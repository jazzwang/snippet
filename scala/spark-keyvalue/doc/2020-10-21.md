# 2020-10-21

## 目標

- 測試 Scala Linter [scalafix](https://scalacenter.github.io/scalafix/docs/users/installation.html) plugin

## 步驟

- 新增 `addSbtPlugin("ch.epfl.scala" % "sbt-scalafix" % "0.9.21")` 到 `project\plugins.sbt`

```diff
diff --git a/scala/spark-keyvalue/project/plugins.sbt b/scala/spark-keyvalue/project/plugins.sbt
index 72477a2..e9f7f4e 100644
--- a/scala/spark-keyvalue/project/plugins.sbt
+++ b/scala/spark-keyvalue/project/plugins.sbt
@@ -1 +1,2 @@
 addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
+addSbtPlugin("ch.epfl.scala" % "sbt-scalafix" % "0.9.21")
```

- 測試跑 `sbt "scalafix RemoveUnused"` 時出現以下錯誤訊息，因此需要修改 `build.sbt`

```bash
~/git/snippet/scala/spark-keyvalue$ sbt "scalafix RemoveUnused"
[info] Loading settings for project spark-keyvalue-build from plugins.sbt ...
[info] Loading project definition from /Users/jazzwang/git/snippet/scala/spark-keyvalue/project
[info] Loading settings for project root from build.sbt ...
[info] Set current project to spark-keyvalue (in build file:/Users/jazzwang/git/snippet/scala/spark-keyvalue/)
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[error] (Compile / scalafix) scalafix.sbt.InvalidArgument: The semanticdb-scalac compiler plugin is required to run semantic rules like RemoveUnused.
[error] To fix this problem for this sbt shell session, run `scalafixEnable` and try again.
[error] To fix this problem permanently for your build, add the following settings to build.sbt:
[error] 
[error] inThisBuild(
[error]   List(
[error]     scalaVersion := "2.11.11",
[error]     semanticdbEnabled := true,
[error]     semanticdbVersion := scalafixSemanticdb.revision
[error]   )
[error] )
[error] 
[error] Total time: 9 s, completed Oct 21, 2020 8:55:17 PM
```

- 參考上述錯誤訊息，新增 `inThisBuild(...)` 進 `build.sbt`

```diff
diff --git a/scala/spark-keyvalue/build.sbt b/scala/spark-keyvalue/build.sbt
index c4009e1..f4d3bba 100644
--- a/scala/spark-keyvalue/build.sbt
+++ b/scala/spark-keyvalue/build.sbt
@@ -12,9 +12,18 @@ lazy val root = (project in file("."))
       "org.apache.spark"  %%  "spark-core"    % "2.2.1",
       "org.apache.spark"  %%  "spark-sql"     % "2.2.1",
       scalaTest % Test
-    )
+    ),
+    scalacOptions += "-Ywarn-unused-import" // required by `RemoveUnused` rule
   )
 
+inThisBuild(
+  List(
+    scalaVersion := "2.11.11",
+    semanticdbEnabled := true,
+    semanticdbVersion := "4.1.9"
+  )
+)
+
 // https://stackoverflow.com/questions/24996437/how-to-execute-a-bash-script-as-sbt-task/25005
 import scala.sys.process._
 lazy val distclean = taskKey[Unit]("Clean up temporary files and directories")
```

- ( 2020-10-21 21:19:53 ) 備註：原本定義的 `scalafixSemanticdb.revision` 是 `4.3.22`。可是預設的 maven repo 找不到，所以只好根據 maven repo 手動改成 `4.1.9`
    - 參考文件：https://www.scala-sbt.org/1.x/docs/Resolvers.html

```shell
[error] (update) sbt.librarymanagement.ResolveException: Error downloading org.scalameta:semanticdb-scalac_2.11.11:4.3.22
[error]   Not found
[error]   Not found
[error]   not found: /Users/jazzwang/.ivy2/local/org.scalameta/semanticdb-scalac_2.11.11/4.3.22/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/scalameta/semanticdb-scalac_2.11.11/4.3.22/semanticdb-scalac_2.11.11-4.3.22.pom
[error] Total time: 2 s, completed Oct 21, 2020 9:02:18 P
```

- ( 2020-10-21 21:24:50 ) 刻意在程式碼中加入不需要的 `import java.io._`，然後再跑一次 `sbt "scalafix RemoveUnused"`，結果程式碼就自動移除了不必要的 import

```shell
~/git/snippet/scala/spark-keyvalue$ code src/main/scala/example/Hello.scala
~/git/snippet/scala/spark-keyvalue$ sbt "scalafix RemoveUnused"
[info] Loading settings for project spark-keyvalue-build from plugins.sbt ...
[info] Loading project definition from /Users/jazzwang/git/snippet/scala/spark-keyvalue/project
[info] Loading settings for project root from build.sbt ...
[info] Set current project to spark-keyvalue (in build file:/Users/jazzwang/git/snippet/scala/spark-keyvalue/)
[info] Compiling 1 Scala source to /Users/jazzwang/git/snippet/scala/spark-keyvalue/target/scala-2.11/classes ...
[warn] /Users/jazzwang/git/snippet/scala/spark-keyvalue/src/main/scala/example/Hello.scala:6:16: Unused import
[warn] import java.io._
[warn]                ^
[warn] one warning found
[info] Running scalafix on 1 Scala sources
[success] Total time: 15 s, completed Oct 21, 2020 9:24:06 PM
```
- ( 2020-10-21 21:32:53 ) 沒特別做設定，再跑 `sbt compile` 的時候也會『**警告**』不需要的 import

```shell
~/git/snippet/scala/spark-keyvalue$ sbt compile
[info] Loading settings for project spark-keyvalue-build from plugins.sbt ...
[info] Loading project definition from /Users/jazzwang/git/snippet/scala/spark-keyvalue/project
[info] Loading settings for project root from build.sbt ...
[info] Set current project to spark-keyvalue (in build file:/Users/jazzwang/git/snippet/scala/spark-keyvalue/)
[info] Executing in batch mode. For better performance use sbt's shell
[info] Compiling 1 Scala source to /Users/jazzwang/git/snippet/scala/spark-keyvalue/target/scala-2.11/classes ...
[warn] /Users/jazzwang/git/snippet/scala/spark-keyvalue/src/main/scala/example/Hello.scala:6:16: Unused import
[warn] import java.io._
[warn]                ^
[warn] one warning found
[success] Total time: 10 s, completed Oct 21, 2020 9:32:28 PM
```

- ( 2020-10-21 21:53:49 ) 調整 `ThisBuild` 的寫法，並嘗試啟用 `scalafixOnCompile` 並加入 `.scalafix.conf` 設定檔

```diff
diff --git a/scala/spark-keyvalue/build.sbt b/scala/spark-keyvalue/build.sbt
index c4009e1..6e4c31e 100644
--- a/scala/spark-keyvalue/build.sbt
+++ b/scala/spark-keyvalue/build.sbt
@@ -4,6 +4,10 @@ ThisBuild / scalaVersion     := "2.11.11"
 ThisBuild / version          := "0.1.0"
 ThisBuild / organization     := "com.example"
 ThisBuild / organizationName := "example"
+ThisBuild / semanticdbEnabled := true
+ThisBuild / semanticdbVersion := "4.1.9"
+ThisBuild / scalafixOnCompile := true
+ThisBuild / scalacOptions     += "-Ywarn-unused-import" // required by `RemoveUnused` rule
 
 lazy val root = (project in file("."))
   .settings(
```


## 參考文件

- https://github.com/mramshaw/Scala-Linters

-----

# 2020-10-21 [Scala] 測試 `scalafix` command-line

- [X] https://scalacenter.github.io/scalafix/docs/users/installation.html#command-line
- [X] https://get-coursier.io/docs/cli-installation

## 1. 先安裝 Coursier


- ( 2020-10-21 23:16:51 ) macOS 安裝方式
```
$ brew install coursier/formulas/coursier
$ cs
```
- ( 2020-10-21 23:17:26 ) Windows 安裝方式
    - Note that this must be run with `cmd.exe`, not `PowerShell`.
```
> bitsadmin /transfer cs-cli https://git.io/coursier-cli-windows-exe "%cd%\cs.exe"
> .\cs --help
```

## 2. 安裝 `scalafix`

- ( 2020-10-21 23:18:11 )

```
cs install scalafix
./scalafix --version # Should say 0.9.21
```

## 3. 檢查程式碼

- ( 2020-10-21 23:38:14 ) 解決了 scalaOptions 的問題，不過還是遇到 `SemanticDB` 的問題。
```
~/git/snippet/scala/spark-keyvalue$ scalafix -r ExplicitResultTypes --scalac-options="-Ywarn-unused"
error: error while loading Object, Missing dependency 'object scala.native in compiler mirror', required by /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jre/lib/rt.jar(java/lang/Object.class)
error: SemanticDB not found: project/Dependencies.scala
error: SemanticDB not found: project/plugins.sbt
error: SemanticDB not found: build.sbt
error: SemanticDB not found: src/test/scala/example/HelloSpec.scala
error: SemanticDB not found: src/main/scala/example/Hello.scala
```
- ( 2020-10-21 23:39:49 ) 文件寫說：`Semantic rules` 必須要先用 `SemanticDB compiler plugin` 編譯過才行。

> Scalafix is a refactoring and linting tool. Scalafix
supports both syntactic and semantic linter and rewrite
rules. Syntactic rules can run on source code without
compilation. **Semantic rules can run on source code that has
been compiled with the SemanticDB compiler plugin.**

- ( 2020-10-21 23:42:14 ) 根據 [Built-in Rules](https://scalacenter.github.io/scalafix/docs/rules/overview.html) 的分類：
    - Semantic Rules: `ExplicitResultTypes`, `NoAutoTupling`, `RemoveUnused`
    - Syntactic Rules: `DisableSyntax`,`LeakingImplicitClassVal`,`NoValInForComprehension`,`ProcedureSyntax`
- ( 2020-10-21 23:48:55 ) 在 `src/main/scala/example/Hello.scala` 中加入 `trait A { def doSomething }`
```diff
~/git/snippet/scala/spark-keyvalue$ code src/main/scala/example/Hello.scala
~/git/snippet/scala/spark-keyvalue$ git diff
diff --git a/scala/spark-keyvalue/src/main/scala/example/Hello.scala b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
index b5703d5..3a20b51 100644
--- a/scala/spark-keyvalue/src/main/scala/example/Hello.scala
+++ b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
@@ -29,3 +29,5 @@ object Hello extends Greeting with App {
 trait Greeting {
   lazy val greeting: String = "hello"
 }
+
+trait A { def doSomething }
\ No newline at end of file
```
- ( 2020-10-21 23:49:25 ) 然後跑 `ProcedureSyntax` rule
```
~/git/snippet/scala/spark-keyvalue$ scalafix -r DisableSyntax -r LeakingImplicitClassVal -r NoValInForComprehension -r ProcedureSyntax
```
- ( 2020-10-21 23:50:29 ) 確實會被修正成 `trait A { def doSomething: Unit }`
```diff
~/git/snippet/scala/spark-keyvalue$ git diff
diff --git a/scala/spark-keyvalue/src/main/scala/example/Hello.scala b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
index b5703d5..74d6f9d 100644
--- a/scala/spark-keyvalue/src/main/scala/example/Hello.scala
+++ b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
@@ -29,3 +29,5 @@ object Hello extends Greeting with App {
 trait Greeting {
   lazy val greeting: String = "hello"
 }
+
+trait A { def doSomething: Unit }
\ No newline at end of file
```

## (Optional) SemanticDB

- ( 2020-10-21 23:57:20 ) 用 `Coursier` 安裝 `SemanticDB`

```shell
cs install metac
```

- ( 2020-10-22 00:02:27 ) 嘗試產生對應的 SemanticDB 放在 `META-INF`，不過看樣子還需要解決相依性的問題。

```
~/git/snippet/scala/spark-keyvalue$ metac src/main/scala/example/Hello.scala
src/main/scala/example/Hello.scala:3: error: object apache is not a member of package org
import org.apache.spark.sql.SparkSession
           ^
src/main/scala/example/Hello.scala:4: error: object apache is not a member of package org
import org.apache.hadoop.io._
           ^
src/main/scala/example/Hello.scala:5: error: object apache is not a member of package org
import org.apache.hadoop.mapreduce.lib.input._
           ^
src/main/scala/example/Hello.scala:10: error: not found: value SparkSession
  val spark = SparkSession.builder
              ^
4 errors
```

# 2020-10-27

[TOC]

## Wartremover - A flexible Scala linter

- http://www.wartremover.org/doc/install-setup.html
- https://github.com/mramshaw/Scala-Linters#wartremover

- 修改 `project/plugins.sbt`，加上

```scala
addSbtPlugin("org.wartremover" % "sbt-wartremover" % "2.2.1")
```

- 修改 `build.sbt`，加上

```scala
wartremoverErrors ++= Warts.unsafe
```

- 這裡使用的是 `unsafe` 的警告，也有 `Warts.all`、`Wart.Any`, `Wart.Nothing`, `Wart.Serializable`

- 當執行 `sbt run` 時會出現以下警告

```shell
~/git/snippet/scala/spark-keyvalue$ sbt run
[info] Loading settings for project spark-keyvalue-build from plugins.sbt ...
[info] Loading project definition from /Users/jazzwang/git/snippet/scala/spark-keyvalue/project
[info] Loading settings for project root from build.sbt ...
[info] Set current project to spark-keyvalue (in build file:/Users/jazzwang/git/snippet/scala/spark-keyvalue/)
[info] Compiling 1 Scala source to /Users/jazzwang/git/snippet/scala/spark-keyvalue/target/scala-2.11/classes ...
[error] /Users/jazzwang/git/snippet/scala/spark-keyvalue/src/main/scala/example/Hello.scala:24:24: [wartremover:Any] Inferred type containing Any
[error]   records.keys.foreach(println)
[error]                        ^
[error] /Users/jazzwang/git/snippet/scala/spark-keyvalue/src/main/scala/example/Hello.scala:25:26: [wartremover:Any] Inferred type containing Any
[error]   records.values.foreach(println)
[error]                          ^
[error] /Users/jazzwang/git/snippet/scala/spark-keyvalue/src/main/scala/example/Hello.scala:23:41: [wartremover:StringPlusAny] Implicit conversion to string is disabled
[error]   System.out.println("RDD Partitions: " + records.getNumPartitions)
[error]                                         ^
[error] three errors found
[error] (Compile / compileIncremental) Compilation failed
[error] Total time: 8 s, completed Oct 27, 2020 9:45:54 AM
```

- ( 2020-10-27 09:48:41 ) 修改這幾行以後，就沒有警告了～

```diff
diff --git a/scala/spark-keyvalue/src/main/scala/example/Hello.scala b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
index b5703d5..07753c0 100644
--- a/scala/spark-keyvalue/src/main/scala/example/Hello.scala
+++ b/scala/spark-keyvalue/src/main/scala/example/Hello.scala
@@ -20,9 +20,9 @@ object Hello extends Greeting with App {
   //       in the sample dataset, we'll use ','
   conf.set("mapreduce.input.keyvaluelinerecordreader.key.value.separator",",")
   val records = sc.newAPIHadoopFile("dataset",classOf[KeyValueTextInputFormat], classOf[Text], classOf[Text])
-  System.out.println("RDD Partitions: " + records.getNumPartitions)
-  records.keys.foreach(println)
-  records.values.foreach(println)
+  System.out.println("RDD Partitions: " + records.getNumPartitions.toString)
+  records.keys.foreach(k => println(k))
+  records.values.foreach(v => println(v))
   spark.stop()
 }
```
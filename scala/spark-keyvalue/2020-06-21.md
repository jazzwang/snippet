# 2020-06-21

## Test with `spark-shell`

```sh
~/git/spark-keyvalue$ cd
~$ mkdir -p spark-keyvalue
~$ cd spark-keyvalue/
~/spark-keyvalue$ mkdir -p dataset
~/spark-keyvalue$ echo > dataset/input1 < EOF
-bash: EOF: No such file or directory
~/spark-keyvalue$ cat > dataset/input1 << EOF
> k1,v1
> k2,v2
> k3,v3
> EOF
~/spark-keyvalue$ cat > dataset/input2 << EOF
> k4,v4
> k5,v5
> k6,v6
> EOF
~/spark-keyvalue$ cat > dataset/input3 << EOF
> k7,k7
> k8,v8
> k9,v9
> k10,v10
> EOF
~/spark-keyvalue$ spark-shell

	...(skip)...

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_252)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.hadoop.io._
scala> import org.apache.hadoop.mapreduce.lib.input._
scala> val conf = sc.hadoopConfiguration
conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml

scala> conf.set("mapreduce.input.keyvaluelinerecordreader.key.value.separator",",")

scala> val file = sc.newAPIHadoopFile("dataset",classOf[KeyValueTextInputFormat], classOf[Text], classOf[Text])
file: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text)] = dataset NewHadoopRDD[0] at newAPIHadoopFile at <console>:30

scala> file.getNumPartitions
res2: Int = 3

scala> file.partitions.size
res1: Int = 3

scala> file.keys.collect
res3: Array[org.apache.hadoop.io.Text] = Array(k10, k10, k10, k10, k6, k6, k6, k3, k3, k3)

scala> file.values.collect
res4: Array[org.apache.hadoop.io.Text] = Array(v10, v10, v10, v10, v6, v6, v6, v3, v3, v3)

scala> file.values.foreach(println)
v1
k7
v4
v8
v5
v2
v6
v9
v3
v10

scala> file.keys.foreach(println)
k1
k7
k2
k8
k3
k9
k10
k4
k5
k6

scala> :quit
```

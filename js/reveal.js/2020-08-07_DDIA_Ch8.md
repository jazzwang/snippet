---
presentation:
    theme: night.css
    width: 1920
    height: 1080
    slideNumber: true
    hideAddressBar: true
    history: true
---

<style>
.reveal code { color: #e7ad52; }
.reveal strong, .reveal b { color: #ffccff; }
.reveal em { color: #ccff99; }
quote { color: #e7ad52; }
</style>

<!-- slide -->

## Design Data-Intensive Application (DDIA)

Ch.8 The Trouble with Distributed Systems

By [Jazz Yao-Tsung Wang](https://slideshare.net/jazzwang)
2020-08-07 @ Taipei, Taiwan

<!-- slide -->

### 本章大綱

- *"完全故障" vs “部分故障"*
    - 單機軟體 vs 分散式軟體
    - 超級電腦 vs 雲服務
- 常見分散式系統會遇到的問題
    - 不可靠的網路 `Unreliable Networks`
    - 不可靠的時間 `Unreliable Clocks`
- 知識、真理與謊言

<!-- slide vertical=true -->

### "完全故障" vs "部分故障"

- **單機軟體的「確定性」** <small>`deterministic`</small> <!-- .element: class="fragment" -->
    - 只要硬體沒問題，每次執行單機軟體的結果應該是一樣的 <!-- .element: class="fragment" -->
    - 如果硬體有問題，就會是「藍色螢幕」或 Kernel Panic <!-- .element: class="fragment" -->
    - *完全故障* <small>`fault`</small> <!-- .element: class="fragment" -->
- **分散式軟體的「不確定性」** <small>`nondeterministic`</small> <!-- .element: class="fragment" -->
    - 什麼事情都可能會出錯（網路/電力/空調/機櫃) <!-- .element: class="fragment" -->
    - *部分故障* <small>`partial failure`</small> <!-- .element: class="fragment" -->

<!-- slide vertical=true -->

### 分散式軟體的不確定性

- 奇聞軼事 <small>`anecdote`</small>
    - 電源供應器故障 <small>`PDU [power distribution unit] failures`</small> <!-- .element: class="fragment" -->
    - 網路集線器故障 <small>`switch failures`</small> <!-- .element: class="fragment" -->
    - 整個機櫃跳電 <small>`accidental power cycles of whole racks`</small> <!-- .element: class="fragment" -->
    - 整個資料中心骨幹網路故障 <small>`whole-DC backbone failures`</small> <!-- .element: class="fragment" -->
    - 整個資料中心電力故障 <small>`whole-DC power failures`</small>  <!-- .element: class="fragment" -->
    - 低血糖的司機開著福特貨卡撞上資料中心的冷卻空調系統 <!-- .element: class="fragment" -->
    <small><quote>
    a hypoglycemic driver smashing his Ford pickup truck into a DC’s HVAC [heating, ventilation, and air conditioning] system
    </quote></small> 

<!-- slide vertical=true -->

### 雲端運算與超級電腦

- 大規模運算系統 <small>`large-scale computing system`</small> 的頻譜兩端
    - **超級電腦** <small>`High-performance Computing (HPC)`</small> <!-- .element: class="fragment" -->
        - *同質性*高 <small>`homogeneous`</small>
        - Ex. 電腦叢集 PC Cluster
            - *特殊規格*硬體，搭配*高速網路*連結 Ex. [InfiniBand](https://en.wikipedia.org/wiki/InfiniBand)
            - *共享記憶體* <small>`remote direct memory access (RDMA)`</small>
    - **雲端運算** <small>`Cloud Computing`</small> <!-- .element: class="fragment" -->
        - *異質性*高 <small>`heterogeneous`</small>
            - 採用*一般規格*的硬體，*故障率較高*
            - 克洛斯網路拓墣 <small>[Clos Network Topology](https://en.wikipedia.org/wiki/Clos_network) </small>
        - *多用戶 異地資料中心* <small>`multi-tenant datacenters`</small>
        - 隨需自助服務 / 動態資源池 <small>`elastic/on-demand resource allocation`</small>
        - 依用量計價 <small>`metered billing`</small>

<!-- slide vertical=true -->

### HPC 與 Cloud 的不同處

- 作者認為 HPC 的工作（Job）通常靠**查核點** `Checkpoints` <!-- .element: class="fragment" -->
    - Job 不成功就視同失敗(`All or Nothing`)。其表現比較類似單機軟體。
- Cloud 比較像實作 **內部服務** (`internal services`)，使用者的期待包括：<!-- .element: class="fragment" -->
    - *上線服務* (`Online`) - 對比 HPC 的「*批次運算* (`Batch`)」
    - 低網路延遲 `low latency` (像在內網一樣)
    - 無法接受停機維修
- 大型系統的基本假設：<!-- .element: class="fragment" -->
    - 每天都有硬體會故障，維運人員要花很多時間處理這些故障
- 分散式系統必須確保就算遇到硬體故障，也能持續運作 <!-- .element: class="fragment" -->
    - 比喻：**網路作業系統**
    - 第四章提到的 `Rolling Upgrade`
- 雲服務的特徵：**跨異地資料中心**的通訊 <!-- .element: class="fragment" -->

<!-- slide vertical=true -->

### 基於不可靠的元件，打造可靠的系統

- Building a reliable system from unreliable components
    - **錯誤檢查碼** `Error-correcting codes`
    - TCP 相較於 IP，可以提供更可靠的傳輸
- 雖然上層的軟體元件設計可以解決掉「部分」底層軟硬體造成的故障，
但並**無法解決所有的故障問題**。只能讓剩下的問題易於理解與排除。

<!-- slide -->

### 本章大綱

- "完全故障" vs “部分故障"
    - 單機軟體 vs 分散式軟體
    - 超級電腦 vs 雲服務
- 常見分散式系統會遇到的問題
    - *不可靠的網路 `Unreliable Networks`*
    - 不可靠的時間 `Unreliable Clocks`
- 知識，真理與謊言

<!-- slide vertical=true -->

### Shared-nothing System

- <small>`題外話：Hadoop MapReduce 的一個重點觀念`</small>
- **一群靠網路串聯的機器**
    - 每台機器*只能存取自己的資源* (Ex.記憶體與硬碟)
    - 只能*透過網路要求存取其他機器的資源*
- 是目前打造網路服務的**主流架構** <small>`dominant approach`</small>
    - 不需要特殊規格的硬體
    - 可透過*異地跨機房的備援* <small>`redundancy`</small> 來達到高可靠性

<!-- slide vertical=true -->

### 非同步網路 <small>`Asynchronous packet networks`</small> 的各種狀況

- 六種非同步網路的異常狀況
    - *封包遺失* (掉包) `packet lost` - Ex. 有人拔掉網路線
    - *封包延遲* `latency` - Ex. 等在佇列/網路負載過重
    - *目的端異常* - Ex. 有人關掉那台機器
    - 目的端*暫無回應* - Ex. 那台機器負載過重
    - 目的端已處理要求，但*回應封包遺失*
    - 目的端已處理要求，但*回應封包延遲*
- 即使套用「逾時 `Timeout` 」機制，還是無從知道遠端的機器是否有收到回應。

<img src='https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0801.png' width='1024'>

<!-- slide vertical=true -->

### 真實網路故障範例 <small>`Network Faults in Practice`</small>

- 連不到某台機器 
    - Ex. 單機網卡故障 / 網路線故障 / Switch 網路孔故障
    - 靈異：
        - 網卡 Drop 掉進來的封包，可是又可以正常送出封包？！
        - 網路線只有單方向傳輸能力？！
- 連不到機櫃裡一半的機器 - Ex. Switch 設定問題
- 跨國連線失敗 - Ex. **鯊魚咬斷海纜**
- 一群機器因為網路問題造成「**Deadlock**」 
    - 縱使網路恢復了，這群機器還是沒辦法恢復正常運作
    - 例如：先前提過的「[**腦裂問題** `Split-Brain`](https://en.wikipedia.org/wiki/Split-brain_(computing))」

<!-- slide vertical=true -->

### 偵測網路故障 <small>`Detecting Faults`</small>

- 許多系統需要偵測故障的節點(`Node`), Ex.
    - 負載平衡器 (`Load Balancer`)
    - 分散式資料庫 - 第五章的 `single-leader application`
- 能明確得知網路故障的四種狀況
    - 無法連線到指定的「**連線埠**(`Port`)」 
        - 軟體當掉了～作業系統釋出連線埠的狀況～
        - TCP 連線會收到 `RST` 或 `FIN` 的回應
        - 對比：如果只是部分軟體異常，將無法確認到底處理了多少資料
    - **隔離腳本**監控到某個背景程序不存在
        - 高可用性（`High Availability`）的隔離腳本(`Fencing Script`)
        - Ex. HBase 的 RegionServer 或 Hadoop 的 NameNode 都仰賴 ZooKeeper 來判斷
    - 從**網路設備管理介面**查詢到硬體層的連線故障
        - 前提：可以存取到 Switch (Hub) / Router 的管理介面
    - 路由器回覆 ICMP **找不到該 IP 位址**
        - 補充：有興趣的可以去了解一下 ARP, ICMP 的運作方式
- 作者建議：
    - 儘管 TCP 會 Retry 連線，但最好在 Application Layer 也要有 Retry 跟 Timeout 機制。

<!-- slide vertical=true -->

### 逾時機制 與 無上限的延遲 <small>`Timeouts and Unbounded Delays`</small>

- Timeout 的兩難
    - 太長：使用者要等很久才能確定發生故障
    - 太短：可能只是「假警報」（Ex. 單純只是負載較重）
- 潛在的負載問題
    - 比喻：有人請假，其他人就得負責更多工作（代班）。
    - 如果每個節點都宣稱其他節點已經掛點了～整個系統就沒辦法運作了！
- 理想的狀態：
    - 可以確保網路延遲最高為 `d`
    - 可以確保運算時間最高為 `r`
    - **理想的 Timeout 等於 `2d+r`**
- 現實狀況：
    - 網路延遲並無法確保上限 `Unbounded Delay`
    - 運算時間也無法確保上限
- 一般常用的 Timeout 是 *「round-trip time 的平均值」*

<!-- slide vertical=true -->

### 網路壅塞與排隊 <small>`Network congestion and queuing`</small>

- 網路壅塞 通常發生在 **多個來源端** 都要送資料給 **一個目的端**
    - `Tx` 網卡 Output Queue (如果封包很多要等著送出去，就會排隊)
    - 交換器 Switch Queue (如果滿了，會把進來的封包丟掉 `packet drop`)
    - `Rx` 網卡 Input Queue (如果 CPU 很忙，就會排隊)
- TCP 也有「**流量管制** `Flow Control`」(`back-pressure`) 的設計
    - 可避免網路過載 / 目的端過載。但這個機制也延長了網路的延遲
    - **Trade-off: `TCP` vs `UDP`**
- 在共享的雲端環境裡，*其他用戶的異常用量*也可能增加您的網路延遲。
    - 建議量測 Round-trip time，甚至*根據平均值動態調整 Timeout*

<img src='https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0802.png' width='1024'>

<!-- slide vertical=true -->

### 同步 與 非同步 網路 <small>`Synchronous Versus Asynchronous Networks`</small>

| 電話語音網路(`telephone network`) | 數據網路 |
|---------------------------------|--------|
| *專用* 連線 `circuit` | *非專用*，閒置時就不佔用線路 |
| 預留*專屬* 頻寬 `fixed bandwidth` | *動態* 頻寬 `dynamic bandwidth` |
| *同步* 網路 `Aynchronous` | *非同步* 網路 `Asynchronous` |
| *有上限* 的延遲 `bounded delay` | *無上限* 的延遲 `unbounded delay` |
| 擅長傳遞*固定的流量* | 擅長處理*突發的流量* `bursty traffic` |
| ATM, InfiniBand | Ethernet |

--

- 現在多半採用數據網路是*考量成本效益後的結果*
- 雖然存在 **QoS (quality of service)** 跟 **admission control** (進場管制) 的機制，
但目前多用戶的雲端資料中心並*沒有啟用這些機制*，開發者必須*自己實驗決定 Timeout*。

<!-- slide -->

### 本章大綱

- "完全故障" vs “部分故障"
    - 單機軟體 vs 分散式軟體
    - 超級電腦 vs 雲服務
- 常見分散式系統會遇到的問題
    - 不可靠的網路 `Unreliable Networks`
    - *不可靠的時間 `Unreliable Clocks`*
- 知識、真理與謊言

<!-- slide vertical=true -->

### 時鐘 `Clock` 的重要性

- **系統需求**
    - 這個要求(`Request`)*逾時*了嗎？
    - 這個服務的平均 99 百分位*反應時間*是多長？
    - 過去五分鐘內，這個服務平均每秒接收到幾次查詢？
    - 這個快取(`Cache`)是否*過期*了？
    - 這個日誌檔(`Log`)裡，錯誤訊息的時間戳記(`timestamp`)是什麼時候？
- **商務需求**
    - 使用者待在這個網站多久了？
    - 這篇文章什麼時候發佈的？
    - 這個 E-mail 提醒該在什麼日期什麼時間點遞送呢？

<!-- slide vertical=true -->

### 網路校時的重要性

- 分散式系統仰賴網路傳輸，一定會有網路延遲。
- 每台機器的*硬體時鐘*是靠自己的*石英震盪器*控制，多少會有誤差值。
- 仰賴 *網路校時* (`Network Time Protocol`, *NTP*) 來定期同步多台機器的時間。

--

- 試想：
    - 如果兩台機器的時間差 5 ms，而 Timeout 也剛好是 5 ms，會發生什麼事情？

--

- 經驗談：
    - 實務管理電腦叢集時，就會發現如果沒設定網路校時，
    甚至 *Timezone* 設定不一致，時間認定不一（*是否假定 BIOS 時間為 UTC/GMT*）
    一座 Hadoop 或 HBase 叢集，會出現一些不如預期的行為。

<!-- slide vertical=true -->

### `Time-of-Day Clock` ↔ `Monotonic Clock`

| Time-of-Day Clock | Monotonic Clock |
|--|--|
| 根據 `wall-clock time` 回傳日期與時間 | 根據石英振盪器回傳的日期與時間 |
| 通常與 NTP Server 校正 | NTP 只能微調石英振盪器的頻率 | 
| `clock_gettime(CLOCK_REALTIME)` | `clock_gettime(CLOCK_MONOTONIC)` |
| `System.currentTimeMillis()` | `System.nanoTime()` |
| 閏秒會造成時間重設問題 | 確切的數值並不重要<br/>通常拿*兩次結果的差值*來衡量時間長度 |
| 舊系統顆粒度太大(10ms) | 通常*顆粒度較小* (microseconds 以下) |

--

- `wall-clock time` / `epoch`
    - 1970-01-01 0:00 UTC
    - 不計算閏秒 `leap second`

<!-- slide vertical=true -->

### 時鐘同步與精準度

- Monotonic Clock 不需要同步。Time-of-Day Clock 仰賴外部標準時間。
- 石英振盪器的時鐘 跟 網路標準時間 都存在潛在的不精準性：
    - 石英振盪器 會因為 `環境溫度` 而造成 *時鐘飄移* (`drift`)
        - `6 ms` 飄移 - 30 秒 NTP 同步一次 / `17 秒` 飄移 - 1 天 NTP 同步一次
    - 如果 local time 跟 NTP Server 的**時間差過大，會拒絕同步**，並強制重設。
        - 應用程式會觀察到時間戳記一下子往後，一下子往前。
    - NTP 網路校時 會因為 *網路延遲* 而不準確
        - `35 ms` 到 `1 秒` 的誤差
- **實務分享**
    - 確定 NTP 校時通訊協定沒*被防火牆擋掉*
    - 「*閏秒*」（一分鐘 = 59 秒 或 61 秒）造成許多大型系統的問題
    - *虛擬機器*的「硬體時鐘」也是虛擬的。
    在共享環境下，運行虛擬機器的應用程式可能會看到 `10ms` 的時間差。
    - 遊戲玩家可能想規避「試玩時間限制」而刻意把硬體時鐘往前調到幾年前。
- **增加精準度的方式**
    - *GPS* 接收器 / PTP 通訊協定 (`Precision Time Protocol`)
    - *銫原子鐘* (`atomic (caesium) clock`)

<!-- slide vertical=true -->

### 仰賴時鐘準確度的場景

- 事實：*不正確的時間設定很容易被忽略*。
    - CPU 有問題 或 網路設定錯誤，很容易就會被注意到。 
    - 網路校時失敗，石英振盪器故障，比較難察覺到異狀。
- 建議：如果軟體需要仰賴時鐘的精確度，最好*持續監控每台機器的時間差*。

--

- 四種需要仰賴時鐘精準度的場景
    - 確保事件的順序 `Ordering of events across multiple nodes`
    - 值得信任的時間值 `Clock reading has a confidence value`
    - 快照隔離 `Synchronized clocks for global snapshots`
    - 運算暫停 `Process Pause`

<!-- slide vertical=true -->

### Case 1: Timestamps for ordering events

- multi-leader replication 的 `last write wins (LWW)` 法則 (Cassandra/Riak)
    - Node 1 跟 Node 3 的時間差: `3 ms`
    - `x = 1` @ 42.004 (實際上*早發生*，但**時間戳記較晚**)
    - `x = 2` @ 42.003 (實際上*後發生*，但**時間戳記較早**)
    - Node 2 認為 `x = 1` 才是對的，丟掉 `x = 2`。

 <img src='https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0803.png' width='1280'/>

<!-- slide vertical=true -->

### Case 2: Clock readings has a confidence value

- 多數系統回傳的時間值是不可靠的
    - 不確定性 = `網路校時` 的誤差 + `石英振盪器` 的誤差
- 例外：`Google’s TrueTime API` in `Spanner`
    - 回傳 `[earliest, latest]` 最早跟最晚兩個可能的時間戳記
    - 註：7/17 讀書會中有提到打破 CAP 理論的 [Spanner](https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf)

<!-- slide vertical=true -->

### Case 3: Synchronized clocks for global snapshots

- 在第七章 `“Snapshot Isolation and Repeatable Read”` 
討論到 `snapshot isolation`
    - 實作上需要一個 *單調遞增 `monotonically increasing` 的 Transaction ID*
- 如果資料庫節點分散在異地的資料中心，又希望 Transation ID 反應因果順序，採用 `Time-of-Day Clock` 來當 Transaction ID 並不可行！
- Google Spanner 仰賴 TrueTime API 來實作 Snapshot Isolation
    - A = [A~earliest~, A~latest~] and B = [B~earliest~, B~latest~]
    - 沒有重疊：A~earliest~ < A~latest~ < B~earliest~ < B~latest~
- Google Spanner 在提交 read-write transaction 前，刻意延遲一段等待時間，以確保因果順序。
    - 因此 Google Spanner 的成功建立在 GPS/原子鐘 之上。同步誤差： `7 ms`

<!-- slide vertical=true -->

### Case 4: Process pause

- 假設場景：
    - single leader per partition
    - 使用 `lease` 來確保自己還是 leader
    - 假定 `lease` 還有 10 秒以上，就處理 Request。
- 會遇到的狀況：如何確保處理 Request 的時間低於 10 秒？
    - JVM 的 "stop-the-world" *Garbage Collector (GC)* 
    - 虛擬機器的 `suspend` (休眠) 與 `live migration` (在實體機之間搬移)
    - 筆電的休眠(suspend)/還原(resume)
    - CPU 的 *Context-switch* / Hypervisor 在不同虛擬機器之間切換
    - 因為*硬碟 I/O* 造成的延遲
        - 如果是 網路磁碟 (Ex. AWS EBS) 還可能因為 *網路延遲* 變得更慢
    - *記憶體不足* `swapping to disk (paging)`
    - Unix 系統的 `SIGSTOP` 信號 (Ctrl-Z) 會暫停運行中的程序，
    直到收到 `SIGCONT` 信號 (Ex.*人為操作疏失*)

<!-- slide vertical=true -->

### 硬即時系統 Hard Real-time System

- 「即時 (`Real-time`)」代表系統必須保證在指定的 `deadline` 前做出回應 (做完 or 沒做完)
- `real-time operation system (RTOS)`
    - 確保 程序(process) 排程(schedule) 在指定時間內取得 CPU 資源
    - 函式庫的文件要寫執行時間的最糟/最久紀錄
    - 限制或停用動態記憶體配置(`dynamic memory allocation`)
- 因為 RTOS 研發上燒錢，通常只用在像是飛行安全/火箭控制等嵌入式系統。
Server-side 資料處理系統考量成本效益，自然就不會採取 RTOS。

<!-- slide vertical=true -->

### 減輕 Garbage Collection 的衝擊

- 將 GC 視為是「計畫中」的停機維護
    - JVM 告知 Application 將需要 GC
    - Application 停止送 Request 到那台機器
    - 處理完手上的 Request
    - 實行 GC
- 規律重開程序 `restart process periodically`
    - *呂布治百病*

<!-- slide -->

### 本章大綱

- "完全故障" vs “部分故障"
    - 單機軟體 vs 分散式軟體
    - 超級電腦 vs 雲服務
- 常見分散式系統會遇到的問題
    - 不可靠的網路 `Unreliable Networks`
    - 不可靠的時間 `Unreliable Clocks`
- *知識、真理與謊言*

<!-- slide vertical=true -->

### 多數決的真理 The Truth Is Defined by the Majority (1/2)

- 情境一：
    - 甲只能正常接收訊息，無法正常送出訊息
    - 乙、丙、丁：送給甲的訊息怎麼都沒有得到回應，一分鐘以後，宣告「甲已經死了」
    - 甲：我還沒死啊？（可是沒人聽得到）
- 情境二：
    - 甲只能正常接收訊息，無法正常送出訊息
    - 甲（心想）：奇怪，我送出去的訊息，怎麼沒有得到乙、丙、丁的回應？
    啊！應該網路有問題！（可是還是無能為力）
    - 乙、丙、丁：送給甲的訊息怎麼都沒有得到回應，一分鐘以後，宣告「甲已經死了」
- 情境三：
    - 甲本來都可以正常接收訊息與送出訊息
    - 甲（突然呆了一分鐘）：處理 GC
    - 乙、丙、丁：送給甲的訊息怎麼都沒有得到回應，一分鐘以後，宣告「甲已經死了」
    - 甲（醒來）：啊～剛剛講到這裡....咦？你們怎麼都不理我？
    - 乙、丙、丁（心想）：這傢伙是怎麼了？突然又活過來了～好像什麼事都沒發生過～

<!-- slide vertical=true -->

### 多數決的真理 The Truth Is Defined by the Majority (2/2)

- 每台機器不能以自己「認知」的狀況當作「真理」
- 多數分散式系統仰賴「*法定人數* `quorum`」（*過半數多數決*）的機制
    - 第九章會再詳談 共識演算法 `consensus algorithms`

<!-- slide vertical=true -->

### 「領袖」與「鎖」 The leader and the lock

- 很多時候，系統裡只能有「一台」負責做某件事：
    - the leader of a database partition - 避免 split brain
    - (only one transaction or client) the holder of lock 
- 實作上，往往仰賴 `lease` 或 `lock` 來達成。
- 實務上，如果遇到 `process pause` 時，該怎麼避免兩個 client 同時認為自己是 leader?
    - Ex. HBase 要確保一次只能有一個 client 寫入某個檔案

<img src='https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0804.png' width='1024'/>

<!-- slide vertical=true -->

### Fencing Token

- 延續上面的討論，解法是讓 lease 或 lock 也有版本號碼
    - `Lock service` 每次發出去一把新的，就增加一個數字。
    - 稱為 `fencing token`
    - `Client` 發出 `Request` 時也得帶入 `fencing token`
- 範例：用 `ZooKeeper` 當 `Lock service`
    - 可以拿 transaction ID `zxid` 或 node version `cversion` 當 `fencing token`

<img src='https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0805.png' width='1024'/>

<!-- slide vertical=true -->

### Byzantine Faults

- 拜占庭將軍問題 `Byzantine Generals Problem` / `Two Generals Problem`
- 多個將軍必須要同意作戰計劃，然而中間的阻礙是有「叛徒」混在他們的陣營裡。
    - 忠誠的將軍，會陳述「事實」 / 叛徒會傳遞「謊言」
- 範例：
    - 飛航系統的 CPU 或 記憶體，受到輻射影響而毀損。
    - 多個組織參與的系統，可能有成員想要作弊或詐騙其他成員。(Ex. 比特幣)
- Server-side data system 一般比較不用擔心 `Byzantine fault-tolerant`
- Web application 就比較需要擔心，因為 Client (Browser) 操縱在使用者手上
    - 輸入驗證 `input validation`, 消毒 `sanitization`,  `output escaping`
    - 避免 SQL injection, cross-site scripting
    - 通常不會套用 `Byzantine fault-tolerant protocols` 
    而只仰賴 server 來確保使用者的行為是符合權限(`authority`)。
- **Peer-to-Peer Network** (Ex. Bitcoin) 因為*沒有集中的權限控管機制*，故特別需要容錯。
- 多數 `Byzantine fault-tolerant protocol` 仰賴 `三分之二` 以上的節點是正常運作的！

<!-- slide vertical=true -->

### weak forms of “lying”

- 有時候因為硬體問題、軟體的 Bug、錯誤的設定，都可能會產生「*不經意的謊言*」
- 範例：
    - *網路封包* 
        - 因為 硬體問題/作業系統臭蟲/驅動程式/路由器問題 而毀損
        - 通常可以透過 TCP/UDP 的 `checksum` 解決
        - 如果底層規避檢查，還可以靠 `Application-level protocol checksum`
    - 外部服務通常會對輸入的值做消毒（避免 SQL Injection)
      *內部服務*往往會對輸入的值比較寬鬆，但最好還是要有 `sanity-checking`
    - *NTP Client*
        - 設定多台 NTP Server 可以避免某一台 NTP Server 設定錯誤造成的問題
        - 不要只設定一台 NTP Server

<!-- slide vertical=true -->

### System Model and Reality (1/2)

- `system model` 系統模型
    - an abstraction that describes what things an algorithm may assume.
    演算法基本假設的抽象描述
- 三種與 `timing` 相關的系統模型
    - `Synchronous model` 同步模型 (不實際)
        - *bounded* network delay
        - *bounded* process pauses
        - *bounded* clock error.
    - `Partially synchronous model` 部分同步模型 (相對實際)
        - 假定多數狀態下 network, process, clock 都在可控制範圍
        - 但額外推估 network, process, clock error 很大的狀況
    - `Asynchronous model` 非同步模型
        - 假定連 clock 都沒有
        - 非常受限

<!-- slide vertical=true -->

### System Model and Reality (2/2)

- 三種與 `node failure` 相關的系統模型
    - `Crash-stop faults`
        - crash 就不會再回應
    - `Crash-recovery faults`
        - crash 以後，可以在未知時間後重新恢復回應
        - 假定有穩定的儲存系統(來儲存先前的狀態 `state`)
    - `Byzantine (arbitrary) faults`
        - 可以做任何事情，包括 `欺騙`
- 近期多數採用 `partially synchronous model` 
  搭配 `crash-recovery faults`

<!-- slide -->

### 結語 Summary

鬼故事全集!!各種會出錯的狀況!!
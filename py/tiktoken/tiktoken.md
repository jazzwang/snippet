# tiktoken

- PyPI
  - http://pypi.org/project/tiktoken
- Git Repo
  - https://github.com/openai/tiktoken

## 2025-10-27

- ç·£èµ·ï¼š
  - ä½¿ç”¨ä¸åŒ LLM æ¨¡å‹æ™‚ï¼Œcontext window (max token) æœƒå½±éŸ¿è¼¸å‡ºçµæœã€‚æ‰€ä»¥æƒ³çŸ¥é“å¦‚ä½•ä¼°ç®— token å€‹æ•¸ã€‚
- æœå°‹ï¼š
  - å• Gemini 2.5 Flash
<table><tr><td>Q:</td><td>
<!-- add your Prompt bellow-->
are there python program or library that can count how many tokens does a plain text file have?
</td></tr><tr><td>A:</td><td>
<!-- add Chatbot Response bellow-->
Yes, there are several Python programs and libraries that can count tokens in a plain text file, but the correct approach depends on **how you define a "token"**.

Since token counting is crucial for Large Language Models (LLMs), I'll cover both general-purpose tokenization and LLM-specific tokenization.

-----

## 1\. LLM-Specific Token Counting

If you're interested in the token count for use with an **LLM API** (like Gemini, GPT, or Llama), you need to use the **specific tokenizer** associated with that model, as tokens are often **sub-word units** (Byte Pair Encoding or BPE).

### A. For Gemini Models (Google)

You can use the official Google GenAI SDK's `count_tokens` method.

**Library:** `google-genai`

```python
from google import genai
import os

# 1. Initialize the client (Make sure to set your API key as an environment variable)
client = genai.Client()

def count_tokens_gemini(file_path, model_name="gemini-2.5-flash"):
    """Counts tokens in a file using the Gemini tokenizer."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        response = client.count_tokens(
            model=model_name,
            contents=[text]
        )
        return response.total_tokens
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Example Usage:
# Assuming you have a file named 'my_text_file.txt'
# token_count = count_tokens_gemini('my_text_file.txt')
# print(f"Token count for Gemini: {token_count}")
```

### B. For OpenAI Models (GPT)

The official library for counting tokens for OpenAI's models (like GPT-4 and GPT-3.5) is **`tiktoken`**.

**Library:** `tiktoken`

```python
import tiktoken

def count_tokens_openai(file_path, model_name="gpt-4"):
    """Counts tokens in a file using the tiktoken library."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # Get the appropriate encoding for the model
        encoding = tiktoken.encoding_for_model(model_name)

        # Encode the text and count the number of resulting tokens
        tokens = encoding.encode(text)
        return len(tokens)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Example Usage:
# token_count = count_tokens_openai('my_text_file.txt')
# print(f"Token count for GPT-4: {token_count}")
```

-----

## 2\. General-Purpose Token Counting (NLP/Words)

If you define a "token" as a **word, punctuation mark, or similar unit** for general Natural Language Processing (NLP), you can use libraries like **NLTK** or **spaCy**.

### Using NLTK (Natural Language Toolkit) ğŸ“š

**Library:** `nltk`

```python
import nltk
from nltk.tokenize import word_tokenize

# nltk.download('punkt') # Run this once to download the required resource

def count_tokens_nltk(file_path):
    """Counts tokens (words and punctuation) using NLTK."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # Tokenize the text into words and punctuation
        tokens = word_tokenize(text)
        return len(tokens)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None

# Example Usage:
# token_count = count_tokens_nltk('my_text_file.txt')
# print(f"NLTK token count: {token_count}")
```

### Simple Word Count (Quick & Dirty)

For a very simple approach where a "token" is defined as any sequence of non-whitespace characters (like using the Unix `wc -w` command), you can use basic Python:

```python
def count_words_simple(file_path):
    """Counts 'words' by splitting on whitespace."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        # Split by any whitespace and filter out empty strings
        words = text.split()
        return len(words)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None

# Example Usage:
# word_count = count_words_simple('my_text_file.txt')
# print(f"Simple word count: {word_count}")
```
</td></tr></table>
- åˆæ­¥å¾çµæœå¯ä»¥çŸ¥é“ä¸åŒçš„ LLM æ¨¡å‹ï¼Œå¯èƒ½å› ç‚º encoding çš„æ–¹æ³•ä¸åŒï¼Œæ‰€ä»¥å°æ–¼ token çš„èªå®šä¹Ÿä¸åŒï¼Œæ‰€ä»¥çµ¦å‡ºä¸‰ç¨®ä¸åŒçš„å¯¦ä½œï¼ˆç¬¬å››ç¨®ç”¨ç©ºç™½åˆ‡ token çš„æ–¹æ³•ä¸è©¦ç”¨ä¸­æ—¥æ–‡èªç³»ï¼‰
  - Gemini - ç”¨ `google-genai` å‡½å¼åº«
  - OpenAI - ç”¨ `tiktoken` å‡½å¼åº«
  - å»£ç¾© - ç”¨ `nltk` å‡½å¼åº«
- ä»¥ä¸‹å¯¦æ¸¬ `tiktoken` çš„å¯¦ä½œ

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup Hands-on Lab Environment\n",
        "\n",
        "## 1.1: Install `colab-xterm` and `uv`\n",
        "\n",
        "- Reference: https://github.com/InfuseAI/colab-xterm?tab=readme-ov-file#usage\n",
        "```python\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "```"
      ],
      "metadata": {
        "id": "JDy5_0I6tX1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yxVS1f7ssNoN"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm uv\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2: Install `aider-chat` using `uv tool`\n",
        "\n",
        "- Reference: https://aider.chat/docs/install.html#install-with-uv\n",
        "```bash\n",
        "python -m pip install uv  # If you need to install uv\n",
        "uv tool install --force --python python3.12 --with pip aider-chat@latest\n",
        "```"
      ],
      "metadata": {
        "id": "kg3A5rqttkOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv tool install --force --python python3.12 --with pip aider-chat@latest\n",
        "!uv tool update-shell"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6plXHjJQs1iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3: Install `Ollama`\n",
        "\n",
        "- reference: https://ollama.com/download/linux\n",
        "```bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```"
      ],
      "metadata": {
        "id": "A4XSHN2KtPTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "0rXsN_UHtOgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi      ## show GPU VRAM\n",
        "!df -h $(pwd)    ## show Disk Space Usage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CueLQD4Vux9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3: Start Ollama Server\n",
        "\n",
        "- We want to run the following command in the background.\n",
        "\n",
        "```bash\n",
        "ollama start &\n",
        "```\n",
        "\n",
        "- Known issue:\n",
        "  - running with `!ollama start &` will block the following Notebook Cell.\n",
        "- Known solution:\n",
        "  - tried `%%bash --bg` cell magic, but it does not work. (test with `ps ax`)\n",
        "- Another solution:\n",
        "  - use `subprocess` instead\n",
        "  - Reference: https://stackoverflow.com/a/65194735"
      ],
      "metadata": {
        "id": "s5616ZYTvuhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"/usr/local/bin/ollama\",\"start\"])"
      ],
      "metadata": {
        "id": "AO8XP1KqmnwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4: Load Google Gemma 3 12B model into Ollama\n",
        "\n",
        "```bash\n",
        "ollama pull gemma3:12b\n",
        "```"
      ],
      "metadata": {
        "id": "wG0lq99-xWxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if ollama server is running\n",
        "!ps ax | grep ollama"
      ],
      "metadata": {
        "id": "dNKIKWVLnBxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:12b\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "9jCKzZ6uIb5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5: Checkout a example Git repo\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart.git\n",
        "```"
      ],
      "metadata": {
        "id": "fTTHGsIYxgmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L1VYG12D7xV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6: Running `Aider`\n",
        "\n",
        "```bash\n",
        "aider --model ollama/gemma3:12b --yes-always --no-auto-commits\n",
        "```"
      ],
      "metadata": {
        "id": "CgClLNV5MP8n"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Note: This Python Notebook is designed for running on Google Colab.\n",
        "    Please click the following icon to open it on Google Colab.\n",
        "\n",
        "<p><center><a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a></center></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sEQKwXgRVv1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup Hands-on Lab Environment\n",
        "\n",
        "## 1.1: Install `colab-xterm` and `uv`\n",
        "\n",
        "- Reference: https://github.com/InfuseAI/colab-xterm?tab=readme-ov-file#usage\n",
        "```python\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "```"
      ],
      "metadata": {
        "id": "JDy5_0I6tX1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yxVS1f7ssNoN"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm uv\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2: Install `aider-chat` using `uv tool`\n",
        "\n",
        "- Reference: https://aider.chat/docs/install.html#install-with-uv\n",
        "```bash\n",
        "python -m pip install uv  # If you need to install uv\n",
        "uv tool install --force --python python3.12 --with pip aider-chat@latest\n",
        "```"
      ],
      "metadata": {
        "id": "kg3A5rqttkOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv tool install --force --python python3.12 --with pip aider-chat@latest"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6plXHjJQs1iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3: Install `Ollama`\n",
        "\n",
        "- reference: https://ollama.com/download/linux\n",
        "```bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```"
      ],
      "metadata": {
        "id": "A4XSHN2KtPTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if ! which ollama > /dev/null; then                ## save some time if ollama was already installed\n",
        "  curl -fsSL https://ollama.com/install.sh | sh\n",
        "fi"
      ],
      "metadata": {
        "id": "0rXsN_UHtOgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi      ## show GPU VRAM\n",
        "!df -h $(pwd)    ## show Disk Space Usage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CueLQD4Vux9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3: Start Ollama Server\n",
        "\n",
        "- We want to run the following command in the background.\n",
        "\n",
        "```bash\n",
        "ollama start &\n",
        "```\n",
        "\n",
        "- Known issue:\n",
        "  - running with `!ollama start &` will block the following Notebook Cell.\n",
        "- Known solution:\n",
        "  - tried `%%bash --bg` cell magic, but it does not work. (test with `ps ax`)\n",
        "- Another solution:\n",
        "  - use `subprocess` instead\n",
        "  - Reference: https://stackoverflow.com/a/65194735"
      ],
      "metadata": {
        "id": "s5616ZYTvuhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"/usr/local/bin/ollama\",\"start\"])"
      ],
      "metadata": {
        "id": "AO8XP1KqmnwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4: Load Google Gemma 3 4B model into Ollama\n",
        "\n",
        "```bash\n",
        "ollama pull gemma3:4b\n",
        "```"
      ],
      "metadata": {
        "id": "wG0lq99-xWxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if ollama server is running\n",
        "%%bash\n",
        "sleep 3\n",
        "ps ax | grep ollama"
      ],
      "metadata": {
        "id": "dNKIKWVLnBxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ollama pull gemma3:4b\n",
        "ollama list"
      ],
      "metadata": {
        "id": "9jCKzZ6uIb5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5: Checkout a example Git repo\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart.git\n",
        "```"
      ],
      "metadata": {
        "id": "fTTHGsIYxgmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L1VYG12D7xV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6: Running `Aider` as single batch job\n",
        "\n",
        "Reference: grep from `aider --help`\n",
        "```\n",
        "Modes:\n",
        "  --message COMMAND, --msg COMMAND, -m COMMAND\n",
        "                        Specify a single message to send the LLM, process\n",
        "                        reply then exit (disables chat mode) [env var:\n",
        "                        AIDER_MESSAGE]\n",
        "```\n",
        "\n",
        "```bash\n",
        "aider --model ollama/gemma3:4b --no-auto-commits --no-gitignore -m \"/ask could you give me a high-level overview of this git repository?\"\n",
        "```"
      ],
      "metadata": {
        "id": "CgClLNV5MP8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ollama list\n",
        "export PATH=\"/root/.local/bin:$PATH\"\n",
        "export OLLAMA_API_BASE=\"http://127.0.0.1:11434\"\n",
        "cd gemini-fullstack-langgraph-quickstart\n",
        "aider --model ollama/gemma3:4b --no-auto-commits --no-gitignore -m \"/ask what program language is used inthis git repository?\""
      ],
      "metadata": {
        "id": "HvuDTeOVXxWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7: Running `Aider` interactively\n",
        "\n",
        "```bash\n",
        "aider --model ollama/gemma3:4b --no-auto-commits --no-gitignore\n",
        "```"
      ],
      "metadata": {
        "id": "vzcXC61ImpnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "k95LlUrLbzmx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
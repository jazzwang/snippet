# OpenAI models

### 2025-04-18

- 2025-04-16: [Introducing OpenAI o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)

## OpenAI o3-pro

### 2025-06-10

- 2025-06-10: [Launching OpenAI o3-pro—available now for Pro users in ChatGPT and in our API](https://help.openai.com/en/articles/9624314-model-release-notes)

## OpenAI o4-mini

- 2025-04-16: [Introducing OpenAI o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)

## OpenAI GPT-4.1

- 2025-04-14: [Introducing GPT-4.1 in the API](https://openai.com/index/gpt-4-1/)

## OpenAI gpt-oss

- 2025-08-05:
  - Introducing gpt-oss
  - https://openai.com/index/introducing-gpt-oss/
- Try gpt-oss
  - https://gpt-oss.com/
- Guides
  - https://cookbook.openai.com/topic/gpt-oss
- Model Card
  - https://openai.com/index/gpt-oss-model-card
- Git Repo
  - https://github.com/openai/gpt-oss
- HuggingFace
  - 

    **Download [gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) and [gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) on Hugging Face**

> Welcome to the gpt-oss series, [OpenAI's open-weight models](https://openai.com/open-models/) designed for powerful reasoning, agentic tasks, and versatile developer use cases.
>
> We're releasing two flavors of these open models:
> 
> -   `gpt-oss-120b` --- for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)
> -   `gpt-oss-20b` --- for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)
> 
> Both models were trained <mark>using our [harmony response format](https://github.com/openai/harmony) and should only be used with this format; otherwise, they will not work correctly.</mark>
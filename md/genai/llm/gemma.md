# Google Gemma

## 2025-01-14

- https://huggingface.co/bartowski/gemma-2-2b-it-GGUF

- 下載 `gemma-2-9b-it-Q2_K.gguf`

```bash
jazzw@JazzBook:~$ cd .ollama/models/
jazzw@JazzBook:~/.ollama/models$ mkdir gemma-2-9b-it-gguf
jazzw@JazzBook:~/.ollama/models$ cd gemma-2-9b-it-gguf/
jazzw@JazzBook:~/.ollama/models/gemma-2-9b-it-gguf$ wget https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q2_K.gguf

Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3805397952 (3.5G) [binary/octet-stream]
Saving to: 'gemma-2-9b-it-Q2_K.gguf'

gemma-2-9b-it-Q2_K.gguf                100%[============================================================================>]   3.54G  1.73MB/s    in 34m 10s

2025-01-14 21:30:34 (1.77 MB/s) - 'gemma-2-9b-it-Q2_K.gguf' saved [3805397952/3805397952]
```

- 用最簡單的 Modelfile 匯入 Gemma 2 9B Instruct Q2_K 模型

```bash
jazzw@JazzBook:~/.ollama/models/gemma-2-9b-it-gguf$ cat > Modelfile << EOF
FROM gemma-2-9b-it-Q2_K.gguf
EOF
jazzw@JazzBook:~/.ollama/models/gemma-2-9b-it-gguf$ ollama create gemma-2-9b-it -f Modelfile
transferring model data 100%
using existing layer sha256:2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403
using autodetected template gemma-instruct
using existing layer sha256:2490e7468436707d5156d7959cf3c6341cc46ee323084cfa3fcf30fe76e397dc
creating new layer sha256:f01f72e2d77f367b87c2ef63b4deb185b031d32ed6beee859cd13207d14440de
writing manifest
success
jazzw@JazzBook:~/.ollama/models/gemma-2-9b-it-gguf$ ollama list
NAME                    ID              SIZE      MODIFIED
gemma-2-9b-it:latest    51dee1323fb5    3.8 GB    6 seconds ago
jazzw@JazzBook:~/.ollama/models/gemma-2-9b-it-gguf$ ollama run gemma-2-9b-it:latest
>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /show model
Unknown command '/show model'. Type /? for help
>>> /show
Available Commands:
  /show info         Show details for this model
  /show license      Show model license
  /show modelfile    Show Modelfile for this model
  /show parameters   Show parameters for this model
  /show system       Show system message
  /show template     Show prompt template

>>> /show modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM gemma-2-9b-it:latest

FROM C:\Users\jazzw\.ollama\models\blobs\sha256-2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403
TEMPLATE """{{- $system := "" }}
{{- range .Messages }}
{{- if eq .Role "system" }}
{{- if not $system }}{{ $system = .Content }}
{{- else }}{{ $system = printf "%s\n\n%s" $system .Content }}
{{- end }}
{{- continue }}
{{- else if eq .Role "user" }}<start_of_turn>user
{{- if $system }}
{{ $system }}
{{- $system = "" }}
{{- end }}
{{- else if eq .Role "assistant" }}<start_of_turn>model
{{- end }}
{{ .Content }}<end_of_turn>
{{ end }}<start_of_turn>model
"""
PARAMETER stop <start_of_turn>
PARAMETER stop <end_of_turn>

>>>
>>> Send a message (/? for help)
```
- 蠻特別的，自動從 GGUF 檔中，偵測到 Template `gemma-instruct`

```bash
using autodetected template gemma-instruct
```

- ( 2025-01-14 21:46:10 )
- `ollama start` 的 log - 可以看到 GGUF 原始 Model 的 metadata
```bash
[GIN] 2025/01/14 - 21:42:14 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 21:42:14 | 200 |       539.3µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/01/14 - 21:42:37 | 200 |       519.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 21:42:49 | 201 |    8.3508931s |       127.0.0.1 | POST     "/api/blobs/sha256:2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403"
[GIN] 2025/01/14 - 21:42:49 | 200 |    107.1596ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/01/14 - 21:42:55 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 21:42:55 | 200 |      1.0702ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/01/14 - 21:43:07 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 21:43:07 | 200 |     46.8456ms |       127.0.0.1 | POST     "/api/show"
time=2025-01-14T21:43:08.599+08:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=C:\Users\jazzw\.ollama\models\blobs\sha256-2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403 gpu=GPU-e8624b08-30b4-9c97-b054-cf2ed44d5da2 parallel=1 available=7443841024 required="5.2 GiB"
time=2025-01-14T21:43:08.628+08:00 level=INFO source=server.go:104 msg="system memory" total="31.2 GiB" free="16.2 GiB" free_swap="14.1 GiB"
time=2025-01-14T21:43:08.629+08:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=43 layers.offload=43 layers.split="" memory.available="[6.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.2 GiB" memory.required.partial="5.2 GiB" memory.required.kv="672.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="3.5 GiB" memory.weights.repeating="2.8 GiB" memory.weights.nonrepeating="717.8 MiB" memory.graph.full="507.0 MiB" memory.graph.partial="1.2 GiB"
time=2025-01-14T21:43:08.638+08:00 level=INFO source=server.go:376 msg="starting llama server" cmd="C:\\Users\\jazzw\\scoop\\apps\\ollama\\current\\lib\\ollama\\runners\\cuda_v12_avx\\ollama_llama_server.exe runner --model C:\\Users\\jazzw\\.ollama\\models\\blobs\\sha256-2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403 --ctx-size 2048 --batch-size 512 --n-gpu-layers 43 --threads 8 --no-mmap --parallel 1 --port 53122"
time=2025-01-14T21:43:08.696+08:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-01-14T21:43:08.696+08:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-01-14T21:43:08.697+08:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-01-14T21:43:09.092+08:00 level=INFO source=runner.go:945 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
time=2025-01-14T21:43:09.751+08:00 level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(clang)" threads=8
time=2025-01-14T21:43:09.753+08:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:53122"
time=2025-01-14T21:43:09.979+08:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7099 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from C:\Users\jazzw\.ollama\models\blobs\sha256-2efce51c0d774a47c247c07ec6f109f670e3da4f4eec0f9322d72e3635622403 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma2
llama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it
llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192
llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584
llama_model_loader: - kv   4:                         gemma2.block_count u32              = 42
llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16
llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256
llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000
llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000
llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/gemma-2-9b-it-GGUF/gemma-...
llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 294
llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128
llama_model_loader: - type  f32:  169 tensors
llama_model_loader: - type q2_K:  168 tensors
llama_model_loader: - type q3_K:  126 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 217
llm_load_vocab: token to piece cache size = 1.6014 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma2
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 3584
llm_load_print_meta: n_layer          = 42
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_swa            = 4096
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 2
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 9B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 9.24 B
llm_load_print_meta: model size       = 3.54 GiB (3.29 BPW)
llm_load_print_meta: general.name     = gemma-2-9b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOG token        = 1 '<eos>'
llm_load_print_meta: EOG token        = 107 '<end_of_turn>'
llm_load_print_meta: max token length = 48
llm_load_tensors: offloading 42 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 43/43 layers to GPU
llm_load_tensors:          CPU model buffer size =   717.77 MiB
llm_load_tensors:        CUDA0 model buffer size =  3623.33 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_kv_cache_init:      CUDA0 KV buffer size =   672.00 MiB
llama_new_context_with_model: KV self size  =  672.00 MiB, K (f16):  336.00 MiB, V (f16):  336.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.99 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   507.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    15.01 MiB
llama_new_context_with_model: graph nodes  = 1690
llama_new_context_with_model: graph splits = 2
time=2025-01-14T21:43:11.766+08:00 level=INFO source=server.go:594 msg="llama runner started in 3.07 seconds"
```
# Getting Started with Google Cloud Learning Path

- https://www.cloudskillsboost.google/paths/8

[TOC]

## 02: Implementing Cloud Load Balancing for Compute Engine

- https://www.cloudskillsboost.google/paths/8/course_templates/648

### Implementing Cloud Load Balancing for Compute Engine

#### Set Up Network Load Balancers

- https://www.cloudskillsboost.google/paths/8/course_templates/648/labs/567898

#### Set Up Application Load Balancers

- https://www.cloudskillsboost.google/paths/8/course_templates/648/labs/567899

#### Using an Internal Application Load Balancer

- https://www.cloudskillsboost.google/paths/8/course_templates/648/labs/567900

#### Implement Load Balancing on Compute Engine: Challenge Lab

- https://www.cloudskillsboost.google/paths/8/course_templates/648/labs/567901

### Your Next Steps

## 03: Google Cloud Fundamentals: Core Infrastructure

- https://www.cloudskillsboost.google/paths/8/course_templates/60

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531558

Hi, and welcome to the Google Cloud Fundamentals: Core Infrastructure training course. The goal of this course is to provide you with an overview of Google Cloud. Google Cloud offerings can be broadly categorised as compute, storage, big data, machine learning, and application services for web, mobile, analytics, and back-end solutions. Through a combination of videos, quizzes, and hands-on labs, you’ll learn the value of Google Cloud and how cloud solutions factor into business strategies. The intended target audience of today’s course consists of solutions developers, systems operations professionals, and solution architects planning to deploy applications and create application environments on Google Cloud. The course will also be useful for business decision makers evaluating Google Cloud. While you should be happy to hear that we’ll be finding out about services and concepts that are specific to Google Cloud in this course, do keep in mind that, as a ‘fundamentals’ level course, some content will be geared towards learners who are entirely new to cloud technologies. This course has no prerequisites, although it’s helpful be familiar with application development, Linux operating systems, systems operations, and data analytics/machine learning to best understand the technologies covered. There are seven key learning objectives that we’re hoping to achieve. By the end of this course, you should be able to: Identify the purpose and value of Google Cloud products and services. Define how infrastructure is organized and controlled in Google Cloud. Explain how to create a basic infrastructure in Google Cloud. Select and use Google Cloud storage options. Describe the purpose and value of Google Kubernetes Engine. Identify the use cases for serverless Google Cloud services. And combine Google Cloud knowledge with prompt engineering to improve Gemini responses. OK, all set? Let’s begin!

### Introducing Google Cloud

#### Cloud computing overview

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531559

Let’s start at the beginning with an overview of cloud computing. The cloud is a hot topic these days, but what exactly is it? The US National Institute of Standards and Technology created the term cloud computing, although there is nothing US-specific about it. Cloud computing is a way of using information technology (IT) that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they require without the need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they’re flexible, so customers can be. If customers need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. And that's it, that's the definition of cloud. But why is the cloud model so compelling nowadays? To understand why, we need to look at some history. The trend towards cloud computing started with a first wave known as colocation. Colocation gave users the financial efficiency of renting physical space, instead of investing in data center real estate. Virtualized data centers of today, which are the second wave, share similarities with the private data centers and colocation facilities of decades past. The components of virtualized data centers match the physical building blocks of hosted computing—servers, CPUs, disks, load balancers, and so on—but now they’re virtual devices. With virtualization, enterprises still maintain the infrastructure; but it also remains a user-controlled and user-configured environment. Several years ago, Google realized that its business couldn’t move fast enough within the confines of the virtualization model. So Google switched to a container-based architecture— a fully automated, elastic third-wave cloud that consists of a combination of automated services and scalable data. Services automatically provision and configure the infrastructure used to run applications. Today, Google Cloud makes this third-wave cloud available to Google customers. Google believes that, in the future, every company, regardless of size or industry, will differentiate itself from its competitors through technology. Increasingly, that technology will be in the form of software. Great software is based on high-quality data. This means that every company is, or will eventually become, a data company.

#### IaaS and PaaS

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531560

The move to virtualized data centers introduced customers to two new types of offerings: infrastructure as a service, commonly referred to as IaaS, and platform as a service, or PaaS. IaaS offerings provide raw compute, storage, and network capabilities, organized virtually into resources that are similar to physical data centers. Compute Engine is an example of a Google Cloud IaaS service. PaaS offerings, in contrast, bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. App Engine is an example of a Google Cloud PaaS service. In the IaaS model, customers pay for the resources they allocate ahead of time; in the PaaS model, customers pay for the resources they actually use. As cloud computing has evolved, the momentum has shifted toward managed infrastructure and managed services. Leveraging managed resources and services allows companies to concentrate more on their business goals and spend less time and money on creating and maintaining their technical infrastructure. It allows companies to deliver products and services to their customers more quickly and reliably. Serverless is yet another step in the evolution of cloud computing. It allows developers to concentrate on their code, rather than on server configuration, by eliminating the need for any infrastructure management. Serverless technologies offered by Google include Cloud Run, which allows customers to deploy their containerized microservices based application in a fully-managed environment. and Cloud Run functions, which manages event-driven code as a pay-as-you-go service. While it’s outside the scope of this course, you might have heard about software as a service, SaaS, and wondered what it is and how it fits into the Cloud ecosphere. SaaS provides the entire application stack, delivering an entire cloud-based application that customers can access and use. Software as a Service applications are not installed on your local computer. Instead, they run in the cloud as a service and are consumed directly over the internet by end users. Popular Google applications such as Gmail, Docs, and Drive, that are a part of Google Workspace, are all examples of SaaS.

#### The Google Cloud network

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531561

Google Cloud runs on Google’s own global network. It’s the largest network of its kind, and Google has invested billions of dollars over many years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by leveraging more than 100 content caching nodes worldwide. These are locations where high demand content is cached for quicker access, allowing applications to respond to user requests from the location that will provide the quickest response time. Google Cloud’s locations underpin all of the important work we do for our customers. From redundant cloud regions to high- bandwidth connectivity via subsea cables, every aspect of our infrastructure is designed to deliver your services to your users, no matter where they are around the world. Google Cloud’s infrastructure is based in seven major geographic locations: North America, South America, Europe, Africa, the Middle East, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency, the latter of which measures the time a packet of information takes to travel from its source to its destination. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy. You can run also resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, such as a natural disaster. Some of Google Cloud’s services support placing resources in what we call a multi-region. For example, Spanner multi-region configurations allow you to replicate the database's data not just in multiple zones, but in multiple zones across multiple regions, as defined by the instance configuration. These additional replicas enable you to read data with low latency from multiple locations close to or within the regions in the configuration, like The Netherlands, and Belgium. The number of zones and regions Google Cloud supports is increasing all the time. You can find the most up-to-date numbers at cloud.google.com/about/locations.

#### Environmental impact

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531562

The virtual world, which includes Google Cloud’s network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy. Altogether, existing data centers use roughly 2% of the world’s electricity. With this in mind, Google works to make their data centers run as efficiently as possible. Just like our customers, Google is trying to do the right things for the planet. We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals. Therefore, it’s useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a standard that maps out a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste. As an example of how this is being done, here’s Google’s data center in Hamina, Finland. This facility is one of the most advanced and efficient data centers in the Google fleet. Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world. In our founding decade, Google became the first major company to be carbon neutral. In our second decade, we were the first company to achieve 100% renewable energy. By 2030, we aim to be the first major company to operate completely carbon free.

#### Security

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531563

Nine of Google’s services have more than one billion users each, and so you can be assured that security is always on the minds of Google's employees. Design for security is prevalent throughout the infrastructure that Google Cloud and Google services run on. Let's talk about a few ways Google works to keep customers' data safe. The security infrastructure can be explained in progressive layers, starting from the physical security of our data centers, continuing on to how the hardware and software that underlie the infrastructure are secured, and finally, describing the technical constraints and processes in place to support operational security. We begin with the Hardware infrastructure layer which comprises three key security features: The first is hardware design and provenance. Both the server boards and the networking equipment in Google data centers are custom-designed by Google. Google also designs custom chips, including a hardware security chip that's currently being deployed on both servers and peripherals. The next feature is a secure boot stack. Google server machines use a variety of technologies to ensure that they are booting the correct software stack, such as cryptographic signatures over the BIOS, bootloader, kernel, and base operating system image. This layer's final feature is premises security. Google designs and builds its own data centers, which incorporate multiple layers of physical security protections. Access to these data centers is limited to only a very small number of Google employees. Google additionally hosts some servers in third-party data centers, where we ensure that there are Google-controlled physical security measures on top of the security layers provided by the data center operator. Next is the Service deployment layer, where the key feature is encryption of inter-service communication. Google’s infrastructure provides cryptographic privacy and integrity for remote procedure call (“RPC”) data on the network. Google’s services communicate with each other using RPC calls. The infrastructure automatically encrypts all infrastructure RPC traffic that goes between data centers. Google has started to deploy hardware cryptographic accelerators that will allow it to extend this default encryption to all infrastructure RPC traffic inside Google data centers. Then we have the User identity layer. Google’s central identity service, which usually manifests to end users as the Google login page, goes beyond asking for a simple username and password. The service also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also employ secondary factors when signing in, including devices based on the Universal 2nd Factor (U2F) open standard. On the Storage services layer we find the encryption at rest security feature. Most applications at Google access physical storage (in other words, “file storage”) indirectly via storage services, and encryption using centrally managed keys is applied at the layer of these storage services. Google also enables hardware encryption support in hard drives and SSDs. The next layer up is the Internet communication layer, and this comprises two key security features. Google services that are being made available on the internet, register themselves with an infrastructure service called the Google Front End, which ensures that all TLS connections are ended using a public-private key pair and an X.509 certificate from a Certified Authority (CA), as well as following best practices such as supporting perfect forward secrecy. The GFE additionally applies protections against Denial of Service attacks. Also provided is Denial of Service (“DoS”) protection. The sheer scale of its infrastructure enables Google to simply absorb many DoS attacks. Google also has multi-tier, multi-layer DoS protections that further reduce the risk of any DoS impact on a service running behind a GFE. The final layer is Google's Operational security layer which provides four key features. First is intrusion detection. Rules and machine intelligence give Google’s operational security teams warnings of possible incidents. Google conducts Red Team exercises to measure and improve the effectiveness of its detection and response mechanisms. Next is reducing insider risk. Google aggressively limits and actively monitors the activities of employees who have been granted administrative access to the infrastructure. Then there’s employee U2F use. To guard against phishing attacks against Google employees, employee accounts require use of U2F-compatible Security Keys. Finally, there are stringent software development practices. Google employs central source control and requires two-party review of new code. Google also provides its developers libraries that prevent them from introducing certain classes of security bugs. Additionally, Google runs a Vulnerability Rewards Program where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications. You can learn more about Google’s technical-infrastructure security at cloud.google.com/security/security-design.

#### Open source ecosystems

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531564

Some organizations are afraid to bring their workloads to the cloud because they're afraid they'll get locked into a particular vendor. However, if, for whatever reason, a customer decides that Google is no longer the best provider for their needs, we provide them with the ability to run their applications elsewhere. Google publishes key elements of technology using open source licenses to create ecosystems that provide customers with options other than Google. For example, TensorFlow, an open source software library for machine learning developed inside Google, is at the heart of a strong open source ecosystem. Google provides interoperability at multiple layers of the stack. Kubernetes and Google Kubernetes Engine give customers the ability to mix and match microservices running across different clouds, while Google Cloud Observability lets customers monitor workloads across multiple cloud providers.

#### Pricing and billing

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531565

To round off this section of the course, let’s take a brief look at Google Cloud’s pricing structure. Google was the first major cloud provider to deliver per-second billing for its infrastructure-as-a-service compute offering, Compute Engine. In addition, per-second billing is now also offered for users of Google Kubernetes Engine (our container infrastructure as a service), Dataproc (which is the equivalent of the big data system Hadoop, but operating as a service), and App Engine flexible environment VMs (a platform as a service). Compute Engine offers automatically applied sustained-use discounts, which are automatic discounts that you get for running a virtual machine instance for a significant portion of the billing month. Specifically, when you run an instance for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. Custom virtual machine types allow Compute Engine virtual machines to be fine-tuned with optimal amounts of vCPU and memory for their applications so that you can tailor your pricing for your workloads. Our online pricing calculator can help estimate your costs. Visit cloud.google.com/products/calculator to try it out. Now, you’re probably thinking, “How can I make sure I don’t accidentally run up a big Google Cloud bill?” You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month’s spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you’ll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud Console that allows you to monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack, protecting both account owners and the Google Cloud community as a whole. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources you can have in your projects. For example, by default, each Google Cloud project has a quota allowing it no more than 15 Virtual Private Cloud networks. Although projects all start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531566

### Resources and Access in the Cloud

#### Google Cloud resource hierarchy

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531567

In this section of the course we’ll look at the functional structure of Google Cloud. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. At the first level are resources. These represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are also inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let’s take a look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they’re billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can’t be changed after creation. They’re what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don’t have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It’s helpful to know that these Google-generated numbers exist, but we won’t explore them much in this course. They’re mainly used internally by Google Cloud to keep track of resources. Google Cloud’s Resource Manager tool is designed to programmatically help you manage projects. It’s an API that can gather a list of all the projects associated with an account, create new projects, update existing projects, and delete projects. It can even recover projects that were previously deleted,and can be accessed through the RPC API and the REST API. The third level of the Google Cloud resource hierarchy is folders. Folders let you assign policies to resources at a level of granularity you choose. The resources in a folder inherit policies and permissions assigned to that folder. A folder can contain projects, other folders, or a combination of both. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. As previously mentioned, the resources in a folder inherit policies and permissions from that folder. For example, if you have two different projects that are administered by the same team, you can put policies into a common folder so they have the same permissions. Doing it the other way--putting duplicate copies of those policies on both projects–could be tedious and error-prone. if you needed to change permissions on both resources, you would now have to do that in two places instead of just one. To use folders, you must have an organization node, which is the very topmost resource in the Google Cloud hierarchy. Everything else attached to that account goes under this node, which includes folders, projects, and other resources. There are some special roles associated with this top-level organization node. For example, you can designate an organization policy administrator so that only people with privilege can change policies. You can also assign a project creator role, which is a great way to control who can create projects and, therefore, who can spend money. How a new organization node is created depends on whether your company is also a Google Workspace customer. If you have a Workspace domain, Google Cloud projects will automatically belong to your organization node. Otherwise, you can use Cloud Identity, Google’s identity, access, application, and endpoint management platform, to generate one. Once created, a new organization node will let anyone in the domain create projects and billing accounts, just as they could before. folders underneath it and put projects into it. Both folders and projects are considered to be “children” of the organization node.

#### Identity and Access Management (IAM)

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531568

When an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The “who” part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A “who” is also called a “principal.” Each principal has its own identifier, usually an email address. The “can do what” part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. When a principal is given a role on a specific element of the resource hierarchy, the resulting policy applies to both the chosen element and all the elements below it in the hierarchy. You can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. Deny policies, like allow policies, are inherited through the resource hierarchy. There are three kinds of roles in IAM: basic, predefined, and custom. The first role type is basic. Basic roles are quite broad in scope. When applied to a Google Cloud project, they affect all resources in that project. Basic roles include owner, editor, viewer, and billing administrator. Let’s look at these basic roles in a bit more detail. Project viewers can access resources but can’t make changes. Project editors can access and make changes to a resource. And project owners can also access and make changes to a resource. In addition, project owners can manage the associated roles and permissions and set up billing. Often companies want someone to control the billing for a project but not be able to change the resources in the project. This is possible through a billing administrator role. A word of caution: If several people are working together on a project that contains sensitive data, basic roles are probably too broad. Fortunately, IAM provides other ways to assign permissions that are more specifically tailored to meet the needs of typical job roles. This brings us to the second type of role, predefined roles. Specific Google Cloud services offer sets of predefined roles, and they even define where those roles can be applied. Let’s look at Compute Engine, for example, a Google Cloud product that offers virtual machines as a service. With Compute Engine, you can apply specific predefined roles—such as “instanceAdmin”—to Compute Engine resources in a given project, a given folder, or an entire organization. This then allows whoever has these roles to perform a specific set of predefined actions. But what if you need to assign a role that has even more specific permissions? That’s when you’d use a custom role. Many companies use a “least-privilege” model in which each person in your organization is given the minimal amount of privilege needed to do their job. So, for example, maybe you want to define an “instanceOperator” role to allow some users to stop and start Compute Engine virtual machines, but not reconfigure them. Custom roles will allow you to define those exact permissions. Before you start creating custom roles, please note two important details. First, you’ll need to manage the permissions that define the custom role you’ve created. Because of this, some organizations decide they’d rather use the predefined roles. And second, custom roles can only be applied to either the project level or organization level. They can’t be applied to the folder level.

#### Service accounts

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531569

Imagine you have a Compute Engine virtual machine running a program that needs to access other cloud services regularly. Instead of requiring a person to manually grant access each time the program runs, you can give the virtual machine itself the necessary permissions. This is where service accounts come in. Service accounts allow you to assign specific permissions to a virtual machine, so it can interact with other cloud services without human intervention. Let’s say you have an application running in a virtual machine that needs to store data in Cloud Storage, but you don’t want anyone on the internet to have access to that data - just that particular virtual machine. You can create a service account to authenticate that VM to Cloud Storage. Service accounts are named with an email address, but instead of passwords they use cryptographic keys to access resources. So, if a service account has been granted Compute Engine’s Instance Admin role, this would allow an application running in a VM with that service account to create, modify, and delete other VMs. Service accounts do need to be managed. For example, maybe Alice needs to manage which Google accounts can act as service accounts, while Bob just needs to be able to view a list of service accounts. Fortunately, in addition to being an identity, a service account is also a resource, so it can have IAM policies of its own attached to it. This means that Alice can have the editor role on a service account, and Bob can have the viewer role. This is just like granting roles for any other Google Cloud resource.

#### Cloud Identity

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531570

When new Google Cloud customers start using the platform, it’s common to log in to the Google Cloud Console with a Gmail account and then use Google Groups to collaborate with teammates who are in similar roles. Although this approach is easy to start with, it can present challenges later because the team’s identities are not centrally managed. This can be problematic if, for example, someone leaves the organization. With this setup, there’s no easy way to immediately remove a user’s access to the team’s cloud resources. With a tool called Cloud Identity, organizations can define policies and manage their users and groups using the Google Admin Console. Admins can log in and manage Google Cloud resources using the same usernames and passwords they already use in existing Active Directory or LDAP systems. Using Cloud Identity also means that when someone leaves an organization, an administrator can use the Google Admin Console to disable their account and remove them from groups. Cloud Identity is available in a free edition and also in a premium edition that provides capabilities to manage mobile devices. If you’re a Google Cloud customer who is also a Google Workspace customer, this functionality is already available to you in the Google Admin Console.

#### Interacting with Google Cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531571

There are four ways to access and interact with Google Cloud. The Google Cloud console, the Google Cloud SDK and Cloud Shell, the APIs, and the Google Cloud app. Let’s explore each of those now. First is the Google Cloud console, which is Google Cloud’s graphical user interface, or GUI, that helps you deploy, scale, and diagnose production issues in a simple web-based interface. With the Google Cloud console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Google Cloud console also provides a search facility to quickly find resources and connect to instances via SSH in the browser. Second is through the Google Cloud SDK and Cloud Shell. The Google Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These include the Google Cloud CLI, which provides the main command-line interface for Google Cloud products and services, and bq, a command-line tool for BigQuery. When installed, all of the tools within the Google Cloud SDK are located under the bin directory. Cloud Shell provides command-line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent 5 gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. With Cloud Shell, the Google Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The third way to access Google Cloud is through application programming interfaces, or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Google Cloud console includes a tool called the Google APIs Explorer that shows which APIs are available, and in which versions. You can try these APIs interactively, even those that require user authentication. So, suppose you’ve explored an API, and you’re ready to build an application that uses it. Do you have to start coding from scratch? No. Google provides Cloud Client libraries and Google API Client libraries in many popular languages to take a lot of the drudgery out of the task of calling Google Cloud from your code. Languages currently represented in these libraries are Java, Python, PHP, C#, Go, Node.js, Ruby, and C++. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app provides up-to-date billing information for your projects and billing alerts for projects that are going over budget. You can set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app also offers alerts and incident management. You can download the Google Cloud app at cloud.google.com/app.

#### Google Cloud Fundamentals: Getting Started with Cloud Marketplace

- https://www.cloudskillsboost.google/paths/8/course_templates/60/labs/531572

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531573

### Virtual Machines and Networks in the Cloud

#### Virtual Private Cloud networking

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531574

In this section of the course, we’re going to explore how Google Compute Engine works with a focus on virtual networking. Many users start with Google Cloud by defining their own virtual private cloud inside their first Google Cloud project or by starting with the default virtual private cloud. So, what is a virtual private cloud? A virtual private cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud – like Google Cloud! On a VPC, customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but this private cloud is hosted remotely by a public cloud provider. This means that VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing. VPC networks connect Google Cloud resources to each other and to the internet. This includes segmenting networks, using firewall rules to restrict access to instances, and creating static routes to forward traffic to specific destinations. Here's something that tends to surprise a lot of new Google Cloud users: Google VPC networks are global. They can also have subnets, which is a segmented piece of the larger network, in any Google Cloud region worldwide. Subnets can span the zones that make up a region. This architecture makes it easy to define network layouts with global scope. Resources can even be in different zones on the same subnet. The size of a subnet can be increased by expanding the range of IP addresses allocated to it, and doing so won’t affect virtual machines that are already configured. For example, let’s take a VPC network named vpc1 that has two subnets defined in the asia-east1 and us-east1 regions. If the VPC has three Compute Engine VMs attached to it, it means they’re neighbors on the same subnet even though they’re in different zones. This capability can be used to build solutions that are resilient to disruptions yet retain a simple network layout.

#### Compute Engine

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531575

Earlier in the course, we explored infrastructure as a service, or IaaS. Now let’s explore Google Cloud’s IaaS solution: Compute Engine. With Compute Engine, users can create and run virtual machines on Google infrastructure. There are no upfront investments, and thousands of virtual CPUs can run on a system that’s designed to be fast and to offer consistent performance. Each virtual machine contains the power and functionality of a full-fledged operating system. This means a virtual machine can be configured much like a physical server: by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system. A virtual machine instance can be created via the Google Cloud console, which is a web-based tool to manage Google Cloud projects and resources, the Google Cloud CLI, or the Compute Engine API. The instance can run Linux and Windows Server images provided by Google or any customized versions of these images. You can also build and run images of other operating systems and flexibly reconfigure virtual machines. A quick way to get started with Google Cloud is through the Cloud Marketplace, which offers solutions from both Google and third-party vendors. With these solutions, there’s no need to manually configure the software, virtual machine instances, storage, or network settings, although many of them can be modified before launch if that’s required. Most software packages in Cloud Marketplace are available at no additional charge beyond the normal usage fees for Google Cloud resources. Some Cloud Marketplace images charge usage fees, particularly those published by third parties, with commercially licensed software, but they all show estimates of their monthly charges before they’re launched. At this point, you might be wondering about Compute Engine’s pricing and billing structure. For the use of virtual machines, Compute Engine bills by the second with a one-minute minimum, and sustained-use discounts start to apply automatically to virtual machines the longer they run. So, for each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every additional minute. Compute Engine also offers committed-use discounts. This means that for stable and predictable workloads, a specific amount of vCPUs and memory can be purchased for up to a 57% discount off of normal prices in return for committing to a usage term of one year or three years. And then there are Preemptible and Spot VMs. Let’s say you have a workload that doesn’t require a human to sit and wait for it to finish–such as a batch job analyzing a large dataset. You can save money, in some cases up to 90%, by choosing Preemptible or Spot VMs to run the job. A Preemptible or Spot VM is different from an ordinary Compute Engine VM in only one respect: Compute Engine has permission to terminate a job if its resources are needed elsewhere. Although savings are possible with preemptible or spot VMs, you'll need to ensure that your job can be stopped and restarted. Spot VMs differ from Preemptible VMs by offering more features. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. However, the pricing is, currently the same for both. In terms of storage, Compute Engine doesn’t require a particular option or machine type to get high throughput between processing and persistent disks. That’s the default, and it comes to you at no extra cost. And finally, you’ll only pay for what you need with custom machine types. Compute Engine lets you choose the machine properties of your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types or by creating your own custom machine types.

#### Scaling virtual machines

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531576

As we’ve just seen, with Compute Engine, you can choose the most appropriate machine properties for your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types, or by creating custom machine types. To do this, Compute Engine has a feature called Autoscaling, where VMs can be added to or subtracted from an application based on load metrics. The other part of making that work is balancing the incoming traffic among the VMs. Google’s Virtual Private Cloud (VPC) supports several different kinds of load balancing, which we’ll explore shortly. With Compute Engine, you can in fact configure very large VMs, which are great for workloads such as in-memory databases and CPU-intensive analytics, but most Google Cloud customers start off with scaling out, not up. The maximum number of CPUs per VM is tied to its “machine family” and is also constrained by the quota available to the user, which is zone-dependent. Specifications for currently available VM machine types can be found at cloud.google.com/compute/docs/machine-types

#### Important VPC compatibilities

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531577

Now let’s explore some of the most important Virtual Private Cloud compatibility features. Much like physical networks, VPCs have routing tables. VPC routing tables are built-in so you don’t have to provision or manage a router. They’re used to forward traffic from one instance to another within the same network, across subnetworks, or even between Google Cloud zones, without requiring an external IP address. Another thing you don’t have to provision or manage for Google Cloud is a firewall. VPCs provide a global distributed firewall, which can be controlled to restrict access to instances through both incoming and outgoing traffic. Firewall rules can be defined through network tags on Compute Engine instances, which is really convenient. For example, you can tag all your web servers with, say, “WEB,” and write a firewall rule saying that traffic on ports 80 or 443 is allowed into all VMs with the “WEB” tag, no matter what their IP address happens to be. You’ll remember that VPCs belong to Google Cloud projects, but what if your company has several Google Cloud projects, and the VPCs need to talk to each other? With VPC Peering, a relationship between two VPCs can be established to exchange traffic. Alternatively, to use the full power of Identity Access Management (IAM) to control who and what in one project can interact with a VPC in another, you can configu

#### Cloud Load Balancing

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531578

Previously, we explored how virtual machines can autoscale to respond to changing loads. But how do your customers get to your application when it might be provided by four VMs one moment, and by 40 VMs at another? That’s done through Cloud Load Balancing. The job of a load balancer is to distribute user traffic across multiple instances of an application. By spreading the load, load balancing reduces the risk that applications experience performance issues. Cloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. And because the load balancers don’t run in VMs that you have to manage, you don’t have to worry about scaling or managing them. You can put Cloud Load Balancing in front of all of your traffic: HTTP or HTTPS, other TCP and SSL traffic, and UDP traffic too. Cloud Load Balancing provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if backends become unhealthy. Cloud Load Balancing reacts quickly to changes in users, traffic, network, backend health, and other related conditions. And what if you anticipate a huge spike in demand? Say, your online game is already a hit; do you need to file a support ticket to warn Google of the incoming load? No. No so-called “pre-warming” is required. Google Cloud offers a range of load balancing solutions that can be classified based on the OSI model layer they operate at and their specific functionalities. Application Load Balancers operate at the application layer and are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content-based routing and SSL/TLS termination. Application Load Balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules you define. They are highly flexible and can be configured for both internet-facing (external) and internal applications. Network Load Balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types: Proxy Network Load Balancers also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments. Unlike proxy Network Load Balancers, passthrough Network Load Balancers do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well-suited for applications that require direct server return or need to handle a wider range of IP protocols.

#### Cloud DNS and Cloud CDN

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531579

One of the most famous free Google services is 8.8.8.8, which provides a public Domain Name Service to the world. DNS is what translates internet hostnames to addresses, and as you might imagine, Google has a highly developed DNS infrastructure. It makes 8.8.8.8 available so that everyone can take advantage of it. But what about the internet hostnames and addresses of applications built in Google Cloud? Google Cloud offers Cloud DNS to help the world find them. It’s a managed DNS service that runs on the same infrastructure as Google. It has low latency and high availability, and it’s a cost-effective way to make your applications and services available to your users. The DNS information you publish is served from redundant locations around the world. Cloud DNS is also programmable. You can publish and manage millions of DNS zones and records using the Cloud console, the command-line interface, or the API. Google also has a global system of edge caches. Edge caching refers to the use of caching servers to store content closer to end users. You can use this system to accelerate content delivery in your application by using Cloud CDN - Content Delivery Network. This means your customers will experience lower network latency, the origins of your content will experience reduced load, and you can even save money. After an Application Load Balancer is set up, Cloud CDN can be enabled with a single checkbox. There are many other CDNs available out there, of course. If you are already using one, chances are, it’s a part of Google Cloud’s CDN Interconnect partner program, and you can continue to use it.

#### Connecting networks to Google VPC

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531580

Many Google Cloud customers want to connect their Google Virtual Private Cloud networks to other networks in their system, such as on-premises networks or networks in other clouds. There are several effective ways to accomplish this. One option is to start with a Virtual Private Network connection over the internet and use Cloud VPN to create a “tunnel” connection. To make the connection dynamic, a Google Cloud feature called Cloud Router can be used. Cloud Router lets other networks and Google VPC, exchange route information over the VPN using the Border Gateway Protocol. Using this method, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But using the internet to connect networks isn't always the best option for everyone, either because of security concerns or because of bandwidth reliability. So, a second option is to consider “peering” with Google using Direct Peering. Peering means putting a router in the same public data center as a Google point of presence and using it to exchange traffic between networks. Google has more than 100 points of presence around the world. Customers who aren’t already in a point of presence can work with a partner in the Carrier Peering program to get connected. Carrier peering gives you direct access from your on-premises network through a service provider's network to Google Workspace and to Google Cloud products that can be exposed through one or more public IP addresses. One downside of peering, though, is that it isn’t covered by a Google Service Level Agreement. If getting the highest uptimes for interconnection is important, using Dedicated Interconnect would be a good solution. This option allows for one or more direct, private connections to Google. If these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%. Also, these connections can be backed up by a VPN for even greater reliability. Another option we’ll explore is Partner Interconnect, which provides connectivity between an on-premises network and a VPC network through a supported service provider. A Partner Interconnect connection is useful if a data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if the data needs don’t warrant an entire 10 GigaBytes per second connection. Depending on availability needs, Partner Interconnect can be configured to support mission-critical services or applications that can tolerate some downtime. As with Dedicated Interconnect, if these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%, but note that Google isn’t responsible for any aspects of Partner Interconnect provided by the third-party service provider, nor any issues outside of Google's network. And the final option is Cross-Cloud Interconnect. Cross-Cloud Interconnect helps you establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud network with your network that's hosted by a supported cloud service provider. Cross-Cloud Interconnect supports your adoption of an integrated multicloud strategy. In addition to supporting various cloud service providers, Cross-Cloud Interconnect offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps.

#### Getting Started with VPC Networking and Google Compute Engine

- https://www.cloudskillsboost.google/paths/8/course_templates/60/labs/531581

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531582

### Storage in the Cloud

#### Google Cloud storage options

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531583

Every application needs to store data, like media to be streamed or perhaps even sensor data from devices, and different applications and workloads require different storage database solutions. Google Cloud has storage options for structured, unstructured, transactional, and relational data. In this section of the course, we’ll explore Google Cloud’s five core storage products: Cloud Storage, Cloud SQL, Spanner, Firestore, and Bigtable. Depending on your application, you might use one or several of these services to do the job.

#### Cloud Storage

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531584

Let’s begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as “objects” and not as a file and folder hierarchy (file storage), or as chunks of a disk (block storage). These objects are stored in a packaged format which contains the binary form of the actual data itself, as well as relevant associated meta-data (such as date created, author, resource type, and permissions), and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. Cloud Storage is Google’s object storage product. It allows customers to store any amount of data, and to retrieve it as often as needed. It’s a fully managed scalable service that has a wide variety of uses. A few examples include serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users via Direct Download. Cloud Storage’s primary use is whenever binary large-object storage (also known as a “BLOB”) is needed for online content such as videos and photos, for backup and archived data and for storage of intermediate results in processing workflows. Cloud Storage files are organized into buckets. A bucket needs a globally unique name and a specific geographic location for where it should be stored, and an ideal location for a bucket is where latency is minimized. For example, if most of your users are in Europe, you probably want to pick a European location, so either a specific Google Cloud region in Europe, or else the EU multi-region. The storage objects offered by Cloud Storage are immutable, which means that you do not edit them, but instead a new version is created with every change made. Administrators have the option to either allow each new version to completely overwrite the older one, or to keep track of each change made to a particular object by enabling “versioning” within a bucket. If you choose to use versioning, Cloud Storage will keep a detailed history of modifications -- that is, overwrites or deletes -- of all objects contained in that bucket. If you don’t turn on object versioning, by default new versions will always overwrite older versions. With object versioning enabled, you can list the archived versions of an object, restore an object to an older state, or permanently delete a version of an object, as needed. In many cases, personally identifiable information may be contained in data objects, so controlling access to stored data is essential to ensuring security and privacy are maintained. Using IAM roles and, where needed, access control lists (ACLs), organizations can conform to security best practices, which require each user to have access and permissions to only the resources they need to do their jobs, and no more than that. There are a couple of options to control user access to objects and buckets. For most purposes, IAM is sufficient. Roles are inherited from project to bucket to object. If you need finer control, you can create access control lists. Each access control list consists of two pieces of information. The first is a scope, which defines who can access and perform an action. This can be a specific user or group of users. The second is a permission, which defines what actions can be performed, like read or write. Because storing and retrieving large amounts of object data can quickly become expensive, Cloud Storage also offers lifecycle management policies. For example, you could tell Cloud Storage to delete objects older than 365 days; or to delete objects created before January 1, 2013; or to keep only the 3 most recent versions of each object in a bucket that has versioning enabled. Having this control ensures that you’re not paying for more than you actually need.

#### Cloud Storage: Storage classes and data transfer

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531585

There are four primary storage classes in Cloud Storage. The first is Standard Storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline Storage. This is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline Storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline Storage, Coldline Storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive Storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes has differences, it’s worth noting there are several characteristics that apply across all of these storage classes. These include: Unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience, which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data. Cloud Storage has no minimum fee because you pay only for what you use, and prior provisioning of capacity isn’t necessary. And from a security perspective, Cloud Storage always encrypts data on the server side, before it’s written to disk, at no additional charge. Data traveling between a customer’s device and Google is encrypted by default using HTTPS/TLS, which is Transport Layer Security. Regardless of which storage class you choose, there are several ways to bring data into Cloud Storage. Many customers simply carry out their own online transfer using gcloud storage, which is the Cloud Storage command from the Cloud SDK. Data can also be moved in by using a drag and drop option in the Cloud Console, if accessed through the Google Chrome web browser. But what if you have to upload terabytes or even petabytes of data? Storage Transfer Service enables you to import large amounts of online data into Cloud Storage quickly and cost-effectively. The Storage Transfer Service lets you schedule and manage batch transfers to Cloud Storage from another cloud provider, from a different Cloud Storage region, or from an HTTP(S) endpoint. And then there is the Transfer Appliance, which is a rackable, high-capacity storage server that you lease from Google Cloud. You connect it to your network, load it with data, and then ship it to an upload facility where the data is uploaded to Cloud Storage. You can transfer up to a petabyte of data on a single appliance. Cloud Storage’s tight integration with other Google Cloud products and services means that there are many additional ways to move data into the service. For example, you can import and export tables to and from both BigQuery and Cloud SQL. You can also store App Engine logs, Firestore backups, and objects used by App Engine applications, like images. Cloud Storage can also store instance startup scripts, Compute Engine images, and objects used by Compute Engine applications.

#### Cloud SQL

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531586

Google Cloud’s second core storage option is Cloud SQL. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It’s designed to hand off mundane, but necessary and often time-consuming, tasks to Google—like applying patches and updates managing backups, and configuring replications—so your focus can be on building great applications. Cloud SQL doesn't require any software installation or maintenance. It can scale up to 128 processor cores, 864 GB of RAM, and 64 TB of storage. It supports automatic replication scenarios, such as from a Cloud SQL primary instance, an external primary instance, and external MySQL instances. Cloud SQL supports managed backups, so backed-up data is securely stored and accessible if a restore is required. The cost of an instance covers seven backups. Cloud SQL encrypts customer data when on Google’s internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. A benefit of Cloud SQL instances is that they are accessible by other Google Cloud services, and even external services. Cloud SQL can be used with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. Compute Engine instances can be authorized to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might use, like SQL Workbench, Toad, and other external applications using standard MySQL drivers.

#### Spanner

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531587

The third core storage option offered by Google Cloud is Spanner. Spanner is a fully managed relational database service that scales horizontally, is strongly consistent, and speaks SQL. Battle tested by Google’s own mission-critical applications and services, Spanner is the service that powers Google’s $80 billion business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, strong global consistency, and high numbers of input and output operations per second. We’re talking tens of thousands of reads and writes per second or more.

#### Firestore

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531588

Google Cloud’s fourth core storage option is Firestore. Firestore is a flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development. With Firestore, data is stored in documents and then organized into collections. Documents can contain complex nested objects in addition to subcollections. Each document contains a set of key-value pairs. For example, a document to represent a user has the keys for the firstname and lastname with the associated values. Firestore’s NoSQL queries can then be used to retrieve individual, specific documents or to retrieve all the documents in a collection that match your query parameters. Queries can include multiple, chained filters and combine filtering and sorting options. They're also indexed by default, so query performance is proportional to the size of the result set, not the dataset. Firestore uses data synchronization to update data on any connected device. However, it's also designed to make simple, one-time fetch queries efficiently. It caches data that an app is actively using, so the app can write, read, listen to, and query data even if the device is offline. When the device comes back online, Firestore synchronizes any local changes back to Firestore. Firestore leverages Google Cloud’s powerful infrastructure: automatic multi-region data replication, strong consistency guarantees, atomic batch operations, and real transaction support.

#### Bigtable

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531589

The last of Google Cloud’s core storage options we’re going to explore is Bigtable. Bigtable is Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding which storage option is best, customers often choose Bigtable if: They’re working with more than 1TB of semi-structured or structured data. Data is fast with high throughput, or it’s rapidly changing. They’re working with NoSQL data. This usually means transactions where strong relational semantics are not required. Data is a time-series or has natural semantic ordering. They’re working with big data, running asynchronous batch or synchronous real-time processing on the data. Or they’re running machine learning algorithms on the data. Bigtable can interact with other Google Cloud services and third-party clients. Using APIs, data can be read from and written to Bigtable through a data service layer like Managed VMs, the HBase REST Server, or a Java Server using the HBase client. Typically this is used to serve data to applications, dashboards, and data services. Data can also be streamed in through a variety of popular stream processing frameworks like Dataflow Streaming, Spark Streaming, and Storm. And if streaming is not an option, data can also be read from and written to Bigtable through batch processes like Hadoop MapReduce, Dataflow, or Spark. Often, summarized or newly calculated data is written back to Bigtable or to a downstream database.

#### Comparing storage options

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531590

Now that we’ve covered Google Cloud’s core storage options, let’s do a comparison to help highlight the most suitable service for a specific application or workflow. Consider using Cloud Storage if you need to store immutable blobs larger than 10 megabytes, such as large images or movies. This storage service provides petabytes of capacity with a maximum unit size of 5 terabytes per object. Consider using Cloud SQL or Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides up to 64 terabytes, depending on machine type, and Spanner provides petabytes. Cloud SQL is best for web frameworks and existing applications, like storing user credentials and customer orders. If Cloud SQL doesn’t fit your requirements because you need horizontal scalability, not just through read replicas, consider using Spanner. Consider Firestore if you need massive scaling and predictability together with real time query results and offline query support. This storage service provides terabytes of capacity with a maximum unit size of 1 megabyte per entity. Firestore is best for storing, syncing, and querying data for mobile and web apps. Finally, consider using Bigtable if you need to store a large number of structured objects. Bigtable doesn’t support SQL queries, nor does it support multi-row transactions. This storage service provides petabytes of capacity with a maximum unit size of 10 megabytes per cell and 100 megabytes per row. Bigtable is best for analytical data with heavy read and write events, like AdTech, financial, or IoT data. Depending on your application, it’s possible that you might use one, or several, of these services to do the job. You may have noticed that BigQuery hasn’t been mentioned in this section of the course. This is because it sits on the edge between data storage and data processing, and is covered in more depth in other courses. The usual reason to store data in BigQuery is so you can use its big data analysis and interactive querying capabilities, but it’s not purely a data storage product.

#### Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL

- https://www.cloudskillsboost.google/paths/8/course_templates/60/labs/531591

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531592

### Containers in the Cloud

#### Introduction to containers

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531593

In this section of the course we’ll explore containers and help you understand how they are used. Infrastructure as a service, or IaaS, allows you to share compute resources with other developers by using virtual machines to virtualize the hardware. This lets each developer deploy their own operating system (OS), access the hardware, and build their applications in a self-contained environment with access to RAM, file systems, networking interfaces, etc. This is where containers come in. The idea of a container is to give the independent scalability of workloads in PaaS and an abstraction layer of the OS and hardware in IaaS. A configurable system lets you install your favorite runtime, web server, database, or middleware, configure the underlying system resources, such as disk space, disk I/O, or networking, and build as you like. But flexibility comes with a cost. The smallest unit of compute is an app with its VM. The guest OS might be large, even gigabytes in size, and take minutes to boot. As demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly. A container is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware. It only requires a few system calls to create and it starts as quickly as a process. All that’s needed on each host is an OS kernel that supports containers and a container runtime. In essence, the OS is being virtualized. It scales like PaaS but gives you nearly the same flexibility as IaaS. This makes code ultra portable, and the OS and hardware can be treated as a black box. So you can go from development, to staging, to production, or from your laptop to the cloud, without changing or rebuilding anything. As an example, let’s say you want to scale a web server. With a container, you can do this in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host. That's just a simple example of scaling one container running the whole application on a single host. However, you'll probably want to build your applications using lots of containers, each performing their own function like microservices. If you build them this way and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts. The hosts can scale up and down and start and stop containers as demand for your app changes or as hosts fail.

#### Kubernetes

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531594

A product that helps manage and scale containerized applications is Kubernetes. So to save time and effort when scaling applications and workloads, Kubernetes can be bootstrapped using Google Kubernetes Engine (GKE). So, what is Kubernetes? Kubernetes is an open-source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud which is a virtual machine running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Deploying containers on nodes by using a wrapper around one or more containers is what defines a Pod. A Pod is the smallest unit in Kubernetes that you can create or deploy. It represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per Pod, but if you have multiple containers with a hard dependency, you can package them into a single Pod and share networking and storage resources between them. The Pod provides a unique network IP and set of ports for your containers and configurable options that govern how your containers should run. One way to run a container in a Pod in Kubernetes is to use the kubectl run command, which starts a Deployment with a container running inside a Pod. A Deployment represents a group of replicas of the same Pod and keeps your Pods running even when the nodes they run on fail. A Deployment could represent a component of an application or even an entire app. To see a list of the running Pods in your project, run the command: $ kubectl get pods. Kubernetes creates a Service with a fixed IP address for your Pods, and a controller says "I need to attach an external load balancer with a public IP address to that Service so others outside the cluster can access it." In GKE, the load balancer is created as a network load balancer. Any client that reaches that IP address will be routed to a Pod behind the Service. A Service is an abstraction which defines a logical set of Pods and a policy by which to access them. As Deployments create and destroy Pods, Pods will be assigned their own IP addresses, but those addresses don't remain stable over time. A Service group is a set of Pods and provides a stable endpoint (or fixed IP address) for them. For example, if you create two sets of Pods called frontend and backend and put them behind their own Services, the backend Pods might change, but frontend Pods are not aware of this. They simply refer to the backend Service. To scale a Deployment, run the kubectl scale command. In this example, three Pods are created in your Deployment, and they're placed behind the Service and share one fixed IP address. You could also use autoscaling with other kinds of parameters. For example, you can specify that the number of Pods should increase when CPU utilization reaches a certain limit. So far, we’ve seen how to run imperative commands like expose and scale. This works well to learn and test Kubernetes step-by-step. But the real strength of Kubernetes comes when you work in a declarative way. Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like, and Kubernetes determines how to do it. You accomplish this by using a Deployment config file. You can check your Deployment to make sure the proper number of replicas is running by using either kubectl get deployments or kubectl describe deployments. To run five replicas instead of three, all you do is update the Deployment config file and run the kubectl apply command to use the updated config file. You can still reach your endpoint as before by using kubectl get services to get the external IP of the Service and reach the public IP address from a client. The last question is, what happens when you want to update a new version of your app? Well, you want to update your container to get new code in front of users, but rolling out all those changes at one time would be risky. So in this case, you would use kubectl rollout or change your deployment configuration file and then apply the change using kubectl apply. New Pods will then be created according to your new update strategy. Here’s an example configuration that will create new version Pods individually and wait for a new Pod to be available before destroying one of the old Pods.

#### Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531595

So now that we have a basic understanding of containers and Kubernetes, let’s talk about Google Kubernetes Engine, or GKE. GKE is a Google-hosted managed Kubernetes service in the cloud. The GKE environment consists of multiple machines, specifically Compute Engine instances, grouped together to form a cluster. You can create a Kubernetes cluster with Kubernetes Engine, but how is GKE different from Kubernetes? From the user’s perspective, it’s a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes responsibility for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need of a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes. Let’s examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. Autopilot also helps produce a strong security posture. And Autopilot also promotes operational efficiency. The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode. You can create a Kubernetes cluster with Kubernetes Engine by using the Google Cloud console or the gcloud command that's provided by the Cloud software development kit. GKE clusters can be customized, and they support different machine types, number of nodes, and network settings. Kubernetes provides the mechanisms through which you interact with your cluster. Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks, set policies, and monitor the health of deployed workloads. Running a GKE cluster comes with the benefit of advanced cluster management features that Google Cloud provides. These include: Google Cloud's load-balancing for Compute Engine instances, Node pools to designate subsets of nodes within a cluster for additional flexibility, Automatic scaling of your cluster's node instance count, Automatic upgrades for your cluster's node software, Node auto-repair to maintain node health and availability, And logging and monitoring with Google Cloud Observability for visibility into your cluster. To start up Kubernetes on a cluster in GKE, all you do is run this command: $> gcloud container clusters create k1

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531596

### Applications in the Cloud

#### Cloud Run

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531597

So far in this course, we’ve provided an introduction to Google Cloud and explored the options and benefits related to using virtual machines, networks, storage, and containers in the Cloud. In the final section of the course, we’ll turn our attention to developing applications in the Cloud. We’ll begin with Cloud Run, which is a managed compute platform that runs stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It’s built on Knative, an open API and runtime environment built on Kubernetes. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you‘ll never pay for over-provisioned resources. The Cloud Run developer workflow is a straightforward three-step process. First, you write your application using your favorite programming language. This application should start a server that listens for web requests. Second, you build and package your application into a container image. And third, the container image is pushed to Artifact Registry, where Cloud Run will deploy it. Once you’ve deployed your container image, you’ll get a unique HTTPS URL back. Cloud Run then starts your container on demand to handle requests, and ensures that all incoming requests are handled by dynamically adding and removing containers. Because Cloud Run is serverless, it means that you, as a developer, can focus on building your application and not on building and maintaining the infrastructure that powers it. For some use cases, a container-based workflow is great, because it gives you a great amount of transparency and flexibility. Sometimes, you’re just looking for a way to turn source code into an HTTPS endpoint, and you want your vendor to make sure your container image is secure, well-configured and built in a consistent way. With Cloud Run, you can do both. You can use a container-based workflow, as well as a source-based workflow. The source-based approach will deploy source code instead of a container image. Cloud Run then builds the source and packages the application into a container image. Cloud Run does this using Buildpacks - an open source project. Cloud Run handles HTTPS serving for you. That means you only have to worry about handling web requests, and you can let Cloud Run take care of adding the encryption. The pricing model on Cloud Run is unique; as you only pay for the system resources you use while a container is handling web requests, with a granularity of 100ms, and when it’s starting or shutting down. You don’t pay for anything if your container doesn’t handle requests. Additionally, there is a small fee for every one million requests you serve. The price of container time increases with CPU and memory. A container with more vCPU and memory is more expensive. You can use Cloud Run to run any binary, as long as it’s compiled for Linux sixty-four bit. Now, this means you can use Cloud Run to run web applications written using popular languages, such as: Java, Python, Node.js, PHP, Go, and C++. You can also run code written in less popular languages, such as: Cobol, Haskell, and Perl. As long as your app handles web requests, you’re good to go.

#### Development in the cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531598

Many applications contain event-driven parts. For example, an application that lets users upload images. When that event takes place, the image might need to be processed in a few different ways, like converting the image to a standard format, converting a thumbnail into different sizes, and storing each new file in a repository. This function could be integrated into the application, but then you’d have to provide compute resources for it–whether it happens once a millisecond or once a day. With Cloud Run functions, you write a single-purpose function that completes the necessary image manipulations and then arrange for it to automatically run whenever a new image is uploaded. Cloud Run functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events, without the need to manage a server or a runtime environment. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also connect and extend cloud services. You’re billed to the nearest 100 milliseconds, but only while your code is running. Cloud Run functions supports writing source code in a number of programming languages. These include Node.js, Python, Go, Java, . Net Core, Ruby, and PHP. For more information about the supported specific version, refer to the runtimes documentation. Events from Cloud Storage and Pub/Sub can trigger Cloud Run functions asynchronously, or you can use HTTP invocation for synchronous execution.

#### Hello Cloud Run

- https://www.cloudskillsboost.google/paths/8/course_templates/60/labs/531599

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531600

### Prompt Engineering

#### Prompt Engineering

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531601

Generative AI and large language models are proving to be powerful tools, but to leverage their capabilities, it's important to understand their architecture. It's also important to consider recommended practices when implementing these technologies. The goal of this module, titled Google Cloud: Prompt Engineer Guide, is to help with these important steps. In this guide to prompt engineering, you’ll get answers to the questions: What is generative AI? What is a large language model? What is prompt engineering? You’ll also explore prompt engineering best practices. Before we delve into this lesson, let's define the interchangeably used terms such as 'generative AI' and 'Large Language Model' (LLM). While both terms describe AI models capable of generating human-like responses based on input prompts in many references, it's important to note they're not identical. Generative AI encompasses a broader range of models capable of generating various types of content beyond just text, while LLM specifically refers to a subset of generative AI models focusing on language tasks. We'll thoroughly explore each term in this lesson. So let's begin with an important question: What is generative AI? Generative artificial intelligence, which is commonly referred to as gen AI, is a subset of artificial intelligence that is capable of creating text, images, or other data using generative models, often in response to prompts. It has grown in popularity hugely since 2021 but artificial intelligence has been around since the mid 1950s. By the way, a prompt is a specific instruction, question, or cue given to a computer program or user to initiate a specific action or response, but we examine this more later. In its current format, gen AI models are like conversational programs that can generate content based on the inputs supplied. Gen AI models learn the patterns and structure from input training data and then create new data with similar characteristics. Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, and sales. However, rather than exploring all generative AI applications, this training will specifically focus on articulating prompts to harness the power of gen AI effectively. Let me introduce a scenario that we’ll make reference to throughout this section of the training. We’ve included it to help put some of this theory into practice. Meet Sasha, a cloud architect, who needs to create a prototype design of Google Cloud VPC network architecture for Cymbal Bank. Sasha wants to save time by combining her existing knowledge of cloud architecture and generative AI tools to create a usable prototype design. Sasha was excited to learn about Gemini, since having the tool inside the Google Cloud Console means that she can access it without any additional installs. We’ll check back in with Sasha later. Let’s spend some time exploring large language models, which are a highly sophisticated computer programs trained on gigantic amounts of data that can be text or images. But how are they trained? And why do they need training at all? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. In this context, large refers to: The size of the training dataset, which can sometimes be at the petabyte scale. And the number of parameters. Parameters are the memories and knowledge that the machine has learned during model training. They determine the ability of a model to solve a problem, such as predicting text, and can reach billions or even trillions in size. General-purpose means that the models can sufficiently solve common problems. This is thanks to the commonality of a human language, regardless of the specific tasks. Saying LLMs are pre-trained and fine-tuned, means… …that they have been pre-trained for a general purpose with a large dataset… ...and then fine-tuned for specific goals with a much smaller dataset. But how are LLMs trained? When you submit a prompt to an LLM, it calculates the probability of the correct answer from its pre-trained model. The probability is determined through a task called pre-training. Pre-training an LLM involves feeding a massive dataset of text, images, and code to the model so that it can learn the underlying structure and patterns of the language. This process helps the model to understand and generate human language more effectively. In this way, the LLM works like a fancy autocomplete, suggesting the most common correct response to the prompt. But sometimes the LLM gives a completely wrong answer. This is called a hallucination. Hallucinations are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. This happens because LLMs can only understand the information they were trained on. This means that they might not be aware of your business's proprietary or domain-specific data. Also, they do not have access to real-time information. To make matters worse, LLMs only understand the information that is explicitly given to them in the prompt. In other words, they often assume that the prompt is true. They also do not have the ability to ask for more context information. Ultimately, an LLM does not know anything outside of what it was trained on, and it cannot truly know if that information is accurate. But what causes a hallucination. Hallucinations can be caused by a number of factors, including: The model is not trained on enough data. The model is trained on noisy or dirty data. The model is not given enough context. The model is not given enough constraints. Hallucinations can be a problem for LLMs because they can make the output text difficult to understand. They can also make the model more likely to generate incorrect or misleading information. But we will see in the Prompt Engineering section that there are things we can do to minimize this problem. OK, but knowing where the sun is will not help Sasha with her current task. Lucky for Sasha, Google Cloud offers a generative AI model called Gemini, [[Pause here for 5 seconds]] which can act as an always-on collaborator. This gen AI-powered assistant can help a wide range of Google Cloud users, including developers, data scientists, and operators. To provide an integrated assistance experience, Gemini is embedded in many Google Cloud products. Gemini has access to a massive range of data, including Google Cloud documentation, tutorials, and samples. With the right prompts, it can produce detailed suggestions and guides on what resources will best suit Sasha’s current challenge and their configuration. Gemini can even create detailed gcloud commands and insert them into Cloud Shell for her. She just needs to articulate her needs in a way that gets the best response from Gemini. For example, if she uses the prompt “How can I create a network that uses IPv4 and IPv6 addresses?”, she will get a response that details how to do just that. You’ve learned that a large language model is a huge object model containing a massive dataset of text. But how can you extract the information you need from this dataset? This is where prompt engineering comes in. A prompt is the text that you feed to the model, and prompt engineering is a way of articulating your prompts to get the best response from the model. The better structured a prompt is, the better the output from the model will be. Let’s explore what this means. Prompts can be in the form of a question, and are categorized into four categories: zero-shot, one-shot, few-shot, and role prompts. Zero-shot prompts do not contain any context or examples to assist the model. For example, the prompt “What’s the capital of France?” does not provide any examples of what a capital is. Clearly, that is not too important for this example. But for more specific and technical prompts, an example would help refine the scope of the response from Gemini. One-shot prompts, however, provide one example to the model for context. Here, we ask for the capital of France again, but we provide Italy and Rome as an example. And few-shot prompts provide at least two examples to the model for context. Here, our prompt is updated to also include Japan and Tokyo in our examples. And then, there are role prompts which require a frame of reference for the model to work from as it answers the questions. In our example, we state “I want you to act as a business professor. I’ll give you a term, and you will correctly explain its meaning. Make sure your answers are always right. What is ROI? “ For Sasha’s needs, using role prompts might be the best solution. She can define what is required and in what context. This means that the LLM will have a clear point of reference when supplying an answer. Now that you’ve seen the types of prompts you can create, let’s explore the two elements of a prompt: the preamble and the input. The preamble refers to the introductory text you provide to give the model context and instructions before your main question or request. Think of it as setting the stage for the LLM to better understand what you want. It can include the context for the task, the task itself, and some examples to guide the model. The input is the central request you're making to the LLM. It’s what the instruction or task will act upon, for example “Comment: I don’t know what to think about the video. The review is:” Based on the preamble, Gemini reviews the input and suggests if the review is positive, neutral, or negative. It is worth noting that not all the components are required for a prompt, and the format can change depending on the task at hand. The element order can also change. Let's amend Sasha’s original prompt “How can I create a network that uses IPv4 and IPv6 addresses?” and add a role context to the input fed into Gemini. She also adds the detail of needing a dual stack subnet. The new prompt is “I want you to act as a cloud architect in Google Cloud. How can I use gcloud to create a network that uses IPv4 and IPv6 subnets?” But since Gemini maintains its own interaction context, she could have just asked “I want you to act as a cloud architect in Google Cloud. How can I adjust the gcloud command provided to create a subnet to ensure the subnet is dual stack?” Now that you’ve had a chance to explore what Gen AI is, what large language models are and how they’re trained, and what prompt engineering is, it’s time to explore some prompt engineering best practices. The first best practice is to write detailed and explicit instructions. The more vague the prompt, the more chance that the model will produce a result that is not usable. Be clear and concise in the prompts that you feed into the model. Next, be sure to define boundaries for the prompt. It’s better to instruct the model on what to do rather than what not to do. If the model gets stuck, give it a few 'fallback' outputs that work in various situations. For example, something like "I'm still learning about that" to use when unsure. Another best practice is to adopt a persona for your input. Adding a persona for the model can provide meaningful context to help it focus on related questions, which can help improve accuracy. This prompt would help Sasha, the cloud architect, get started with prototyping a network architecture for Cymbal Bank. And finally, it’s a recommended practice to keep each sentence concise. Longer sentences can sometimes produce suboptimal results. It’s best to break long sentences in a prompt into a series of shorter sentences and simpler tasks. So, let’s return to Sasha, and use what we have learned so far. Sasha updates her prompt to: “You're a cloud architect. You want to build a Google Cloud VPC network that can be centrally managed. You also connect to other VPC networks in your company's other regions. You don't want to have many different sets of firewall policies to maintain. What sort of network architecture would you recommend?” With this new prompt, Gemini proposes a hub-and-spoke network architecture, which fits Sasha’s needs exactly. By refining and amending her prompts, Sasha has articulated her requirements in a way that Gemini can respond with the correct focus and level of detail.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/60/quizzes/531602

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/8/course_templates/60/video/531603

Congratulations on completing the Google Cloud Fundamentals: Core Infrastructure training course. Before you go, let’s take a few minutes to review what we’ve covered. In module 1, you were introduced to Google Cloud and cloud computing. Specifically, you explored: The concept of managed infrastructure and managed services, through IaaS, infrastructure as a service, and PaaS, platform as a service. The Google Cloud network. Google Cloud’s focus on security throughout our infrastructure. How Google publishes key elements of technology using open source licenses. And Google Cloud’s pricing structure and billing tools. In module 2, you learned about the Google Cloud Resource Hierarchy, which is made up of four levels: resources, projects, folders, and an organization node. You also learned about: Defining policies and their downward inheritance. When to use Identity and Access Management, or IAM, And the four ways to access and interact with Google Cloud: through the Google Cloud console, the Cloud SDK and Cloud Shell, APIs, and the Google Cloud App. In module 3, you explored how Compute Engine works, with a focus on virtual machines and virtual networking. You were introduced to: The VPC, or virtual private cloud. Compute Engine’s Autoscaling feature. And important Google Virtual Private Cloud compatibility features, like routing tables, firewalls, VPC peering, and shared VPC, all of which result in the need for less network management. You also explored Cloud Load Balancing, a fully distributed, software-defined, managed service for all your traffic. Finally, you compared how on-premises or other-cloud networks can be interconnected with a Google VPC. In module 4, you explored Google Cloud's five core storage options: Cloud Storage, Bigtable, Cloud SQL, Spanner, and Firestore. You also examined the four storage classes that make up Cloud Storage: Standard Storage, which is used for frequently accessed hot data, Nearline Storage and Coldline Storage, which are used for less-frequently accessed cool data, and Archive Storage. In module 5, you learned about containers, which are invisible boxes around your code and its dependencies. You were introduced to containers, along with: Kubernetes, an open-source platform for managing containerized workloads and services. And Google Kubernetes Engine (GKE), a Google-hosted managed Kubernetes service in the cloud. In module 6, the focus was on developing applications in the cloud. You explored: Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. And Cloud Run functions, a lightweight, event-based, asynchronous compute solution to create single-purpose functions. Finally, in module 7, you explored how to combine Google Cloud knowledge with prompt engineering to improve Gemini responses. You discovered answers to the following questions: What is generative AI? What is a large language model? And what is prompt engineering? You ended the module by identifying prompt engineering best practices. We hope that this course is just the beginning of your Google Cloud journey. For more training and hands-on practice, explore the different learning paths available at cloud.google.com/training. And if you’re interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you might consider working toward a Google Cloud certification. You can learn more about Google Cloud’s certification offerings at cloud.google.com/certification. Thanks for completing this course. We’ll see you next time!

#### Course resources

- https://www.cloudskillsboost.google/paths/8/course_templates/60/documents/531604

### Your Next Steps

## 04: Set Up an App Dev Environment on Google Cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/637

### Perform Foundational Infrastructure Tasks in Google Cloud

#### Cloud Storage: Qwik Start - Cloud Console

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526668

#### Cloud Storage: Qwik Start - CLI/SDK 

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526669

#### Cloud IAM: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526670

#### Cloud Monitoring: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526671

#### Cloud Run Functions: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526672

#### Cloud Run Functions: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526673

#### Pub/Sub: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526674

#### Pub/Sub: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526675

#### Pub/Sub: Qwik Start - Python

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526676

#### Set Up an App Dev Environment on Google Cloud: Challenge Lab

- https://www.cloudskillsboost.google/paths/8/course_templates/637/labs/526677

### Your Next Steps

## 05: Introduction to AI and Machine Learning on Google Cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/593

### Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565154

[MUSIC] Artificial intelligence, AI, and machine learning, ML are rapidly evolving, especially in the field of generative AI. Generative AI allows machines to produce content based on human input, which opens a wide range of possibilities that were not available even a few months ago. If you are a developer, you might want to incorporate new AI capabilities in your applications or use them to improve productivity. If you are a data scientist, you might want to train the machine learning model with your own data to solve a business problem. If you are a machine learning engineer, you might want to build an ML pipeline and deploy it to production. Even if you are not an AI professional, you might be curious about cutting edge advancements such as generative AI and how they can spark new business ideas. If you are any of these people, we have a course for you. Introduction to AI and machine learning on Google Cloud. I'm Doctor Yoanna Long, an AI and machine learning educator at Google Cloud. I was a college professor for 15 years and I'm passionate about making complex concepts simple so everyone can understand and use AI. I'm here to help you embark on the learning journey and succeed in this course, so what is this course about? This course presents a toolbox which is full of AI technologies and tools offered by Google. These technologies and tools are organized into layers to make navigation easier. You begin with the AI foundation layer where you learn about cloud essentials like compute, storage, and network and data tools such as data pipelines and data analytics. These tools help you start your journey from data to AI. You then move on to the AI development layer where you explore different options to build machine learning project, including out of the box solutions, low code or no code, and DIY, do it yourself. You also walk through the workflow to train and serve a machine learning model using Vertex AI, the AI development platform provided by Google Cloud. Finally, you are introduced to generate AI and you learn how generative AI empowers the AI development and AI solutions layer. After completing this course, you will be able to recognize the data to AI technologies and tools offered by Google Cloud. Leverage generative AI capabilities in applications, choose between different options to develop an AI project on Google Cloud, and build machine learning models end-to-end using Vertex AI. How can you succeed in this course? Here are some suggestions, write down three keywords after each lesson, lab and module. These keywords can be points you learned, use cases to apply, and new business ideas. Look at the list and see how the terms relate to each other. You can then draw lines to connect the terms or group them together in a way that makes sense to you. This practice will strengthen your understanding of the concepts. Apply what you learned to your own work. This is the best way to develop your skills as an AI practitioner. The evolution and the capabilities of AI are fascinating. As you learn more about this technology, you are better prepared to meet the challenges of today and tomorrow, let's get started.

### AI Foundations

#### Introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565155

Welcome to the first module of this course, AI Foundations. The first question you may have is quickly, how does AI help you innovate business processes and improve business efficiency? Lesson 1 answers this question by showing you a use case powered by recent AI technologies like generative AI. You may also wonder why Google and how to start an AI project on Google Cloud. Lesson 2 illustrates an AI ML framework and helps you navigate through the whole course. Next, you explore Google Cloud's infrastructure, focusing on compute and storage. You then examine the products that support your journey from data to AI on Google Cloud. After that, you advance to more AI and ML content, starting with ML model categories, which provides context to understand ML model building. You then explore Big Query and specifically Big Query ML and walk through the steps to build an ML model with SQL commands. Finally, you complete a hands on lab to build your first ML model on Google Cloud with Big Query ML. Let's get started.

#### Why AI

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565156

Why AI and why Google? These might be your first questions. Let's explore an example to understand how AI can enhance business efficiency and transform operations. Coffee on Wheels, an international company that sells coffee on trucks in cities like London, New York, San Francisco, and Tokyo, provides a compelling case study. Coffee on Wheels is facing three main challenges: Coffee on Wheels is facing three main challenges: Location selection and route optimization Predicting popular locations for truck placement, and optimizing routes based on weather and traffic conditions. Sales forecast and real-time monitoring Forecasting sales and monitoring performance in real-time. Marketing campaign automation Automating marketing campaigns to increase efficiency and effectiveness. Recognizing the potential of AI, Coffee on Wheels sought assistance from Data Beans, a digital native company, to leverage data and AI technologies to resolve their business challenges. Let's take a tour of the demo. Choose one of the four current locations, such as London. The dashboard displays overall statistics across cities, including revenue, operating margin, and the number of trucks. This information is generated by data tools like BigQuery and Looker, as well as AI tools and models like Gemini and Vertex AI. On the right, you can view the final data for London with a summary. It shows how London's revenue compares to the average, and provides insights into revenue per truck and customer loyalty. In the top left corner, the dashboard displays the weather and generates route suggestions based on weather conditions. For example, if lower temperatures are forecasted, it might suggest a new itinerary that focuses on covered areas. You can click "show updated route" and "publish route" to implement these changes. By clicking on a specific time on the timeline, you can see route suggestions based on city events. For example, if there's a football game happening, it might suggest rerouting trucks to avoid congestion. Clicking on the truck sign provides a detailed dashboard with information such as street view and revenue forecast. To monitor the performance of the business in real-time, you can access a dashboard by clicking "show menu." If an item is underperforming, you can click the "generate" button to get suggestions for a new item. Additionally, you have the option to generate marketing campaigns by selecting "yes" to "save the suggestion." This feature enables you to automatically create campaigns that include both text and images. You can further streamline your marketing efforts by sending campaign emails to targeted customers with just a click of the "post" button. Finally, you can generate an operational report and export the insights to any format, such as Google Slides. To customize the application using code, click "how it's made." This reveals the tools and technologies, including BigQuery, Gemini, and Vertex AI, that were used to create the app. You can also click "open in notebook" to access the sample code in the code development environment. Isn’t it remarkable? The process is actually straightforward: Multimodal input: this involves incorporating various forms of data, such as text (customer reviews), images (coffee and dessert pictures), and videos (real-time street view). 2. Prediction and generation: this is powered by data analytics like customer segment analysis, predictive AI like sales forecasting, and generative AI like marketing campaign automation. 3. Visual output: the insights and reports are then presented visually, empowering businesses to make real-time data-driven decisions and optimize their operations. Behind the scenes, many Google products collaborate to make this application possible. For example, Gemini multimodal enables data acquisition, BigQuery provides data analytics, Vertex AI handles ML development, and Looker and Google APIs contribute to data visualization and app creation. The course will explore these tools in more depth later, giving you the chance to learn about them in detail. This application's development encompasses the entire data to AI lifecycle. It includes data ingestion, data analytics, data engineering, model training, testing, and deployment. These processes are supported by Google's unified development platforms. Additionally, Google's AI development platform enables the utilization of various types of AI, including predictive AI for tasks like sales forecasting, generative AI for tasks like automating marketing campaigns, and hybrid approaches that combine both. By leveraging the application, Coffee on Wheels gained the following benefits: Streamlined business processes in key areas such as marketing, digital commerce, and back-office operations. Modernized customer service through features such as automated comment replies and actionable consumer insights and predictions. Enhanced employee productivity through the utilization of GenAI for code assistance and marketing content generation.

#### AI/ML architecture on Google Cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565157

You might be excited about the capabilities and potentials of AI, but how to start your AI projects with Google? You explore the AI and ML toolbox in this lesson. So, why should you trust Google for AI? Firstly, Google has been an AI-first company and used AI to power its products since the beginning. Second, Google is a leader in AI and ML innovations. Thirdly, Google is a believer in responsible AI. Let's take a brief look at how Google began innovating with data and AI in its products. Data is the basis of AI. Historically speaking, Google experienced challenges related to data processing quite early. As a search engine, Google needed to constantly invent new data processing methods to index the World Wide Web and keep up with the rapidly growing internet. The innovation started in the early 2000s from Google File System, or GFS, which is the foundation for Cloud Storage, and MapReduce, which aims to manage large-scale data processing. To address the challenges of different types of data, whether structured or unstructured, and different processing requirements, either streaming or batch, Google has continued to invent multiple products and technologies in data analytics and engineering. For example, BigQuery, the data warehouse widely used on Google Cloud, can analyze large amounts of data and build ML models at the same time with SQL, Structured Query Language. In 2015, Pub/Sub was invented to help build data pipelines for streaming analytics. In the field of AI, Google has also contributed with many key technologies. For example, the widely-used ML library in Python for general purposes scikit-learn was a Google summer coding project back in 2007. TensorFlow, an open-source ML platform for training deep learning neural networks was developed by Google in 2015. And it has evolved in multiple versions. The transformer, the basis of all generative AI (or gen AI) applications seen today, was invented by Google in 2017. From this, large language models evolved rapidly, including Bidirectional Encoder Representations from Transformers (BERT) in 2018, Language Model for Dialogue Applications (LaMDA) in 2021, Pathways Language Model (PaLM) in 2022, Gemini, the most recent cutting-edge genAI model to process multimodal data released at the end of 2023, and Gemma, an open model released at the beginning of 2024. In terms of AI products, Google built an end-to-end AI development platform that evolves from AutoML in 2018, to Al Platform in 2019, and Vertex AI in 2021. In 2023, Google also announced a series of generative AI products on Vertex AI such as Vertex AI Studio, Model Garden, and Search and Conversation. You’ll explore all of these AI technologies and products in depth later in this course. Google is a leader in AI and ML innovations. Here are four major reasons: The ML development platform is empowered by Google's state-of-the-art ML models. So you build on excellence. Google provides an end-to-end development platform to convert an ML model from experiment to production so you can be more efficient. Google provides a unified data-to-AI platform so you can develop data and AI projects with ease. Like Google Cloud itself, AI services are based on an efficient and scalable infrastructure so you can get more for less. AI comes with its own set of unique challenges. Google integrates responsible AI into its AI principles. Responsible AI refers to the development and use of artificial intelligence systems in a way that prioritizes ethical considerations, fairness, accountability, safety, and transparency. We have three AI principles that guide our work. These are concrete standards that actively govern our research and product development and affect our business decisions. Here’s an overview of each one. 1. Bold innovation. We develop AI that assists, empowers, and inspires people in almost every field of human endeavor, drives economic progress, and improves lives, enables scientific breakthroughs, and helps address humanity’s biggest challenges. 2. Responsible development and deployment. Because we understand that AI, as a still-emerging transformative technology, poses evolving complexities and risks, we pursue AI responsibly throughout the AI development and deployment lifecycle, from designing to testing to deployment to iteration, learning as AI advances and uses evolve. 3. Collaborative progress, together. We make tools that empower others to harness AI for individual and collective benefit. Establishing principles were a starting point rather than an end. They are a foundation that establishes what Google stands for, what to build, and why to build it, and they are core to the success of Google’s enterprise AI offerings. To learn more about responsible AI and how Google implements it in research and product design, please check the Google doc in the reading list. In sum, Google’s AI principles revolve around being an AI-first company with a rich history of AI development, a leader in AI innovations, and a practitioner of responsible AI ethics. Now you know why you can trust Google for artificial intelligence and machine learning. The next question you may have is : How do I start? Assuming you are a data scientist and you want to develop an ML model from beginning to end, what options do you have? Say you are an ML engineer, and you want to build an ML pipeline to automatically monitor the model performance. What tools do you have? Or perhaps you are an AI developer who would like to leverage generative AI capabilities in an application. What products can you use? You’ll find all the answers in this course. This course presents a toolbox that is based on the AI/ML framework on Google Cloud. The toolbox is organized into three layers to make navigation easier. You begin with the AI foundations layer in this module, module one. Here, you learn about cloud essentials like compute and storage. Also about data and AI products that support your journey from data to AI. You then advance to the AI development layer. In module 2, you focus on the options to develop an ML model from beginning to end on Google Cloud. There, you explore out-of-the-box solutions like pre-built APIs, low or no-code solutions like AutoML, and do-it-yourself approaches like custom training. In module 3, you walk through the workflow to build an ML model using Vertex AI, the end-to-end AI development platform. From data preparation, to model training, and finally model serving. Additionally, you learn how to automate this ML workflow using the Vertex AI Pipelines SDK. Finally, in module 4, you are introduced to generative AI, how it works, the tools to develop generative AI projects, and how generative AI empowers AI solutions. Regardless your role, module one provides you with an overview of the AI/ML framework on Google Cloud the cloud infrastructure, and data and AI products. If you are an AI developer, a data scientist, or an ML engineer who would like to build your own ML models, module two to four will walk you through all the details you need to know. If you are a business user and would like to leverage AI/ML in your applications, AI solutions in module 4 help you navigate through a use case, tools, and technologies.

#### Google Cloud infrastructure

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565158

Let’s explore Google Cloud infrastructure. Google has been working with data and artificial intelligence since its early days as a company in 1998. Ten years later, in 2008, Google Cloud was launched to provide secure and flexible cloud computing and storage services. You can think of the Google Cloud infrastructure in terms of three layers. At the base layer is networking and security, which lays the foundation to support all of Google’s infrastructure and applications. On the next layer sit compute and storage. Google Cloud separates, or decouples, as it’s technically called, compute and storage so they can scale independently based on need. The top layer includes data and AI/machine learning products, which enable you to perform tasks to ingest, store, process, and deliver business insights, data pipelines, and ML models. Thanks to Google Cloud, these tasks can be accomplished without a need to manage and scale the underlying infrastructure. Let’s begin with compute. Organizations with growing data needs often require lots of compute power to run data and AI jobs. And as organizations design for the future, the need for compute power only grows. Google offers a range of computing services. The first is Compute Engine. Compute Engine is an Iaas, or infrastructure as a service offering, which provides compute, storage, and network resources virtually that are similar to a physical machine. You use the virtual compute and storage resources the same as you manage them locally. Compute Engine provides maximum flexibility for those who prefer to manage server instances themselves. The second is Google Kubernetes Engine, or GKE. GKE runs containerized applications in a cloud environment, as opposed to on an individual virtual machine like Compute Engine. A container represents code packaged up with all its dependencies. The third computing service offered by Google is App Engine, a fully managed PaaS, or platform as a service, offering. PaaS offerings bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. Then there is Cloud Run, a fully managed compute platform that enables you to run requests or event-driven stateless workloads without having to worry about servers. It abstracts away all infrastructure management so you can focus on writing code, and it automatically scales up and down from zero, so you never have to worry about scale configuration. Cloud Run charges you only for the resources you use, so you never pay for over-provisioned resources. And finally, there is Cloud Run functions, which executes code in response to events, like when a new file is uploaded to Cloud Storage. It’s a completely serverless execution environment, which means you don’t need to install any software locally to run the code and you are free from provisioning and managing servers. Cloud Run functions is often referred to as Functions as a Service. Where does the processing power come from? It’s from the hardware: from computer chips. However, traditional computer chips, like a CPU, or central processing unit, and even the more recent GPU, or graphics processing unit, may no longer scale to adequately reach the rapid demand for ML. To help overcome this challenge, in 2016, Google introduced the Tensor Processing Unit, or TPU. TPUs are Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. TPUs act as domain-specific hardware, as opposed to general-purpose hardware like CPUs and GPUs. This allows for higher efficiency by tailoring the architecture to meet the computation needs in a domain, such as the matrix multiplication in machine learning. TPUs are generally faster than current GPUs and CPUs for AI and ML applications. They are also significantly more energy-efficient. Cloud TPUs have been integrated across Google products, making this state-of-the-art hardware and supercomputing technology available to Google Cloud customers. Let’s now examine storage. For proper scaling capabilities, compute and storage are decoupled. That is one major difference between cloud and desktop computing. With cloud computing, compute and storage can scale separately. Most applications require a database and storage solution of some kind. Google Cloud offers fully managed database and storage services. These include: Cloud Storage Cloud Bigtable Cloud SQL Cloud Spanner Firestore And BigQuery How do you choose from these products and services? Well, it depends on the data type and business needs. Let’s look at the data type, which includes unstructured versus structured data. Unstructured data is information stored in a non-tabular form such as documents, images, and audio files. Unstructured data is usually suited to Cloud Storage. Cloud Storage has four primary storage classes. The first is standard storage. Standard storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that is stored for only brief periods of time. The second storage class is nearline storage. This is best for storing infrequently accessed data, like reading or modifying data once per month or less, on average. Examples include data backups, long-tail multimedia content, or data archiving. The third storage class is coldline storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to nearline storage, coldline storage is meant for reading or modifying data at most once every 90 days. The fourth storage class is archive storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Alternatively, there is structured data, which represents information stored in tables, rows, and columns. Structured data comes in two types: transactional workloads and analytical workloads. Transactional workloads stem from online transaction processing systems, which are used when fast data inserts and updates are required to build row-based records. This is usually to maintain a system snapshot. They require relatively standardized queries that impact only a few records. Then there are analytical workloads, which stem from online analytical processing systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. Once you’ve determined if the workloads are transactional or analytical, you then need to identify whether the data will be accessed using SQL or not. So, if your data is transactional and you need to access it using SQL, then two options are Cloud SQL and Spanner. Cloud SQL works best for local to regional scalability, while Spanner works best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional, NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery is likely the best option. BigQuery, Google’s data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. It’s best for real-time, high-throughput applications that require only millisecond latency.

#### Data and AI products

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565159

In this lesson, let's investigate the primary data and AI products on Google Cloud. In the last lesson, you examined Google Cloud Infrastructure, specifically compute and storage. The final layer of the Google Cloud infrastructure that is left to explore is Data and AI products. As you explored in the earlier lesson, Google offers a range of Data and AI tools, so how do you know which is best for your business needs? Let's look closer at the list of products, which can be divided into four general categories along the data to AI workflow. You gather data from multiple sources through ingestion and process. The data is then saved in different types of storage based on data type and business needs. You analyze the data and visualize the results, and you can further train an ML model with historical data to either predict future trends or generate new content. These tools are seamlessly connected on Google Cloud, making it easy for data scientists and AI developers to transition from data to AI. For example, Big Query provides embedded AI features that allow you to directly call SQL commands to train an ML model. Additionally, on Vertex AI, the AI development platform, you can use SQL commands in a notebook to import data from Big Query and further train ML models. Let's cover the products available in each stage of the data to AI workflow. The first category is ingestion and process, which includes products that are used to digest both real time and batch data. The list includes Pub Sub, Data Flow, data PC and Cloud data fusion. Please check cloud.google.com/training for more training on data ingestion and process. The second product category is data storage, and you'll recall from earlier that there are six storage products, Cloud storage, which saves unstructured data such as text image, audio and video, Big Query, Cloud SQL, Spanner, Big Table, and Fire Store. Big Query, Cloud SQL, and Spanner focus on SQL databases, while Big Table and Fire Store are no SQL databases. The third product category is analytics. The major analytics tool is big query, which you'll explore more later in this module. Big Query is a fully managed data warehouse that can be used to analyze data through SQL commands. In addition to big query, you have business intelligence, BI tools to analyze data and visualize results. For example, the Looker family, which includes comprehensive BI tools to visualize, analyze, model, and govern business data. The final product category is AI and machine learning, which includes both AI development tools and AI solutions. These products are either integrated with generative AI or embedded with generative AI capabilities. The major product to support AI development is Vertex AI, which is a unified platform and includes multiple tools such as AutoML for predictive AI, workbench, and Colab Enterprise for coding, and Vertex AI Studio and Model Garden for generative AI. AI solutions are built on the ML development platform and include state of the art technologies to meet both horizontal and vertical market needs. These include document AI, Contact Center AI, Vertex AI search for retail and healthcare data engine. These products unlock insights that only large amounts of data can provide. Many of them have been recently embedded with generative AI capabilities. For example, Contact Center AI is equipped with chat bots that are powered with large language models to understand natural language and engage in conversations like a human agent. You'll explore the machine learning options and workflow together with these products in greater detail later.

#### ML model categories

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565160

Before you start building ML models, let's take a look at model categories. First, let's pause to clarify two terms, artificial intelligence and machine learning. You may note that people often use the terms interchangeably, but they do have some difference. Artificial intelligence, or AI, is an umbrella term that includes anything related to computers mimicking human intelligence. Some examples of AI applications include robots and self-driving cars. Machine learning is a subset of artificial intelligence that allows computers to learn without being explicitly programmed. This is in contrast to traditional programming, where the computer is told explicitly what to do. Machine learning mainly includes supervised and unsupervised learning. You might also hear the terms deep learning or deep neural networks. This is a subset of machine learning that adds layers in between input data and output results to make a machine learn at more depth. You'll learn more about neural networks and deep learning later in this course. Lastly is generative AI, which produces content and performs tasks based on requests. Generative AI relies on training extensive models like large language models. These models are a type of deep learning model. So what's the difference between supervised and unsupervised learning? Imagine two types of problems. In problem one, you are asked to classify dogs and cats from a very large set of pictures. You already know the difference between dogs and cats, so you label each picture and pass the labeled pictures to a machine. By learning from the data, in this case, pictures with the answers or labels, supervised learning is being enacted, allowing the machine to tell if a new picture represents a dog or a cat in the future. In problem two, you are asked to classify breeds of dogs. Unfortunately, this time you don't know how many of them and are not able to label the pictures, so you send these unlabeled pictures to a machine. In this case, the machine learns from the data without answers and finds underlying patterns to group the animals, this is an example of unsupervised learning. Put simply, supervised learning deals with labeled data, is task-driven, and identifies a goal. Unsupervised learning, however, deals with unlabeled data, is data-driven, and identifies a pattern. An easy way to distinguish between the two is that supervised learning provides each data point with a label or an answer, while unsupervised does not. There are two major types of supervised learning, the first is classification, which predicts a categorical variable, such as determining whether a picture shows a cat or or a dog. In ML, you use models like a logistic regression model to solve classification problems. The second type of supervised learning is regression, which predicts a numerical variable like forecasting sales for a product based on its past sales. You use ML models like a linear regression model to solve regression problems. There are three major types of unsupervised learning. The first is clustering, which groups together data points with similar characteristics and assigns them to clusters, like using customer demographics to determine customer segmentation. You use ML models like k-means clustering to solve clustering problems. The second type is association which identifies underlying relationships like a correlation between two products to place them closer together in a grocery store for a promotion. You use association rule learning techniques and algorithms like a priori to solve association problems. And the third type of unsupervised learning is dimensionality reduction, which reduces a number of dimensions or features in a data set to improve the efficiency of a model. For example, combining customer characteristics like age, driving violation history, or car type to create a simplified rule for calculating and and insurance quote. You use ML techniques like principal component analysis to solve these problems. All right, time to test your learning. You are asked to predict customer spending based on purchase history. Is this supervised or unsupervised learning? Yes, that's supervised learning because you have the labeled data, the amount the customers have spent, and you want to predict their future purchases. Is this a classification or regression problem? Yes, it's a regression problem because it predicts a continuous number future spending. Which ML model should you use, a logistic regression or a linear regression? Yes, a linear regression. A logistic regression model is for classification problems while a linear regression model is for regression problems. Lets look at another scenario. Imagine you are using the same dataset, however, this time you are asked to identify customer segmentation. You don't want to base your judgment on stereotypes such as age or gender, so you use a computer for help. Is this supervised or unsupervised learning? Yes, it's unsupervised learning because you don't have each customer labeled as belonging to a certain segment, instead, you want the computer to discover the underlying pattern. Is it a clustering association or dimensionality deduction problem? Yes, identifying customer segmentation is a clustering problem. Which ML model should you use? Logistic regression, linear regression, or k-means clustering analysis? Right, it's a clustering analysis scenario. You will find these models within BigQuery ML, auto ML and custom training later on in this course.

#### BigQuery ML

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565161

With the different types of ML models in your mind. Let's apply concept to practice, in this lesson, you explore BigQuery ML and walk through the steps to build an ML model with SQL commands. You learned about BigQuery, the primary data analytics tool on Google Cloud, from the previous lesson. BigQuery provides two services in one, it's a fully managed storage facility to load and store datasets, and it's a fast SQL-based analytical engine. The two services are connected by Googles high speed internal network. Its this super fast network that allows BigQuery to scale both storage and compute independently based on demand. Although it started out solely as a data warehouse, over time it has evolved to provide features that support the data to AI lifecycle, meaning you can perform both data analytics and build ML models within BigQuery. In this lesson, you explore BigQuery capabilities to build ML models and walk through the steps and key SQL commands to do so. If you've worked with ML models before, you know that building and training them can be very time intensive. You must import and prepare the data, then experiment with different ML models and tune the parameters to improve the model performance. You also need to go back and forth to train the model with new data and features. And finally, you need to deploy the model to make predictions. This is an iterative process that requires a lot of time and resources. Now, with BigQuery ML, you can manage tabular data and execute ML models in one place with just a few steps. BigQuery ML tunes the parameters for you and helps you manage the ML workflow. Let's walk through the phases of a machine learning project and the key SQL commands. In phase 1, you extract, transform, and load data into BigQuery. If it isn't there already. If you're already using other Google products like YouTube for example, look out for easy connectors to get that data into BigQuery. Before you build your own pipeline. You can enrich your existing data warehouse with other data sources by using SQL joins. In phase 2, you select and preprocess features. You can use SQL to create the training dataset for the model to learn from BigQuery ML does some of the preprocessing for you like one hot encoding of your categorical variables? One hot encoding converts your categorical data into numeric data that is required by a training model. In phase 3, you create the model inside BigQuery. This is done by using the create model command. In this example, you want to create an ML model to predict customer purchasing behavior, specifically if they will buy the product in the future, you give the model a name ecommerce classification. You then specify the model type. Remember the previous lesson about ML model types. If you want to predict whether a customer will buy or not, which ML model should you use? That's right, a logistic regression model is the answer because you are solving a classification problem other than the logistic regression model, Bigquery ML supports other popular ML models. Including regression models such as linear regression and other models such as k means clustering and time series forecasting models. In addition to providing different types of machine learning models, BigQuery ML supports ML OPs machine learning operations. MLOPs turns your ML experiment to production and helps deploy, monitor, and manage the ML models. You'll learn more about MLOPs later in this course. You are recommended to start with simple options such as logistic regression and linear regression. And use the results as a benchmark to compare against more complex models such as DNN deep neural networks, which take more time and computing resources to train and deploy. After specifying the model type, you also need to define the label column. Why, remember the two major categories of ML models, supervised and unsupervised. The former deals with labeled data and predicts a goal, whereas the latter handles unlabeled data and identifies a hidden pattern. Is this a supervised or unsupervised model? Of course its a supervised classification problem, thus a labeled column. From there, you can run the query. In phase 4, after your model is trained, you can execute an ML evaluate query to evaluate the performance of the trained model on your evaluation data set. It's here that you specify which evaluation metrics the model will access, such as accuracy, precision, and recall. You'll explore these metrics later in the next module. Finally, in phase 5, when you're happy with your model performance, you can then use it to make predictions. To do so, invoke the ML predict command on your newly trained model to return with predictions and the model's confidence in those predictions. With the results your label field will have predicted added to the field name. This is your model's prediction for that label. You'll practice all these steps and the SQL commands using BigQuery ML in the hands on lab.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565162

Now it's time to get some hands-on practice building a machine learning model in BigQuery. In the lab that follows this video, you'll use e-commerce data from the Google Merchandise Store, shop.merch.google. The site's visitor and order data has been loaded into BigQuery, and you'll build a machine learning model to predict whether a visitor will return for more purchases later. You'll get practice creating a dataset in BigQuery, training an ML model, evaluating the model, and using the model for prediction. Let's get started.

#### Predict Visitor Purchases with BigQuery ML

- https://www.cloudskillsboost.google/paths/8/course_templates/593/labs/565163

#### Summary

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565164

In the first module of this course, you learned about AI foundations. Let's have a quick recap. You started with the story of coffee on wheels, which demonstrates how AI enables business processes, transformation and efficiency enhancement. You were then introduced to the toolbox of AI and ML on Google Cloud, which consists of three layers, AI foundations, AI development and AI solutions. This course focuses on the second layer which delves into both predictive AI and generative AI. Next, you explored the Google Cloud infrastructure, specifically compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. Additionally, you examined data and AI products which enable you to perform tasks to support the data to AI journey from data ingestion, storage and analytics to AI and machine learning. After that, you advance to the fundamental ML concepts, including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. With these concepts in mind, you can choose from different models and follow the steps to build an ML model to fit your needs with BigQuery ML. Finally, you had a hands on lab where you applied those steps to build your own ML model using SQL commands. This concludes the overview of the AI foundations module. In the next module, you'll advance to AI development and explore the different options to build an AI and ML project. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/593/quizzes/565165

#### Reading

- https://www.cloudskillsboost.google/paths/8/course_templates/593/documents/565166

### AI Development Options

#### Introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565167

In the first section of this course, you learned about AI Foundations, specifically on Cloud essentials like compute, storage, and networking. You also explored data tools like BigQuery, which helps you start the journey from data to AI. Now you'll explore how to develop an AI project on Google Cloud, specifically, the options available to build a machine learning, or ML model. You begin by comparing AI development options on Google Cloud, from pre-made to low-code and no-code, and finally a do-it-yourself approach. You then delve into the first option, pre-trained APIs, which use pre-trained ML models that don't require any training data. Next, you are introduced to Vertex AI, Google's unified platform to build an ML model end-to-end and support both no-code and code development. After that, you explore AutoML on Vertex AI, a low or no-code option to automate the ML development from data preparation to model training and model serving with your own training data. Finally, you explore the last option, custom training. Which allows you to manually code ML projects with tools like Python, Keras and TensorFlow. Consolidating the background knowledge gained throughout this module, you conclude with a hands on practice using the natural language API to identify subjects and analyze sentiment in text. Let's get started

#### AI developement options

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565168

Let's begin with Google Cloud's AI development options, which include out-of-the-box, low-code and no-code, and do it yourself. You'll explore the pros and cons of each and examine a decision tree, to help you consider which option might be best for your business problem. Imagine that you're helping your organization use AI to transform your business model and operation. Where and how should you start? Let's say you are a business user or an application developer, and you don't have training data and lack experience developing an ML model. However, you really want to use AI to automatically label and classify customer feedback. What can you do? Or perhaps you're a data analyst and you have some training data and experience using SQL. How can you build a custom ML model with your existing skills? Maybe you're a data scientist and you have a large amount of data, and want to train a custom ML model. However, you don't want to spend hours tuning ML parameters from the beginning. What choices do you have? Or what if you're an ML engineer or scientist, and enjoy using do-it-yourself models and code to operationalize the ML pipeline. What tools can you use? Well, you can find tools on Google Cloud to help you achieve your goals. From pre-configured solutions such as pre-trained APIs, to low or no code solutions such as BigQuery ML and AutoML, to a completely DIY approach using a code based solution by using custom training. Let's look at each of them. Google Cloud offers four options for building machine learning models. The first option is to use pre-trained APIs. API stands for application programming interface. This option lets you use pre-trained machine learning models, so you don't need to build your own if you don't have training data or machine learning expertise in the house. The second option is BigQuery ML, which you learned about in the previous module. This option uses SQL queries to create and execute machine learning models in BigQuery. If you already have your data in BigQuery, and your problems fit the predefined ML models offered by BigQuery ML, this could be your choice. The third option is AutoML. Which is a no-code solution that helps you build your own machine learning models on Vertex AI, through a point and click interface. Finally, there is custom training, through which you can code your very own machine learning environment, training and deployment. This option allows you flexibility and control over the ML pipeline. Let's compare the four options to help you decide which one to use for building your ML model. Note that the technologies change constantly and this is only a brief guideline. BigQuery ML only supports tabular data, whereas the other three support tabular image, text, and video. Pre-trained APIs also process audio. In terms of training data size, pre-trained APIs do not require any training data. Whereas BigQuery ML and custom training, require a large amount of data. Pre-trained APIs and AutoML, are user-friendly with low requirements for machine learning and coding expertise. Whereas custom training has the highest requirement, and BigQuery ML requires you to understand SQL. At the moment, you can't tune the hyperparameters with pre-trained APIs or AutoML. However, you can experiment with hyperparameters by using BigQuery ML and custom training. Pre-trained APIs require no time to train a model, because they directly use pre-trained models from Google. The time to train a model for the other three options depends on the specific project. Normally, custom training takes the longest time, because it builds the ML model from the beginning unlike AutoML and BigQuery ML. The best option depends on your business needs and ML expertise. Budget is also an important consideration. Visit Google Cloud's website for detailed pricing information. If you have little ML experience and no intention to train your own ML models, using pre-trained APIs might be the best choice. Pre-trained APIs address common perceptual tasks such as vision, video, and natural language. They are ready to use without any model development effort. If your data engineers, scientists, or analysts are familiar with SQL and already have data in BigQuery, BigQuery ML lets you use SQL queries to build predefined ML models. If you wish to build custom models with your own training data while you spend minimal time coding, then AutoML on Vertex AI is your choice. AutoML allows you to focus on business problems instead of the underlying model architecture and provisioning. If your ML engineers and data scientists want full control of ML workflow, Vertex AI custom training, lets you train and serve custom models with code on Vertex AI Workbench or Google Colab. Let's walk through pre-trained APIs, AutoML, and custom training one-by-one later in this module.

#### Pre-trained APIs

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565169

Now that you’ve been introduced to the different AI development options available with Google Cloud, let’s now focus on the first: pre-trained APIs. Good machine learning models require lots of high-quality training data. You should aim for hundreds of thousands of records to train a custom model. But what if you don't have that kind of data? How can you use AI to serve your purposes? Pre-trained APIs are a great place to start. API stands for application programming interface, and they define how software components communicate with each other. Imagine APIs are electric sockets. Different regions have different standards, for example, the US has type A and B whereas Europe has type F. As a traveller, you only need to know which adapter to use without worrying about what’s behind the wall and how to build the electric network. The same principle applies to APIs. As a user, you only need to know which API to fit your code without worrying about the implementation of the APIs. Or in other words, how to train and deploy ML models. After you plug the API to your code, you can directly use the functions. Pre-trained APIs are offered as services. In many cases, they can act as building blocks to create the application you want without the expense or complexity of creating your own models. They save the time and effort of building, curating, and training a new dataset so you can just directly deal with predictions. So, what are the pre-trained APIs provided by Google Cloud? Let’s explore a short list. Speech, text, and language APIs For example, the Natural Language API derives insights from text using pre-trained large language models. It recognizes the entities and sentiment of a sentence. Image and video APIs For example, the Vision API recognizes content in static images, and the Video Intelligence API recognizes motion and action in video. Document and data APIs For example, the Document API processes documents like text extraction and form parser. It can be used in specialized use cases like lending, contracts, procurement, and identity documents. Conversational AI APIs For example, the Dialogflow API builds conversational interfaces. Let’s try out the Natural Language API in a browser. Scroll down to try the API by uploading a paragraph of text, which can be in over ten different languages including Chinese, English, and Spanish. Feel free to view the full list by clicking See supported languages. For example, if you use the sample paragraph about Google, and click ANALYZE, You find four types of analysis: entity, sentiment, syntax, and category. Entity analysis identifies the subjects in the text including: A proper noun, such as the name of a particular person, place, organization, or thing. In this case, the Natural Language API automatically detects Google as an organization, Mountain View as a location, and Sundar Pichai as a person. Common nouns such as goods are also addressed. In this case, It identifies Android and phone as consumer goods. How can entity analysis be applied to solve your business problems? How about automatic tagging? Say you have tons of legal documents and you want to auto-tag the main words of each document. How about document classification? Say you want to classify your documents to different categories based on key information in the text. Or what about information extraction, so you can generate summaries based on the key entities in the doc? There are many other usages of entity analysis. Take a minute to think about how to apply it to your use cases. Sentiment analysis is used to identify the emotions indicated in the text such as positive, negative, and neutral. Score ranges between -1.0 (negative) and 1.0 (positive) and magnitude indicates the overall strength of emotion within the given text, between between zero and positive infinity. The Natural Language API can analyze the sentiment for the entire document and at entity level. How can sentiment analysis be used solve your business problems? Well, it can be used to analyze the emotion of customer feedback, social network comments, and conversations. You can then use this data as feedback to adjust your offerings. In addition to entity and sentiment analysis, the Natural Language API can analyze syntax and extract linguistic information for further language model training in a specific field. It can also do category analysis for the entire text. For example, this text is about an internet and telecommunications company. The UI, or user interface, demonstrates the major features of the Natural Language API. When you’re ready to build a production model, you can integrate the APIs into your code. We’ll show you how in the hands-on lab later in this module. In summary, you can build AI projects and applications without training your own ML models or providing training data. Instead, you can use pre-trained AI models through APIs provided by Google. In addition to the above APIs, generative AI APIs have rapidly evolved recently. These APIs allow you to use different foundation models to generate various types of content. Some examples include Gemini multimodal, processing data in multiple modalities like text, image, and video. Embeddings for both text and multimodal, converting multimodal data into numerical vectors that can be processed by ML models, especially gen AI foundation models. Gemini for text and chat, performing language tasks and conducting natural conversations. Imagen for Image, generating images and captions. Chirp for speech, building voice-enabled applications. And Codey for code, generating, completing, and chatting about code. As gen AI advances every day, you’ll find new APIs and models emerging in the near future. In the latter part of this course, you get a chance to delve deeper into the multifaceted realm of generative AI.

#### Vertex AI

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565170

In this lesson, you'll explore Vertex AI, which is the unified platform that supports various technologies and tools on Google Cloud to help you build an ML project from end to end. For years now, Google has invested time and resources into developing Data and AI. Google had developed crucial technologies and products from scikit-learn as a Google Summer Coding project back in 2007 to Vertex AI and generative AI today. As an AI first company, Google has applied AI technologies to many of its products and services, like Gmail, Google Maps, Google Photos, and Google Translate, just to name a few. But developing these technologies doesn't come without challenges. Some traditional challenges include handling large quantities of data, determining the right machine learning model to train the data and harnessing the required amount of computing power. Then there are challenges around getting ML models into production. Some of the challenges can be scalability, monitoring, and continuous integration, delivery, and training. In fact, according to Gartner, only half of enterprise ML projects get past the pilot phase. There are also ease of use challenges. Many tools on the market require advanced coding skills, which can take a data scientists focus away from model configuration. Without a unified workflow, data scientists often have difficulties finding tools. Google's solution to many of the production and ease of use challenges is Vertex AI, a unified platform that brings all the components of the machine learning ecosystem and workflow together. What exactly does a unified platform mean? There are two primary aspects. Firstly, it means that Vertex AI provides an end-to-end ML pipeline to prepare data and create, deploy, and manage models over time and at scale. For instance, during the data readiness stage, users can upload data from wherever it's stored, Cloud Storage, BigQuery, or a local machine. Then during the feature readiness stage, users can create features, which are the process data that will be put into the model and then share them with others by using the feature store. After that, it's time for training and hyperparameter tuning. This means that when the data is ready, users can experiment with different models and adjust hyperparameters. Finally, during deployment and model monitoring, users can set up the pipeline to transform the model into production by automatically monitoring and performing continuous improvements. You'll learn how to do this later in this course when you explore MLOps. Second, vertex AI is a unified platform that encompasses both predictive AI and generative AI. Predictive AI allows for sales forecasting and classification, while generative AI enables the creation of multimodal content. Vertex AI allows users to build ML models with either AutoML a no code solution, or custom training a code based solution. AutoML provides an easy to navigate UI. It lets data scientists focus on what business problems to solve instead of how to code and deploy an ML solution. Custom training gives data scientists and ML engineers more control over the development environment and process. They can use tools like Vertex AI workbench and Colab to DIY their ML projects. One convenient feature is that data scientists can now write SQL with workbench on Vertex AI to seamlessly connect BigQuery and Vertex AI. Being able to perform such a wide range of tasks in one unified platform has many benefits. This can be summarized with four Ss. It's seamless. Vertex AI provides a smooth user experience from uploading and preparing data all the way to model training and production. It's scalable. The machine learning operations or MLOps provided by Vertex AI helps to monitor and manage the ML production and therefore scale the storage and computing power automatically. It's sustainable. All of the artifacts and features created using Vertex AI can be reused and shared. It's speedy. Vertex AI produces models that have 80% fewer lines of code than competitors. In addition to AutoML and custom training, Vertex AI also provides tools for generative AI. You can use these tools to generate content and embed generative AI into your applications. We will discuss generative AI technologies and tools later in this course.

#### AutoML

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565171

In the last lesson you learned about Vertex AI, a unified platform that supports both AutoML, a no code solution, and custom training a code based solution. In this lesson, you'll explore AutoML in depth, including the technologies used to power automated ML development. Auto ML, which stands for automated machine learning aims to automate the process to develop and deploy an ML model. If you've worked with ML models before, you know that building them can be extremely time consuming because you need to repeatedly add new data and features. Try different models and tune parameters to achieve the best results. When AutoML was first introduced in January of 2018, the goal was to save the manual work from data scientists and automate machine learning pipelines from preprocessing data to model training and deployment. Since 2021, AutoML features are embedded in vertex AI and have become part of the platform. But how could this be done? How can you trust AutoML to generate the best results without bias and do so in a speedy way? Let's look deeper to explore how AutoML works and the main technologies behind it. AutoML is powered by the latest research from google, it's an ongoing endeavor. There are four distinct phases, phase 1 is data processing. After you upload a dataset, AutoML provides functions to automate part of the data preparation process. For example, it can convert numbers, date time, text categories, arrays of categories, and nested fields into a certain format of data so that it can be fed into an ML model. Phase 2 includes searching the best models and tuning the parameters. Two critical technologies support this auto search, the first is called neural architect search, which helps search the best models and tune the parameters automatically. And the second is called transfer learning, which helps speed the searching by using pre trained models. Let's look at transfer learning first, machine learning is similar to human learning it learns new things based on existing knowledge. AutoML has already trained many different models with large amounts of data, these trained models can be used as a foundation model to solve new problems with new data. A typical example are large language models, LLMs which are general purpose and can be pre trained and fine tuned for specific purposes. LLMs are trained for general purposes to solve common language problems, such as: text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields, such as: retail, finance and entertainment, using a relatively small size of field datasets. Transfer learning is a powerful technique that lets people with smaller data sets or less computational power, achieve great results by using pre trained models trained on similar larger datasets. Because the model learns through transfer learning it doesn't have to learn from the beginning, so it can generally reach higher accuracy with much less data and computation time than models that don't use transfer learning. Now, let's look at neural architecture search, the goal of neural architecture search is to find optimal models among many options specifically, AutoML tries different architectures and models and compares against the performance between models to find the best ones. For instance, auto ML can search through multiple advanced ML models and automatically tune the parameters to find the best fit for your data. In phase 3 the best models are assembled from phase 2 and prepared for prediction in phase four. Note that AutoML does not rely on one single model, but on the top number of models. The number of models depends on the training budget, but is typically around ten. The assembly can be as simple as averaging the predictions of the top number of models relying on multiple top models instead of one greatly improves the accuracy of prediction. By applying these advanced ML technologies, AutoML automates the pipeline from feature engineering to architecture search, to hyperparameter tuning and to model assembly. It might seem that AutoML can do a better job than a human to find the optimal models that fit your data, perhaps, the best feature of auto ML is that it provides a no code solution. You can point and click through a UI to build an ML model with your own data. You'll walk through the details from preparing training data to train your model and finally get predictions in the next module.

#### Custom training

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565172

Now, let's look at custom training. You do it yourself solution to build an ML project. You explored the options that Google Cloud provides to build machine learning models by using pre-trained APIs, where you directly call Google's train models to solve your problems. BigQuery ML, where you write a few lines of SQL code to train your own model, and AutoML, where you build your ML model with your own data through a UI. But if you want to create your own machine learning environment to experiment with and build your own pipeline, you need custom training. Before any coding begins, you must determine what environment you want your ML training code to use. There are two options, a pre-built container or a custom container. A pre-built container is like a furnished kitchen with cabinets, appliances, and cookware. If your ML training needs a platform like Python, TensorFlow, and PyTorch, and you're not particular about the underlying infrastructure to run on or to use our kitchen analogy, which oven or knife you use, a pre-built container is probably your best choice. A custom container alternatively, is like an empty room. You define the exact appliances and tools that you prefer to cook with. That means you must determine the details like the environment, machine type, and discs when creating the custom container. In terms of the tools to code your ML model, you can use Vertex AI Workbench. You can think of Vertex AI Workbench as a Jupiter Notebook deployed in a single development environment that supports the entire data science workflow from exploring to training, and then deploying a machine learning model. You can also use Colab Enterprise, which was integrated into Vertex AI platform in 2023, so data scientists can code in a familiar environment. After you decide the working environment, the next step is to start writing code. These days, you don't have to code from scratch. Instead, you can leverage ML libraries. An ML library is a collection of pre written code that can be used to perform machine learning tasks. These libraries can save developers time and effort by providing them with the tools they need to build machine learning models without having to write everything from the beginning. As a data scientist, you might already be familiar with popular ML libraries like TensorFlow, Scikit-learn, and PyTorch. They are open-source and widely used by a large community of users and developers. Let's explore TensorFlow, an end to end open platform for machine learning supported by Google. TensorFlow contains multiple abstraction layers. You use TensorFlow APIs to develop and train ML models. The TensorFlow APIs are arranged hierarchically with the high level APIs built on the low level APIs. The lowest layer is hardware. TensorFlow can run on different hardware platforms, including CPU, GPU, and TPU. The next layer is the low level TensorFlow APIs, where you can write your own operations in C++ and call the core basic and numeric processing functions written in Python. The third layer is the TensorFlow model libraries, which provide the building blocks such as neural network layers and evaluation metrics to create a custom ML model. The high level TensorFlow APIs like Keras sit on top of this hierarchy. They hide the ML building details and automatically deploy the training. They can be your most used APIs. Note that Vertex AI fully hosts TensorFlow from low level to high level APIs. Regardless of which abstraction level you are writing your TensorFlow code at, Vertex AI gives you a managed service. Now let's look at an example of using TF Keras, a high level TensorFlow library commonly used to build a simple regression model. Typically, it takes three fundamental steps. In Step 1, you create a model where you piece together the layers of a neural network. In Step 2, you compile the model, where you specify hyper parameters such as performance evaluation and model optimization. Finally, you train your model to find the best fit. Assume you already imported necessary packages like TensorFlow and uploaded the data. The first step is to create a model by using TF Keras sequential. To demonstrate, you can define your model as a three layer neural network. You'll explore more details about neural networks such as activation functions and the next module. The next step is to compile the model by specifying how you want to train it by using the method compile. For instance, you can decide how to measure the performance by specifying a loss function. You can also optimize the training by pointing to an optimizer. The last step is to train the model by using the method fit. For instance, you can define the input, the training data, and the output, the predicted results. You can also decide how many iterations you want to train the model by specifying the number of epoxs. After you train the model and are satisfied with the performance, you can then deploy the model and make predictions. Apart from TensorFlow, Google is constantly introducing new frameworks. One of the most promising frameworks is JAX. JAX is a high performance numerical computation library that is highly flexible and easy to use. It offers new possibilities for both research and production environments.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565173

Now it's time for some hands-on practice. In the following lab, you'll use the Natural Language API to analyze texts. Specifically, you'll identify entities and analyzeSentiment with code. Before you begin, let's briefly review the main features of the Natural Language API you learned in the previous lesson. You can identify entities which are subjects in the inputted texts, such as Google as a company name, and Mountain View as a location. You can identify the sentiment which indicates a motion at both the overall document and individual subject level. You can analyzeSyntax and extract linguistic information, such as the relationship between words, and you can also classify the text to categories based on topics or keywords. Similar to assigning a tag to a piece of text. You perform all of this analysis through a UI, which is a quick and efficient way to demonstrate and test these features. However, if you want to incorporate these features in production, you must embed the APIs into code. Using APIs in your code is similar to ordering a sandwich at a Deli. You order from the menu and get your food without worrying about how it was made in the kitchen. The same concept applies to using APIs. You only need to know three things. The features, like the menu, the input, like the order, and the output, like the sandwich. Like a menu, features are the types of requests that you can make to the Natural Language API. Like a food order, the input is how you construct the requests. Then the sandwich you receive after you place the order is the response or output. With this, you can determine next steps. What are the different types of requests that you can make? The Natural Language API provides several methods for performing analysis and annotations on your text. You'll practice with most of them in the lab. For entity analysis, you can use the analyzeEntities method. The sentiment analysis is performed through the analyzeSentiment method at the entire texts level, and analyzeEntitiesSentiment at the individual entity and subject level. The syntactic analysis is performed with the analyzeSyntax method. The content classification is performed by using the classifyText method. Now, how do you construct those requests? The Natural Language API is a rest API and consists of JSON requests and responses. A simple JSON request for entity analysis looks like the code shown here, where you define the type of the document. For example, plain text, the language like EN, which stands for English. The content, which can be the text itself or the file location and Cloud Storage, and finally, the encoding type like UTF8. After you construct the request, you need to call the API. Just like after you decide what you want at a Deli, you then need to place the order with a counter person. Here is an example to call the API with curl. Curl stands for client URL, and is a command line tool to transfer data between client and server. You can also use other programming languages such as Python and Java SDKs to call the APIs. Typically, the vendors of the products and services you are using to find the APIs and provide the SDKs in different languages for you to choose. In this example, you call the Natural Language API feature analyzeEntities. Pass the request.json file that you just constructed and save the response to result.json file. Finally, how should you handle the responses? You can review the result by using a command like cat result.json, or parse it for further usage. Equipped with the technical details in this lab, you'll use the Natural Language API to extract entities, analyzeSentiment and analyzeSyntax. By completing the lab, you'll get practice creating a Natural Language API request and calling the API with curl, extracting entities and running sentiment analysis on text, performing linguistic analysis on text, and creating a Natural Language API request in a different language. Let's start

#### Entity and Sentiment Analysis with the Natural Language API

- https://www.cloudskillsboost.google/paths/8/course_templates/593/labs/565174

#### Summary

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565175

This concludes the module on AI development options. Let's do a quick recap. You compared AI development options from a ready-made approach to low-code and no-code to do it yourself. You started with pre-trained APIs, which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google's unified platform to build a machine learning project from end-to-end. You learned about AutoML, which is a low or no-code tool on Vertex AI that lets you automate ML development from data preparation to model training and model serving with your own data. Then you learned about custom training, which lets you manually code ML projects by using tools like Python, TensorFlow, and Vertex AI workbench. Finally, you've got hands-on practice with a lab on the Natural Language API, which identifies subjects and analyzes sentiment in text. In the next module, you'll explore the AI development workflow, where you walk through the ML pipeline and build an ML model end-to-end. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/593/quizzes/565176

#### Reading

- https://www.cloudskillsboost.google/paths/8/course_templates/593/documents/565177

### AI Development Workflow

#### Introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565178

In the previous section of this course, you learned about different options to develop an AI project on Google Cloud, from ready to use approaches like pre-trained APIs to low or no-code solutions like AutoML, and to DIY solutions such as custom training. In this section, you further investigate how to develop an AI project on Google Cloud. Specifically, you walk through the ML workflow and explore how to create an automated pipeline. You begin with an overview of the ML workflow and then focus on the 1st stage of the workflow, data preparation, which includes data uploading and feature engineering. After that, you advance to the 2nd stage, model development. This includes model training and evaluation. You then proceed to the 3rd stage, model serving. This includes model deployment and monitoring. Next, you learn about machine learning operations, or MLOps, which takes ML models from development to production. You'll also be shown an example of how to build a pipeline to automate the training, evaluation, and deployment of an ML model by using vertex AI pipelines. You conclude this module with a hands on lab, where you walk through the three stages to build an ML model with auto ML on vertex AI. Gaining a solid grasp of ML terminology requires a clear understanding of how a neural network learns. This module offers an optional lesson that delves into the neural networks learning process along with the key terminologies. If you're already familiar with the ML theories, feel free to skip this lesson. Let's get started.

#### ML workflow

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565179

Let's look at the ML workflow and walk through the main stages. Building an ML model is actually not too different from serving food in a restaurant. You start by preparing raw ingredients and finish by serving the dishes on the table. There are three main stages to the ML workflow with Vertex AI. The first stage is data preparation, which includes two steps, data uploading, and feature engineering. A model needs a large amount of data to learn from. The quality and quantity of the data, decide how much and how well the machine learns. The data used in machine learning can be real time streaming data or batch data. The data can also be structured or unstructured. Structured data is data that can be easily stored in tables, such as numbers and text. Unstructured data is data that cannot be easily stored in tables, such as images and videos. The second stage of the ML workflow is model development. A model needs a tremendous amount of iterative training. This is when training and evaluation form a cycle to train a model and then evaluate the model and then train the model more. The third and final stage is model serving. A model needs to actually be used in order to predict results. This is when the machine learning model is deployed and monitored. If you don't move an ML model into production, it has no use and remains only a theoretical model. Compare this process to serving food in a restaurant. Data preparation is when you prepare the raw ingredients. Model development is when you experiment with different recipes, and model serving is when you finalize the menu to serve the meal to customers. Now, it's important to note that an ML workflow is not linear. It's iterative. For example, during model training, you might need to return to the raw data and generate more useful features to feed the model. When monitoring the model during model serving, you might find data drifting, or the accuracy of your prediction might suddenly drop. You might need to check the data sources and adjust the model parameters. Fortunately, these steps can be automated with ML Ops. You'll learn more about this later in this module. Although the main stages remain the same, you have two options to set up the workflow with Vertex AI. The first choice is to use AutoML, a no code solution that lets you build an ML model through UI. It's user friendly and doesn't require a lot of ML expertise. Also, no coding skills are needed. Alternatively, you can code the workflow with Vertex AI Workbench or Colab using Vertex AI Pipelines. Vertex AI Pipelines is essentially a toolkit that includes pre-built SDKs and software development kits, which are the building blocks of a pipeline. Coding the pipeline is a good option if you're an experienced ML engineer or data scientist and want to automate the workflow programmatically. Let's focus on AutoML next and then explore the code-based approach later in this module.

#### Data preparation

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565180

Let's start with the first stage of the ML workflow, data preparation. During this stage, you must upload data and then prepare it for model training and feature engineering. The data can come from Cloud storage, big query, or even your local machine. AutoML supports four types of data; image, tabular, text, and video. For each data type, AutoML solves different types of problems called objectives. For image data, you can train the model to classify images, either single label or multi label. Single label means that you can only assign one tag to an image like dog or cat. Whereas multi label means that you can assign multiple tags to one image, like dog, large, and brown. You can also train the model to detect objects and discover image segmentation. For tabular data, you can train the model to solve regression, classification, or forecasting problems. Forecasting is vital to many industries like retail. To learn more about how to build a forecasting model, please check the course titled, Introduction to Vertex Forecasting and Time Series and Practice in the reading list. For text data, you can train the model to classify text, extract entities, and conduct sentiment analysis. Do these tasks sound familiar? Right. You learned about entity extraction and sentiment analysis when you explored natural language APIs in the previous module. Finally, for video data, you can train the model to recognize video actions, classify videos, and track objects. Although the tasks can be similar, pre trained APIs rely on Google's pre trained ML models with no customer data, whereas AutoML trains a custom model using your own data. In reality, you might not be restricted to just one data type and one objective. Instead, you might need to combine multiple data types and different objectives to solve a business problem. AutoML is a powerful tool that can help you do this. After the data is uploaded to AutoML, the next step is preparing it for model training with feature engineering. Imagine you're preparing a meal. Your data is like your ingredients, such as carrots, onions, and tomatoes. Before you start cooking, you'll need to peel the carrots, chop the onions, and rinse the tomatoes. This is what feature engineering is like. The data must be processed before the model starts training. A feature refers to a factor that contributes to the prediction. It's like an independent variable in statistics or a column in a table. Preparing features can be both challenging and tedious. To help, Vertex AI provides a service called Vertex AI Feature Store, which is a centralized repository to manage, serve, and share features. It aggregates the features from different sources in Big Query and makes them available for both real time, often called online, and batch, often called offline serving. Vertex AI automates the feature aggregation to scale the process with low latency. Additionally, Vertex AI features store is ready for the challenge of generative AI. It can manage and serve embeddings, which is the crucial data type in gen AI. It also supports retrieving similar items in real time, ensuring low latency. The workflow to setup and start real time online serving using Vertex AI Feature store can be summarized as follows. First, prepare the data source in big query. Optionally, register the data sources by creating feature groups and features. Set up online store and feature view resources to connect the feature data sources with online serving clusters and serve the latest feature values online from a feature view. So what are the benefits of vertex AI feature store? First, features are shareable for training and serving. They are managed and served from a central repository, maintaining consistency across your organization. Second, features are reusable. This helps to save time and reduces duplicated efforts. Third, features are scalable. They automatically scale to provide low latency serving so you can focus on developing the logic to create them without worrying about deployment. Fourth, features are easy to use. Vertex AI feature store is built on an easy to navigate user interface.

#### Model development

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565181

Let's advance to the second stage model development, where you train the model and evaluate the result. Now that our data is ready, which, if you return to the cooking analogy is the ingredients, it's time to train the model. This is like experimenting with the recipes. This stage involves two steps, model training, which is like cooking the recipe, and model evaluation, which is like testing how good the meal is, this process might be iterative. To set up an ML model, you need to specify a few things. First of all, is the training method where you tell Vertex AI the dataset you just uploaded from the preparation stage. Depending on the data type, whether it's tabular, image, text or video, you specify the training objective. This is the goal of the model training and the task you want to solve. Then you decide the training method, AutoML, without code or custom training using code. The next step is to determine the training details. For example, if you are training the model to solve a supervised learning problem such as regression and classification, you must choose the target column from your dataset. In training options, you can choose certain features to participate in the training and transform the data type if needed. Finally, you specify the budget in pricing and then click "Start Training." AutoML will train the model for you and choose the best performed models amongst thousands of others. Do you recall the powerful technologies behind AutoML? The credit there goes to neural architecture search and transfer learning. While you're experimenting with a recipe, you need to keep tasting it to ensure that it meets expectations. This is the evaluation portion of the model development stage. Vertex AI provides extensive evaluation metrics to help determine a model's performance. Let's focus on the metrics of recall and precision when evaluating the performance of classification models. To do this, you'll use a confusion matrix. A confusion matrix is a specific performance measurement for machine learning classification problems. It's a table with combinations of predicted and actual values. To keep things simple, we assume the output includes only two classes. Let's explore an example. The first is true positive, which can be interpreted as the model predicted positive, and that's true. The model predicted that this is an image of a cat and it actually is. The opposite of that is true negative, which can be interpreted as the model predicted negative, and that's true. The model predicted that the image is not a cat and it actually isn't. Then there is false positive, otherwise known as a Type 1 error, which can be interpreted as the model predicted positive and that's false. The model predicted that the image is a cat, but it actually isn't. Finally, there is false negative, otherwise known as a Type 2 error, which can be interpreted as the model predicted negative and that's false. The model predicted that the image is not a cat, but it actually is. A confusion matrix is the foundation of many other metrics used to evaluate the performance of a machine learning model. Let's look at the two popular matrix, recall and precision that you will encounter in the lab. Recall refers to all the positive cases and looks at how many were predicted correctly. This means that recall is equal to the true positives, divided by the sum of the true positives and false negatives. Precision refers to all the cases predicted as positive and how many are actually positive. This means that precision is equal to the true positives, divided by the sum of the true positives and false positives. Imagine you're fishing with a net, using a wide net, you caught both fish and rocks, 80 fish out of 100 total fish in the lake plus 80 rocks. The recall in this case is 80%, which is calculated by the number of fish caught 80, divided by the total number of fish in the lake, 100. The precision is 50%, which is calculated by taking the number of fish caught 80, and dividing it by the number of fish and rocks collected 160. Let's say you wanted to improve the precision, so you switched to a smaller net. This time, you caught 20 fish and zero rocks. The recall becomes 20%, 20 out of 100 fish collected, and the precision becomes 100%, 20 out of 20 total fish and rocks collected. Precision and recall are often a trade off. Depending on your use case, you might need to optimize for one or the other. Consider a classification model where Gmail separates emails into two categories, spam and not spam. If the goal is to catch as many potential spam emails as possible, Gmail might want to prioritize recall. In contrast, if the goal is to only catch the messages that are definitely spam without blocking other emails, Gmail might want to prioritize precision. Vertex AI visualizes the precision recall curve so that it can be adjusted based on the problem that needs to be solved. You'll get the opportunity to practice adjusting precision and recall in the AutoML lab. In addition, to the confusion matrix and the metrics generated to measure recall and precision, the other useful measurement is feature importance. In Vertex AI, feature importance is displayed through a bar chart to illustrate how each feature contributes to a prediction. The longer the bar or the larger the numerical value associated with a feature, the more important it is. This information helps decide which features are included in a machine learning model to predict the goal. You will observe the feature importance chart in the lab as well. Feature importance is just one example of Vertex AI's comprehensive machine learning functionality called explainable. Explainable AI is a set of tools and frameworks to help understand and interpret predictions made by machine learning model. Please check the reading list if you want to learn more about explainable AI on Google Cloud.

#### Model serving

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565182

Let's focus on the third stage of the ML workflow, model serving. The recipes are ready, and now it's time to serve the meal. This represents the final stage of the machine learning workflow, model serving. Model serving consists of two steps. First, model deployment, which you can compare to serving a meal to a hungry customer, and second model monitoring, which is like checking with the wait staff to ensure that the restaurant is operating efficiently. It's important to note that model management exists throughout this whole workflow to manage the underlying machine learning infrastructure. This lets data scientists focus on what to do and not how to do it. Let's start with model deployment, which is the exciting time when the model is implemented and ready to serve. You have two primary options. Deploy the model to an endpoint for real-time predictions or often called online predictions. This option is best when immediate results with low latency are needed, such as making instant recommendations based on a user's browsing habits whenever they're online. A model must be deployed to an endpoint before it can be used to serve real-time predictions. Request the prediction job directly from the model resource for batch prediction. This option is best when no immediate response is required. For example, sending out marketing campaigns every other week, based on the user's recent purchasing behavior and what's currently popular on the market. Batch prediction does not require deploying the model to an endpoint. You can deploy a model either using the UI on Vertex AI or using code by calling APIs. You'll practice building an endpoint later in the lab. Beyond making predictions in the Cloud, deploying the model off Cloud is also possible. This approach is generally adopted when the model needs to be deployed in a specific environment to mitigate latency, ensure privacy, or enable offline functionality. For instance, consider an IOT Internet of Things application like object detection that utilizes a camera feed in a manufacturing plant. In such use case, the added latency of relying on the Cloud can be impractical. Once the model is deployed and begins making predictions or generating contents, it is important to monitor its performance. The backbone of automating ML workflow on Vertex AI is a tool kit called Vertex AI Pipelines. It automates monitors, and governs machine learning systems by orchestrating the workflow in a serverless manner. Imagine you're in a production control room and Vertex AI Pipelines is displaying the production data on screen. If something goes wrong, it automatically triggers and displays a warning based on a pre-defined threshold. With Vertex AI Workbench, which is a notebook tool, you can define your own pipeline using SDKs. You can do this with pre-built pipeline components, which means that you primarily need to specify how the pipeline is put together using components as building blocks. You'll explore more details about Vertex AI Pipelines in the next lesson. It's with these final two steps, model deployment, and model monitoring that you complete the exploration of the machine learning workflow. The restaurant is open and operating smoothly, bon appetit.

#### MLOps and workflow automation

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565183

In this lesson, you explore an advanced topic, MLOps and workflow automation. You learned from the previous lessons to build an ML model through three main stages, data preparation, model development, and model serving. You have two approaches to build an end-to-end workflow, one is codeless through Google Cloud Console like AutoML on Vertex AI. But what if you want to automate this workflow to achieve continuous integration, training, and delivery? Here comes the other option, to code a pipeline that automates the ML workflow. Machine learning operations, or MLOps, play a big role. MLOps combines machine learning development with operations and applies similar principles from DevOps, or development operations, to machine learning models. MLOps aims to solve production challenges related to machine learning. In this case, this refers to building an integrated machine learning system and operating it in production. These are considered to be some of the biggest pain points by the ML practitioners community because both data and code are constantly evolving in machine learning. Practicing MLOps means automating and monitoring each step of the ML system construction and operating it in production to enable continuous integration, training, and delivery. The backbone of MLOps on Vertex AI is a toolkit called Vertex AI Pipelines, which supports both Kubeflow Pipelines, or KFP, and TensorFlow Extended, or TFX. If you already use TensorFlow to build ML models that process terabytes of structured data, it makes sense to use TFX and turn that code into an ML pipeline. Otherwise, KFP can be a good alternative. Learn more about how to choose between Kubeflow Pipelines, SDK, and TFX from the reading list. An ML pipeline contains a series of processes and runs in two different environments. First is the experimentation, development, and test environment, and the second is the staging, pre-production, and production environment. In the development environment, you start from data preparation, which includes data extraction, analysis, and preparation, to model development like training, evaluation, and validation. The result is a trained model that can be entered in model registry. Once the model is trained, the pipeline moves to the staging and production environment where you serve the model, which includes prediction and monitoring. Each of these processes can be a pipeline component, which is a self-contained set of code that performs one task of a workflow. You can think of a component as a function, which is a building block of a pipeline. You can either build a custom component on your own or leverage the pre-built components provided by Google. If you want to accomplish a specific task to tailor your ML workflow, such as determining a special threshold for model deployment, you may need to code a custom component. Before doing so, check the pre-built components offered by Google Cloud, you may find a pipeline component to reuse or customize to suit your needs. Learn more about using Google Cloud pipeline components in your pipeline in the reading list. All these components are like pieces of an ML pipeline. You need to assemble them together to automate the entire ML workflow. Organizations often implement ML automation in three phases. Phase 0 is the starting point, where you have not configured any MLOps. You typically use the GUI, or graphical user interface, workflow such as AutoML for training, deployment, and serving. Phase 0 is critical because it helps you build an end-to-end workflow manually before you automate it. In phase 1, you start automating your ML workflow by building components using Vertex AI Pipelines SDKs. An example of a component would be the training pipeline. It is in this phase that you develop the building blocks for future use. In phase 2, you integrate the separate components to form an entire workflow and to achieve CI, CT, and CD. Let's look at an example, assume you want to build a pipeline to train, evaluate, and deploy an AutoML model that classifies beans into one of seven types based on their characteristics. You have two main steps, build a pipeline and then run it. To build a pipeline, you first plan it as a series of components, which can be a combination of custom and pre-built. To promote reusability, each component should have a single responsibility. Second, you build any custom components that are needed, for example, you create a component called classification model eval metrics. You use this component to compare the evaluation metrics to a threshold after the model is trained and determine whether the model should be deployed. If the model performs well, you deploy it, otherwise, you retrain the model. Third, you assemble the pipeline by adding pre-built components. For example, TabularDatasetCreateOp creates a tabular dataset in Vertex AI given a data source either in cloud storage or BigQuery. AutoMLTabularTrainingJobRunOp kicks off an AutoML training job for a tabular dataset. EndpointCreateOp creates an endpoint in Vertex AI. And ModelDeployOp deploys a given model to an endpoint in Vertex AI. You also include a custom component, classification_model_eval_metrics, from the previous step to compare the performance of the trained model to a threshold. After the pipeline is built, you must compile and run it. First you compile it using the compiler, compiler.compile or compile commands, and then you define and run the pipeline job. The good news is you don't have to create a pipeline from the beginning. Vertex AI provides a few templates, like the one for classification/regression of tabular data with AutoML, to help you start your journey. Now you have an automated pipeline to train, evaluate, and deploy an ML model. This pipeline will check the performance of the model constantly and decide whether it should be deployed or retrained without your intervention. The nice thing is that Google Cloud also visualizes the pipeline based on the coding, with which you can easily check the components and the corresponding artifacts. This example demonstrated the overall process to build a pipeline. To know more about pipeline details, please practice with the coding example in the demo, Introduction to Vertex AI Pipelines, in the reading list for this course.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565184

Let's put what you've just learned into practice with a hands on lab. In this lab, you'll use AutoML, a no code tool to build a machine learning model to predict loan risk. The data set used in the lab relates to loans from a financial institution and has 2,050 data points. AutoML requires at least 1,000 data points in a dataset. The goal is to practice working through the three phases of the machine learning workflow, data preparation, model development and model serving. Before you start the lab, lets explain the details about model evaluation so that you can interpret the training results. Lets start with a review of the confusion matrix. Youll get a similar result like this in the lab, pause for a second and try to interpret this matrix yourself. What does it tell you? 100% true positive rate, this means the model perfectly identifies everyone who will repay their loan. It never misses a good borrower, that's fantastic. Note that the true positive rate equals true positives divided by the sum of true positives and false negatives. If the terms sound unfamiliar, please refer to the previous example where you learned about the confusion matrix. 87% true negative rate, the model also correctly identifies 87% of people who wont repay defaulters. Note that the true negative rate equals true negatives divided by the sum of false positives and true negatives. 13% false positive rate, this means the model mistakenly identifies 13% of good borrowers, those who actually repay as defaulters. This can lead to the bank rejecting potentially good customers, which is not ideal. Note that the false positive rate equals false positives divided by the sum of false positives and true negatives. Finally 0% false negative rate. This means the model never incorrectly identifies a borrower who will repay their loan as a potential defaulter. Note that the false negative rate equals false negatives divided by the sum of true positives and false negatives. Let's look at the precision recall curve you will encounter in the upcoming AutoML lab. The confidence threshold determines how a machine learning model counts the positive cases. A higher threshold increases the precision but decreases the recall. A lower threshold decreases the precision but increases recall. Moving the confidence threshold to zero produces the highest recall of 100% and the lowest precision of 50%. So what does that mean? That means the model predicts that 100% of loan applicants will be able to repay a loan they take out. However, actually only 50% of people were able to repay the loan. Using this threshold to identify the default cases in this example can be risky, because it means that you only get half of the loan investments back. Now lets view the other extreme by moving the threshold to one. This will produce the highest precision of 100% with the lowest recall of 1%. What does this mean? It means that all the people who were predicted to repay the loan, 100% of them, actually did. However, you rejected 99% of loan applicants by only offering loans to 1% of them, that's a pretty big business loss for your company. These are both extreme examples, but it's important that you always try to set an appropriate threshold for your model. Now that we've reviewed, let's start the lab.

#### How a machine learns

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565185

To understand machine learning, you must first understand how neural networks learn. This includes exploring this learning process and the terms associated with it. How do machines learn and how do they assess their learning? Before you dive into building an ML model, let's take a look at how a neural network learns. You may already know about the various neural networks such as deep neural networks or DNN, convolutional neural networks or CNN, recurrent neural networks or RNN, and more recently, large language models or LLMs. These networks are used to solve different problems. All these models stem from the most basic artificial neural network or ANN. ANNs are also referred to as neural networks or shallow neural networks. Let's focus on ANN to see how a neural network learns. An ANN has three layers, an input layer, a hidden layer, and an output layer. Each node represents a neuron. The lines between neurons stimulate synopsis, which is how information is transmitted in a human brain. For instance, if you input article titles from multiple resources, the neural network can tell you which media outlet or platform the article belongs to, such as GitHub, New York Times, and Tech Crunch. How does an ANN learn from examples and then make predictions? Let's examine how it works in depth. Let's assume you have two input neurons or nodes, one hidden neuron and one output neuron. Above the link between neurons are weights. The weights retain information that a neural network learned through the training process and are the mysteries that a neural network aims to discover. The first step is to calculate the weighted sum. This is done by multiplying each input value by its corresponding weight and then summing the products. It normally includes a bias component, BI. However, to focus on the core idea, ignore it for now. The second step is to apply an activation function to the weighted sum. What is an activated function and why do you need it? Let's pause your curiosity for just a moment and then get back to that soon. In the third step, the weighted sum is calculated for the output layer, assuming multiple neurons in the hidden layers. The fourth step is to apply an activation function to the weighted sum. This activation function can be different from the one applied to the hidden layers. The result is the predicted y, which consists of the output layer. You use y hat to represent the predicted result and y as the actual result. Now, let's get back to the activation functions. What does an activation function do? Well, an activation function is used to prevent linearity or add non-linearity. What does that mean? Think about a neural network. Without activation functions, the predicted result y hat will always be a linear function of the input x, regardless of the number of layers between input and output. Let's walk through this for clarity. Without the activation function, the value of the hidden layer h equals a total of W_1*X_1 and W_2*X_2. Please note that to make this illustration easy, we ignored the bias component B, which you often see in other ML materials. The output y hat, therefore, equals to W_3*h, and eventually equals to a total of constant number A*x_1 and a constant number b*x_2. In other words, the output y is a linear combination of the input x. If y is a linear function of x, you don't need all the hidden layers, but only one input and one output. You might already know that linear models do not perform well when handling comprehensive problems. That's why you must use activation functions to convert a linear network to a non-linear one. What are widely used activation functions? You can use the rectified linear unit or ReLU function, which turns an input value to zero if it's negative or keeps the original value if it's positive. You can use the sigmoid function, which turns the input to a value between zero and one, and hyperbolic tangent Tahn function, which shifts the sigmoid curve and generates a value between -1 and +1. Another interesting and important activation function is called Softmax. Think about Sigmoid. It generates a value 0-1 and is used for binary classification in logistic regression models. An example for this would be deciding whether an email is spam. What if you have multiple categories such as GitHub, New York Times, and Tech Crunch? Here you must use softmax, which is the activation function for multiclass classification. It maps each output to a zero, one range in a way that the total adds up to one. Therefore, the output of softmax is a probability distribution. Skipping the math, you can conclude that softmax is used for multiclass classification, whereas sigmoid is used for binary class classification in logistic regression models. Also, note that you don't need to have the same activation function across different layers. For instance, you can have ReLU for hidden layers and softmax for the output layer. Now that you understand the activation function and get a predicted y, how do you know if the result is correct? You use an assessment called loss function or cost function to measure the difference between the predicted y and the actual y. Loss function is used to calculate errors for a single training instance, whereas cost function is used to calculate errors from the entire training set. Therefore, in step 5, you calculate the cost function to minimize the difference. If the difference is significant, the neural network knows that it did a bad job in predicting and must go back to learn more and adjust parameters. Many different cost functions are used in practice. For regression problems, means squatted error or MSE is a common one used in linear regression models. MSE equals the average of the sum of squared differences between y hat and y. For classification problems, cross-entropy is typically used to measure the difference between the predicted and actual probability distributions in logistic regression models. If the difference between predicted and actual results is significant, you must go back to adjust weights and biases to minimize cost function. The potential sixth step is called back-propagation. The challenge now is how to adjust the weights. The solution is slightly complex, but indeed the most interesting part of a neural network. The idea is to take cost functions and turn them into a search strategy. That's where gradient descent comes in. Gradient descent refers to the process of walking down the surface formed by the cost function and finding the bottom. It turns out that the problem of finding the bottom can be divided into two different and important questions. The first is, which direction should you take? The answer involves the derivative. Let's say you start from the top left. You calculate the derivative of the cost function and find it's negative. This means the angle of the slope is negative and you are at the left side of the curve. To get to the bottom, you must go down and right. Then at one point, you are on the right side of the curve and you calculate the derivative again. This time, the value is positive, and you must slide again to the left. You calculate the derivative of the cost function every time to decide which direction to take. Repeat this process according to gradient descent, and you will eventually reach the regional bottom. The second question in finding the bottom is what size should the steps be? The step size depends on the learning rate, which determines the learning speed of how fast you bounce around to reach the bottom. Step size or learning rate is a hyperparameter that is set before training. If the step size is too small, your training might take too long. If the step size is too large, you might bounce from wall to wall or even bounce out of the curve entirely without converging. When step size is just right, you're set. The seventh and last step is iteration. One complete pass of the training process from step 1 to step 6 is called an epoch. You can set the number of epochs as a hyperparameter in training. Weights or parameters are adjusted until the cost function reaches its optimum. You can tell that the cost function has reached its optimum when the value stops decreasing even after many iterations. This is how a neural network learns. It iterates the learning by continuously adjusting weights to improve behavior until it reaches the best result. This is similar to a human learning lessons from the past. We have illustrated a simple example with two input neurons or nodes, one hidden neuron, and one output neuron. In practice, you might have many neurons in each layer, regardless of the number of neurons in the input, hidden, and output layer, the fundamental process of how a neural network learns remains the same. Learning about neural networks can be exciting, but also overwhelming with a large number of new terms. Let's take a moment to review them. In a neural network, weights and biases are parameters which are learned by the machine during training. You have no control of the parameters except to set the initial values. The number of layers and neurons, activation functions, learning rate, and epochs are hyperparameters, which are decided by a human before training. The hyperparameters determine how a machine learns. For example, the learning rate decides how fast a machine learns and the number of epochs defines how many times the learning iterates. Normally, data scientists choose the hyperparameters and experiment with them to find the optimum combination. However, if you use tools like AutoML, it automatically selects the hyperparameters for you and saves you plenty of experiment time. You also learned about cost or loss functions, which are used to measure the difference between the predicted and actual value. They are used to minimize error and improve performance. You use backpropagation to modify weights and bias if the difference is significant and gradient descent to decide how to tune the weights and bias and when to stop. These terms are your best friend when building an ML model. You'll revisit them in upcoming lessons and labs.

#### Vertex AI: Predicting Loan Risk with AutoML

- https://www.cloudskillsboost.google/paths/8/course_templates/593/labs/565186

#### Summary

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565187

This concludes the module on the AI development workflow. Let's do a quick recap. In this module, you learned about the three main stages of the machine learning workflow with the help of the restaurant analogy. In stage 1, data preparation, you uploaded data and applied feature engineering. This translated to gathering our ingredients and then chopping and prepping them in the kitchen. In stage 2, model development development, the model was trained and evaluated. This is where you experimented with the recipes and tasted the meal to ensure that it turned out as expected. And in the final stage model, serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end to end. One is through a user interface like you practiced in the AutoML lab, the other is with code, which you are shown using pre-built SDKs with Vertex AI pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training and delivery. We hope you enjoyed this module. Next, you'll advance to Generative AI, which is the most recent AI development that offers lots of exciting opportunities. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/593/quizzes/565188

#### Reading

- https://www.cloudskillsboost.google/paths/8/course_templates/593/documents/565189

### Generative AI

#### Introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565190

In the previous module, you learned how to develop an ML model from beginning to end. More specifically, you walked through the ML workflow and explored how to convert it into an automated pipeline. In this module, you learn about Generative AI, the most recent AI innovation that offers a wide range of exciting opportunities. You explore the generative AI tools that assist AI development and the integration of its capabilities into AI solutions. To begin, it is important to understand what generative AI or GenAI entails, how it functions, and how to create a GenAI project on Google Cloud. You then explore Gemini multimodal, one of the latest foundation models trained by Google, and how to use it with Vertex AI Studio, a primary tool for a developer to access and tune GenAI models. After that, you delve into the art and science of prompt design and a more advanced topic, model tuning. In addition to Vertex AI Studio, you can rely on Model Garden, a model library, to access different GenAI models. Finally, you explore how GenAI is embedded into the AI solutions like CCAI or contact center AI. With the GenAI tools and knowledge in hand, you conclude your learning journey with a hands-on lab using Vertex AI Studio to create prompts and conversations. Let's get started.

#### Generative AI and workflow

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565191

To begin, it is important to understand what generative AI or gen AI entails, how it functions, and how to create a gen AI project on Google Cloud. Imagine you are a marketing manager spending hours creating compelling content and distributing it through multiple channels, both traditional and new media. Or you're a data scientist writing SQL commands to run complex queries on various data sources. Or perhaps you're an app developer creating a chatbot in a specific field like healthcare. Well, generative AI can be your new go to friend for all of these tasks. Generative AI is transforming how we interact with technology. So what is generative AI? It is a type of artificial intelligence that generates content for you. What kind of content? Well, the generated content can be multimodal, including text, code, images, speech, video, and even 3D. When given a prompt or a request, generative AI can help you with a variety of tasks. Such as launching marketing campaigns, generating code, creating chatbots, extracting information, summarizing documents, and providing virtual assistance. And these are just some examples. Then how does AI generate new content? It learns from a massive amount of existing content such as text, image and video. The process of learning from existing content is called training, which results in the creation of a foundation model. A foundation model is usually a large model in the sense of a significant number of parameters, massive size of training data, and high requirements of computational power. Google has a long history innovating in the area of foundation models and gen AI technologies. Starting from Transformer in 2017, which lays the foundation for all modern gen AI applications seen today. To Gemini in 2023, which extends the word general in artificial general intelligence or AGI in the sense of handling multimodal processing. At present, the primary foundation models trained by Google include Gemini for multimodal processing, Gemma, a lightweight open model for language generation, Codey for code generation, and imagen for image processing. Please note that this list is subject to change as foundation models evolve. Additionally, Gemini has the potential to replace some of these models as it can process data across multiple modalities. A foundation model can then be used directly to generate content and solve general problems such as content extraction and document summarization. It can also be trained further with new datasets in your field to solve specific problems such as financial model generation and healthcare consulting. This results in the creation of a new model that is tailored to your specific needs. This leads to the next, pre-trained and fine-tuned. Meaning, to pre-train a foundation model for a general purpose with a large dataset, and then fine-tune it for specific aims with a much smaller dataset. Imagine training a dog. Often you train your dog basic commands such as sit, come, down, and stay. These commands are normally sufficient for general purposes in everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog or a guide dog or a hunting dog, you add special trainings. This similar idea applies to pre-trained versus fine-tuned models. For example, a large language model, which is a typical type of generative AI foundation model, is trained for general purposes to solve common language problems. Such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment by using relatively small datasets from these fields. Powered by the foundation models like large language models or LLMs, generative AI is driving new opportunities to enhance productivity, save operational costs, and create new values. You might have seen these opportunities from the use case about coffee on wheels in module one, where you use gen AI capabilities to automate the marketing campaign, generate customer feedback, and optimize truck route. This sounds exciting. How can you access gen AI models and create a gen AI project on Google Cloud? Recall your best friend Vertex AI, which is the end-to-end AI development platform supporting both predictive AI and generative AI. Aside from the capabilities you learned in previous modules for powering the experimentation, training, and implementation of predictive ML models. Vertex AI offers a range of tools for tuning your gen AI models and developing gen AI projects like Vertex AI Studio and Model Garden. Before you dive into these tools, let's walk through the gen AI workflow on Google Cloud. Input prompt, via the vertex AI studio UI input a prompt, a natural language request to gen AI models. Responsible AI and safety measures. The prompt undergoes responsible AI and safety checks configurable through the UI or code. Foundation models, the screened prompt proceeds to foundation models like Gemini, multimodal, or other gen AI models like Imagen and Codey, based on your choices. Model customization, optionally customize gen AI models to fit your data and use cases by further tuning them. Results grounding, gen AI models return results that undergo grounding and citation checks to prevent hallucinations. Final response, the final response appears on the Vertex AI Studio UI after a final check through responsible AI and safety measures. Lets focus on Vertex AI Studio, the interface to assess gen AI models and model garden, the repository of gen AI models in the following lessons.

#### Gemini multimodal

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565192

Let's explore Gemini multimodal, one of the latest foundation models trained by Google. In the previous lesson, you walked through the Gen AI workflow where Vertex AI studio provides an intuitive interface between developers and the foundational models. It enables you to build GenAI applications in a low code or even no-code environment, where you can rapidly test and prototype models, tune and customize models using your own data. Augment them with real world, up to date information deploy models efficiently in production environments with auto generated code. Let's uncover the capabilities of Gemini multimodal and delve into practical ways to access it with vertex AI Studio. So what is a multimodal model? It's a large foundation model that is capable of processing information from multiple modalities, including text, image, and video. The generated content can also be in multiple modalities. For example, you can send the model a photo of a plate of cookies and ask it to give you a recipe for those cookies. Gemini is such a multimodal model trained by Google, you can access or even tune it with vertex AI Studio. How can Gemini help you with your business use cases, Gemini excels at a diverse range of multimodal use cases. Here are some notable description and captioning, Gemini can identify objects in images and videos, providing detailed or concise descriptions as needed. Information extraction, it can read text from images and videos, extracting important information for further processing. Information analysis, it can analyze the information it extracts from images and videos based on specific prompts. For instance, it can classify expenses on a receipt. Information seeking, Gemini can answer questions or generate Q&A based on the information it extracts from text, images, and videos. Content creation, it can create stories or advertisements using images and videos as inspiration. Data conversion, it can convert text responses to various formats such as HTML and JSON. Can you think of a real world use case to apply Gemini multimodal? In light of these exciting advancements, how can developers engage with Gemini and create applications that leverage multimodal capabilities? There are three primary approaches, each essentially achieving the same objective, using a user interface or UI within the Google Cloud console. This no-code solution is ideal for exploring and testing prompts, using predefined SDKs in different languages like Python and Java, with notebooks like Colab and Workbench. Which are seamlessly integrated within the vertex AI platform, utilizing Gemini APIs in conjunction with command line tools like Curl. Regardless which method to access the Gemini multimodal, you start with a prompt. So what is a prompt? In the world of generative AI, a prompt is a natural language request submitted to a model in order to receive a response, you feed the desired input text like questions and instructions to the model. The model will then provide a response based on how you structured your prompt. Therefore, the answers you get depend on the questions you ask. Lets look at the anatomy of a prompt, which includes one or more of the following input, context, and examples. Input (required), an input represents your request for a response from the model. It can take various forms a question that the model can answer, which is a question input a task that the model can perform, which is a task input. An entity that the model can operate on, which is an entity input, partial input that the model can complete or continue, which is a completion input. Context (optional), a context can serve multiple specify instructions to guide the model's behavior. Provide information for the model to use or reference in generating a response. When you need to supply information or limit responses within the scope of your prompt, include contextual information. For instance, you could assume the role of an IT help desk and consistently advise users to restart their computers regardless of the nature of their inquiries. Examples (optional), examples are pairs of inputs and outputs that demonstrate the desired response format to the model. Incorporating examples in the prompt is an effective technique for tailoring the response format. For example, you can establish the context and assume the role of an IT help desk, and then you provide a few examples. This is called few shot prompting. Couldn't log in, suggest a password reset. Lost Internet connection, suggest checking devices like a modem or a router. Screen went black, suggest restarting the computer. Subsequently, when the question becomes what should I do when my computer freezes? The model may suggest restarting the computer. Context and examples are widely used when training or tuning a GenAI model to behave as you desire. The process of figuring out and designing the input text to get the desired response back from the model is called prompt design, which often involves a lot of experimentation. Youll learn more about prompt and prompt design later in this course. Lets explore the Gemini multimodal feature within vertex AI Studio. Navigate to the Vertex AI Studio overview page and click on multimodal powered by Gemini, try it now. You'll see three a prompt field at the top, a response field at the bottom, and a configuration panel on the right. Click insert media and upload an image. For example, let's use a departure board image from an airport. Enter your first prompt, read the text from the image. Before clicking submit let's look at the configuration on the right. You can choose from a list of models. The default model is usually the most recent cutting edge model, which is currently Gemini Pro vision. The temperature setting controls the degree of randomness in the response, with 0 being the most expected answer and 1 being the most creative. The safety settings allow you to adjust the likelihood of receiving a response that could contain harmful content. Content is blocked based on the probability of harmfulness. For example, for hate speech, you can choose from block few, block some, and block most. Adjust these settings based on your use case and click save. In the next lesson, you'll learn more about advanced settings such as Top-k and Top-p. Once you've completed the configuration, it's time to get the response. Click submit and wait a moment. Here's the result. If the result is not easy to read, you can further adjust your prompt to read the text from the image and put it into two columns, time and destination. Does the result look better? You can also be more adventurous and do some analysis. For example, you can change the prompt to calculate the percentage of the flights to different continents and put them in two columns, percentage and continent. Here is the result. If you desire to further develop the application and make the process productionalized, you can click the code located on the top right corner. There you'll find the code describing the prompt and the settings in the user interface. Alternatively, you can retrieve curl, which serves as the API to call in a command line interface. Additionally, you have the option to open a notebook with the SDK's code of your preferred programming languages such as Python. These automated generated coding and the integrated development environment significantly simplify the production process. Hopefully, this provides a straightforward guide on how to utilize Gemini multimodal with vertex AI Studio. At the end of this course, you practice with prompts and settings in a hands on practice. Enjoy learning.

#### Prompt design

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565193

Let's delve into the art and science of prompt design in this lesson, a fundamental skill for utilizing generative AI effectively. To get started experimenting with the GenAI models, click on new prompt. Let's start with a free form prompt. One way to design a prompt is to simply tell the model what you want. In other words, provide an instruction. For example, generate a list of items I need for a camping trip to Joshua Tree National Park. You send this text to the model and you can see that the model outputs a useful list of items you don't want to camp without. This approach of writing a single command so that the model can adopt a certain behavior is called zero-shot prompting. Generally, there are three methods that you can use to shape the model's response in a way that you desire. Zero-shot prompting is a method where the model is given a prompt that describes the task without additional examples. For example, if you want the LLM to answer a question, you just prompt, what is prompt design? One-shot prompting is a method where the LLM is giving a single example of the task that it is being asked to perform. For example, if you want the LLM to write a poem, you might provide a single example poem. Few-shot prompting is a method where the model is given a small number of examples of the task that it is being asked to perform. Recall the examples of an IT help desk in the previous lesson. You can use the structured mode to design the few-shot prompting by providing a context and additional examples for the model to learn from. The structured prompt contains a few more components. First, you have the context, which instructs how the model should respond. The context applies each time you send a request to the model. Let's say you want to use the model to answer questions about the changes in rainforest vegetation in the Amazon. You can paste this background text as the context. Then you add some examples of questions, such as, what does LGM stand for or what did the analysis from the sediment deposits indicate? You also need to include the corresponding answers to demonstrate how you want the model to respond. Then you can test the prompt you've designed by sending a new question as input. There you go. You've prototyped a Q&A system based on few-shot prompting in just a few minutes. Please note a few best practices around prompt design. The prompt needs to be concise, be specific and clearly defined. Ask one task at a time. Additionally, you can improve response quality by including examples. Adding instructions and a few examples tends to yield good results. However, there's currently no best way to write a prompt. You may need to experiment with different structures, formats, and examples to see what works best for your use case. For more information about prompt design, please check prompt design in the reading list. If you designed a prompt that you think is working pretty well, you can save it and return to it later. Your saved prompts will be visible in the prompt gallery, which is a curated collection of sample prompts that show how generative AI models can work for a variety of use cases. Finally, in addition to testing different prompts and prompt structures, there are a few model parameters you can experiment with to try to improve the quality of the responses. First, there are different models you can choose from. Each model is tuned to perform well on specific tasks. You can also specify the temperature, top P, and top K. These parameters all adjust the randomness of responses by controlling how the output tokens are selected. When you send a prompt to a model, it produces an array of probabilities over the words that could come next. From this array, you need some strategy to decide what it should return. A simple strategy might be to select the most likely word at every time step. But this method can result in uninteresting and sometimes repetitive answers. On the contrary, if you randomly sample over the distribution returned by the model, you might get some unlikely responses. By controlling the degree of randomness, you can get more unexpected, and some might say, creative responses. Back to the model parameters, temperature is the number used to tune the degree of randomness. Low temperature means to narrow the range of possible words to those that have high possibilities and are more typical. In this case, those are flowers and the other words that are located at the beginning of the list. This setting is generally better for tasks like questions and answers, and summarization, where you expect a more typical answer with less variability. High temperature. It means to extend the range of possible words to include those that have low possibility and are more unusual. In this case, those are bugs and other words that are located at the end of the list. This setting is good if you want to generate more creative or unexpected content, like an advertisement slogan. In addition to adjusting the temperature, top K lets the model randomly return a word from the top K number of words in terms of possibility. For example, top two means you get a random word from the top two possible words, including flowers and trees. This approach allows the other high scoring word a chance of being selected. However, if the probability distribution of the words is highly skewed and you have one word that is very likely and everything else is very unlikely, this approach can result in some strange responses. The difficulty of selecting the best top K-value leads to another popular approach that dynamically sets the size of the short list of words. Top P allows the model to return a random word from the smallest subset with the sum of the likelihoods that exceeds or equals to P. For instance, P(0.75) means you sample from a set of words that have a cumulative probability greater than 0.75. In this case, it includes three words, flowers, trees, and herbs. This way, the size of the set of words can dynamically increase and decrease according to the probability distribution of the next word on the list. In sum, Vertex AI Studio provides a few model parameters for you to play with, such as the model type, temperature, top K, and top P. Note that you are not required to adjust them constantly, especially top K and top P.

#### Model tuning

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565194

Having explored prompt design, a fundamental aspect of interacting with GenAI models, lets progress to a more advanced topic, model tuning. You explore different types of tuning and walk through a demo tuning a model with Vertex AI Studio. If you've been prototyping with the GenAI models like in LLM, you might wonder if there are ways to enhance the quality of responses and tailor them to specific domains beyond just prompt design. Here comes model tuning. Let's explore the different tuning methods and how to use Vertex AI studio to launch a tuning job. You have different choices to customize and tune a gen AI model, from less technical prompt design to more technical like distilling. Let's explore them one by one. You are already familiar with prompt design, which lets you tune a GenaI model using natural language without any ML background. The prompt is a request to a model for a desired outcome. To enhance the model's performance, you can provide context and examples to guide its responses. Prompt design does not alter the parameters of the pre trained model. Instead, it improves the model's ability to respond appropriately by teaching it how to react. One benefit of prompt design is that it enables rapid experimentation and customization. Another benefit is that it doesn't require specialized machine learning knowledge or coding skills, making it accessible to a wider range of users. However, producing prompts can be tricky. Small changes in wording or word order can affect the model results in ways that aren't totally predictable, and you cant really fit all that many examples into a prompt. Even when you do discover a good prompt for your use case, you might notice the quality of model responses isn't totally consistent. One way to address the issues is to tune the models using your own data. However, fine tuning the entire model can be impractical due to the high computational resources, cost, and time required. As the name large suggests, LLMs have a vast number of parameters, making it computationally demanding to update every weight. Instead, parameter efficient tuning can be employed. This involves making smaller changes to the model, such as training a subset of parameters or adding additional layers and embeddings. This approach has gained significant attention in the research community as it aims to reduce the challenges of fine tuning large language models by only training a subset of parameters. For example, you can have adapter tuning, which is supervised tuning. It lets you use as few as 100 examples to improve model performance. Reinforcement tuning, which is unsupervised reinforcement learning with human feedback. Distillation, a more technical tuning technique, enables training smaller task specific models with less training data and lower serving costs and latency than the original model. This technique is exclusive to Google Cloud. Through Vertex AI, you can access the newest techniques from Google research available with distilling step-by-step. This technology transfers knowledge from a larger model to a smaller model to optimize performance latency and cost. The rationale is to use a larger teacher model that trains smaller student models to perform specific tasks better and with improved reasoning capabilities. The training and distilling process uses labeled examples and rationales generated by the teacher model to fine tune the student model. Rationales are like asking the model to explain why examples are labeled the way they are. Similar to how you learn better by understanding the reason behind an answer, this type of teaching makes the student model more accurate and robust. Now, let's move to Vertex AI studio and see how to start a tuning job. Please note that the UI may change as the product progresses. From the language section of Vertex AI Studio, tune and distil, select Create Tuned Model. For model details, you can choose from either supervised tuning, which is the adapter tuning as mentioned earlier, or unsupervised tuning, which is reinforcement tuning. Give the tuned model a name, choose the base model, the region, and the output directory. Parameter efficient tuning is ideally suited for scenarios where you have modest amounts of training data. The number of examples can be as low as ten. However, it's recommended to have 100 training examples for better performance. Tuning dataset is where you specify the location of your training dataset. Please note, it needs to be on cloud storage. Your training data should be structured as a supervised training dataset in a JSON L file. Each record or row contains a pair of text data, the input text, which is the prompt, and the output text, which is the expected response from the model. This structure allows the model to learn and adapt to your desired behavior. You can then start the tuning job and monitor the status on the Google Cloud console. When the tuning job completes, you see the tuned model in the vertex AI model registry, and you can deploy it to an endpoint for serving or further test it in vertex AI studio.

#### Model Garden

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565195

In addition to Vertex AI studio, Model Garden with Vertex AI offers access to a wide range of gen AI models not limited to those developed by Google. Model Garden is like a model library where you can search, discover and interact with Google's third parties and open source gen AI models. When you want to build, train and fine tune gen AI models in a comprehensive environment supported by both you, UI and coding, Vertex AI studio can be a good start point. However, if you want to quickly find and apply the right model to solve a problem, model garden is a good choice. Model Garden offers a model card for each model, encapsulating critical details such as overview, use cases and relevant documentation. Furthermore, it seamlessly integrates with Vertex AI studio, allowing you to initiate project development through a user friendly interface. Additionally, it provides access to sample code and facilitates code development via notebooks. You can find three major categories of models, foundation models, tasks-specific solutions, and fine-tunable or open source models. Foundation models are pr trained multitask large models that can be tuned or customized for specific tasks by using vertex AI studio, vertex AI APIs and SDKs. Some of these models are Gemini for multimodal processing, embeddings for creating text, and multimodal embeddings imagin for image, chirp for speech, and Codey for cogeneration. Recall that you learned the APIs for these models in the previous lesson when you explored pretrained APIs. Task specific solutions are pretrained models which are optimized to solve a specific problem. These include some tasks you practiced in the previous section by using natural language APIs like entity analysis, sentiment analysis, and syntax analysis. More interesting tasks like object detector and text translation are also task specific solutions. And finally, there are fine-tunable models. These are mostly open source models that you can fine-tune by using a custom notebook or pipeline. To find the model that best suits your requirements you can also use three filters in your search. They are modalities such as language, vision and speech, tasks like generation, classification and detection, and features such as pipeline, notebook, and one-click deployment support. With all these models to choose from, how can you start your workflow? Model garden lets you use Google's foundation models directly through the Google Cloud console with Vertex AI studio or through code with prebuilt APIs. Tune models in generative AI studio, deploy models, customize and use open source models. Let's walk through a use case of using model garden. Assume you are interested in computer vision from the model garden page let's filter for vision related models. Next in the tasks section, select detection. Great, the search results shows there's an owl vision transformer model, an open source model. It says it's a zero-shot, text-condition object detection model that can query an image with one or multiple text queries. Let's dive into this. Here you see the model card, where you can see more details. As a data scientist, you might want to try using this model, so you click on open notebook. This opens up a colab notebook for the Owl Vision transformer model. This notebook in particular shows how you can deploy the model to an endpoint on Vertex AI. Then send an image to the endpoint to get a prediction, which is the text caption describing what's in the image. As this example illustrates, the flow from finding a model to deploying and using it becomes super easy with model garden.

#### AI solutions

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565196

You learned about Vertex AI Studio and Model Garden the tools provided by Vertex AI to access Gen AI models and develop Gen AI projects. How is Gen AI embedded in AI solutions? You'll explore more in this lesson. AI solutions include both vertical and horizontal solutions. Vertical or industry solutions refer to solutions that solve specific problems within certain industries. Examples include Healthcare Data Engine, which generates healthcare insights and provides services across patients, doctors, and hospitals, and Vertex AI Search for Retail, which enables retailers to provide Google quality search and recommendations on their own digital property, which can help increase conversion rate and reduce search abandonment. Horizontal solutions in contrast, usually refer to solutions that solve similar problems across different industries. For instance, contact center AI or CCAI aims to improve customer service in contact centers through the use of artificial intelligence. It can help automate simple interactions, assist human agents, and unlock caller insights. Document AI uses computer vision, optical character recognition, and natural language processing to create pre-trained models that can extract information from documents. This increases the speed and accuracy of document processing, which can help organizations make better decisions faster and reduce costs. Let's dive into CCAI, explore its main features and learn how generative AI supercharges them. Contact Center artificial intelligence is Google's solution to apply AI in contact centers. The goal is to use AI to increase the customer satisfaction and the operational efficiency while requiring minimum AI expertise. At the center of contact center AI is the conversational core, which empowers three major components; virtual agent, agent assist, and insights. Let's briefly look at how contact center AI works. When a customer contacts contact center AI, the system automatically determines whether the request is routine or complex. Routine requests are passed to a virtual agent, a large language model powered bot that can converse naturally with customers. The virtual agent may then process the request and pass it to back end fulfillment. Complex requests are sent to the agent assist, an AI assistant that helps human agents summarize the problem, gather information, provide solutions, and generate insights. Relevant information and insights can also be saved to the knowledge base throughout the process. Now, let's look at how generative AI is used to transform the Contact Center AI Platform. Today, the Contact Center AI Platform is an end to end AI first call center as a service solution that is integrated with a set of pre built tools and features, including virtual agent, agent assist and insights. The platform will soon be upgraded with natural language understanding powered call and chat, as well as ML driven routing. This means that the platform will be able to understand what customers are saying and direct them to the appropriate person or department. It will also be able to learn from the interactions and improve its routing over time. Today, a contact center AI virtual agent provides 24/7 customer self service. It provides a conversational voice and chat bot that a customer can get help from any time using natural language. It also integrates the function to handoff calls to live agents. Through generative AI, the bots will be faster and easier to use. They will also be able to handle a wider range of customer interactions, answer customer questions in a more comprehensive and accurate way and engage in natural conversations with customers. Today, AI agent assist empowers live agents with step by step help during calls and chats. It also provides agents an automated summary of every interaction afterwards. In the near future, the agent assist will be enhanced with a generative AI feature that can coach live agents on demand. Contact Center AI insights currently use natural language processing to analyze conversations such as sentiment analysis, entity identification, and topic detection. These insights can be used to improve the performance of virtual and live agents. They can also be used to create training materials for agents and to identify areas where customer service can be improved. Soon, insights will generate FAQs automatically to improve customer experience with generative AI capabilities. You can learn more about Google Cloud's growing list of AI solutions and their embedded generative AI capabilities at cloud.google.com/solutions/ai. With the rapid development of AI and machine learning, you might hear exciting news every day. What do you think about the future of AI and machine learning? Here are a few of our expectations. The transition from data to AI is inevitable. Data is the fuel that powers AI, and AI extracts insights and possibilities from data. As AI becomes increasingly powerful and capable, it will learn and adapt in ways that data alone cannot. As a result, AI is becoming more and more essential for businesses that want to stay ahead of the competition. Generative AI becomes more important. It will be used to produce content and improve productivity, which will create new opportunities and offer great potential. These opportunities will be more accessible to all as AI and ML development tools become easier to use, which allows people without a technical background to benefit from their capabilities. What do you think, dear learners?

#### Lab introduction

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565197

With a grasp of the gen AI concepts, it's time to play with Vertex AI studio in a hands-on lab where you analyze images with Gemini multimodal. Explore multimodal capabilities. Design prompts with free form and structured mode. Generate conversations. By the end of this lab, you will be able to use the capabilities of Vertex AI studio discussed in this course. Have fun exploring.

#### Get Started with Vertex AI Studio

- https://www.cloudskillsboost.google/paths/8/course_templates/593/labs/565198

#### Summary

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565199

This concludes the last module on generative AI. Let's do a quick recap. You began by understanding how Gen AI works. Through extensive training on vast amounts of data, foundation models like large language models or LLMs are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the Gen AI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI; Vertex AI Studio, which is the interface between developers and back end Gen AI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune Gen AI models to solve your business problems, and Model Garden, which is a model library, helps you search, discover, and interact with Google and open source Gen AI models. You then explored vertical and horizontal AI solutions and their embedded generative AI capabilities. You concluded with a hands on lab where you used vertex AI Studio to create prompts and conversations. We hope you enjoyed this module and feel motivated to use generative AI in your projects.

#### Quiz

- https://www.cloudskillsboost.google/paths/8/course_templates/593/quizzes/565200

#### Reading

- https://www.cloudskillsboost.google/paths/8/course_templates/593/documents/565201

### Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/8/course_templates/593/video/565202

Congratulations on completing the course Introduction to AI and Machine Learning on Google Cloud. Regardless of your role, an AI developer, data scientist, ML engineer, or simply a passionate AI and machine learning enthusiast, we trust that this course has provided you with valuable insights to enhance your career. Before the final review, let's take a moment to reflect on what you've learned so far. Can you come up with 3-5 keywords and group them together in a way that makes sense to you? Throughout this course, we aim to help you navigate a comprehensive toolbox provided by Google Cloud so you can effectively utilize these tools and resources to build an AI and ML project. Let's walk through the main concepts. You started from AI Foundations, where you focused on Cloud essentials and data tools. You began with Google Cloud infrastructure. Of the three layers, you explored the middle and top layers. On the middle layer sit compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. On the top layer sit data and AI products which enable you to perform tasks to support the data to AI journey. From data ingestion, storage, and analytics to AI and machine learning. After that, you advance to the fundamental ML concepts, including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. By grasping these concepts, you can make an informed decision when selecting an ML model. You then learned the steps to build an ML model by using BigQuery, Google's widely used data warehouse. Finally, you had a hands-on lab where you applied these steps to build your own ML models with SQL commands. In the second module, you advanced to AI development. Specifically, you learned about the options available to build an ML model. You completed three options from a ready-made approach to low and no-code to do it yourself. You started with pre-trained APIs which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google's unified platform to build a machine learning project from end-to-end. You learned about AutoML, which is a low or no-code tool on Vertex AI and lets you build the ML model with your own data through the Google Cloud Console. You also explored custom training, which is a code-based solution on Vertex AI. It allows you to control the working environment and automate the ML workflow by using programming tools that you're familiar with, such as Python, TensorFlow, and Notebooks. Finally, you practiced with a hands-on lab using the Natural Language API to identify subjects and analyze sentiment in text. With a grasp of the various options for developing an ML project, you are now ready to build your own ML model. In the third module, you walked through the ML workflow and explored how to create an automated pipeline. You began with the three main stages of the machine learning workflow with the help of the restaurant analogy. In Stage 1, data preparation, you uploaded d ta and applied feature engineering. This translates to gathering your ingredients and then prepping them in the kitchen. In Stage 2, model development, the model was trained and evaluated. This is where you experiment with the recipes and taste the meal to ensure that it turned out as expected. In the final stage, model serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end-to-end with Vertex AI. One is through a user interface like you practiced in the AutoML lab. The other is with code, which you were shown using pre-built SDKs with Vertex AI pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training, and delivery. In the end, you completed a lab using AutoML on Vertex AI, where you built an ML model to predict loan risk through the Google Cloud Console. In the final module, you were introduced to the most recent development in AI, generative AI. You explored the GenAI development tools and the integration of the GenAI capabilities into AI solutions. You began by understanding how GenAI works through extensive training on vast amounts of data foundation models like large language models, LLMs, are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the GenAI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI Studio, which is the interface between developers and backend GenAI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune GenAI models to solve your business problems. Model Garden, which is a model library helps you search, discover, and interact with Google and open-source GenAI models. You then explored vertical and horizontal AI solutions and their embedded GenAI capabilities. You concluded with a hands-on lab where you used Vertex AI Studio to create prompts and conversations. We hope that this course is the start of your journey into AI and machine learning. Remember what we suggested at the beginning of the course: apply what you've learned to your own work. This is the best way to develop your skills as an AI practitioner. For more training with machine learning and AI, please explore the options available at cloud.google.com/training/machinelearning-ai. If you are interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you may want to consider pursuing a Google Cloud certification. You can learn more about Google Cloud's certification offerings at cloud.google.com/certifications. Thanks for completing this course. We'll see you next time.

#### Reading

- https://www.cloudskillsboost.google/paths/8/course_templates/593/documents/565203

### Your Next Steps

## 06: Prepare Data for ML APIs on Google Cloud

- https://www.cloudskillsboost.google/paths/8/course_templates/631

### Prepare Data for ML APIs on Google Cloud

#### Dataprep: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550121

#### Dataflow: Qwik Start - Templates

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550122

#### Dataflow: Qwik Start - Python

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550123

#### Dataproc: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550124

#### Dataproc: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550125

#### Cloud Natural Language API: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550126

#### Speech-to-Text API: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550127

#### Video Intelligence: Qwik Start

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550128

#### Prepare Data for ML APIs on Google Cloud: Challenge Lab

- https://www.cloudskillsboost.google/paths/8/course_templates/631/labs/550129

### Your Next Steps


# [Game] The Arcade Certification Zone June 2024

[TOC]

- https://www.cloudskillsboost.google/games/5178

## 2024-06-17

### Tagging Dataplex Assets

- https://www.cloudskillsboost.google/games/5178/labs/33840
- GSP1145 

#### Task 1. Create a lake, zone, and asset

```
Dataplex is an intelligent data fabric that enables organizations to centrally discover, manage, monitor, and govern their data across data lakes, data warehouses, and data marts to power analytics at scale.

Data Catalog is a fully managed, scalable metadata management service within Dataplex that you can use to tag data assets and search for assets to which you have access.
```
* dataplex - create lake
```
NAME
    gcloud dataplex lakes create - create a Dataplex lake resource

SYNOPSIS
    gcloud dataplex lakes create (LAKE : --location=LOCATION) [--async]
        [--description=DESCRIPTION] [--display-name=DISPLAY_NAME]
        [--labels=[KEY=VALUE,...]] [--metastore-service=METASTORE_SERVICE]
        [--validate-only] [GCLOUD_WIDE_FLAG ...]
```
```
student_02_38b1b15b5e5c@cloudshell:~ (qwiklabs-gcp-00-a7c2c4c89354)$ gcloud dataplex lakes list --location=us-central1
NAME: orders-lake
DISPLAY_NAME: Orders Lake
LAKE_STATUS: ACTIVE
METASTORE_STATUS: NONE
METASTORE: 
SECURITY_STATUS: 
ACTIVE_ASSETS: 
LABELS: 
```
* Dataplex - Lake - Add zone
```
EXAMPLES
    To create a Dataplex zone with name test-zone within lake test-lake in
    location us-central1 with type RAW, and resource location type
    SINGLE_REGION, run:

        $ gcloud dataplex zones create test-zone --location=us-central \
            --lake=test-lake --resource-location-type=SINGLE_REGION \
            --type=RAW
```
* Attach an asset to a zone
```
EXAMPLES
    To create a Dataplex asset with name test-asset, within zone test-zone, in
    lake test-lake, in location us-central1, with resource type STORAGE_BUCKET,
    with resource name projects/test-project/buckets/test-bucket, run:

        $ gcloud dataplex assets create test-asset --location=us-central \
            --lake=test-lake --zone=test-zone \
            --resource-type=STORAGE_BUCKET \
            --resource-name=projects/test-project/buckets/test-bucket
```
#### Task 2. Create a tag template

#### Task 3. Apply a tag template to Dataplex assets

#### Task 4. Search for assets using tags

### Implementing Security in Dataplex

- https://www.cloudskillsboost.google/games/5178/labs/33841
- GSP1157

#### Task 1. Create a lake, zone, and asset in Dataplex

- 這個練習蠻特別的，有兩個使用者帳號。要先用 User 1 具有 Admin 權限的來建立 Dataplex Lake

#### Task 2. Assign Dataplex Data Reader role to another user

- 從 Dataplex > Secure 將 User 2 加入 Dataplex Data Reader 權限

#### Task 3. Test access to Dataplex resources as a Dataplex Data Reader

- 登出 User 1，改登入 User 2
- 測試上傳 test.csv，但會遇到錯誤。因為 Reader 權限無法上傳到 GCS Bucket

#### Task 4. Assign Dataplex Writer role to another user

- 登出 User 2，改登入 User 1 用管理者身份
- 替 User 2 加上 Writer 權限後，登出 User 1。

#### Task 5. Upload new file to Cloud Storage bucket as a Dataplex Data Writer

- 登出 User 1，改登入 User 2
- 上傳 test.csv 到 GCS Bucket

### Hello Cloud Run

- https://www.cloudskillsboost.google/games/5178/labs/33847

#### Task 1. Enable the Cloud Run API and configure your Shell environment

```bash
student_04_a62d2c4e3939@cloudshell:~ (qwiklabs-gcp-03-f01d32800f97)$ gcloud services enable run.googleapis.com
Operation "operations/acf.p2-685951511411-b6ac0947-b0fc-434a-a6f7-74357c944786" finished successfully.
student_04_a62d2c4e3939@cloudshell:~ (qwiklabs-gcp-03-f01d32800f97)$ gcloud config set compute/region us-east4
WARNING: Property validation for compute/region was skipped.
Updated property [compute/region].
student_04_a62d2c4e3939@cloudshell:~ (qwiklabs-gcp-03-f01d32800f97)$ LOCATION="us-east4"
```
#### Task 2. Write the sample application

```bash
student_04_a62d2c4e3939@cloudshell:~ (qwiklabs-gcp-03-f01d32800f97)$ mkdir helloworld && cd helloworld
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ cat >> package.json << EOF
{
  "name": "helloworld",
  "description": "Simple hello world sample in Node",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "start": "node index.js"
  },
  "author": "Google LLC",
  "license": "Apache-2.0",
  "dependencies": {
    "express": "^4.17.1"
  }
}
EOF
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ cloudshell open index.js
```
- 貼上範例程式：（因為裡面有特殊符號，所以沒辦法直接用 cat 指令產生）

#### Task 3. Containerize your app and upload it to Artifact Registry

```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ cat >> Dockerfile << EOF
# Use the official lightweight Node.js 12 image.
# https://hub.docker.com/_/node
FROM node:12-slim

# Create and change to the app directory.
WORKDIR /usr/src/app

# Copy application dependency manifests to the container image.
# A wildcard is used to ensure copying both package.json AND package-lock.json (when available).
# Copying this first prevents re-running npm install on every code change.
COPY package*.json ./

# Install production dependencies.
# If you add a package-lock.json, speed your build by switching to 'npm ci'.
# RUN npm ci --only=production
RUN npm install --only=production

# Copy local code to the container image.
COPY . ./

# Run the web service on container startup.
CMD [ "npm", "start" ]
EOF
```
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld
Creating temporary archive of 3 file(s) totalling 1.3 KiB before compression.
```
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud container images list
NAME: gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld
Only listing images in gcr.io/qwiklabs-gcp-03-f01d32800f97. Use --repository to list images in other repositories.
```
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud auth configure-docker
WARNING: Your config file at [/home/student_04_a62d2c4e3939/.docker/config.json] contains these credential helper entries:
```
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ docker run -d -p 8080:8080 gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ curl localhost:8080
Hello World!
```
#### Task 4. Deploy to Cloud Run
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud run deploy --image gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld --allow-unauthenticated --region=$LOCATIONon=$LOCATION
Service name (helloworld):  
Deploying container to Cloud Run service [helloworld] in project [qwiklabs-gcp-03-f01d32800f97] region [us-east4]
-  Deploying new service...
```
* deploy success - check "Service URL"
```                
Service [helloworld] revision [helloworld-00001-g25] has been deployed and is serving 100 percent of traffic.
Service URL: https://helloworld-emx4e3t5ja-uk.a.run.app
```
#### Task 5. Clean up
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud container images delete gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld
WARNING: Implicit ":latest" tag specified: gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld
WARNING: Successfully resolved tag to sha256, but it is recommended to use sha256 directly.
Digests:
- gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld@sha256:c46cb85aea13681f87777d8ceda932b2a8a7d7a6ed478d563af04614f259ec0b
  Associated tags:
 - latest
Tags:
- gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld:latest
This operation will delete the tags and images identified by the digests above.

Do you want to continue (Y/n)?  

Deleted [gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld:latest].
Deleted [gcr.io/qwiklabs-gcp-03-f01d32800f97/helloworld@sha256:c46cb85aea13681f87777d8ceda932b2a8a7d7a6ed478d563af04614f259ec0b].
```
```bash
student_04_a62d2c4e3939@cloudshell:~/helloworld (qwiklabs-gcp-03-f01d32800f97)$ gcloud run services delete helloworld --region=us-east4
Service [helloworld] will be deleted.

Do you want to continue (Y/n)?  

Deleting [helloworld]...done.                                                                                                                                                       
Deleted service [helloworld].
```

## 2024-06-18

### Assessing Data Quality with Dataplex

- https://www.cloudskillsboost.google/games/5178/labs/33842
- GSP1158

- enable "Cloud Dataproc API"
- https://console.cloud.google.com/marketplace/product/google/dataproc.googleapis.com

#### Task 1. Create a lake, zone, and asset in Dataplex

- 同上面的步驟，創建 Dataplex Lake, Zone 跟 Asset
- create lake
  - https://console.cloud.google.com/dataplex/lakes
- create Zone
  - https://console.cloud.google.com/dataplex/lakes/ecommerce-lake/create-zone;location=us-east4 (take #1)
  - https://console.cloud.google.com/dataplex/lakes/ecommerce-lake/create-zone;location=us-east1 (take #2)
- load asset
  - https://console.cloud.google.com/dataplex/lakes/ecommerce-lake/zones/customer-contact-raw-zone/create-asset;location=us-east4 (take #1)
  - https://console.cloud.google.com/dataplex/lakes/ecommerce-lake/zones/customer-contact-raw-zone/create-asset;location=us-east1 (take #2)

#### Task 2. Query a BigQuery table to review data quality

- https://console.cloud.google.com/bigquery

#### Task 3. Create and upload a data quality specification file

```bash
student_00_2f1841606671@cloudshell:~ (qwiklabs-gcp-00-495faa914406)$ cat >> dq-customer-raw-data.yaml << EOF                                                                         
metadata_registry_defaults:
  dataplex:
    projects: qwiklabs-gcp-00-495faa914406
    locations: us-east4
    lakes: ecommerce-lake
    zones: customer-contact-raw-zone
row_filters:
  NONE:
    filter_sql_expr: |-
      True
  INTERNATIONAL_ITEMS:
    filter_sql_expr: |-
      REGEXP_CONTAINS(item_id, 'INTNL')
rule_dimensions:
  - consistency
  - correctness
  - duplication
  - completeness
  - conformance
  - integrity
  - timeliness
  - accuracy
rules:
  NOT_NULL:
    rule_type: NOT_NULL
    dimension: completeness
  VALID_EMAIL:
    rule_type: REGEX
    dimension: conformance
    params:
      pattern: |-
        ^[^@]+[@]{1}[^@]+$
rule_bindings:
  VALID_CUSTOMER:
    entity_uri: bigquery://projects/qwiklabs-gcp-00-495faa914406/datasets/customers/tables/contact_info
    column_id: id
    row_filter_id: NONE
    rule_ids:
      - NOT_NULL
  VALID_EMAIL_ID:
    entity_uri: bigquery://projects/qwiklabs-gcp-00-495faa914406/datasets/customers/tables/contact_info
    column_id: email
    row_filter_id: NONE
      - VALID_EMAIL
EOF
```
```bash
student_00_2f1841606671@cloudshell:~ (qwiklabs-gcp-00-495faa914406)$ gsutil cp dq-customer-raw-data.yaml gs://qwiklabs-gcp-00-495faa914406-bucket
Copying file://dq-customer-raw-data.yaml [Content-Type=application/octet-stream]...
- [1 files][  1.0 KiB/  1.0 KiB]                                                
Operation completed over 1 objects/1.0 KiB.     
```
(2024-06-18 18:57:44) -- Take #2
```bash
student_01_c30b3e2c1274@cloudshell:~ (qwiklabs-gcp-01-838ffef09f90)$ cloudshell open dq-customer-raw-data.yaml
student_01_c30b3e2c1274@cloudshell:~ (qwiklabs-gcp-01-838ffef09f90)$ gsutil cp dq-customer-raw-data.yaml gs://qwiklabs-gcp-01-838ffef09f90-bucket
Copying file://dq-customer-raw-data.yaml [Content-Type=application/octet-stream]...
- [1 files][  1.0 KiB/  1.0 KiB]                                                
Operation completed over 1 objects/1.0 KiB.         
```

#### Task 4. Define and run a data quality job in Dataplex

- https://console.cloud.google.com/dataplex/process/create-task/data-quality

#### Task 5. Review data quality results in BigQuery

- [狀況] 由於 `failed_records_query` 的 Query String 太長，所以在 PREVIEW 時，無法複製完整的字串。
- [解法] 另外 Query 一次，只查 `failed_records_query` 並將 Result 存成 CSV。才能順利複製完整的 `failed_records_query` 字串。

#### 備忘：

<mark>Q: 如果用 Terraform 來做 Dataplex 的部署，該如何等待 Lake, Zone 被 create 呢？ Data Quality Job 也需要時間與驗證方式，能夠用 Terraform 做嗎？</mark>

### Streaming Data to Bigtable

- https://www.cloudskillsboost.google/games/5178/labs/33843
- GSP1055

#### Prerequisites

- 要另外完成 "Designing and Querying Bigtable Schemas"，但已經找不到該 Lab

#### Task 1. Create a Bigtable instance and table using commands

(2024-06-18 19:22:53)
```bash
Welcome to Cloud Shell! Type "help" to get started.
To set your Cloud Platform project in this session use “gcloud config set project [PROJECT_ID]”
student_03_f65d165cc39f@cloudshell:~$ gcloud auth list
Credentialed Accounts

ACTIVE: *
ACCOUNT: student-03-f65d165cc39f@qwiklabs.net

To set the active account, run:
    $ gcloud config set account `ACCOUNT`

student_03_f65d165cc39f@cloudshell:~$ gcloud config set account student-03-f65d165cc39f@qwiklabs.net
Updated property [core/account].
student_03_f65d165cc39f@cloudshell:~$ gcloud config set project qwiklabs-gcp-01-7b92655a3976
Updated property [core/project].
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ 
```

##### Create a Bigtable instance

```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ gcloud bigtable instances create sandiego \
--display-name="San Diego Traffic Sensors" \
--cluster-storage-type=SSD \
--cluster-config=id=sandiego-traffic-sensors-c1,zone=us-east1-b,nodes=1

Creating bigtable instance sandiego...done. 
```

##### Configure the Bigtable CLI

```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ echo project = `gcloud config get-value project` \
    >> ~/.cbtrc
Your active configuration is: [cloudshell-25750]

student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cat .cbtrc 
project = qwiklabs-gcp-01-7b92655a3976

student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ echo instance = sandiego \
    >> ~/.cbtrc

student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cat .cbtrc 
project = qwiklabs-gcp-01-7b92655a3976
instance = sandiego
```

##### Create a Bigtable table with column families

```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cbt createtable current_conditions \
    families="lane"
2024/06/18 11:34:17 -creds flag unset, will use gcloud credential
```

#### Task 2. Simulate streaming traffic sensor data into Pub/Sub

- https://console.cloud.google.com/compute/instances

```bash
student-03-f65d165cc39f@training-vm:~$ ls /training
bq_magic.sh  project_env.sh  sensor_magic.sh
```

```bash
student-03-f65d165cc39f@training-vm:~$ git clone https://github.com/GoogleCloudPlatform/training-data-analyst
Cloning into 'training-data-analyst'...
remote: Enumerating objects: 64987, done.
remote: Counting objects: 100% (124/124), done.
remote: Compressing objects: 100% (84/84), done.
remote: Total 64987 (delta 56), reused 89 (delta 38), pack-reused 64863
Receiving objects: 100% (64987/64987), 697.23 MiB | 34.26 MiB/s, done.
Resolving deltas: 100% (41498/41498), done.
student-03-f65d165cc39f@training-vm:~$ source /training/project_env.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    28  100    28    0     0   7646      0 --:--:-- --:--:-- --:--:--  9333
qwiklabs-gcp-01-7b92655a3976
student-03-f65d165cc39f@training-vm:~$ /training/sensor_magic.sh
Copying gs://cloud-training-demos/sandiego/sensor_obs2008.csv.gz...
/ [1 files][ 34.6 MiB/ 34.6 MiB]                                                
Operation completed over 1 objects/34.6 MiB.                                     
INFO: Creating pub/sub topic sandiego
INFO: Sending sensor data from 2008-11-01 00:00:00
INFO: Publishing 477 events from 2008-11-01 00:00:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:05:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:10:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:15:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:20:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:25:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:30:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:35:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 00:40:00
INFO: Sleeping 5.0 seconds
```

#### Task 3. Launch a Dataflow pipeline to write data from Pub/Sub into Bigtable

* 備忘：原來 GCE 的 Web 版 SSH 還可以開多個 Session

##### Launch a Dataflow Pipeline

```
student-03-f65d165cc39f@training-vm:~$ source /training/project_env.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    28  100    28    0     0   9204      0 --:--:-- --:--:-- --:--:-- 14000
qwiklabs-gcp-01-7b92655a3976
student-03-f65d165cc39f@training-vm:~$ cd ~/training-data-analyst/courses/streaming/process/sandiego
student-03-f65d165cc39f@training-vm:~/training-data-analyst/courses/streaming/process/sandiego$ cat run_oncloud.sh
#!/bin/bash

if [ "$#" -lt 3 ]; then
   echo "Usage:   ./run_oncloud.sh project-name bucket-name classname [options] "
   echo "Example: ./run_oncloud.sh cloud-training-demos cloud-training-demos CurrentConditions --bigtable"
   exit
fi

PROJECT=$1
shift
BUCKET=$1
shift
MAIN=com.google.cloud.training.dataanalyst.sandiego.$1
shift

echo "Launching $MAIN project=$PROJECT bucket=$BUCKET $*"

export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:$PATH
mvn compile -e exec:java \
 -Dexec.mainClass=$MAIN \
      -Dexec.args="--project=$PROJECT \
      --stagingLocation=gs://$BUCKET/staging/ $* \
      --tempLocation=gs://$BUCKET/staging/ \
      --region=$REGION \
      --workerMachineType=e2-standard-2 \
      --runner=DataflowRunner"


# If you run into quota problems, add this option the command line above
#     --maxNumWorkers=2 
# In this case, you will not be able to view autoscaling, however.
student-03-f65d165cc39f@training-vm:~/training-data-analyst/courses/streaming/process/sandiego$ ./run_oncloud.sh $DEVSHELL_PROJECT_ID $BUCKET CurrentConditions --bigtable
```

##### Explore the Dataflow pipeline

- https://console.cloud.google.com/dataflow/jobs

#### Task 4. Verify streaming data loaded into Bigtable

```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cbt read current_conditions count=5 \
    columns="lane:.*"9f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cbt read current_conditions count=5 \
2024/06/18 11:57:16 -creds flag unset, will use gcloud credential
----------------------------------------
15#S#1#9223370811321175807
  lane:direction                           @ 1970/01/15-04:25:33.600000
    "S"
  lane:highway                             @ 1970/01/15-04:25:33.600000
    "15"
  lane:lane                                @ 1970/01/15-04:25:33.600000
    "1.0"
  lane:latitude                            @ 1970/01/15-04:25:33.600000
    "32.723248"
  lane:longitude                           @ 1970/01/15-04:25:33.600000
    "-117.115543"
  lane:sensorId                            @ 1970/01/15-04:25:33.600000
    "32.723248,-117.115543,15,S,1"
  lane:speed                               @ 1970/01/15-04:25:33.600000
    "72.0"
  lane:timestamp                           @ 1970/01/15-04:25:33.600000
    "2008-11-01 10:00:00"

----------------------------------------
15#S#1#9223370811321475807
  lane:direction                           @ 1970/01/15-04:25:33.300000
    "S"
  lane:highway                             @ 1970/01/15-04:25:33.300000
    "15"
  lane:lane                                @ 1970/01/15-04:25:33.300000
    "1.0"
  lane:latitude                            @ 1970/01/15-04:25:33.300000
    "32.723248"
  lane:longitude                           @ 1970/01/15-04:25:33.300000
    "-117.115543"
  lane:sensorId                            @ 1970/01/15-04:25:33.300000
    "32.723248,-117.115543,15,S,1"
  lane:speed                               @ 1970/01/15-04:25:33.300000
    "72.1"
  lane:timestamp                           @ 1970/01/15-04:25:33.300000
    "2008-11-01 09:55:00"

----------------------------------------
15#S#1#9223370811321775807
  lane:direction                           @ 1970/01/15-04:25:33.000000
    "S"
  lane:highway                             @ 1970/01/15-04:25:33.000000
    "15"
  lane:lane                                @ 1970/01/15-04:25:33.000000
    "1.0"
  lane:latitude                            @ 1970/01/15-04:25:33.000000
    "32.723248"
  lane:longitude                           @ 1970/01/15-04:25:33.000000
    "-117.115543"
  lane:sensorId                            @ 1970/01/15-04:25:33.000000
    "32.723248,-117.115543,15,S,1"
  lane:speed                               @ 1970/01/15-04:25:33.000000
    "72.3"
  lane:timestamp                           @ 1970/01/15-04:25:33.000000
    "2008-11-01 09:50:00"

----------------------------------------
15#S#1#9223370811322075807
  lane:direction                           @ 1970/01/15-04:25:32.700000
    "S"
  lane:highway                             @ 1970/01/15-04:25:32.700000
    "15"
  lane:lane                                @ 1970/01/15-04:25:32.700000
    "1.0"
  lane:latitude                            @ 1970/01/15-04:25:32.700000
    "32.723248"
  lane:longitude                           @ 1970/01/15-04:25:32.700000
    "-117.115543"
  lane:sensorId                            @ 1970/01/15-04:25:32.700000
    "32.723248,-117.115543,15,S,1"
  lane:speed                               @ 1970/01/15-04:25:32.700000
    "72.1"
  lane:timestamp                           @ 1970/01/15-04:25:32.700000
    "2008-11-01 09:45:00"

----------------------------------------
15#S#1#9223370811322375807
  lane:direction                           @ 1970/01/15-04:25:32.400000
    "S"
  lane:highway                             @ 1970/01/15-04:25:32.400000
    "15"
  lane:lane                                @ 1970/01/15-04:25:32.400000
    "1.0"
  lane:latitude                            @ 1970/01/15-04:25:32.400000
    "32.706184"
  lane:longitude                           @ 1970/01/15-04:25:32.400000
    "-117.120565"
  lane:sensorId                            @ 1970/01/15-04:25:32.400000
    "32.706184,-117.120565,15,S,1"
  lane:speed                               @ 1970/01/15-04:25:32.400000
    "74.1"
  lane:timestamp                           @ 1970/01/15-04:25:32.400000
    "2008-11-01 09:40:00"

student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ 
```

#### Task 5. Stop streaming jobs and delete Bigtable data

##### Stop simulated streaming data

```bash
INFO: Publishing 477 events from 2008-11-01 11:45:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 11:50:00
INFO: Sleeping 5.0 seconds
INFO: Publishing 477 events from 2008-11-01 11:55:00
INFO: Sleeping 5.0 seconds
^CTraceback (most recent call last):
  File "./send_sensor_data.py", line 104, in <module>
    simulate(event_type, ifp, firstObsTime, programStartTime, args.speedFactor)
  File "./send_sensor_data.py", line 67, in simulate
    time.sleep(to_sleep_secs)
KeyboardInterrupt
student-03-f65d165cc39f@training-vm:~$ ^C
```

##### Stop the Dataflow job

- https://console.cloud.google.com/dataflow/jobs
- Stop Job -> Cancel

##### Delete a Bigtable table and instance

```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ cbt deletetable current_conditions
2024/06/18 12:02:15 -creds flag unset, will use gcloud credential
```
```bash
student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ gcloud bigtable instances delete sandiego
Delete instance sandiego. Are you sure?

Do you want to continue (Y/n)?  y

student_03_f65d165cc39f@cloudshell:~ (qwiklabs-gcp-01-7b92655a3976)$ 
```

### Exploring Dataset Metadata Between Projects with Data Catalog

- https://www.cloudskillsboost.google/games/5178/labs/33844
- GSP789

- 2024-06-18 20:06:14
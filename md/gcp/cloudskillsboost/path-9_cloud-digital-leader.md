# Cloud Digital Leader Learning Path

- https://www.cloudskillsboost.google/paths/9

[TOC]

## 01: Digital Transformation with Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/266

### Course Introduction

#### Executive Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509970

Some people imagine that the technical capacity to understand the use of the cloud is all you need in order to transform a business. And I think this would be a big mistake. Hello, my name is Vint Cerf. I'm vice president and Chief Internet Evangelist at Google. My primary job is to make sure there is more Internet out there for everyone. But my big interest is making sure the cloud is useful as well. I think if you're concerned about being ready for cloud, you need to understand what its capabilities are, what it can deliver. You don't necessarily have to understand in great detail how it does it, but you have to appreciate what it can do and how much flexibility it offers in terms of new products and services, or transforming ways in which all businesses operate. Without that insight, without that understanding, it's very hard to steer yourself into an advantageous place in the cloud world. If you think a little bit about the scale of the cloud. You'll appreciate that most companies and certainly most individuals would not be capable of investing in and maintaining the computing capacity and data storage capacity of today's clouds. That transformation means that companies have access to facilities they otherwise could not get access to. Failure to at least attend to new technology could be a fatal risk. And that's why we think everyone should be at least aware of what the new technologies are and whether or not they fit into the corporate structure. Let's talk a little bit about leadership. I think most leaders don't need to know in detail how the Internet works, but they have to have a kind of fundamental appreciation for what it means to get access to cloud based technology through the Internet. Think of the Internet and the cloud as an enabling infrastructure that can be purposed in infinite different ways. So it's important that the leaders at least have a conceptual grasp of what these technologies can do for their companies, their products and services. This is not just about technology. It's about how technology is used. And that requires real vision.

#### Course Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509971

There's a lot of excitement about cloud technology and digital transformation. But you might also have many unanswered questions. For example: What is cloud technology? What does digital transformation mean? How does cloud technology help you or your organization? Where do you even begin? If you've asked yourself any of these questions, you're in the right place. At Google Cloud, we want to provide you with the necessary information and tools for success as you begin your journey to the cloud and digital transformation. This course, “Digital Transformation with Google Cloud” was designed to help you: Understand why and how the cloud revolutionizes businesses. Explain general cloud concepts. And identify the benefits and tradeoffs of using IaaS, or infrastructure as a service; PaaS, or platform as a service; and SaaS, or software as a service. You’ll begin by defining some important terms that you’ll hear throughout the course, and by describing the benefits of adopting cloud technology to digitally transform a business. Next, you’ll explore some fundamental cloud concepts. You’ll learn how migrating to the cloud affects your organization’s flexibility, agility, reliability, and total cost of ownership. You’ll also explore the different types of infrastructure and explore various use cases for them. From there, you’ll learn about cloud computing models, and the shared responsibility between an organization and its cloud provider in hardware, software, security. Throughout the course you’ll be presented with graded knowledge assessments. You must pass these assessments to receive course credit. Okay, let's get started!

### Why Cloud Technology is Transforming Business

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509972

To understand how the cloud is transforming businesses, it’s important to learn about the basics of cloud and cloud technology. In this section of the course, you’ll explore: key terms related to the cloud and digital transformation, the benefits of cloud technology with regard to an organization’s digital transformation, the differences between on-premises infrastructure, public cloud, private cloud, hybrid cloud, and multicloud, and the drivers and challenges that lead organizations to undergo a digital transformation. Let’s get started!

#### Innovations, paradigm shifts, and digital transformation

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509973

Innovation doesn’t come in a linear pattern. It comes in waves. And each of these waves is powered by a breakthrough technology. There was the age of the printing press, the steam engine, electricity, the transportation age, the first computers and, today, data and cloud infrastructure. Each of these inventions triggered thousands of innovations, changing what's possible in life and work. Consider the invention of the printing press. It was revolutionary because it gave everyone access to books, encyclopedias, and even playing cards in their daily lives. It also led to a broader recognition of intellectual property through widely distributed patents, which in turn prepared the world for the first industrial revolution. There was no turning back! Steam-powered engines brought us cars and trains which then radically transformed the transportation industry; allowing businesses to produce and transport goods at scale. The entire Industrial Revolution resulted from new technologies that came together and facilitated new ways of working. In the same way, electricity brought us the light bulb, household appliances and eventually the computer. What the printing press, the steam engine and electricity all have in common is that they’re examples of a paradigm shift: a fundamental and irreversible change in the way that humans work and engage with the world. How do these examples relate to cloud technology? Well, we’re right in the middle of another paradigm shift: one of digital transformation. Cloud technology is transforming how organizations create value how people work, and ultimately, how people live. It’s the catalyst for thousands of innovations that change how we navigate the world, how we interact with media, how we diagnose illness, or how we combat environmental issues. Digital transformation, as a term, has become prominent over the past few years. But what are the key components of a digital transformation, how do they relate to the use of cloud technologies, and why so many organizations pursue it? At Google Cloud, we define digital transformation as when an organization uses new digital technologies, such as public, private, and hybrid cloud platforms to create or modify business processes, culture, and customer experiences to meet the needs of changing business and market dynamics. Organizations choose digital transformation frameworks to foster innovation, generate new revenue streams, , and adapt quickly to market changes and customer needs. Digital transformation helps organizations change how they operate and redefine relationships with their customers, employees, and partners by modernizing their applications, creating new services, and delivering value. For that reason, rapid advances in digital technology are redefining every industry. Many vehicles are now software-driven, and they receive regular updates much like a laptop or phone. In chemistry, big data and artificial intelligence (or AI) facilitates drug discovery. Financial service institutions use cloud’s vast computing power to provide better insights than ever before. With smart analytics that are increasingly embedded in everything and devices that generate exponential amounts of data traditional on-premises computing solutions can no longer suffice. As business innovation becomes more driven by software, the IDC FutureScape report predicts that over 50% of all IT spending will go toward digital transformation and innovation by 2024. In fact, IDC also predicts that, by 2025, more than 90% of new enterprise apps will have AI embedded within them. Leading organizations will rely more heavily on AI to launch new business models, create more customized experiences, and optimize operations to reduce costs. Understanding the scale and power of the cloud is more critical than ever before.

#### What is the cloud?

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509974

So what is the cloud and cloud technology, exactly? And how does it support digital transformation? The cloud is a metaphor for the network of data centers which store and compute information that’s available through the internet. Essentially, instead of describing a complex web of software, servers, computers, networks, and security systems, all of that has been combined into one word: “cloud.” To better understand the cloud, it might help to explore the different ways organizations can implement their information technology (or IT) infrastructure. The list includes on-premises, private cloud public cloud, hybrid cloud, and multicloud implementations. On-premises IT infrastructure, which is often abbreviated to “on-prem,” refers to hardware and software applications that are hosted on-site, located and operated within an organization's data center to serve their unique needs. This implementation is the traditional way of managing IT infrastructure. The benefit of on-premises is that it doesn’t require third-party access which gives owners physical control over the server hardware and software and doesn’t require them to pay for ongoing access. However, to have the computing power to run their required workloads, organizations must buy physical servers and other infrastructure through procurement processes that can take months. These systems require physical space, typically a specialized room with sufficient power and cooling. After configuring and deploying the systems, businesses then need expert personnel to manage them. This long process is difficult to scale when demand spikes or business expands. Organizations often acquire more computing resources than they actually need, which results in low utilization and high overhead. Cloud computing addresses these issues by offering computing resources as scalable, on-demand services. A private cloud is a type of cloud computing where the infrastructure is dedicated to a single organization instead of the general public. This type is also known as single-tenant or corporate cloud. Typically, an organization has to perform the same kind of ongoing maintenance and management for a private cloud as it would for traditional on-premises infrastructure. A private cloud is hosted within an organization’s own private servers, either at an organization’s own data center, at a third-party colocation facility, or by using a private cloud provider. Private cloud computing gives businesses many of the benefits of a public cloud—including self-service, scalability, and elasticity—with more customization available from dedicated on-premises infrastructure. Organizations might use private cloud if they have already made significant investments in their own infrastructure or if, for regulatory reasons, data must be kept on-premises or hosted in a certain way. The public cloud is where on-demand computing services and infrastructure are managed by a third-party provider, such as Google Cloud, and shared with multiple organizations or “tenants” through the public internet. This sharing is why public cloud is known as multi-tenant cloud infrastructure, but each tenant’s data and applications running in the cloud are hidden from other tenants. You can think of it like an apartment building that’s maintained by a property management company. The building has many units and tenants. Each unit might have a slightly different layout, but still has all the amenities a tenant needs to live there. And each unit is locked and private to the tenant who pays for that space. In these lessons, when we refer to “cloud,” unless otherwise stated, we’re talking about the public cloud. Because public cloud has on-demand availability of computing and infrastructure resources, organizations don't need to acquire, configure, or manage those resources themselves, and they only pay for what they use. There are typically three types of cloud computing service models available in public cloud: The first is infrastructure as a service (IaaS), which offers compute and storage services. The second is platform as a service (PaaS), which offers a develop-and-deploy environment to build cloud apps. And the third is software as a service (SaaS), which delivers apps as services, where users get access to software on a subscription basis. We’ll explore these three models in detail later. The final two ways that organizations can implement IT infrastructure is is hybrid cloud or multi-cloud. Although they’re not the same, these two terms are often used interchangeably, so let's take a moment to define them. In a hybrid cloud, applications run in a combination of different environments. The most common hybrid cloud example is combining a public and private cloud environment, like an on-premises data center and a public cloud computing environment, like Google Cloud. The term multicloud describes architectures that combine at least two public cloud providers. Organizations might operate a combination of on-premises and multiple public cloud environments, therefore implementing both hybrid and multicloud simultaneously. So, although hybrid cloud and multicloud are related, they aren’t interchangeable terms. Today, most organizations embrace a multicloud strategy. According to the “Flexera 2022 State of the Cloud Report,”, 89% of respondents reported having a multicloud strategy, and 80% of them take a hybrid approach by combining public and private cloud.

#### The benefits of cloud computing

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509975

So, what are the benefits of cloud computing compared to traditional on-premises infrastructure? It's scalable. Cloud computing gives organizations access to scalable resources and the latest technologies on-demand, so they don’t need to worry about capital expenditures or limited fixed infrastructure. This can significantly accelerate infrastructure deployment time. It’s flexible. Organizations and their users can access cloud services from anywhere scaling services up or down as needed to meet business requirements. It’s agile. Organizations can develop new applications and rapidly get them into production, without worrying about the underlying infrastructure. It offers strategic value. Because cloud providers stay updated with the latest innovations and offer them as services to customers, organizations can get more competitive advantages and a higher return on investment—than if they’d invested in soon-to-be obsolete technologies. This lets organizations innovate and try new ideas faster. It’s secure. Cloud computing security is recognized as stronger than that in enterprise data centers, because of the depth and breadth of the security mechanisms and dedicated teams that cloud providers implement. Finally, it’s cost-effective. No matter which cloud computing service model organizations implement, they only pay for the computing resources they use. They don’t need to overbuild data center capacity to handle sudden spikes in demand or business growth, and they can deploy IT staff to work on more strategic initiatives.

#### Real-world examples: Why it’s critical to transform and embrace new technology

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509976

As the world and business change, keeping technology the same instead of being open to transforming is risky for an organization. Let’s illustrate this by looking at two examples: one embraces new technology and uses it to their advantage, and the other doesn’t. First up is Nintendo. Nintendo has been creating games since 1889! They started with traditional Japanese playing cards, called Hanafuda, which were made possible by the printing press. From there, they have consistently used new technology to transform their business, and become a leader in the gaming industry. They were even among the first to introduce gaming consoles and mobile gaming devices. Still, they didn’t just stop after these successes. Instead, they revolutionized mobile gaming when they launched Pokemon Go in 2016, and then the first cloud gaming console—Nintendo Switch—one year later in 2017. At a time when most of their competitors were failing, Nintendo transformed by using one new technology after the next, consistently maintaining and even expanding its market share and customer base along the way. More recently, Nintendo has been using Google Cloud to bring games to smartphones worldwide. So, what makes Nintendo so successful at transforming? The answer is that they consistently focus on “why” they exist, not “how” they operate. They exist because they want people to play, and naturally, they’ll use any new technology as a resource to achieve this mission. If they focused on liquid crystal displays as the best tool for gaming, then each new technology would have posed a threat to them. Instead, they utilized liquid crystal displays for a while, and then quickly shifted as the next technology became available to continue motivating people to play. By contrast, companies that sold encyclopedias, for example, all focused on “how” they operate (how to print and sell a specific set of books). And this was what they were proud of: a beautiful set of leather-bound books lined up on the shelves of the finest libraries. And because of their high cost, only a few scholars or the elite could afford them. For businesses that made and sold encyclopedias, they needed printing presses well-kept warehouses, bookshelf makers, a way to ship and receive heavy containers, and a good door-to-door selling model. These companies were so focused on manufacturing books that they lost sight of their initial mission: to capture, catalog, and share human knowledge. When new technology such as CD-ROMs became available, this short-term mindset led many of the original encyclopedia companies to view the technology as a threat to their business instead of an opportunity. Ironically, many of the CD-ROM-based encyclopedia providers made the exact same mistake, and were later overtaken by by cloud-based applications such as Wikipedia or traditional encyclopedia companies, such as Britannica, which moved online. Nintendo and encyclopedia companies were both born from the printing era; Nintendo began with traditional playing cards, and encyclopedias stemmed from hard copy books. Because they reacted to technological innovations differently, they experienced different outcomes. The reality is that digital transformation is an ongoing process, not a one-time effort. Today, countless industries around the world are disrupted by digitalization: from healthcare to entertainment, from retail to manufacturing. It's critical that organizations embrace new technology as an opportunity to evolve, serve their customers better, and gain a competitive advantage. This is where cloud computing plays a significant role.

#### Cloud eras

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509977

To understand the cloud computing landscape today and what true digital transformation looks like, we should first understand how we got here. It started with the VM cloud era. VM stands for virtual machine. New organizations, mostly startups, realized that they could forgo ever buying or operating hardware and just start in the cloud. This was a major catalyst for many of the great cloud-native companies that we all rely on today, such as Twitter, Spotify, and PayPal. By the end of this first VM cloud era, very few startups operated their own data centers. Next was the infrastructure cloud era, which is when organizations migrated their IT infrastructure to the cloud. This migration saved costs because infrastructure could scale up and down more quickly and easily. Faster development was possible because companies didn’t need long-term infrastructure planning and security was better. Also, reducing the management load on IT staff let organizations direct more people and resources to focus on building new capabilities. In this last decade of the infrastructure cloud, companies that ignored this migration were left trailing behind. Although the return on investment of these early cloud migrations was important, it didn’t provide compelling transformative or disruptive results or fundamentally change how people worked outside of IT. This is because digital transformation is more than simply migrating and shifting systems to the cloud for cost saving and convenience. As we look ahead, reinventing the future means changing not only where business is done, but how it is done. It requires maximizing the benefits of the cloud and building an environment that enables every person, process, and technology to bring the highest level of innovation to the business. This is what brings us to the transformation cloud era, where organizations are not just making infrastructure decisions, but are truly focusing on transforming. Digitalization is now fundamental, and this era is about spreading transformation among all teams in an organization. To facilitate this degree of constant innovation and progress, today’s most ambitious organizations are building transformation clouds. A transformation cloud is a new approach to digital transformation. It provides an environment for app and infrastructure modernization, data democratization people connections and trusted transactions. It’s built on an easy-to-use platform with customized industry solutions that gives organizations the confidence that they are saving money and creating a more sustainable future for everyone. The result is an organization that benefits from cloud computing to drive innovation, generate new revenue streams, and adapt quickly to market changes and customer needs.

#### Challenges that lead to a digital transformation

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509978

A major indicator for organizations that are accelerating their innovation is how they think about transformation. Instead of asking infrastructure questions about where their apps and services should run, they ask transformation questions about how to build an environment that helps every person, process, and technology to adapt to changing business needs. So, what are the types of problems and questions that make organizations undergo a digital transformation? At Google, when we talk to our customers about their biggest business challenges and what they need to accelerate digital transformation, we consistently hear five themes. First, they want to be the best at understanding and using data. Today, organizations must unify data across streams, lakes, warehouses, and databases so that they can quickly and easily break down data silos, generate real-time insights, and make better business decisions; thus reducing cost and inefficiencies. Second, they want the best technology infrastructure. Organizations are looking for a cloud platform that will serve as their foundation for growth and has the flexibility to innovate securely and adapt quickly based on market needs. Third, they want to create the best hybrid workplace. The fundamental shift in how and where we work requires new, stronger connections and collaboration, and many interactions that took place in person have been digitized. This change requires more intentional connections and collaboration. Fourth, it's critical for organizations to know that their data, systems, and users are secure. The digital world is seeing more severe security issues, so now companies are rethinking their security posture. They must find ways to identify and protect everything from people customers customers to data and transactions in a fast-changing environment. Finally, organizations are prioritizing sustainability as a critical, board-level topic. They want to create a more sustainable future through products and services that minimize environmental impact. These are the top drivers for digital transformation that we see, and the challenges that many organizations face as they navigate their journey.

#### Google’s Transformation Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509979

There are five primary capabilities that form the basis of the transformation cloud. They are: Data Open infrastructure Collaboration Trust And sustainable technology and solutions. Let’s explore each, starting with the data cloud. Data is the key to unlocking value from AI, making it critical for innovation and differentiation. Data is the key to unlocking value from AI, making it critical for innovation and differentiation. Data is the key to unlocking value from AI, making it critical for innovation and differentiation. But becoming a data-driven company can be difficult if datasets are siloed across operational and analytical data stores. According to the NewVantage Partners’ Data and AI Executive Survey 2022, only 26.5% of companies have succeeded in creating a data-driven organization, to realize tangible and measurable value from their data. A data cloud is a unified solution to manage data across the entire data lifecycle, regardless of whether it sits in Google Cloud or in other clouds. It lets organizations identify and process data with great scale, speed, security, and reliability. Leading companies like Ford, Spotify, Wayfair, and UPS use a data cloud to encourage data-driven transformation quickly, securely, and at scale, all with AI built in. Next up is the open infrastructure. Organizations choose to modernize their IT systems on Google’s open infrastructure cloud because it gives them freedom to securely innovate and scale from on-premises, to edge, to cloud on an easy, transformative, and open platform. Open infrastructure cloud brings Google Cloud services to different physical locations, while leaving the operation, governance, and evolution of the services to Google Cloud. Instead of relying on a single service provider or closed technology stack, today most organizations want the freedom to run applications in the place that makes the most sense, using hybrid and multicloud approaches based on open source software. An open infrastructure cloud facilitates faster innovation and reduces lock-in to a single cloud provider by giving organizations the choice by giving organizations the choice and flexibility to build, migrate, and manage their applications across on-premises and multiple clouds. Let’s take a moment to define two terms that are often confused: open standard and open source. Open standard refers to software that follows particular specifications that are openly accessible and usable by anyone. They have guidelines for software functionality, which help avoid vendor lock-in and ensure that the products that use these standards perform in an interoperable way. Examples of open standards are HTTP for requesting content on the web or XML for storing structured data. Open source refers to software whose source code is publicly accessible and free for anyone to use, modify, and share. A decentralized community generally develops open source software as a public collaboration, based on a philosophy of transparency and the open exchange of ideas. Open source plays a critical role in an open cloud to deliver customers the portability they expect. Google has a long history of sharing technology through open source: from projects like Kubernetes, which is now the industry standard in container interoperability in the cloud, to TensorFlow, a platform to help everyone develop and train machine learning models. Another way we provide flexibility is through hybrid and multicloud environments managed by products like Anthos, which is built on open technologies like Kubernetes, Istio, and Knative. And finally, an open infrastructure embraces a partner ecosystem— —and the breadth of solutions it can offer its customers —instead of competing with it. Collaboration helps transform how people connect, create, and collaborate. A transformation cloud isn’t just about technology. People and culture are just as important. Organizations have increased both location and time flexibility in work arrangements since the COVID-19 pandemic began, and hybrid work is here to stay. With the definition of the workplace forever changed, it's essential that information and frontline workers across regions and industries it's essential that information and frontline workers across regions and industries connect, create, and collaborate securely from anywhere, and on any device. This new hybrid work environment needs to support a mix of in-person and remote interactions, including immersive digital and mobile experiences. At Google, for example, we offer a collaboration cloud through Google Workspace. Workspace brings together communication and collaboration apps including Gmail, Chat, Calendar, Drive, Docs, Sheets, and Meet into a people-first experience powered by Google AI. A trusted cloud helps organizations protect what's important with advanced security tools. According to Cybersecurity Ventures, the annual cost of cyber crime is expected to reach $10.5 trillion annually by 2025. Due to the rise of cybersecurity threats, every company is rethinking its security posture. This means finding ways to identify and protect everything, from people and customers to data and transactions —in a fast-changing environment. Organizations see the cloud as more secure than on-premises, and they want to make it simple so that employees, customers, and contractors can safely access their services. They want to create better visibility to find, analyze, resist, and remediate threats at global scale, and benefit from cloud innovations while maintaining control of their digital assets. Finally, a transformation cloud is built on a sustainable foundation, using technology and solutions that help organizations build and work more sustainably. Today, organizations are now encouraged to help create a cleaner, more sustainable world and they need new technologies that help them progress consistently. According to IDC, cloud computing is estimated to save 1 billion metric tons of CO2 emissions by 2024. The largest corporations have the opportunity to lead the way in helping the world reduce its emissions and operate on carbon-free energy always. For that reason, companies are moving to the cloud, and they want a sustainable infrastructure to power their business. At Google Cloud, for example, we partner with customers to decarbonize their digital apps and infrastructure with our sustainable technology and solutions. We proudly operate the cleanest cloud in the industry, with the smartest data centers that are 2 times as energy-efficient as a typical enterprise data center. Moving to Google Cloud can dramatically decrease a customer's IT-related carbon footprint.

#### The Google Cloud Adoption Framework

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509980

So how can organizations approach their cloud journey? Moving to the cloud offers enormous benefits for transforming businesses. Yet there are also risks. The challenge is multi-dimensional, with far reaching implications for the solutions that will run in the cloud and also for the technologies that support them. The people who must implement them and the processes that govern them. The rubric of people, process, and technology is a familiar one. It forms the basis of the Google Cloud Adoption Framework, which was created to support customers on their cloud journey. The value of the Google Cloud adoption Framework is that it serves as a map to help organizations adopt the cloud quickly and effectively by creating a comprehensive action plan for accelerating cloud adoption. It does this by structuring and aligning short term tactical, mid-term, strategic, and long term transformational business objectives. It provides a solid assessment of where an organization is in its cloud journey and actionable programs that get it to where it wants to be. A cloud maturity assessment helps to establish where an organization is currently regarding the cloud adoption themes recognized by Google Cloud. It can quickly reveal any areas where an organization might be weaker or underinvested. This is especially powerful if an organization was previously unaware of this lack of maturity. The Google Cloud adoption framework is more than just a model. It's also a map to real, tangible tasks that organizations need to adopt the cloud. After cloud maturity has been assessed and actions have been recommended, it's easy to scope and structure a cloud adoption program using the framework.

#### Summary

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509981

This brings us to the end of the first section of the Digital Transformation with Google Cloud course. Let's do a quick review of how cloud technology is transforming businesses. Digital transformation is more than lifting and shifting old I.T. infrastructure to the cloud for cost saving and convenience. As we look ahead, reinventing the future means changing not only where business is done, but how it is done. It requires maximizing the benefits of the cloud and building an environment that lets every person, process and technology bring the highest level of innovation to the business. We already see tremendous success with leading organizations that embrace the transformation cloud, move their businesses well beyond infrastructure, and rapidly migrate toward the next era of their cloud evolution.

### Fundamental Cloud Concepts

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509983

To understand the impact that the cloud can have on a business, it’s important to first recognize some of the fundamental cloud concepts. By the end of this section, you’ll be able to: Describe the benefits of moving to cloud infrastructure through customer business use cases. Explain how moving to the cloud shifts an organization's spending from capital expenditure to operational expenditure, and how that affects their total cost of ownership. Identify when private, hybrid, or multicloud infrastructures best apply to different business use cases. Define basic network infrastructure terminology. And explain how Google Cloud supports digital transformation with global infrastructure and data centers connected by a fast, reliable network. Cloud adoption has made a positive impact on some of the world’s leading companies across various industries. Let’s start with an example of how the cloud provided flexibility and improved performance for the Canadian food and pharmacy leader Loblaw. Loblaw is Canada’s largest retailer, with nearly 2,500 corporate, franchised, and associate-owned locations and nearly 200,000 full- and part-time employees. The Vice President of Technology at Loblaw explained: ”We want our tech talent focused on creating better experiences for our customers, not maintaining infrastructure. ” Loblaw took a lift-and-shift approach to accelerate the initial migration to Google Cloud. This means they focused on moving their existing virtual machines on-premises to Compute Engine instances in the cloud without needing to redesign them. However, they designed the cloud architecture to be easily converted to using Google Kubernetes Engine for automated deployment and scaling later. This shows how an open cloud can grow with an organization’s needs. With a more responsive ecommerce site and the ability to handle more traffic without affecting the customer experience, Loblaw could run marketing promotions and generate additional revenue. With these improvements, they expect to reclaim 50% of their Site Reliability Engineers’ time to focus on innovation. Now let’s shift our focus to another example, this time on how the cloud provided scalability and cost reduction. HSBC is one of the world’s largest banking and financial services organizations, serving more than 40 million global customers from offices in 64 countries and territories. Committed to a cloud-first strategy, in January 2021, HSBC embarked on an ambitious project to enhance and future-proof their risk management on the cloud. Their previous on-premises system was not capable of meeting future regulatory and business demands. HSBC built a cloud-native risk management solution that boosts calculation speed to be ten times faster while lowering costs. This equated to three billion calculations per second. The power of a data cloud is that it has almost unlimited resources to process large volumes of data and reduce time to insights. A Global Head of Traded Risk Technology at HSBC explains, “We knew that a cloud-native solution gave us the ability to scale and run at a reduced cost. We did a proof of concept using Google Cloud, and we quickly realized that this could be very successful.” HSBC built a cost-effective platform that is faster and more efficient while meeting their regulatory and compliance requirements. And in a final example, let’s look at how the cloud has brought agility and valuable insights, while maintaining trust, to an organization. The American Cancer Society is a community-based voluntary health organization dedicated to eliminating cancer as a major health problem. Their mission is to free the world from cancer by funding and conducting research, sharing expert information, supporting patients, and spreading the word about prevention. Among women, breast cancer is the most commonly diagnosed type of cancer. Yet, if detected early, it’s also one of the most survivable cancers. Mia M. Gaudet, PhD, is the Scientific Director of Epidemiology Research at the American Cancer Society. Through her research, she’s obtained over 1,700 high-resolution tissue images from participants diagnosed with breast cancer. This valuable data could help them discover factors that could lead to a cancer diagnosis and improve survival rates. The challenge was to identify novel patterns in digital images of breast cancer tissues to potentially improve patient outcomes. Their research group partnered with Slalom, a Google Cloud Premier Partner, and sought to combine machine learning–powered insights with cloud computing performance to improve timeliness and accuracy. By building a machine learning pipeline using Google Cloud AI Platform, now called Vertex AI, they trained models for AI image analysis of tissue scans to find cancer indicators. The team achieved 12 times faster image analysis with enhanced quality and accuracy by removing human limitations. Dr. Gaudet said, "The ability to perform image analysis by using deep learning for epidemiologic breast cancer studies opens a new frontier of research, and Google Cloud makes it easier. We're excited about what we'll find." The American Cancer Society is now equipped with processes and a cloud infrastructure that will be reusable on similar projects, providing a foundation for future work.

#### Total cost of ownership (TCO)

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509984

Organizations often perform a cloud total cost of ownership (or TCO) analysis when they are considering moving to the cloud. This analysis aims to weigh the cost of cloud adoption against the cost of running their current on-premises systems. For on-premises, TCO is associated with assessing the cost of static resources throughout their lifetime. However due to the dynamic nature of the cloud, predicting future costs can be challenging. A common mistake that organizations make when attempting to calculate cloud TCO is to directly compare the running costs of the cloud against their on-premises system. These costs are not equivalent. The cost of on-premises infrastructure is dominated by the initial purchase of hardware and software, but cloud computing costs are based on monthly subscriptions or pay-per-use models. It's also important to consider all the operational costs of running your own data center, such as power, cooling, maintenance, and other support services. A data center is a building or facility that houses a large amount of IT infrastructure, computing, and storage resources in one place. Finally, intangible costs, such as the opportunity cost of not migrating to cloud and the missed benefits, should be considered.

#### Capital expenditures (CapEx) versus operating expenses (OpEx)

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509985

One area where cloud differs from traditional IT is in how managing costs changes when you move to the cloud. With organizations moving from on-premises infrastructure to on-demand cloud services, there’s a major shift in spending from capital expenditures to operating expenses. But what’s the difference between these two? Capital expenditures, or CapEx, are upfront business expenses put toward fixed assets. Organizations buy these items once, and they benefit their business for many years. For example, in IT, these expenditures might mean buying hardware like servers, printers, or cooling systems. Maintaining these assets is also considered CapEx because it extends their lifetime and usefulness. Small businesses can find CapEx spending challenging because large one-time purchases are often high cost. The more money you put toward CapEx means less free cash flow for the rest of the business. And then there are operating expenses, or OpEx, which are recurring costs for a more immediate benefit. This represents the day-to-day expenses to run a business. In IT, these expenses might be yearly services like website hosting or domain registrations, or the subscription fee for cloud services. OpEx covers the spendings on pay-as-you-go items, but are not considered major long-term investments like CapEx items. Understanding the difference between CapEx and OpEx is helpful in recognizing how costs differ between on-premises and the public cloud. In the on-premises CapEx model, cost management and budgeting are a one-time operational process completed annually. Data centers require a huge CapEx investment up front as organizations purchase space, equipment, and software and hire a workforce to run and maintain everything. Forecasting is based on a metric such as historic growth to determine the needs for the next month, quarter, year, or even multiple years. Moving to cloud’s on-demand OpEx model enables organizations to pay only for what they use and only when they use it. Budgeting is no longer a one-time operational process completed annually. Instead, spending must be monitored and controlled on an ongoing basis due to the dynamic nature of cloud use within organizations. How infrastructure is procured has radically changed, too. In a more decentralized cloud world, any employee can create resources in seconds on infrastructure owned and managed by a cloud provider. Organizations save on power, cooling, and floor space; they save on management because they don’t have to install, operate, upgrade, and troubleshoot it themselves. And they're not depreciating the equipment—the cloud provider is. Cloud gives organizations the ability to start small and grow organically instead of having to guess at what is needed next week, next month, and next year. Costs match actual usage and are now operational expenses.

#### Private cloud, hybrid cloud, and multi-cloud strategies

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509986

It's not always possible, or necessary, for an organization to rely solely on the cloud. For example, requirements might call for on-premises infrastructure to work with public cloud services provided by companies, like Google Cloud. With the availability of different cloud options and configurations, it’s important to understand what each means. Let’s explore the definitions of private, hybrid, and multi-cloud, and when an organization might choose each approach. Let’s begin with private cloud, which is when an organization has virtualized servers in its own data centers, or those of a private cloud provider, to create its own private dedicated environment. On-premises servers are also often referred to as private clouds, but generally the distinction can be made that on-premises software runs in a local environment, whereas a private cloud is accessed through the internet. Private cloud computing gives an organization many of the benefits of a public cloud — including self-service, scalability, and elasticity — with more customization available than from dedicated on-premises infrastructure. This approach is often used when an organization has already made significant infrastructure investments, or if, for regulatory reasons, data must be kept on-premises. In contrast, a hybrid cloud is one in which applications are running in a combination of different environments. The most common example is combining a private and public cloud environment, like an on-premises data center, and a public cloud computing environment like Google Cloud. Finally, there’s multicloud, which describes architectures that combine at least two public cloud providers, such as Google Cloud, Amazon Web Services, Microsoft Azure, or others. An organization might choose multicloud if they want to take advantage of the key strengths of different public cloud providers. Organizations may also operate a combination of on-premises and multiple public cloud environments, effectively being both hybrid and multicloud simultaneously. A hybrid cloud approach is one of the most common infrastructure setups today because organizations can continue to use their on-premises servers while also taking advantage of public cloud. According to Gartner, 81% of organizations are working with two or more public cloud providers. Additionally a Flexera state of the cloud report showed 93% of enterprises have a multicloud strategy. So, what is a hybrid or multicloud strategy used for? Let's explore some different business requirements, drivers, and use cases that lead an organization to choose this kind of approach. Access to the latest technologies Running workloads in multiple clouds empowers organizations to leverage the latest innovations and capabilities from each cloud provider, thus taking a best-in-class approach to cloud features and obtaining the scale, security, and agility to innovate fast. Cloud can help organizations build out capabilities, such as advanced analytics services, that might be difficult, or impossible, to implement in existing environments Modernize at the right pace With a hybrid cloud, organizations can migrate applications to the cloud at the pace that makes sense for their business and transform their technical infrastructure over time. Improved return on investment By adding a public cloud provider to their existing on-premises infrastructure, organizations can expand their cloud computing capacity without increasing their data center expenses. This can help reduce CapEx or general IT spending, and improve transparency regarding costs and resource consumption. Flexibility through choice of tools Hybrid and multi-cloud strategies have advantages for organizations as a whole, but specifically benefit development teams that are working on different projects and tackling unique challenges across different lines of business. A wider choice of tools and developer talent can be applied to a particular business problem, which means responding better to changing market demands. It also avoids vendor lock-in concerns. Improve reliability and resiliency Organizations can distribute core workloads across multiple cloud and on-premises infrastructures to reduce downtime and and concerns about over-dependence on a single source of failure. This approach can improve the quality and availability of a service. Maintain regulatory compliance Many industries have rules from governmental or regulatory bodies regarding where their app can operate. Adopting a hybrid solution is an effective way for an organization to ensure compliance with regional data governance, residency, or digital sovereignty requirements. Running apps on-premises Organizations may have regulated applications that must remain on-premises or mainframe systems that are difficult to move to the cloud. A hybrid approach provides the freedom to innovate while still meeting And finally, running apps at remote edge locations Organizations in industries that run distributed apps at remote locations, such as kiosks in retail or networks in telecom, can benefit from hybrid cloud. These apps often require improved performance and low latency, and a hybrid approach lets them run select apps at the network edge.

#### How a network supports digital transformation

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509987

Digital transformation has increased the importance of the network. The ability to connect customers, employees, cloud applications, and devices enables modern organizations to succeed. With every innovation, the underlying apps and services rely on the network to communicate and connect. But how does a reliable networking architecture support a digital transformation strategy? A fast, reliable, and low-latency global network ensures exceptional user experience and high performance. It also makes it easier to communicate and manage data globally. With ever more distributed workforces and online businesses, having virtual network services that can easily scale without adding hardware ensures that organizations can adapt. So, how does a network operate? Let's start with the foundation of the modern internet: fiber-optic networks. Fiber-optic cables contain one or more optical fibers, which are thin strands made of glass or plastic. These fibers are used to transmit data as pulses of light over long distances. Subsea fiber-optic cables carry 99% of international network traffic, yet we barely notice they exist. The first subsea cable was deployed in 1858 for telegraph messages between Europe and North America. A message took over 17 hours to deliver, at 2 minutes and 5 seconds per letter by Morse code. Today, a single cable can deliver a whopping 340 Terabits per second. That's more than 25 million times faster than the average home internet connection! Every shared video, sent email, and downloaded app depends on data traffic that moves through international network infrastructure. But how is this content available to people within milliseconds? A rich ecosystem of companies and local providers build a global infrastructure that provides businesses and people around the world with the best possible internet experience. These include companies like internet service providers (or ISPs). ISPs provide access to the internet to both personal and business customers, handling the traffic between the customer and the internet as a whole. Some examples of ISPs include Verizon, Vodafone, and Softbank. The infrastructure that makes Google’s global reach possible is our network of fiber-optic cables that run on both land and sea. This network connects our data centers and points of presence like highways connect major cities. Google owns and operates data centers all over the world. In these Google data centers, products like Search, Gmail, YouTube, and Google Cloud are run for people and organizations around the world, 24 hours a day, seven days a week. Within this vast global network, how do all the different parts recognize and communicate with each other? There are protocols that make it work. Let's start with an IP address. The IP stands for Internet Protocol, and this address is a series of numbers that can identify a network or the location of a particular device on a network. A domain name is an easy-to-remember name that maps directly to an IP address or set of IP addresses on the internet. It’s the unique name that appears after the @ sign in email addresses and after www. in web addresses. For instance, the domain name example.com might translate to the IP address 198.102.434.8. Other examples of domain names are google.com and youtube.com. And then there’s a Domain Name System, or a DNS. A DNS server stores a database of domain names mapped to IP addresses that can be queried and used by computers to communicate with each other. This system is like the phone book of the web. Every time you visit a website, your computer performs a DNS lookup. A phone book translates a name like "Acme Pizza" into the correct phone number to call; similarly, the DNS translates a web address like "www.google.com" into the IP address of the computer hosting that site. In this case, it’s the Google homepage.

#### Network performance: Bandwidth and latency

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509988

Now that you’ve been introduced to some of the fundamentals of networking, let’s explore how networks perform and are measured. Two important terms in networking are bandwidth and latency. Let’s define them both. Bandwidth is a measure of measure of how much data a network can transfer in a given amount of time. This ​​rate of data transfer is typically measured in terms of “megabits per second” (or Mbps) or “gigabits per second” (or Gbps). Generally speaking, a higher bandwidth allows a computer to download information from the internet more quickly. One way to think of bandwidth is to picture water flowing through a pipe. The bandwidth would be the volume of water a pipe can handle flowing through per second. A wider pipe can handle more water. An internet service provider may provide a home internet connection with 100 MegaBits per second to over 1 GigaBit per second; a data center may have with bandwidth from 10 to 100 GigaBits per second! Having a high bandwidth is useful when sending a large amount of data per second, such as streaming high-definition video, but it’s not the only important measure of network performance. For example, for users playing real-time multiplayer games online, latency will matter much more. For example, for users playing real-time multiplayer games online, latency will matter much more. Network latency is the amount of time it takes for data to travel from one point to another. Often measured in milliseconds, latency, sometimes called lag, describes delays in communication over a network. Going back to our flowing water analogy, latency is the delay from the moment the water pipe is opened until water starts flowing through. Ideally, latency should be as close to zero as possible. However, because it’s a result of the physical distance that data must travel – through wires, fiber optics, routers, and more – to reach its destination, each “hop” along the way adds a small amount of latency to the communication. No matter how much data you can send and receive at once, it can only travel as fast as network latency allows. Imagine an image file took just 10 milliseconds to download with a high-bandwidth connection, but a user had to wait 100 milliseconds before receiving the first byte of data. In this case, the latency, or how much time it took for data packets to travel from one point to another in the network, accounted for most of the time. Cloud computing and mobile technologies have made it easier for developers to reach global audiences, but high latency can drag down an application's performance. Websites run slower for some users depending on their physical location, even if both the user and the server have excellent bandwidth. So the farther a user is from a server, or the more fragmented the network is, the bigger the latency. Reducing latency is essential to reaching users faster.

#### Google Cloud regions and zones

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509989

Google has invested billions of dollars over the years to build its network, which is one of the largest networks of its kind on Earth. It’s designed to give customers the highest possible throughput and lowest possible latencies for their applications. Google Cloud’s infrastructure is based Google Cloud’s infrastructure is based in five major geographic locations: North America, South America, Europe, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency, the latter of which measures the time a packet of information takes to travel from its source to its destination. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy. You can run resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, such as a natural disaster. Some of Google Cloud’s services support placing resources in what we call a multi-region. For example, Cloud Storage lets you place data within the Europe multi-region. That means it's stored redundantly in at least two geographic locations, separated by at least 160 kilometers within Europe like London and Belgium. You can find the most up-to-date numbers for Google Cloud regions and zones at cloud.google.com/about/locations.

#### Google’s edge network

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509990

A recommended best practice for organizations is to keep their traffic on Google's private network for most of its journey, using the same network that powers products like Gmail, Google Search and YouTube allows organizations to take advantage of the performance that global infrastructure provides. When a user opens a Google app or Web page, Google responds to that request from an edge network location that will provide the lowest latency. Understanding Google's Edge Network and how it maintains caches that store popular content near its users helps organizations choose when to hand off traffic to Google. A network's edge is defined as a place where a device or an organization's network connects to the Internet. It's called "the edge" because it's the entry point to the network. Google's Edge Network is how we connect with ISPs to get traffic to and from users. It's made up of network infrastructure that organizations can hand off traffic to based on users needs, performance and cost. Google aims to deliver its services with high performance, high reliability and low latency for users. We have invested in network infrastructure that's aligned with this goal and that also allows us to exchange traffic efficiently and cost effectively with network operators. This is how network infrastructure supports digital transformation.

#### Summary

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509991

This brings us to the end of the second section of the Digital Transformation with Google Cloud course. Let's do a quick review of some fundamental cloud concepts. We covered examples of how customers from different industries were able to transform their business through creating new ways of working when moving to the cloud. Calculating the total cost of ownership differs greatly from the static and long term procurement on premises world to the more dynamic and on demand cloud world. Moving to cloud means shifting spending from a capital expenditure to operational expenses model, enabling organizations to pay only for what they use and only when they use it. We defined private, hybrid, and multi-cloud and described the different business drivers that lead an organization to choose these kinds of approaches. And finally, you learned the importance of a fast, reliable and low latency global network as a foundation to transformation and exceptional user experience.

### Cloud Computing Models and Shared Responsibility

#### Introduction to cloud computing models and shared responsibility

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509993

When moving to the cloud, there are decisions to make about how to manage and operate different cloud services. One of those decisions is around the type of cloud computing service model to use. Organizations typically choose service model types based on their specific business requirements. In this section of the course, you’ll explore three main cloud computing service models: IaaS, or Infrastructure as a Service PaaS, or Platform as a Service and SaaS, or Software as a Service Because the levels of responsibility between an organization and their cloud service provider vary depending on which model is used, you’ll also examine the shared responsibility model between Google Cloud and our customers. By the end of this section, you’ll be able to: Define IaaS, PaaS, and SaaS. Compare and contrast the benefits and tradeoffs of IaaS, PaaS, and SaaS. Determine which computing model applies to various business scenarios and use cases. Describe the cloud shared responsibility model. And identify which responsibilities are the cloud provider’s or the customer’s for on-premises and cloud computing models.

#### Cloud computing service models

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509994

The world of cloud computing has a diverse set of computing service models to choose from, depending on customer requirements. You might have heard of terms like IaaS, PaaS and SaaS. These terms represent the different cloud computing models provided “as a service” by cloud providers. ”As a service” refers to the way IT resources are consumed in these models, and is a key difference between cloud computing and traditional IT. In traditional IT, an organization consumes resources,such as hardware, software, and development tools, by purchasing, installing, managing, and maintaining them in its own on-premises or self-managed data center. Organizations are responsible for all of their IT infrastructure when it's completely on-premises. In cloud computing, the cloud service provider owns, manages, and maintains the resources. The customer consumes those resources, which are provided on a subscription or pay-as-you-go basis. All you need is an internet connection. Cloud computing allows for a third party to be responsible for some part of the infrastructure. This means that organizations then have more time to focus on their core business. Coming up, we're going to explore three different cloud computing service models: Infrastructure as a service, or IaaS, which offers infrastructure resources such as compute and storage. Platform as a service, or PaaS, which offers a develop-and-deploy environment to build cloud apps. And software as a service, or SaaS, which delivers complete applications as services. Each model offers distinct features and functionalities, and knowing the differences between them helps organizations choose one to best fit their business’ needs. It’s important to remember that most organizations that use cloud often use a combination of cloud computing models to solve for different needs. You can visualize these cloud computing models in layers. As you move up the layers from one model to another, each model requires less knowledge and management of the underlying infrastructure. This concept is called abstraction. In cloud architecture, as the level of abstraction increases, less is known about the underlying implementation. The goal of "abstracting away” infrastructure is to reduce complexity by removing unnecessary information and simplifying operations. Think about abstraction in the way that you operate a car. When you turn on the ignition, press the brake, put the car into gear, and accelerate, you’re not thinking about how the engine is physically operating under the hood, right? That complexity is abstracted away from you, so you can focus on driving safely to your destination. Abstraction is one of the core features of cloud computing. When choosing between cloud computing service models, organizations must decide the level of control and management they’ll require, or how much they want to hide technical details and focus on business needs. Let’s use a transportation analogy to see how on-premises, IaaS, PaaS, and SaaS compare with each other. On-premises IT infrastructure is like owning a car. When you buy a car, you’re responsible for its usage and maintenance. Upgrading means buying a new car, which takes time and can be costly. IaaS is like leasing a car. When you lease a car, you choose a car and drive it wherever you want, but the car isn’t yours. Upgrading is easier though, as you can just lease a new car. PaaS is like taking a taxi. You provide specific directions, like the code, but the driver does the actual driving. And SaaS is like going by bus. You still get access to transport, but it's less customizable. Buses have designated routes, and you share the space with other passengers.

#### IaaS (Infrastructure as a service)

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509995

Now let's look at each of these computing models in more detail. We’ll start with infrastructure as a service, or IaaS. IaaS is a computing model that offers the on-demand availability of almost infinitely scalable infrastructure resources, such as compute, networking, storage, and databases as services over the internet. IaaS allows organizations to lease the resources they need instead of having to buy hardware outright, and they only pay for what they use. It provides the same technologies and capabilities as a traditional data center without having to physically maintain or manage all of it. One of the main reasons businesses choose IaaS is to reduce their capital expenditures and transform them into operational expenses. IaaS is appealing because acquiring computing resources to run applications or store data the traditional way requires time and capital. Organizations must purchase equipment through procurement processes that can take months. They must also invest in physical spaces, which are typically specialized rooms with power and cooling. And after deploying the systems, they need IT professionals to manage them. This traditional way is challenging to scale when demand spikes or business grows. Organizations risk running out of capacity, or overbuilding and ending up with underutilized infrastructure. In contrast, IaaS resources are offered as individual services, so organizations can choose what they need. The cloud provider manages the infrastructure, and businesses can concentrate on installing, configuring, and managing software and keeping their data secure. Compute Engine and Cloud Storage are examples of Google Cloud IaaS products. You can create and run virtual machines with Compute Engine, and you can store any type of data with Cloud Storage. So, what are the benefits of IaaS? It’s economical. Because IaaS resources are used on demand and you only pay for what you use, IaaS costs are fairly predictable and easy to budget for. It’s efficient. IaaS resources are regularly available when you need them. As a result, there are fewer delays when infrastructure is expanded resources aren’t wasted by overbuilding capacity. This efficiency leads to faster development lifecycles and ultimately a faster time to market. It boosts productivity. Because the cloud provider is responsible for setting up and maintaining the physical infrastructure, IT departments save time and money. They can then redirect resources to more strategic activities. It’s reliable. IaaS has no single point of failure. Even if one component of the hardware resources fails, the service usually remains available. And it’s scalable. One of the biggest advantages of IaaS in cloud computing is the capability to scale the resources up and down rapidly rapidly according to business needs. So, what scenarios would IaaS be good for? The flexibility and scalability of IaaS is useful for organizations that: Have unpredictable workload volumes or need to move quickly in response to business fluctuations. Require more infrastructure scalability and agility than traditional data centers can provide. Have high business growth that outpaces infrastructure capabilities. Experience unpredictable spikes in demand for infrastructure services. And see low utilization of existing infrastructure resources.

#### PaaS (Platform as a service)

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509996

Platform as a Service, or PaaS, is a computing model that offers a cloud-based platform for developing, running, and managing applications.  PaaS provides a framework for developers that they can build upon and use to create customized applications. PaaS is appealing because it provides a platform for developers to develop, run, and manage their own apps without having to build and maintain the associated infrastructure. They can also use built-in software components to build their applications, which reduces the amount of code they have to write. Cloud Run and BigQuery are examples of Google Cloud PaaS products. Cloud Run is a fully managed, serverless platform for developing and hosting applications at scale, which takes care of provisioning servers and scaling app instances based on demand. BigQuery is a fully managed enterprise data warehouse that manages and analyzes data, and can be queried to answer big data questions with zero infrastructure management. So, what are the benefits of PaaS? It reduces development time. Developers can go straight to coding instead of spending time setting up and maintaining a development environment, which leads to faster time to market. which leads to faster time to market. It's scalable. With PaaS, organizations can purchase additional capacity for building, testing, staging, and running applications whenever they need it. It also allows for applications to be designed to take advantage of the inherent scalability of cloud infrastructure. It reduces management. By abstracting the management of underlying resources even further than IaaS, PaaS offloads infrastructure management, patches, updates, and other administrative tasks to the cloud service provider. This provides a cost-effective way to focus on new functionality. And it's flexible. With support for different programming languages and easy collaboration for distributed teams, PaaS provides developers with the flexibility to deliver various projects—from prototypes to enterprise solutions—on the same platform. So, what scenarios would PaaS be good for? PaaS is suitable for organizations that: Want to create unique and custom applications without investing a lot in owning and managing infrastructure. Want to rapidly test and deploy applications. Have many legacy applications and want to reduce the cost of operations. Have a new app project that they want to deploy quickly by growing and updating the app as fast as possible. Want to only pay for resources while they’re being used. And want to offload time-consuming tasks such as setting up and maintaining application servers and development and testing environments.

#### SaaS (Software as a service)

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509997

Software as a service, or SaaS, is a computing model that offers an entire application, managed by a cloud provider, through a web browser. The cloud provider hosts the application software in the cloud and delivers it through a browser. With this model, you don’t need to download or install any of it. SaaS is appealing because it abstracts technology completely from the consumer; SaaS is appealing because it abstracts technology completely from the consumer; the end user doesn’t need to care about the underlying infrastructure, which is the cloud provider's responsibility. Organizations simply pay a subscription fee for access to a ready-to-use software product. Google Workspace, which includes tools such as Gmail, Google Drive, Google Docs, and Google Meet, is a Google Cloud SaaS product. So, what are the benefits of SaaS? It's low maintenance. SaaS eliminates the need to have IT staff download and install applications on each individual computer. With SaaS, vendors manage all potential technical issues, such as data, servers, storage, and updates in the cloud. This helps to streamline maintenance and support for an organization. It's cost-effective. SaaS is based on a subscription model with a fixed, inclusive, monthly or annual account fee. Predictable costs and per-user budgeting allows for clear financial governance. It's flexible. Everything is available over the internet when a user signs in to their personalized account online. They can access the software from anywhere, any device, anytime. And what scenarios would SaaS be good for? Well, SaaS is suitable for organizations that: Well, SaaS is suitable for organizations that: Want to use standard software solutions that require minimal customization. Don’t want to invest time or internal expertise in maintaining applications or infrastructure. Need more time for IT teams to focus on strategic projects. And need to access apps from various devices and locations.

#### Choosing a cloud computing model

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509998

So, how does an organization decide which cloud computing model is the best option for them? The answer depends on their business needs, required functionality, and available expertise. If they are looking for a highly flexible, scalable service— while maintaining control of their infrastructure— then IaaS is the right choice. This model offers the most control and customization, but also requires the most management responsibilities and technical expertise. If they need a platform designed for building software products, then PaaS would help their business immediately. This provides a cost-effective way to build applications, but still requires some technical expertise and less management. If they want features that are ready to use, without the hassle of installations, then SaaS might be the best option. This represents the least management responsibilities and technical expertise, but also offers the least control and customization. These computing models are not mutually exclusive, though. Depending on the use case, most organizations will use combinations of all three to solve for different business needs. They’ll need to compare their options based on variables such as management level, control, responsibility, flexibility, and expertise needed. For example, imagine a large organization needs to implement a new inventory management system. If they had the in-house expertise to develop it and the willingness to manage the infrastructure, they could build this with IaaS resources. The organization's IT team would have complete control over server configurations, but also bear the burden of managing and maintaining them. They could choose a PaaS solution and build a custom CRM application while offloading management of infrastructure to the cloud service provider; retaining complete control over application features, but reducing the management load. Finally they could choose to buy a ready-made SaaS solution; having no daily management of infrastructure, , but also giving up all control over features and functionality in the software. Each of these options is a viable solution, so organizations must compare the benefits and tradeoffs for each use case. These cloud computing service models give organizations choices, flexibility, and options that on-premises hosting simply can’t provide.

#### The shared responsibility model

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/509999

One area of responsibility where each of the cloud computing models differ is security. When an organization manages its data in its own data centers, that organization is responsible for all aspects of its security. However, as infrastructure is moved to the cloud, some aspects of the responsibility shift to the cloud provider. This concept is called the shared responsibility model. Security in the cloud is a shared responsibility between the cloud provider and the customer. Although direct responsibilities change based on the cloud computing service model, organizations are always in control of securing their data, and the cloud provider is always responsible for securing the infrastructure. At Google Cloud, we defend organizations’ data against threats and fraudulent activity with the same infrastructure and security services we use for our own operations. However, security of the cloud and security in the cloud are two different things. Simply put, the cloud provider is responsible for the security of the cloud, while the customer is responsible for security in the cloud. It's important for organizations to understand how the specific customer responsibilities vary according to the type of cloud computing model used. This is especially important because, according to a Gartner report, 99% of all cloud security failures will result from user error through the year 2025. Organizations must understand their roles and responsibilities in cloud security to guarantee it.

#### How the shared responsibility model works

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/510000

If you look at the various cloud computing models together, you can see where the cloud provider’s responsibility ends and where the customer's responsibility begins. A general guideline for shared responsibility is that "if you configure or store it, you're responsible for securing it." This generally means that a cloud provider is responsible for securing the parts of the cloud that it directly controls, such as hardware, networks, and physical security. At the same time, the customer is responsible for securing anything that they create within the cloud, such as the configurations, access policies, and user data. No matter which cloud provider you use, there is shared responsibility. Let’s examine the ratios of responsibility between Google Cloud as a service provider and our customers. The blue squares represent the parts of the infrastructure security that the customer is responsible for, while the yellow squares represent the elements that Google Cloud is responsible for. Let's begin with on-premises. When an organization runs its own on-premises data centers, security for the infrastructure is solely the responsibility of the organization's internal teams. They are responsible for securing servers and the data stored on them. Next is infrastructure as a service. When an organization transitions to an IaaS computing model, it assigns some IT security responsibilities to Google Cloud. This includes being responsible for the physical resources and sharing responsibility with the customer for the security of the infrastructure and network. The rest, such as the security of the operating system, software stack required to run their applications, and their data, is the responsibility of the customer. This allows customers the most freedom and control, but also places most of the responsibility in their hands. When an organization uses the platform as a service model, more of the responsibility is passed over to Google Cloud. This includes full responsibility for the physical infrastructure, the access and authentication, network security, and guest operating systems. The customer is still responsible for the security of any content, such as code or data, produced on the platform. Lastly, with the software as a service model, Google Cloud is responsible for almost every aspect of security—from the underlying infrastructure to the actual application. Customers still have some security responsibilities, such as application usage, access policies like authentication settings to prevent phishing attacks, and the user content. One important aspect of the shared responsibility model is that customers are always responsible for the security of their data, whether they have on-premises data centers or only pay a monthly subscription for a single user license. The customer controls who or what has access to their data. Google Cloud is committed to keeping customers’ data secure, but security is a shared responsibility, and requires collaboration.

#### Summary

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/510001

This brings us to the end of the third section of the Digital Transformation with Google Cloud course. Let’s do a quick review of cloud computing models and shared responsibility. You learned about the three main cloud computing service models: IaaS, or infrastructure as a service, PaaS, or platform as a service, and SaaS, or software as a service. Each model brings a different level of service and set of products to suit an organization's needs. You also learned about how the main benefits and trade-offs of each cloud computing model can help organizations choose the one that best fits their business. Organizations will be better equipped to make the right resourcing and budget decisions knowing exactly what is involved in each computing model. Finally, you learned about the shared responsibility between the cloud provider and its customers. While the cloud provider will keep the cloud infrastructure safe and secure, it's the responsibility of the customer to keep its data secure.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/266/video/510003

This concludes the “Digital Transformation with Google Cloud” course where you learned about the foundations of cloud technology and saw how this technology is changing business in a digital era. In the first section of the course, Why Cloud Technology is Transforming Business, you learned: key terms related to the cloud and digital transformation, the benefits of cloud technology with regard to an organization’s digital transformation, the differences between on-premises infrastructure, public cloud, private cloud, hybrid cloud, and multicloud, and the drivers and challenges that lead organizations to undergo a digital transformation. In the second section of the course, Fundamental cloud concepts, you learned: the benefits of moving to cloud infrastructure through customer business use cases, the difference between a solution and a product in Google Cloud, how moving to the cloud shifts an organization's spending from capital expenditure to operational expenditure, and how that affects their total cost of ownership, when private, hybrid, or multicloud infrastructures best apply to different business use cases, basic network infrastructure terminology, and how Google Cloud supports digital transformation with global infrastructure and data centers connected by a fast, reliable network. In the third section of the course, Cloud Computing Models and Shared Responsibility, you learned: the definitions, benefits, and tradeoffs of IaaS, PaaS, and SaaS, which computing model applies to various business scenarios and use cases, what the cloud shared responsibility model is, and which responsibilities are the cloud provider’s or the customer’s for on-premises and cloud computing models Now you’ve had a comprehensive introduction to digital transformation, move on to the next course in the series, Exploring Data Transformation with Google Cloud, where you’ll learn about: the value of data, the cloud data transformation journey, Google Cloud data management solutions, and Google Cloud smart analytics and business intelligence solutions.

### Your Next Steps

## 02: Exploring Data Transformation with Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/267

### Course Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512683

Business data is not a new term, because organizations have long applied information about performance and operations to make decisions. With traditional methods, though, data analysis can take days or months, and is often incomplete. In addition, specialized teams are often required to produce complex reports. With cloud technology, this doesn’t need to be the case. Data can now be consumed, analyzed, and used at speed and scale never before possible. In fact, organizations can now benefit from cloud technology to ingest data in real time to train machine learning models and to act in ways that benefit their business. You no longer need to be a data scientist or technical expert to perform data analysis. With that in mind, this course, “Exploring Data Transformation with Google Cloud,” is designed to help you understand the value of data and how it affects customer experiences, learn about the different Google Cloud data management solutions that are available, and explore the ways that Google Cloud products have made data more useful and accessible to a workforce. Throughout the course, you’ll be presented with graded knowledge assessments. You must pass these assessments to receive course credit. OK, let's get started!

### The Value of Data

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512684

The word “data” is used a lot in today’s business world. There’s a good reason for that, because capturing, managing, and using data is central to redefining customer experiences and creating new value in almost every industry. In this section of the course, you’ll explore how data generates business insights and drives decision making, basic data management concepts, like databases, data warehouses, and data lakes, how organizations can create value by using their current data, collecting new data, and sourcing data externally, how the cloud unlocks business value from all types of data, including structured data and previously untapped unstructured data, the data value chain, from the initial creation of data through data activation, and the importance that data governance plays in a successful data journey.

#### How data creates value

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512685

Data is an essential ingredient for driving innovation and differentiation, and is the key to unlocking value from artificial intelligence. Data powers AI-driven business insights, helps companies make better real-time decisions, and is the basis for how companies build and run their applications. We’re generating more data every day, and the complexity and speed of data arrival are changing the business environment. However, the most valuable insights no longer come just from sales, inventory, and personnel data. They are often hidden across unstructured data points from a myriad of sources and systems. Extracting the insights requires the right blend of tools, skills, and strategy. Some data is easy to capture like financial data, because it can be found in databases and spreadsheets. But other data might not be as easy for example, how your customers engage with you across social media platforms. And after you capture data, how do you store it so that you can gain insights from it? With machine learning, or ML, and artificial intelligence, or AI, organizations can generate insights from data, both past and present, and can also perceive, predict, recommend, and categorize data in new ways. For example, ML lets online retailers who use smart analytics tools to ingest real-time customer behavior data while they surface the best suggestions for particular users. So with every click that the user makes, their website experience becomes increasingly personalized. However, some organizations struggle to remove the barriers that sit between them and their data. According to a report by Accenture titled “Closing the data value gap,” 68% of organizations say they are still unable to realize tangible and measurable value from data. Organizations that want to adapt must determine how to close the gap and support value generation. An intelligent data cloud is the key to unlocking more business value.

#### Unlocking business value from data

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512686

Unlocking the value of data is central to digital transformation. To generate insights, you might need to combine different types of data. However, not all data is created and organized the same way. Data can be categorized into three main types: structured, semi-structured, and unstructured. Structured data is highly organized and well-defined. It’s typically stored in a table with relationships between the different rows and columns, like in a spreadsheet or database. Because structured data is organized in this way, it is easy to analyze. For example, it’s common for organizations to use structured data in customer relationship management tools, or CRMs, as they follow customer behavior patterns and trends. Semi-structured data falls somewhere in between structured and unstructured data. It’s organized into a hierarchy, but without full differentiation or any particular ordering. Examples include emails, HTML, JSON, and XML files. Although this data type doesn’t have a formal structure, it contains tags or other markers that make it easier to analyze than unstructured data. Unstructured data is information that either doesn’t have a predefined data model or isn’t organized in a predefined manner. Categories include: Text, which is the most common, and is often generated and collected from sources like documents, presentations, or even social media posts. Data files, like images, audio files, and videos. And infrastructure activity and performance data, like log files from servers, networks, and applications or output data from Internet of Things (IoT) sensors. Organizations can use unstructured data in many ways. For example, a marketing team might analyze social media posts to identify sentiment toward a brand. Or customer service teams might train automated chatbots to augment support staff by analyzing language in customer communications and providing interactive responses. But in general, unstructured data has historically been difficult to analyze. According to Harvard Business Review, on average less than 1% of an organization’s unstructured data is analyzed or used at all. Until recently, tools to tap the potential of unstructured data were either unavailable or prohibitively expensive and complex. What makes this statistic even more concerning is that, according to Gartner research, unstructured data represents 80% to 90% of all new enterprise data. This reveals a staggering gap between the data being generated and the value that it's providing. But, cloud technology has changed that. With the right cloud tools, businesses can extract value from unstructured data by using machine learning to discover trends, or even using application programming interfaces, or APIs, to extract structure from the data. An example of an API is Google Cloud’s Vision API, which uses machine learning to detect products within a picture and can then even label the picture to describe its contents. Understanding the different types of data available can help organizations define what’s possible with the data solutions they have. One of the transformative powers of the cloud is how it can unlock value from structured and the previously untapped, unstructured data.

#### Data management concepts

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512687

Organizations need a modern approach to enterprise data to manage the vast volumes that are produced. The list of options often includes databases, data warehouses, and data lakes. Let’s explore each of these options starting with databases. A database is an organized collection of data stored in tables and accessed electronically from a computer system. Let’s examine two types of databases: relational and non-relational. A relational database stores and provides access to data points that are related to one another. This means storing information in tables, rows, and columns that have a clearly defined schema that represents the structure or logical configuration of the database. A relational database can establish links—or relationships–between information by joining tables, and structured query language, or SQL, can be used to query and manipulate data. Relational databases are highly consistent, reliable, and best suited for dealing with large amounts of structured data. They’re designed for business data processing and storing the online transactional data needed to support the daily operations of a company. A non-relational database, sometimes known as a NoSQL database, is less structured in format and doesn’t use a tabular format of rows and columns like relational databases. Instead, non-relational databases follow a flexible data model, which makes them ideal for storing data that changes its organization frequently or for applications that handle diverse types of data. This includes when large quantities of complex and diverse data need to be organized, or when the data regularly evolves to meet new business requirements. Choosing the right database depends on the use case. Google Cloud relational database products include Cloud SQL and Spanner, while Bigtable is a non-relational database product. We’ll look at these products in more detail later. Let’s explore another data management concept, the data warehouse. Like a database, a data warehouse is a place to store data. However, while a database is designed to capture data for storage, retrieval, and use, a data warehouse is designed to analyze data. A data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources. Think of the data warehouse as the central hub for all business data. Business data might include point-of-sale transactions, marketing automation, or even customer relationship management data. Suited for both ad hoc analysis and custom reporting, a data warehouse can help analyze sales and identify trends, because it can store both current and historical data in one place. This capability can provide a long-range view of data over time, which makes a data warehouse a primary component of business intelligence. BigQuery is Google Cloud's data warehouse offering. We’ll explore BigQuery in more detail later. Although data warehouses handle structured and semi-structured data, they’re not typically the answer for how to handle large amounts of available unstructured data, like images, videos, and documents. Unstructured data, which doesn't conform to a well-defined schema, is often disregarded in traditional analytics. A data lake is a repository designed to ingest, store, explore, process, and analyze any type or volume of raw data, regardless of the source, like operational systems, web sources, social media, or Internet of Things, or IoT. It can store different types of data in its original format; ignoring size limits, and without much pre-processing or adding structure. Having this unprocessed, raw data available for analysis prevents unintentionally contaminating the data or adding bias. It also means that the raw data can be enriched by merging it with other data at the same time. This differs from a data warehouse that contains structured data that has been cleaned and processed, ready for strategic analysis based on predefined business needs. Data lakes often consist of many different products, depending on the nature of the data that is ingested. For example, the best Google Cloud products for storing structured data are Cloud SQL, Spanner, or BigQuery. For semi-structured data, the options include Datastore and Bigtable. And for storing unstructured data, Cloud Storage is an option. Data warehouses and data lakes should be considered complementary instead of competing tools. Although both store data in some capacity, each is optimized for different uses. Traditional data warehouse users are business intelligence analysts who are closer to the business and focus on driving insights from data. These users traditionally use the data to answer questions. Data lake users, and also analysts, include data engineers and data scientists. They’re closer to the raw data with the tools and capabilities to explore, mine, and experiment with the data. These users find answers in the data, but they also find questions. As enterprises are increasingly focused on data-driven decision making, data warehouses and data lakes play a critical role in an organization’s digital transformation journey. Democratization of data lets users gain a deeper understanding of business situations because they have more context than ever before. Today, organizations need a 360-degree real-time view of their businesses to gain a competitive edge.

#### The role of data in digital transformation

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512688

Organizations have access to data like never before. This includes both internal information, called first-party data, and external information, which is usually data about customers and industry, often called second or third-party data. As organizations have digitized their operations, many types of business data have become available, including information about their customers. First-party data is the proprietary customer datasets that a business collects from customer or audience transactions and interactions. These datasets might include information about digital interactions, like the length of time a user spends on a web page. Second-party data often describes first-party data from another organization, such as a partner or other business in their supply chain, that can be easily deployed to augment a company's internal datasets. The organization does not directly own this data, but it’s relevant to their business. Finally, there’s third-party data, which are datasets collected and managed by organizations that don’t directly interact with an organization's customers or business. These datasets might come from government, nonprofit, or academic sources, like weather or public demographic data, or from industry-specific sources like analyst reports or industry benchmarking. Third-party data is often shared or purchased on data marketplaces or exchanges, such as the Google Cloud Marketplace. Using external data can greatly increase the value of data by providing new context and insights. Let’s explore an example of how an airline transformed their business through data. Budget airlines don't provide food as part of their service. Instead, they charge customers for meals if they want them. This solution might seem cost-effective, but it can be difficult to estimate the number of meals required onboard. If the airline overestimates the number of meals needed, they risk wasting food and losing revenue. But if they underestimate, they risk selling out of food; providing poor customer service and losing potential revenue. One budget airline in Asia reimagined how they could solve this problem by using data. They began by identifying factors to help estimate stock, such as the size of the plane and the number of passengers. But they soon discovered that estimates based on these factors were inaccurate. This meant having to think about their data differently by analyzing information such as destination, time of flight, and flight connections before and after each journey. Using this information, they uncovered actionable insights. For example, they learned that flights to and from India required 73% more vegetarian meals. With these new insights, the airline was able to predict the number of meals required more accurately, which in turn provided a more positive customer experience and improved the profitability of their food service. This is just one example of how cloud technology can unlock new value by reimagining data. No matter where you are in your company, you too can use data to solve challenges.

#### The data value chain

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512689

When you think about data processing, it's important to place it within the broader context of the data value chain. Imagine data traveling along an assembly line, like a car in a factory. The assembly line progressively adds parts and value to an object that moves along it. Raw data at the beginning of the line is eventually transformed into actions that humans or machines take. Let’s examine the steps in this data value chain. Data genesis is the initial creation of a unit of data; this could be a click on a website, the swipe of a card, a sensor recording from an IoT device, or countless other examples. It’s the raw material that will eventually be turned into an insight ready for action. Data collection brings that initial unit of data to the assembly line through ingestion. The basic function of ingestion is to extract data from the system in which it’s hosted and bring it to a new system. It can have dramatically different requirements based on the volume, velocity, and variety of the raw data that’s required for a given analysis, and how fast the data needs to be analyzed. Data processing is where the collected raw data is transformed into a form that’s ready to derive insights from. The data will likely need to be adjusted, for example, by merging different datasets together. It can be a single-stage operation, or it can be a complex tree of cascading procedures. In our manufacturing process analogy, this phase is where raw materials take the shape of the pre-assembly parts of a manufactured product. Data storage is where the data lands, can be found, and is ready for analysis and action. As with real-world manufacturing, where storage options vary depending on the type of product that is processed, different types of data can be stored in different ways. For example, NoSQL is available for fast reads and writes, data warehousing for fast access to analysis, and object storage for unstructured data. There are also customized options of these standard stores. Data analysis provides direction for business-oriented action. To continue with our manufacturing line analogy, in this stage, inputs from the data processing stage are assembled into a final product. And finally, the last step in the data value chain is data activation. When an analysis is produced, it needs to be pushed to the relevant business procedures and decision makers so that action can be taken and the value chain completed. The most common points of activation are applications that make automated decisions and business intelligence dashboards that guide humans toward better, more informed decisions. In our manufacturing line example, this is the step where a fully produced product is put to its intended use. There is no one way to assemble a data value chain, as there’s no one way to create a real-world manufacturing line. Similarly, as technologies progress, new inputs become available, your workforce evolves, or the desired output changes, the optimal value chain will also change. However, at its core, the value chain principles hold. We want to use raw data to perform actions that benefit the business.

#### Data governance

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512690

In the last decade, the amount of data produced has increased exponentially, and the cloud has made it easier to collect, store, and analyze it at a lower cost. Organizations are now challenged to democratize and embed data in every decision, while they also ensure that it’s secure and protected from unauthorized use. An effective data governance program can help implement data directives to achieve this. But what exactly is data governance? Data governance means setting internal standards—data policies—that apply to how data is gathered, stored, processed, and disposed of. It governs who can access certain data and what data is under governance. It also involves complying with external standards set by industry associations, government agencies, and other stakeholders. Data governance focuses on making the data available to all stakeholders across the full lifecycle of the data, in a form that they can readily access and use, in a manner that generates the desired business outcomes through insights and analysis, and if relevant, in a way that conforms to regulatory standards and compliance needs. Data governance brings several benefits. It makes data more valuable. Data governance implements processes to ensure high quality data, and provides a platform that makes it easier to share data securely with stakeholders across the organization. It helps users make better, more timely decisions. Through data governance, users throughout an organization get the data they need to reach and service customers, design and improve products and services, and seize opportunities for new revenues. By democratizing data, organizations can embed data in all decision making. It improves cost controls. Data helps organizations manage resources and operate more effectively. Because they can eliminate data duplication caused by information silos, they don’t overbuy—and have to maintain—expensive hardware. It enhances regulatory compliance. An increasingly complex regulatory climate has made it even more important for organizations to establish rigorous data governance practices. They avoid risks associated with noncompliance and proactively anticipate new regulations. It helps earn greater trust from customers and suppliers. By being in auditable compliance with both internal and external data policies, organizations gain the trust of customers and partners. It helps manage risk. With robust data governance, organizations can reduce concerns about exposure of sensitive data to individuals or systems who lack proper authorization, security breaches from malicious outsiders, or even insiders who access data they don’t have the right to see. It allows more personnel access to more data. Strong data governance provides confidence that the right personnel get access to the right data, and that this democratization of data does not negatively impact the organization. It's possible that organizations without an effective data governance program will suffer from compliance violations. This can lead to fines, poor data quality–which generates lower quality insights that impact business decisions, challenges in finding data–which results in delayed analysis and missed business opportunities, and poorly trained data models for AI–which reduces the model accuracy and benefits of using AI. Every organization needs data governance. As businesses throughout all industries progress on their digital transformation journeys, data has quickly become the most valuable asset they possess.

### Google Cloud Data Management Solutions

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512692

Data plays such an integral role in an organization's operations. For this reason, it’s crucial to have an effective way of storing and managing it. Google Cloud offers a wide range of data management products and solutions, each applicable to different business use cases. In this section of the course, you’ll explore: Google Cloud data management options and the differences between them. The different storage classes available with Cloud Storage. How to choose the right storage product to meet the needs of your organization. And ways an organization can migrate and/or modernize their current database in the cloud.

#### Unstructured data storage

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512693

Every application needs to store data, like media to be streamed or even sensor data from devices, and different applications and workloads require different storage solutions. Google Cloud offers several core storage products. This list includes Cloud Storage, Cloud SQL, Spanner, BigQuery, Firestore, and Bigtable. Depending on your use case, you might use one or several of these services to do the job. Let’s begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as “objects” instead of as file storage, which is a file and folder hierarchy, or as block storage, which is chunks of a disk. These objects are stored in a packaged format that contains the binary form of the actual data, and relevant associated metadata–such as creation date, author, resource type, and permissions–and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. This type of data is referred to as unstructured, which means that it doesn’t have a predefined data model or isn’t organized in a predefined manner, as you might find in a structured database format. Cloud Storage lets customers store any amount of data and retrieve it as often as needed. It’s a fully managed, scalable service that has a wide variety of uses, such as serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users through direct download. There are four primary storage classes in Cloud Storage. The first is Standard storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline storage. This option is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline storage, Coldline storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes have differences, it’s worth noting there are several characteristics that apply across all of these storage classes, which include: unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience–which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data.

#### Structured data storage

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512694

In the previous lesson, you saw how Cloud Storage is used to store unstructured data. Now let’s explore some Google Cloud data storage products that are suited for storing structured data. Structured data consists of numbers and values that are organized in a predefined format that’s easily searchable in a relational database. Earlier in the course, we mentioned that a relational database stores information in tables, rows, and columns that have a clearly defined schema that represents the structure or logical configuration of the database. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It’s designed to transfer mundane—but necessary and often time-consuming—tasks to Google, like applying patches and updates, managing backups, and configuring replications, so you can focus on building great applications. Trusted by thousands of the largest enterprises around the world, organizations that use Cloud SQL obtain various benefits. It doesn't require any software installation or maintenance. It supports managed backups, so backed-up data is securely stored and accessible if a restore is required. It encrypts customer data when on Google’s internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. Spanner is a fully managed, mission-critical, relational database service that scales horizontally to handle unexpected business spikes. Battle tested by Google’s own mission-critical applications and services, Spanner is the service that powers Google’s multi-billion dollar business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, which provides data redundancy to reduce downtime when a zone or instance becomes unavailable (the goal is to prevent a single point of failure), strong global consistency, which ensures that all locations where data is stored are updated to the most recent data version quickly, and high numbers of input and output operations per second (tens of thousands of reads and writes per second or more). Both Cloud SQL and Spanner are fully managed database services, but how do they differ? Cloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server with greater than 99.95% availability. Database Migration Service (DMS) makes it easy to migrate your production databases to Cloud SQL with minimal downtime. And then there is Spanner, which is a fully managed relational database with unlimited scale, strong consistency, and up to 99.999% availability with zero downtime for planned maintenance and schema changes. This globally distributed, ACID-compliant cloud database automatically handles replicas, sharding, and transaction processing, so you can quickly scale to meet any usage pattern and ensure success of products. When considering which option is best for your business, consider this: if you have outgrown any relational database, are sharding your databases for throughput high performance, need transactional consistency, global data, and strong consistency, or just want to consolidate your database, consider using Spanner. If you don’t need horizontal scaling or a globally available system, Cloud SQL is a cost-effective solution. The final structured data storage solution that we’ll explore is BigQuery. BigQuery is a fully-managed data warehouse. As we’ve already learned, a data warehouse is a large store that contains petabytes of data gathered from a wide range of sources within an organization and is used to guide management decisions. Because it’s fully managed, BigQuery takes care of the underlying infrastructure, so users can focus on using SQL queries to answer business questions, without having to worry about deployment, scalability, and security. BigQuery provides two services in one: storage and analytics. It’s a place to store petabytes of data. For reference, one petabyte is equivalent to 11,000 movies at 4k quality. BigQuery is also a place to analyze data, with built-in features like machine learning, geospatial analysis, and business intelligence. Data in BigQuery is encrypted at rest by default without any action required from a user. Encryption at rest is encryption used to protect data that’s stored on a disk, including solid-state drives, or backup media. BigQuery provides seamless integration with the existing partner ecosystem. Businesses can tap into our ecosystem of system integrators and data integration partners to help enhance analytics and reporting. These integrations mean that BigQuery lets organizations make the most of existing investments in business intelligence, data ingestion, and data integration tools. Industry research shows that 90% of organizations have a multicloud strategy, which adds complexity to data integration, orchestration, and governance. BigQuery works in a multicloud environment, which lets data teams eradicate data silos by using BigQuery to securely and cost effectively analyze data across multiple cloud providers. BigQuery also has built-in machine learning features so that ML models can be written directly in BigQuery by using SQL. And if other professional tools—such as Vertex AI from Google Cloud—are used to train ML models, datasets can be exported from BigQuery directly into Vertex AI for a seamless integration across the data-to-AI lifecycle.

#### Semi-structured data storage

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512695

Semi-structured data contains elements of both structured and unstructured data. It does have some defining or consistent characteristics, but generally doesn’t follow a structure as rigid as a relational database. Semi-structured data is easier to organize because it usually contains some organizational properties, such as tags or metadata. An example of semi-structured data is an email message. While the actual content of the email is unstructured, it does contain structured data such as the name and email address of the sender and recipient, the time sent, and so on. Google Cloud offers two semi-structured data storage products, Firestore and Bigtable. Firestore is a flexible, horizontally scalable, NoSQL cloud database for storing and syncing data in real-time. Firestore can be directly accessed by mobile and web applications. Firestore performs data storage in the form of documents, with the documents being stored in collections. Documents support a wide variety of data types, such as nested objects, numbers, and strings. One of Firestore’s main features is automatic scaling. It’s been designed to scale automatically depending on user demand, but retains the same level of performance irrespective of database size. Firestore also provides offline usage through a comprehensive database on users’ devices. Offline data access ensures that applications run without interruption, even if the user gets disconnected from the internet. And then there’s Bigtable, Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle large workloads at consistent low latency, which means Bigtable responds to requests quickly, and high throughput, which means it can send and receive large amounts of data. For this reason, it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding on a storage option, you might choose Bigtable if you’re working with more than 1 TB of semi-structured or structured data, data is fast with high throughput, or it’s rapidly changing, you’re working with NoSQL data, data is a time-series or has natural ordering, you’re working with big data and running batch or real-time processing on the data, or you’re running machine learning algorithms on the data.

#### Choosing the right storage product

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512696

So, you’ve learned about the different storage options that Google Cloud offers, but in what scenarios should you use each one? Ultimately, it’s a combination of the data type that needs to be stored and the business need. If data is unstructured, then Cloud Storage is the most appropriate option. You have to decide a storage class: Standard, Nearline, Coldline, or Archive. Or whether to let the Autoclass feature decide that for you. If data is structured or semi-structured, choosing a storage product will depend on whether workloads are transactional or analytical. Transactional workloads stem from online transaction processing, or OLTP, systems, which are used when fast data inserts and updates are required to build row-based records. An example of this is point-of-sale transaction records. Then there are analytical workloads, which stem from online analytical processing, or OLAP systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. An example here would be analyzing sales history to see trends and aggregated views. After you determine if the workloads are transactional or analytical, you must determine whether the data will be accessed by using SQL. So, if your data is transactional and you need to access it by using SQL, then Cloud SQL and Spanner are two options. Cloud SQL works best for local to regional scalability, and Spanner is best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery might be the best option. BigQuery, Google’s data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. It’s best for real-time, high-throughput applications that require only millisecond latency.

#### Database migration and modernization

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512697

Running modern applications on legacy, on-premises databases requires overcoming expensive, time-consuming challenges around latency, throughput, availability, and scaling. With database modernization, organizations can move data from traditional databases to fully managed or modern databases with relative ease. There are different ways that an organization can migrate or modernize their current database in the cloud. The most straightforward method is a lift and shift platform migration. This is where databases are migrated from on-premises and private cloud environments to the same type of database hosted by a public cloud provider, such as Google Cloud. Although this solution makes the database more difficult to modernize, it does bring with it the benefits of minimal upheaval, and having data and infrastructure managed by the cloud provider. Alternatively, a managed database migration allows the migration of databases from SQL Server, MySQL, PostgreSQL, and others to a fully managed Google Cloud database. Although this migration requires careful planning and might cause slight upheaval, a fully managed solution lets you focus on higher priority work that really adds value to your organization. Google Cloud’s Database Migration Service (DMS) can easily migrate your databases to Google Cloud, or Datastream can be used to synchronize data across databases, storage systems, and applications. Let’s look at a real life use case. With 18 fulfillment centers, 38 delivery centers, and a catalog of more than 22 million items, the online retailer Wayfair needed a way to quickly move from their on-premises data centers, which ran on SQL Server, to Google Cloud. This had to be achieved without inconveniencing their team of over 3,000 engineers, their tens of millions of customers, or their 16,000 supplier partners. So, the goal was to lift and shift their workloads as quickly as possible with minimal changes, and then use cloud databases to modernize those workloads. Wayfair chose Google Cloud because of the clear path for shifting workloads to the cloud by using Cloud SQL for SQL Server. Google Cloud provided the flexibility to be deliberate about which engine and product to run Wayfair’s systems on going forward. They liked how they could run SQL Server on virtual machines (VMs), for example, but could also benefit from database offerings like Cloud SQL and Spanner. Now that migration is complete, they also use Google Kubernetes Engine (GKE) and Compute Engine VMs to host the services built by the Google Cloud team. They also use Pub/Sub and Dataflow for sending operational data to their analytical store in BigQuery.

### Making Data Useful and Accessible

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512699

It’s not always easy for organizations to make smart business decisions based on the data they’ve collected or produced. And too often there can be blockers in place that make analyzing it difficult for part, or all, of a workforce. With Google Cloud, that doesn’t need to be the case. In the final module of this course, you’ll explore: How Looker makes it easy for a workforce to access the data they need, when they need it. How streaming analytics in real time can make data more useful. Two Google Cloud products that modernize data pipelines: Pub/Sub and Dataflow.

#### Business intelligence and insights using Looker

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512700

When data is in a database, a fair amount of effort and expertise might still be required to uncover insights. This goal can be achieved by using a business intelligence solution. However, the challenge that organizations often face is identifying the right business intelligence solution. Some solutions are too complex and not accessible by those outside the data engineering or data analysis teams. This means other teams have to put in requests and wait for answers, which defeats the purpose of gaining real-time insights. Other solutions let everyone in the business perform their own data analysis, but they can only perform their analysis with a selection of the available data. This means that only a few people, or possibly no one, has a full view of the organization’s business data. Looker is a Google Cloud business intelligence (BI) platform designed to help individuals and teams analyze, visualize, and share data. This includes creating interactive dashboards and reports that are easy to understand and share. By having a reliable authority for business data, anyone on a team can explore it, ask and answer their own questions, and create visualizations. This approach empowers organizations to not just uncover insights but also act on them. Looker supports BigQuery, along with more than 60 different SQL databases. Together, BigQuery and Looker provide rich, interactive dashboards and reports without compromising performance, scale, security, or data freshness. Looker is also 100% web-based, which makes it easy to integrate into existing workflows and share with multiple teams at an organization. So how can Looker be used? Let's explore an example. Diamond Resorts, a global leader in hospitality, offers destinations, events, and experiences to help people recharge, connect, and enjoy. They had previously used a mixture of complex Excel workbooks and legacy BI tools to track important metrics. Each business unit operated and ran their own siloed data initiatives. As a result, there were: No common view of business or single authority for common metrics Redundant data engineering efforts, because work was never shared or used across the organization And inconsistent project prioritization, because decisions were driven primarily on intuition as opposed to actual data Also, infrastructure did not meet business requirements with: Executive reporting efforts that took months to complete Data that was duplicated across multiple business units without proper governance Multiple reporting tools and data warehouses throughout the business And infrastructure that didn’t support advanced analytics aspirations Diamond Resorts wanted to create a single common cloud-based architecture that was fully-managed; establishing data governance and enabling the business to be more data-driven, while they set the foundation for advanced analytics efforts. They migrated to the cloud and began using Looker to help improve business agility. This decision let them gain access to real-time insights in less than 3 months. It helped them to navigate COVID changes with important operational metrics such as daily booking and cancellations, while it also provided a 360-degree customer view. And in addition to this, manual reporting for the Yield Management team was decreased by hours each day. The chief information officer said, “Projects that we anticipated coming in future years were suddenly ready to be tackled within weeks.” This is just one example of how an effective business intelligence solution can let businesses transform to better serve their customers.

#### Streaming analytics

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512701

Data traditionally is moved in batches. Batch processing often processes large volumes of data at the same time, with long periods of latency. An example is payroll and billing systems that have to be processed on either a weekly or monthly basis. Although this approach can be efficient to handle large volumes of data, it doesn’t work with time-sensitive data that’s meant to be streamed, because that data can be stale by the time it’s processed. Streaming analytics is the processing and analyzing of data records continuously instead of in batches. Generally, streaming analytics is useful for the types of data sources that send data in small sizes, often in kilobytes, in a continuous flow as the data is generated. This results in the analysis and reporting of events as they happen. Sources of streaming data include equipment sensors, clickstreams, social media feeds, stock market quotes, app activity, and more. Companies use streaming analytics to analyze data in real time and provide insights into a wide range of activities, such as metering, server activity, geolocation of devices, or website clicks. Use cases include: Ecommerce: User clickstreams can be analyzed to optimize the shopping experience with real-time pricing, promotions, and inventory management. Financial services: Account activity can be analyzed to detect abnormal behavior in the data stream and generate a security alert. Investment services: Market changes can be tracked and settings adjusted to customer portfolios based on configured constraints, such as selling when a certain stock value is reached. News media: User click records can be streamed from various news source platforms and the data can then be enriched with demographic information to better serve articles that are relevant to the targeted audience. Utilities: Throughput across a power grid can be monitored and alerts generated or workflows initiated when established thresholds are reached. Google Cloud offers two main streaming analytics products to ingest, process, and analyze event streams in real time, which makes data more useful and accessible from the instant it’s generated. Pub/Sub ingests hundreds of millions of events per second, but Dataflow unifies streaming and batch data analysis and builds cohesive data pipelines. A data pipeline represents a series of actions, or stages, that ingest raw data from different sources and then move that data to a destination for storage and analysis. You'll explore these products in more detail in the next section.

#### Pub/Sub and Dataflow

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512702

One of the early stages in a data pipeline is data ingestion, which is where large amounts of streaming data are received. Data, however, may not always come from a single, structured database. Instead, the data might stream from a thousand, or even a million, different events that are all happening asynchronously. A common example of this is data from IoT, or Internet of Things, applications. These can include sensors on taxis that send out location data every 30 seconds or temperature sensors around a data center to help optimize heating and cooling. Pub/Sub is a distributed messaging service that can receive messages from various device streams such as gaming events, IoT devices, and application streams. The name is short for Publisher/Subscriber, or publish messages to subscribers. After messages have been captured from the streaming input sources you need a way to pipe that data into a data warehouse for analysis. This is where Dataflow comes in. Dataflow creates a pipeline to process both streaming data and batch data. “Process” in this case refers to the steps to extract, transform, and load data, sometimes referred to as ETL. A popular solution for pipeline design is Apache Beam. It’s an open source, unified programming model to define and execute data processing pipelines, including ETL, batch, and stream processing. Dataflow handles much of the complexity for infrastructure setup and maintenance, and is built on Google’s infrastructure. This product allows for reliable auto scaling to meet data pipeline demands. Dataflow is serverless and fully managed. Serverless computing means that software developers can build and run applications without having to provision or manage the back-end infrastructure. For example, Google Cloud manages infrastructure tasks on behalf of the users, like resource provisioning, performance tuning, and ensuring pipeline reliability. And a fully managed environment is one where software can be deployed, monitored, and managed without needing an operations team. You can create this environment by using automation tools and technologies. Using a serverless and fully managed solution like Dataflow means that you can spend more time analyzing the insights from your datasets and less time provisioning resources to ensure that your pipeline will successfully complete its next cycles.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/267/video/512704

This brings us to the end of the Exploring Data Transformation with Google Cloud course. Let’s do a quick recap. In the first section of the course, The Value of Data, you learned how data generates business insights and drives decision making; basic data management concepts, like databases, data warehouses, and data lakes; how organizations can create value by using their current data, collecting new data, and sourcing data externally; how the cloud unlocks business value from all types of data, including structured data and previously untapped unstructured data; about the data value chain, from the initial creation of data to data activation; and the importance that data governance plays in a successful data journey. In the second section of the course, Google Cloud Data Management Solutions, you learned about Google Cloud data management options and the differences between them; about the different storage classes available with Cloud Storage; how to choose the right storage product to meet the needs of your organization; and ways an organization can migrate and/or modernize their current database in the cloud. Finally, in the third section of the course, Making Data Useful and Accessible, you learned how Looker makes it easy for a workforce to access the data they need, when they need it; how streaming analytics in real time can make data more useful; and about two Google Cloud products that modernize data pipelines, Pub/Sub and Dataflow. Now that you’ve had a comprehensive introduction to data transformation, move on to the next course in the series, Innovating with Google Cloud Artificial Intelligence, where you’ll learn about the fundamentals of artificial intelligence and machine learning, selecting Google Cloud AI solutions, and building and using Google Cloud AI solutions. We’ll see you next time!

### Your Next Steps

## 03: Innovating with Google Cloud Artificial Intelligence

- https://www.cloudskillsboost.google/paths/9/course_templates/946

### Course Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452214

Artificial intelligence and machine learning represent an important evolution in information technologies that are quickly transforming a wide range of industries. As organizations digitally transform, they can find themselves with lots of data. As time progresses, the amount of data they have only grows. Although that data is really valuable, it can be very laborious to collect, process, and analyze. New tools and methodologies are needed to manage what's being collected, analyze it for insights, and then act on those insights. What do these organizations do? This is where artificial intelligence and machine learning come in. This course; Innovating with Google Cloud Artificial Intelligence, is designed to help you explore important AI and machine learning or ML concepts, and understand how they can bring value to your business, learn about the AI and ML solutions that Google Cloud offers, and understand how Google Cloud's pre-trained APIs, AutoML, and custom AI and ML products can help transform your business. Throughout the course, you'll be presented with graded knowledge assessments. You must pass these assessments to receive course credit. Let's get started.

### AI and ML Fundamentals

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452215

Google has nine products with over 1 billion users. Android, Google Chrome, Gmail, Google Drive, Google Maps, Google Search, the Google Play Store, Youtube and Google photos. Artificial intelligence and machine learning were integrated into these products to make the user experience of each even more efficient and productive. This includes features like search in photos, recommendations in Youtube, smart composing in Gmail, and traffic predictions in Google Maps. Google continues to innovate products powered by new technologies such as generative AI, which can produce content for you. As you consider how AI and ML could provide a benefit to your business, understanding the basics is important. In this section of the course, you'll explore the difference between artificial intelligence and machine learning, how machine learning differs from data analytics and business intelligence. Different types of problems that AI solutions are suited to solve. The importance of using quality data for machine learning, and the importance of responsible and explainable AI.

#### AI and ML defined

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452216

People commonly use the terms artificial intelligence, AI, and machine learning, ML interchangeably. The confusion is understandable because artificial intelligence and machine learning are closely related. However, these trending technologies differ in several ways, including scope and application. Before we advance, let's define each of the terms. Artificial intelligence is a broad field which refers to the use of technologies to build machines and computers that can mimic cognitive functions associated with human intelligence. These functions include, being able to see, understand, and respond to spoken or written language, analyze data, make recommendations and more. Although artificial intelligence is often thought of as a system in itself, it's a set of technologies implemented in a system to let it reason, learn, and act to solve a complex problem. Machine learning is a subset of AI that lets a machine learn from data without being explicitly programmed. It relies on various models to analyze large amounts of data, learn from the insights, and then make predictions and informed decisions. Machine learning algorithms improve performance over time as they are trained or exposed to more data. Machine learning models at the output, or what the program learns from running an algorithm on training data. When more data is used, the model improves. One helpful way to remember the difference between the two is to imagine them as umbrella categories. Artificial intelligence is the overarching term that covers a variety of specific approaches and algorithms. Machine learning sits beneath that umbrella, but so do other major sub fields such as deep learning, robotics, expert systems, and natural language processing. Another area of AI you may be hearing a lot about is generative AI. This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data. Google applies generative AI to products like Google Workspace to help users easily automate different types of tasks, like generating summaries of long documents. Google also provides development tool kits, such as generative AI APIs to developers to help them create customized products and services. Generative AI can be used in a variety of applications, such as conversational bots, content generation, document synthesis, and product discovery.

#### How AI and ML differ from data analytics and business intelligence

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452217

Within your organization, perhaps you're familiar with a specific dashboard that analysts view every day. Or maybe managers review a particular report each month. Both the dashboard and the report are examples of backward looking data. They look at what happened in the past. Most data analysis and business intelligence is based on historical data, used to calculate metrics or identify trends. But to create value in your business, you need to use that data to make decisions for future business. This is where artificial intelligence and machine learning come in. They're the key to unlocking these capabilities. Let's consider an example to emphasize this point. Maya leads the business strategy and operations team for an international airline to establish a trend in customer purchasing patterns, she's looking at historical annual reports. She can use this data to generate dashboards that present information such as customer demographic distribution and sales in recent years. But there's nothing new or transformational about this decision making process. Maya is simply using data analytics to illustrate what's happened in the past. But what if Maya could predict the satisfaction rate of each flight, or predict customer complaints and get ahead of them? To do this effectively, she needs access to a lot more data and use ML models to make predictions for future business. The data she needs might include the number of passengers per flight, the duration of each flight, the customer satisfaction ratings per flight, and the number of customer complaints per flight. She also needs to understand factors that contributed to customer complaints, weather reports, seasonal indicators, and the time to resolution data for customer complaints. With all of these various data points, Maya might predict the quality of a single flight and its customer complaints. But there are hundreds of flights each day. The real value for Maya would come from being able to make predictive insights for all flights all year round. More importantly, it would be far more valuable if she could dynamically adjust pricing or staff assignments, or even catering based on the predictions. Remember, ML provides a method to teach a computer how to solve problems by feeding examples of the correct answers. With access to the right data, Maya can use machine learning to uncover these types of predictive insights to benefit the airline and its customers.

#### Problems that ML is suited to solve

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452218

Machine learning lets computer systems continuously adjust and enhance themselves as they accrue more experiences. For this reason, when more data is put into them, the results are more accurate. With this in mind, ML is suited to solve four common business problems. The first is replacing or simplifying rule based systems. Let's use Google Search as an example. Suppose you want to search for the Giants, a US sports team. If you type in Giants, should the search results show you the San Francisco Giants or the New York Giants? One's a baseball team based in California and the other is an American football team based in New York. In years gone by, the search engine used hand coded rules to decide which sports team to show user. If the query is Giants and the user is in the Bay Area, show them results about San Francisco Giants. If the user is in the New York area, show them results about NY Giants. If the user is anywhere else, show them results about tall people. This was for just one query. If you multiply this process by millions of different queries and users each day, you can probably imagine how complex the whole code base became. This is a perfect problem for ML to solve. If all the data that's available shows which search results users clicked on per query, a machine learning model can be trained to predict the rank for search results. A second business problem ML can help solve relates to automating processes. ML is designed to make predictions and repeated decisions at scale. Let's explore another example, this time from a property developer headquartered in Thailand called Ananda Development. For every sale, both an Ananda Development inspector and the buyer have to conduct a detailed check of the property. This was a manual, time-consuming process that was prone to much human error. Inspectors would visually check hundreds of items a day for problems, list any issues on paper and then photograph the findings. Multiplied across several projects, this workload adds up. Ananda Development decided to create a mobile application to make this process more efficient. Inspectors would verbally describe defects and critical issues to the application that ran on their smartphones. The application would then track and document the inspection results. In planning the application, the business realized it would need to recognize and convert to text, Thai language, speech and a version of English spoken by many Thai people. They decided to automate this process using Google's speech-to-text API. Furthermore, Ananda Development wanted to establish a pathway to use machine learning to complete condominium inspections by using remotely piloted drones. They decided to automate that process by using the Cloud Vision API to capture images of defects and automatically classify information about each one. Within three months of implementation, Ananda Development had saved around 130 hours of inspection time and over 100,000 US dollars in manpower costs. The inspection process is now more efficient and accurate. And as another benefit, buyers also receive copies of electronic inspection reports and updated status notes as defects are repaired. So far, you heard about ML problems that use structured data to make predictions at scale. A third type of business problem that ML can help solve is understanding unstructured data like images, videos, and audio. This example comes from Ocado, one of the world's largest online only grocery supermarkets. Previously, when Ocado received emails, they would all go to a central mailbox for sorting and forwarding by a human. This process was time-consuming and led to a poor customer experience. To improve and scale this process, Ocado used ML's ability to process natural language to identify the customer's sentiment and the topic of each message, so that they could route it immediately to the relevant department. This eliminated multiple rounds of reading and triaging, and ultimately improved customer satisfaction, and retention. And finally, there's personalization. Many businesses use ML to personalize user experiences and YouTube is a great example of personalization in action. When you watch a video on YouTube, you've probably noticed there's a list of recommended videos that are up next. When your video finishes, a new video will play and YouTube wants it to be interesting and useful for you. By using ML to provide personalized recommendations, YouTube can deliver a better customer experience. Many businesses use this same approach to surface product recommendations on their websites that are personalized to individual users. Other businesses use personalization to surface new content like music recommendations or films to stream. It's important to remember that ML models aren't standalone solutions and that solving complex business challenges requires combinations of models. There are of course, many more applications of machine learning for businesses and you can learn even more about them in our machine learning courses.

#### Why ML requires high-quality data

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452219

Data is used by machine learning models to derive predictive insights and make repeated decisions. However, the accuracy of those predictions relies on large volumes of data that is correct and free of errors. Data is considered low quality if it's not aligned to the problem, or is biased in some way. If you feed an ML model low quality data, it's like teaching a child with incorrect information. An ML model can't make accurate predictions by learning from incorrect data. So, how can you ensure that you have quality data when training an ML model? To assess it's quality, data is evaluated against six dimensions. Completeness, uniqueness, timeliness, validity, accuracy, and consistency. Let's explore what each of these mean in more detail. The completeness of data refers to whether all the required information is present. If the data is incomplete, then the model will not learn all the patterns that are necessary to make accurate predictions. Take, for example, the training of an ML model that's reliant on a data set of customer transactions. If some transactions are missing critical information, such as the date of the transaction, the accurate training of the model will be affected. Data should be unique. If a model is trained on a data set with a high number of duplicates, the ML model may not be able to learn accurately. This is because it will be confused by the duplicate records and won't be able to accurately identify patterns. For example, if you're training a model to identify a breed of dog from a photo, it's important to have images of many different unique breeds. If the data set contains many thousands of images, but most of them are just photos of Labradors, the model will find it difficult to correctly identify most other breeds accurately. The timeliness of the data refers to whether the data is up-to-date and reflects the current state of the phenomenon that's being modeled. If the data is not timely, then the model might be making predictions based on outdated or irrelevant information. Training an ML model to predict stock market fluctuations might rely on a data set of stock prices. If the data is several months old, it's untimely for making current predictions. Validity means the data conforms to a set of predefined standards and definitions, such as type and format. Validity also ensures that data is in an acceptable range. An example of invalid data is a date of 08-12-2019, when the standard format is defined as year, month, and date. Accuracy reflects the correctness of the data, such as the correct birth date or the accurate number of units sold. For example, in a data set of images, some images might be labeled as dogs when they actually show cats. Note how accuracy is different from validity. Whereas validity focuses on type, format, and range, accuracy is focused on form and content. Finally, the consistency of the data refers to whether the data is uniform and doesn't contain any contradictory information. If data is inconsistent, then an ML model might be unable to make accurate predictions. If the same entity appears with different names or values across different parts of the data, it would lead to inconsistent data. For example, in a dataset of customer information, the same customer might appear as John Smith in one place, and J.Smith in another. Remember, data is the only lens through which a model views the world. Anything the model can't see, it assumes doesn't exist. So it's your responsibility to provide the model with complete and correct data. The good news is that most of these problems can be solved simply by getting more high quality data, but you have to be purposeful in collecting that data.

#### The importance of responsible and explainable AI

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452220

AI has significant potential to help solve challenging problems, including advancing medicine, understanding language, and fueling scientific discovery. To realize that potential, it's critical that AI is used responsibly. To that end, Google has established principles that guide Google AI applications, best practices to share our work with communities outside of Google and programs to operate rationalize our efforts. The principles state that AI should be socially beneficial, avoid creating or reinforcing unfair bias, be built and tested for safety, be accountable to people, incorporate privacy design principles, uphold high standards of scientific excellence. And be made available for uses that accord with these principles. In addition to these principles, Google will not design or deploy AI in the following application areas. Technologies that cause or are likely to cause overall harm. Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people. Technologies that gather or use information for surveillance, violating internationally accepted norms. And technologies whose purpose contravenes widely accepted principles of international law and human rights. Although these are Google's own guiding AI principles, we urge other organizations to develop their own set of principles that encourage responsible AI development. It's also important for organizations to debug and improve ML model performance and help others understand their model's behavior. Organizations building ML models also need help with detecting and resolving bias, drift, and other gaps in their data and models. In addition, having human interpretable explanations of your ML models will help grow end-user trust and improve transparency. Explainable AI is Google Cloud's set of tools and frameworks to help you understand and interpret predictions made by your machine learning models. These tools are natively integrated with several Google products and services to ensure transparent AI development.

### Google Cloud’s AI and ML Solutions

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452222

Historically, artificial intelligence and machine learning were not accessible to ordinary people. Most of the people capable of developing AI and ML solutions were specialty engineers, who were scarce in number and expensive to hire. The reality is that ML is more accessible now than ever before, which allows more people to build, even those without the technical expertise. Google Cloud offers four options for building machine learning models. The first option is BigQuery ML. This is a tool for using SQL queries to create and execute machine learning models in BigQuery. If you already have your data in BigQuery and your problems fit the predefined ML models, this could be your choice. The second option is to use pre trained APIs, or application programming interfaces. This option lets you use machine learning models that were built and trained by Google, so you don't have to build your own ML models if you don't have enough training data or sufficient machine learning expertise in house. The third option is AutoML, which is a no code solution, letting you build your own machine learning models on Vertex AI through a point and click interface. And finally, there's custom training through which you can code your very own machine learning environment, the training, and the deployment, which gives you flexibility and provides control over the ML pipeline. In this second section of the course, you'll learn more about these four options for building machine learning models, and you'll also learn about some of Google's other AI solutions.

#### BigQuery ML

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452223

Machine learning on large data sets requires extensive programming and knowledge of ML frameworks. These requirements restrict solution development to a very small set of people within each company, and they exclude data analysts who understand the data but have limited machine learning knowledge and programming expertise. Although BigQuery started solely as a data warehouse, over time it has evolved to provide additional features that support the data to AI life cycle. BigQuery ML democratizes the use of machine learning by empowering data analysts, but primary data warehouse users, to build and run models by using existing business intelligence tools and spreadsheets. Predictive analytics can guide business decision making across the organization. Using Python or Java to program an ML solution isn't necessary. Models are trained and access directly in BigQuery by using SQL, which is a language familiar to data analysts. BigQuery ML brings machine learning to the data. It reduces complexity because fewer tools are required. It also increases speed of production because moving and formatting large amounts of data for Python-based ML frameworks is not required for model training in BigQuery. BigQuery ML also integrates with Vertex AI, Google Cloud's end to end AI and ML platform. When BigQuery ML models are registered to the Vertex AI model registry, they can be deployed to endpoints for online prediction.

#### Pre-trained APIs

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452224

Google Cloud's pre trained API's are a great option If you don't have your own training data. These are ideal in situations where an organization doesn't have specialized data scientists, but it does have business analysts and developers. This is the fastest and lowest effort of the machine learning approaches, but is less customizable than the others. Google Cloud's pre trained API's can help developers build smart apps quickly by providing access to ML models for common tasks like analyzing images, videos, and text. API's can be deployed in a virtual private cloud, on premises, or in Google's public cloud regardless of the level of ML experience. Let's imagine a developer building a mobile app that users will submit photos to the developer needs the app to recognize what the images are and filter out any that aren't safe for work. The developer might choose Vision API. This offers powerful, pre trained machine learning models, which use Google data to automatically detect faces, objects, text, and even sentiment in images. The developer can use Vision API to assign labels to images and quickly classify them into millions of predefined categories. The natural language API is another out of the box, pre trained API. If a business has a contact form on its website that receives many messages every day. This data can be difficult and time intensive to manually handle, categorize an action. Natural language API discovers syntax, entities and sentiment in text and classifies texts into a predefined set of categories. In this case, it can decide if comments represent complaints, Praise, and attempt to learn more about your business and more. Google also offers several other pre trained API's. The Cloud Translation API converts texts from one language to another. The speech to text API converts audio to text for data processing. The text to speech. API converts text into high quality voice audio. And the video intelligence API recognizes motion and action in video. How well a machine learning model is trained depends on how much data is available to train it. As you might expect, Google has lots of images, texts and ML researchers to train its pre trained models. This means less work for you and a faster return on your investment.

#### AutoML

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452225

Another more custom way to use machine learning to solve problems is to train models by using your own data. This is where Vertex AI comes in. Vertex AI brings together Google Cloud services for building ML under one unified user interface. You can use your own training data with Vertex AI to manage and build ML projects. AutoML and Vertex AI lets you build and train machine learning models from end to end by using graphical user interfaces. Often referred to as GUIs without writing a line of code. This means that after your data is ingested into Vertex AI, AutoML chooses the best machine learning model for you by comparing different models and tuning parameters. What once required, a lot of manual work is done automatically and quickly, which results in a trained model that is both accurate and customized to your data. This lets machine learning practitioners focus on the problems that they are trying to solve. Instead of the details of machine learning. AutoML is a great option for businesses that want to produce a customized ML model, but are not willing to spend too much time coding and experimenting with thousands of models. Let's go back to our image recognition example, which used Vision API, a pre-existing model trained with Google data. Imagine you work for a car manufacturing company. Vision API can tell you the difference between generic images found in Google databases, like the difference between a wheel and a door. But it can't help a car manufacturer distinguish between good or defective parts. In this case, a developer could use an AutoML vision instance and train it with your specialized data. This automates the training of machine learning models, which means that you could upload a batch of images and train an image classification model through the easy to use graphical interface of AutoML. Models can be further optimized and deployed directly from the cloud. Now let's focus on another feature of AutoML. Earlier you saw how the natural language API could be used for processing entries into an online contact form. But if your text examples don't fit neatly into the natural language API, sentiment based or vertical topic based classification scheme, and you want to use your own specialized data instead, you need to use AutoML natural language. AutoML natural language lets you build and deploy custom machine learning models. The analyzed documents, categorize them, identify entities within them, or assess attitudes within them. You can use the AutoML user interface to upload your training data and test your custom model without a single line of code. Vertex AI makes this customization possible. Those examples are just a few of the many Google Cloud ML offerings. You can also find APIs that categorize videos, convert audio to text, or text to audio, understand natural language, translate from one language to another, and more. In fact, in many of the most innovative applications for machine learning, several of these applications are combined.

#### Custom models

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452226

Vertex AI is also the essential platform for creating custom end to end machine learning models. This means not only are models trained with your own data, but the models are custom built as well. Vertex AI provides a suite of products to help at each stage of the ML workflow, from gathering data to future engineering, building models, and finally, deploying and monitoring those models. As this approach is fully custom built, end to end, its process takes the longest and requires a specialized team of data scientists and engineers. However, these fully custom ML models are the most specialized to your needs, and give your business the most differentiation and innovative results. Vertex AI contains tools that assist programmers with virtual machine imaging in data labeling, training, and predictions. It also provides pre-built algorithms. It's important to remember that although these tools are the building blocks to using your data at every stage, there is no one size fits all approach. Every use case requires a different combination of tools and products.

#### TensorFlow

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452227

All Machine Learning models are built on top of Google Cloud's AI foundational infrastructure. A part of this foundation is TensorFlow, which is an end to end open source platform for machine learning. TensorFlow has a flexible ecosystem of tools, libraries, and community resources that enable researchers to innovate in ML and developers to build and deploy ML powered applications. First developed for Google's Internal use, TensorFlow is now open source so that everyone can benefit. TensorFlow takes advantage of the Tensor Processing Unit, or TPU, which is Google's custom developed application specific integrated circuit used to accelerate machine learning workloads. TPUs act as domain specific hardware as opposed to general purpose hardware with CPUs and GPUs. With TPUs, the computing speed increases more than 200 times. This means that instead of waiting 26 hours for results with a state of the art GPU, you only need to wait for 7.9 minutes for a full cloud TPU pod to deliver the same results. Cloud TPUs have been integrated across Google products, and this state of the art hardware and supercomputing technology is available with Google Cloud products and services.

#### AI solutions

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452228

Beyond the customizable options, Google Cloud has also created a set of full AI solutions aimed to solve specific business needs. Contact Center AI provides models for speaking with customers and assisting human agents, increasing operational efficiency, and personalizing customer care to transform your contact center. Document AI unlocks insights by extracting and classifying information from unstructured documents such as invoices, receipts, forms, letters, and reports. The extracted data can then be saved in a database or exported to another application for further analysis. Discovery AI for retail uses machine learning to select the optimal ordering of products on a retailer's e-commerce site when shoppers choose a category like winter jackets or kitchen ware. Over time, the AI learns the ideal product ordering for each page on the site by using historical data, optimizing how and what products are shown for accuracy, relevance, and likelihood of making a sale. And Cloud Talent Solution uses AI with job search and talent acquisition capabilities, matches candidates to ideal jobs faster, and allows employers to attract and convert higher quality candidates. These are just some of the fully built AI solutions offered by Google Cloud.

#### Considerations when selecting Google Cloud AI/ML solutions

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452229

Google Cloud offers a range of AI and ML solutions and products, but there are several decisions and trade-offs to consider when selecting which to employ. The first consideration is speed. How quickly do you need to get your model to production? AI projects can typically take anywhere 3-36 months to plan and implement, depending on the scope and complexity of the use case. But business decision makers often underestimate the time it will take. Pre-trained API's require no model training, because that time-consuming task has already been carried out. Custom training usually takes the longest time because it builds the ML model from the beginning, unlike autoML and Big query ML. The next consideration is differentiation. How unique is your model, or how unique does it need to be? Google Cloud offers a range of outs of the box solutions for organizations that want to quickly use ML models in their day to day business operations. These include image recognition solutions and chatbots, which are quick to deploy and can be applied in various use cases. Alternatively, Vertex AI, which is Google Cloud's unified platform for building, deploying, and managing AI solutions, can give ML engineers and data scientists full control of the ML workflow. Vertex AI custom training lets you train and serve custom models with code on vertex workbench, which results in highly bespoke ML models. Another consideration is the expertise required when embarking on an AI or ML project. Infusing AI into business processes requires roles such as data engineers, data scientists, and machine learning engineers among others. Organizations should consider their current team and then determine a people strategy, which could include reusing or repurposing existing resources, upskilling and training current staff, or hiring or working with outside consultants or contractors. Google Cloud's AI and ML products vary from those that can be employed by data analysts and business intelligence teams, right up to those more suited to ML engineers and data scientists. The final consideration is the effort required to build an AI solution. This depends on several factors, including the complexity of the problem, the amount of data available, and the experience of the team. Google Cloud can help provide solutions for projects at both ends of the scale. However, any AI undertaking will generally require much time, effort, and expertise to have a worthwhile impact on business operations.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/946/video/452231

This brings us to the end of the Innovating with Google Cloud Artificial Intelligence course, let's do a quick recap. In the first section of the course titled AI and ML fundamentals, you explored the difference between artificial intelligence and machine learning. How machine learning differs from data analytics and business intelligence, different types of problems that AI solutions are suited to solve. The importance of using high quality data for machine learning, and the importance of responsible and explainable AI. And in the second section of the course titled Google Cloud's AI and ML solutions. You learnt about BigQuery ML, Pre-trained APIs, AutoML and custom models, which are both part of Vertex AI. Tensorflow, existing AI solutions and what you should consider when choosing a Google Cloud AI or ML solution. Now that you've had a comprehensive introduction to artificial intelligence and machine learning on Google Cloud. You can move on to the next course in the series, Modernize Infrastructure and Applications with Google Cloud. Where you'll learn about, why modernization and migration to the cloud is an important step in an organization transformation journey. Options for and advantages of running compute workloads in the cloud. Using containers and serverless computing in application modernization, the business value of application programming interfaces, APIs. And the business reasons for choosing hybrid or multi-cloud strategies, we'll see you next time.

### Your Next Steps

## 04: Modernize Infrastructure and Applications with Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/265

### Course Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510815

New organizations born in the cloud are challenging old business models. Scale is no longer a competitive advantage it's the norm. But what does this mean for organizations founded before the cloud era? They want to know how to not only survive, but thrive. The answer lies in how organizations structure and use their It resources. Like moving away from investing their own resources to run and maintain existing IT infrastructure. With the cloud, organizations can shift their focus to creating new, higher value products and services. And it's not just about infrastructure. With the cloud, organizations can develop and build new applications to drive better engagement with customers and employees faster, securely and at scale. Enterprises are also seeing significant financial benefits from adopting the cloud. As their approach to IT moves from buying fixed capacity to paying only for what they use, they are changing the economics of technology investment. For many organizations, infrastructure and application modernization are the foundation for digital transformation. So, considering that, let's explore the goals of this course. Modernized infrastructure and applications with Google Cloud was designed to help introduce you to common terminology related to infrastructure and application modernization. Explore options available to run compute workloads in the cloud, including virtual machines, containers and server less architecture. And examine options to modernize application development through rehosting and APIs. Throughout the course, you're presented with graded knowledge assessments. You must pass these assessments to receive course credit. Okay, let's get started.

#### Important cloud migration terms

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510816

You'll hear some common terminology when learning about modernizing infrastructure and applications in the Cloud. Let's introduce or remind you of some of these terms. The first is workload. In Cloud computing, a workload is a specific application, service, or capability that can be run in the Cloud or on premises. Workloads include containers, databases, and virtual machines. Sometimes workloads get retired. Retiring a workload means removing it from a platform. A workload might be retired because it's unnecessary, not cost effective, secure, or compatible with a specific platform. Alternatively, workloads are often retained. Retaining a workload means that it's intentionally kept. When a workload is retained, it's typically kept on premises or in a hybrid Cloud environment. This means that the workload will continue to be managed by the business and will not be subject to the same level of Cloud provider control. Many workloads are rehosted in Cloud computing. Rehost refers to the migration of a workload to the Cloud without changing anything in the workload's code or architecture. This is often done as a first step in Cloud migration because it's the simplest and quickest way to run a workload in the Cloud. This process is often referred to as lift and shift. However, rehosting also has some drawbacks, including it doesn't use all the benefits of Cloud computing. Managing workloads that were rehosted without making any changes can be difficult. Scaling workloads that were re hosted without making any changes can also be difficult. Then there's replatform. In Cloud computing, replatform refers to the process of migrating a workload to the Cloud while making some changes to the workloads code or architecture. This process is often called move and improve. Replatforming lets organizations benefit from the Cloud's scalability, reliability and cost effectiveness, improve the performance of their workloads, and reduce the cost of their workloads. However, replatforming also has some drawbacks, including it can be a complex and time consuming process. Making the necessary changes to the workload's code or architecture can be difficult and testing the changes to the workload's code or architecture can also be difficult. Sometimes workloads are refactored, which refers to the process of changing the code of a workload. For example, an organization might refactor a workload to use either a Cloud-based microservices architecture or a Cloud-based server-less architecture. We'll explore what those concepts mean later in this course. Refactoring has some benefits. It means that workloads can become more efficient, scalable, or secure, and a valuable investment for organizations that want to use all Cloud capabilities. That being said, a possible drawback for organizations is that refactoring a workload can be a complex and time consuming process. Finally, Cloud modernization can inspire and incentivize organizations to reimagine. In Cloud computing, reimagine refers to the process of rethinking how an organization uses technology to achieve its business goals. This can involve reconsidering the organization's current Cloud strategy and its use of other technologies such as artificial intelligence and machine learning. Reimagining Cloud computing can help organizations to improve their efficiency, reduce costs, and increase agility. It can also help organizations better meet the needs of their customers and partners.

### Modernizing Infrastructure in the Cloud

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510817

In the context of the Cloud, compute refers to a machine's ability to process information. Associated tasks include storing, retrieving, comparing, and analyzing the information. Instead of relying on local servers and storage devices, Cloud computing uses a network of remote servers to provide on-demand access to various computing resources, including applications, storage, and processing power. This technology has become increasingly popular in recent years due to its flexibility, scalability, and cost-effectiveness. In this section of the course, you'll learn about the benefits that Cloud computing can bring to an organization and explore three Cloud computing options, virtual machines, containers, and serverless.

#### The benefits of running compute workloads in the cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510818

Why should an organization consider running compute workloads in the Cloud? Let's explore some benefits that running compute workloads in the Cloud can bring to an organization. We'll begin with total cost of ownership, or TCO, which is a measure of the total cost of a system or solution over its lifetime. It includes the cost of the initial purchase, maintenance and operation, along with any other associated costs. Cloud computing can help businesses save money on IT costs by eliminating the need to purchase and maintain physical infrastructure. Cloud providers offer a pay-as-you-go model, which means that organizations only pay for the resources used. They also offer discounts for long term commitments, which can further reduce TCO for businesses that are planning to use Cloud services for a long period. Next, there is scalability, which refers to the ability to increase or decrease the number of resources such as servers, storage, and bandwidth that are available to a Cloud-based application to meet changing demand. Scalability is important because it provides a means to meet changing demand without having to make large upfront investments in infrastructure. If a business experiences a sudden spike in demand, it can easily scale up its Cloud resources to meet the demand. Conversely, if they experience reduced demand, infrastructure can quickly scale down its Cloud resources to save money. Another benefit to Cloud computing is reliability. Cloud providers offer a high degree of reliability and up-time, which gives businesses confidence that their data and applications will be available when they need them. Cloud providers have many ways to ensure the reliability of their services. Google Cloud for example has multiple data centers located in different parts of the world. This helps to ensure that if one data center goes down, the others can continue to operate. Cloud providers also use various technologies to monitor their services and automatically detect and fix problems. Next is security. Cloud computing providers offer a high level of security for data and applications. Organizations need to be sure that their data is being kept safe. In addition to physical data center security, Cloud security features include data encryption, identity and access management, network security, virtual private Clouds, and monitoring services that can detect and respond to security threats in real time. These security features can also help to ensure compliance with government or industry regulations. Running compute workloads in the Cloud offers a high degree of flexibility for organizations. Organizations can choose the Cloud services that best meet their needs at any point in time, and then change or adapt those services when necessary. For example, a business that needs to increase the amount of storage space that it uses can easily add more storage space to its Cloud storage service. Finally, another benefit of running compute workloads in the Cloud is abstraction. Abstraction refers to how Cloud providers remove the need for customers to understand the finer details of the infrastructure implementation by providing management of the hardware, software, and certain aspects of security and networking. For example, a Cloud storage provider might provide a way for customers to store files so that they don't have to worry about the finer details of how the files are stored on the Cloud providers' infrastructure. Abstraction also lets Cloud providers offer many services. For example, Google Workspace lets customers run productivity applications so that they don't have to worry about the details of how the applications are actually run or maintained on Google's infrastructure. Running compute workloads in the Cloud can help organizations get their products and services to market faster by eliminating the need to develop and maintain their own infrastructure. At the same time, it provides a platform for innovation by providing access to the latest technologies and tools as and when they are released.

#### Virtual machines

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510819

Traditionally, various technological pressures compelled many organizations to tightly bind specific computing hardware resources to specific applications. Virtualization technology relieved these pressures. Virtualization is a form of resource optimization that lets multiple systems run on the same hardware. These systems are called virtual machines or VMs. This means that they share the same pool of processing, storage, and networking resources. VMs enable organizations to run multiple applications at the same time on a server in a way that is efficient and manageable. Compute Engine is Google Cloud's infrastructure as a service product, that lets users create and run virtual machines on Google infrastructure. There are no upfront investments, and thousands of virtual CPUs can run on a system that's designed to be fast and to offer consistent performance. Each virtual machine contains the power and functionality of a full fledged operating system. This means a virtual machine can be configured much like a physical server by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system. A virtual machine instance can be created through the Google Cloud console, which is a web-based tool to manage Google Cloud projects and resources and Google Cloud CLI command line interface by using infrastructure automation tools such as Terraform or the Compute Engine API. An API, or application programming interface, is a set of instructions that allows different software programs to communicate with each other. We'll learn about API's in more detail later in this course. When you use virtual machines, Compute Engine bills by the second with a one-minute minimum, and sustained-use discounts start to apply automatically to virtual machines the longer they run. So,for each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every incremental hour of use. Compute Engine also offers committed-use discounts. This means that when committing to use resources for either a 1-year or 3-year period, discounts are offered over the on-demand prices. And then there are Preemptible and Spot VMs. Let's say that a workload doesn't require a human to sit and wait for it to finish, such as a batch job analyzing a large dataset. Costs can be reduced, in some cases by up to 90%, by choosing Preemptible or Spot VMs to run the job. A Preemptible or Spot VM is different from an ordinary Compute Engine VM in only one respect: Compute Engine might preemptively interrupt Spot VMs to reclaim the capacity at any time. Although savings are possible with Preemptible or Spot VMs, it needs to be ensured that a job can be stopped and restarted without impact. Spot VMs differ from Preemptible VMs by offering more features. For example, Preemptible VMs can only run for up to 24 hours at a time, but Spot VMs don't have a maximum runtime. However, the pricing is currently the same for both. And finally, Compute Engine lets users choose the machine properties of their instances, like the number of virtual CPUs, the operating system, and the amount of memory, by using a set of predefined machine types, or by creating custom machine types.

#### Containers

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510820

Infrastructure as a service, or IS, lets users share compute resources with other developers by using virtual machines to virtualize the hardware. This lets each developer deploy their own operating system, access the hardware, and build their applications in a self-contained environment with access to the necessary system resources. Containers follow the same principle as virtual machines. They provide isolated environments to run software services and optimize resources from one piece of hardware. However, they're even more efficient. The key difference between virtual machines and containers is that virtual machines virtualize an entire machine down to the hardware layers. Whereas containers only virtualize software layers above the operating system level. Containers start faster and use a fraction of the memory compared to booting an entire operating system. A container is packaged with your application and all of its dependencies, so it has everything it needs to run. Containers can be independently developed, tested, and deployed, and are well suited for a microservices based architecture. This architecture is made up of smaller individual services that run containerized applications, that communicate with each other through APIs or other lightweight communication methods, such as REST or gRPC. Containers let developers create predictable environments isolated from other system resources. So if a customer asks for a new feature or a change in the application, developers can easily make an update to that particular part of the application without affecting the REST. Containers can run virtually and anywhere, which makes development and deployment easy.

#### Managing containers

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510821

Containers improve agility, enhance security, optimize resources and simplify managing applications in the cloud. Many organizations have a mix of virtual machines and containers. However, as their It infrastructure setup becomes more complex, they often need a way to manage their services and machines. For example, an organization can have millions and millions of containers. This require as keeping them secure and ensuring that they operate efficiently can require significant oversight and management. Kubernetes, originally developed by Google, is an open-source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them, and easily deploy rollouts and rollbacks. This improves application reliability and reduces the time and resources needed to spend on management and operations. Google Kubernetes Engine or GKE is a Google hosted, managed Kubernetes service in the Cloud. The GKE environment consists of multiple machines, specifically compute engine instances grouped to form a cluster. GKE clusters can be customized, and they support different machine types, numbers of nodes, and network settings. GKE makes it easy to deploy applications by providing an API and a Web based console. Applications can be deployed in minutes and can be scaled up or down as needed. GKE also provides many features that can help monitor applications, manage resources, and troubleshoot problems. Let's explore how Ubie, a Japan based healthcare technology startup, reduced their infrastructure costs and maintenance requirements with Google Kubernetes Engine. Founded in 2017, Ubie's goal is to get people the right medical care when they need it, and it does this with products designed for hospitals and individuals. Ubie for hospital, their flagship product, is AI powered questionnaire software that lets patients provide medical details before an appointment. Ubie initially relied on an alternative, Cloud, to make Ubie for Hospital available in Japan. As the business added new customers, they needed an infrastructure that could support daily deployments and provide a secure gateway to connect Ubie to a wide range of customer networks and settings. Ubie evaluated available options and decided to use Kubernetes in Google Kubernetes Engine. Google Kubernetes Engine Autopilot, a mode that enables full management of an entire cluster's infrastructure and provides per-pod billing, presented a compelling option for the business to run Ubie for Hospital more efficiently and cost effectively. With GKE Autopilot, Ubie could eliminate the need to configure and monitor clusters while only paying for running pods. The shift reduced Ubie's infrastructure costs by 20%, and GKE Autopilot has helped the business eliminate Ubie for Hospital infrastructure maintenance and upgrade tasks that could take hours and days to complete. Another popular option for running containerized applications on Google Cloud is Cloud Run. Cloud Run is a fully managed serverless platform to deploy and run containerized applications without needing to worry about the underlying infrastructure. After your application code is containerized and deployed to Cloud Run, Google Cloud takes care of scaling and managing the infrastructure automatically. Cloud Run is ideal for running stateless applications that need to scale up and down quickly in response to traffic. This makes cloud run most suitable for simple and lightweight applications such as web applications. In summary, GKE is ideal when lots of control is required over a Kubernetes Environment and there are complex applications to run. Alternatively, Cloud Run is ideal for when a simple, fully managed serverless platform that can scale up and down quickly is required.

#### Serverless computing

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510822

Another option for modernizing Cloud applications is serverless computing. Serverless computing doesn't mean there's no server, it means that resources like compute power are automatically provisioned in the background as needed. The advantage here is that organizations won't pay for compute power unless they're running a query or application. At its simplest definition, serverless means that businesses provide the code for whatever function they want and the public Cloud provider does everything else. Imagine you provide software to businesses that help employees manage their corporate expenses. You want to add a feature that lets users upload an image with their expense receipt. In this case, the ability to upload an image is called a function. You as the software development company write the code for that function directly into your public Cloud platform. From there, the public Cloud provider manages everything else. One type of serverless computing solution is called function as a service. Some functions are a response to specific events, like file uploads to Cloud storage, or changes to database records. You write the code that defines the response to those events and the Cloud provider does everything else. Google Cloud offers many serverless computing products. The first is Cloud Run, which is a fully managed environment for running containerized applications. With this product, you don't have to worry about the underlying infrastructure. Then there is Cloud functions, which is the platform for hosting simple single purpose functions that are attached to events emitted from your Cloud infrastructure and services. For example, sending a notification to a mobile device when a new order is placed on a website. There is also App Engine, which is a service to build and deploy web applications. Serverless computing has many benefits, reduced operational costs. The Cloud provider is responsible for the infrastructure and its maintenance. Therefore, the application owner does not need to invest in the infrastructure or the human resources required to manage it. Scalability. Serverless computing provides automatic scaling of computing resources based on the applications demand. The Cloud provider manages the scaling process and the application owner only pays for the resources they use. Faster time to market, the need for infrastructure, setup and configuration is eliminated, which reduces the time required to deploy applications. This feature lets the application owner focus on writing code and quickly deploying new features. Reduce development costs. The development process is simplified because developers can focus on the application's logic and not on the underlying infrastructure. Improved resilience. Serverless computing offers improved resilience and availability as the Cloud provider automatically manages the infrastructure's failover and disaster recovery capabilities. Pay per use pricing model, The application owner only pays for the computing resources they use. This reduces the cost of unused resources and helps optimize costs. How might an organization benefit from Cloud computing infrastructure technology? Let's explore an example specializing in educational technology. Mashme.io provides video collaboration experiences for over 3 million users in 73 countries. Connecting 250 full HD live video streams in real time is a major technical challenge. Latencies need to be kept very low to achieve the face to face experience, and continuous integration in deployment is vital to avoid disruptive downtime for global clients. Meanwhile, costs have to be kept to a minimum to keep the solution affordable for a growing start up. To meet those needs, Mashme.io chose to use Google Kubernetes Engine. Every teacher we speak to tells us that latency is the most important thing for educational video conferencing. Says Mashme.io meet founder Victor Sanchez Belmar. Low latency means having servers close to every student that connects to Mashme.io. With students connecting from around the world, Google Cloud has the global network to make that happen. The view was that setting up data centers around the world with your own hardware is a good way for a start up to never start. Instead, Mashme.io started using Google's global network with App Engine before moving to Google Cloud with their own docker containers, and finally, to Google Kubernetes Engine. This allowed them to update their nodes and services in an almost continuous way without disruption. Students didn't lose an hour or even a second of class.

### Modernizing Applications in the Cloud

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510824

In the previous section of the course, you explored the benefits and options for modernizing It infrastructure with the cloud. Now let's focus on application modernization. But first, what exactly is an application? In its basic form, an application is a computer, program or software that helps users do something. And in this digital age, applications are everywhere. Consider how many application patients you interact with each day, from checking email to tracking your fitness with wearable technology that links to an app on your phone. Customers expect intuitive, well functioning applications that can help them do things faster. Applications have been developed on premises for years and often still are. However, on premises application development often slows organizations down. Deploying an application on premises can be time consuming and can also require specialized IT teams. Changes can often take six months or more to implement, which can create friction within different parts of an organization. With cloud technology, businesses can modernize, develop, and manage applications in new ways, which makes them more agile and responsive to user needs. In this section of the course, you'll compare traditional and modern cloud application development methods. Explore considerations and tools for rehosting legacy applications in the cloud. Define application programming interfaces, or APIs. Examine the benefits of maintaining and managing APIs with Apigee API management, and consider scenarios when a hybrid or multi cloud strategy might be beneficial.

#### The benefits of modern cloud application development

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510825

Thanks to advances in cloud technology, the way that software applications are developed has drastically changed. With modern cloud application development, software development is flexible, scalable, and uses the latest cloud computing technologies to build and deploy applications. In the past, the traditional software development approach, often referred to as monolithic applications, required all the components of an application to be developed and deployed as a single, tightly coupled unit, typically using a single programming language. There are many benefits to the modern cloud application development approach. Let's explore a few. We'll begin with architecture. Modern cloud applications are typically built as a collection of microservices. Microservices are independently deployable, scalable and maintainable components that can be used to build a wide range of applications. This can help organizations bring business value to market faster because features can be released as they're completed without waiting for the rest of the application to be complete. Regarding deployment, modern applications are typically deployed to the cloud and can use managed or partially managed services. Managed services take care of the day-to-day management of cloud-based infrastructure, such as patching, upgrades, and monitoring. This can free up staff to focus on other tasks, such as developing new applications. Partially managed services offer a hybrid approach, where businesses manage some aspects of their cloud-based applications themselves and the cloud provider manages others. In terms of cost, modern cloud applications use a pay as you go pricing model, which can make them extremely cost effective when configured efficiently. That means that organizations don't always need to pay for resources they aren't fully utilizing. Developers can also use prebuilt APIs, which we'll explore later in this section of the course, and other tools offered by the cloud provider to build and deploy their applications quicker. And then there's scalability. Modern cloud-based applications can easily be scaled up or down to meet user demands. Modern cloud applications are designed to be highly available and resilient with built in features like load balancing, which is the process of distributing network traffic evenly across multiple servers that support an application. And automatic failover, which is a process that allows a cloud-based application to automatically switch to a backup server if a failure occurs. Additionally, cloud service providers typically offer robust monitoring and management tools that allow developers to quickly identify and respond to issues, which can further improve the reliability of cloud applications.

#### Rehosting legacy applications in the cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510826

When a business decides to modernize  and move its operations to the cloud,  it might be running several  specialized legacy applications  that aren’t compatible with  cloud-native applications.   In these situations, a business  might take a rehost migration path,  commonly referred to as lift and shift, where an application is moved from an  on-premises environment to a cloud environment without making  any changes to the application itself. Rehosting applications brings with it   the many benefits of cloud computing  that we explored earlier, such as  cost savings, scalability,  reliability, and security.   However, there are also some potential  drawbacks to choosing a rehost migration   path for legacy applications, including: Complexity: rehosting can be a complex   process. Businesses need to carefully  plan the migration process and ensure   that they have the right resources in place. Risk: migrating applications to the cloud   always involves some risk. Businesses  need to carefully assess and identify   potential risks and ensure that they have  a plan in place in case of any problems.   Vendor lock-in: by moving applications to the  cloud, businesses might become locked into a   particular cloud provider. This can potentially  make it difficult to switch providers later.   Google Cloud offers many solutions for  rehosting specialized legacy applications.   The first is Google Cloud VMware Engine, which  helps migrate existing VMware workloads to   the cloud without having to rearchitect  the applications or retool operations.   With Google Cloud VMware Engine, organizations  can maintain their existing VMware   environments and operational processes, while benefiting from the scalability,  security, reliability of Google Cloud.   By doing this, organizations can also access  a range of Google Cloud services such as  BigQuery, AI/ML,  and Google Kubernetes Engine, which lets them modernize their   application environment and use new  capabilities and technologies as needed.   And for organizations with  legacy applications on Oracle,  Google Cloud offers Bare Metal Solution.   This is a fully managed cloud infrastructure  solution that lets organizations run their   Oracle workloads on dedicated,  bare metal servers in the cloud.

#### Application programming interfaces (APIs)

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510827

Implementing a software service can be complex and changeable. And if each software service that an organization uses has to be coded for each implementation, the result can be fragile and error-prone. One way to make things easier is to use APIs or application programming interfaces. Earlier in this course, you saw how cloud providers offer a variety of resources and services for running applications and performing computational tasks in the cloud. However, to fully use these resources and services, applications need to be able to interact with them in a standardized and efficient way. This is where APIs come in. An API is a set of instructions that lets different software programs communicate with each other. Think of it as an intermediary between two different programs, which provides a standardized and predictable way for them to exchange data and interact. An API is like a waiter in a restaurant. The waiter takes orders from customers, communicates with the kitchen, and then brings the food back to the customers. Similarly, an API takes requests from one software program, the customer, communicates with another program, the kitchen, and then returns a response, the food, back to the requesting program, the customer. APIs can be used in many different applications, from social media platforms to mobile apps and web services. They let developers access functionality and data from other programs without having to write all the code themselves, saving time and effort. Google itself provides many APIs that let developers access its products and services. These include APIs that use the power of Google to search across a website or collection of websites, APIs that let developers access Google Maps data such as maps, directions and traffic information, and APIs that let developers translate text from one language to another. In fact, many Google Cloud products and services have documented APIs. Using APIs can create new business opportunities for organizations and improve online experiences for users. For example, an organization could expose an API that allows customers to track their shipments or check their account balances from within a third party app. There's also an opportunity for organizations to create new products that let other companies access their data or services through an API. Let's explore why an organization might consider this business opportunity. APIs can be used to create new products and services. An organization could create an API that allows developers to access data from its database. This data could then be used to create new products and services. APIs can be used to generate new revenue streams. An organization could charge developers to access its APIs. This could generate new revenue streams for the organization and help to offset the cost of developing and maintaining the APIs. APIs can create partnerships. By exposing APIs, organizations can create partnerships with other companies or developers which can lead to new business opportunities and collaborations. By carefully considering the needs of their customers and partners, organizations can develop APIs that provide value and help to grow their businesses.

#### Apigee API Management

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510828

When an organization has implemented API's, it's important to maintain and manage them effectively. This can be done using a platform such as Apigee API management, Google Cloud's API management service to operate API's with enhanced scale security and automation. Apigee is a popular choice for organizations that need to manage their API's because it offers many benefits. It helps organizations secure their API's by providing features such as authentication, authorization and data encryption. It tracks and analyzes API usage with real time analytics and historical reporting. It helps with developing and deploying API's through a visual API editor and a test sandbox. It offers API versioning, API documentation, and even API throttling, which is the process of limiting the number of API requests a user can make in a certain period. AccuWeather has enjoyed great success, sharing its world class weather data through APIs with a range of global partners who have built applications for connected cars, smart homes, wearables, smart TV's, mobile devices, and more. But the company wanted to get its data into the hands of a new customer, individual developers. It needed a way to engage this audience and tailor its offerings to the varying needs of developers and monetize those different offering levels accordingly. To implement a simple and fast way for developers to start building with an appropriate level of API calls and features for their needs, AccuWeather realized it required a sophisticated API management platform. One that enabled different tiers of offerings by bundling API's into different products, each with their own rate limits and pricing. With Apigee managing API's for AccuWeather, their users can customize API consumption to their specific needs. While Apigee helps attract and build that traffic. With the customizable Apigee developer portal, developers can sign up quickly, learn about the AccuWeather API's and test them out. With built in analytics, AccuWeather can keep close tabs on who's signing up, what traffic they're producing and from where, and also observe unexpected patterns in traffic activity.

#### Hybrid and multi-cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510829

As you've seen throughout this course, organizations can thrive with the help of cloud. But the reality is that most of the world's enterprise computing still happens on premises. The path to the cloud can be complex and full of difficult decisions and sometimes workloads remain on premises due to compliance or operational concerns. How can organizations modernize their IT infrastructure without completely migrating to the cloud? How can they maintain flexibility and avoid lock in? Two options are hybrid and multi cloud solutions. A hybrid cloud environment comprises some combination of on premises or private cloud infrastructure and public cloud services. This is the situation many organizations are currently in, where some of their data and applications have been migrated to the cloud, while others remain on premises. Interconnects between the private and public clouds allow interoperability. A multi-cloud environment is where an organization uses multiple public cloud providers as part of its architecture. This is ideal for organizations that need flexibility and secure connectivity between the different networks. An organization might choose to use hybrid cloud multi-cloud or a combination of both if they want to incorporate specific elements of a public cloud to benefit from the main strengths of that provider. This lets organizations keep parts of the system's infrastructure on premises while they move other parts to the cloud. This way they create an environment that is uniquely suited to the organization's needs. Move only specific workloads to the cloud because a full scale migration is not required for it to work, benefit from the flexibility, scalability, and lower computing costs offered by Cloud services for running specific workloads. Add specialized services such as machine learning, content caching, data analysis, long term storage, and IOT or Internet of Things. To the organization's computing resources toolkit. How can Google Cloud help in this context? Google's answer to modern hybrid and Multi-cloud distributed systems and services management is called GKE Enterprise. GKE Enterprise is a managed production ready platform for running Kubernetes applications across multiple cloud environments. It provides a consistent way to manage Kubernetes, clusters, applications and services regardless of where they are running. Some of the benefits of GKE enterprise include Multi-cloud and hybrid-cloud support. GKE enterprise can run Kubernetes clusters on Google Cloud, AWS, Azure, and other public clouds. Centralized management GKE Enterprise provides a single centralized console for managing Kubernetes clusters and applications, security and compliance. GKE Enterprise includes many features that help secure Kubernetes clusters and applications and comply with industry regulations, networking and load balancing. GKE Enterprise includes a number of features that help network and load balance Kubernetes applications, monitoring and logging GKE Enterprise provides a rich set of tools for monitoring and maintaining application consistency across an entire network, whether on premises in the cloud or in multiple clouds.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/265/video/510831

This brings us to the end of the modernized infrastructure and applications with Google Cloud course. Let's do a quick recap. In the first section of the course titled course introduction, you explored some important cloud migration terminology. In the second section, titled modernizing infrastructure in the cloud, you were introduced to, the benefits of running compute workloads in the cloud. Virtual machines, containers and how to manage them, and serverless computing. And in the final section of the course, modernizing applications in the cloud, you learned about the benefits of modern cloud application development. Rehosting legacy applications in the cloud, APIs and API management with Apigee, and using hybrid and multi-cloud solutions. Now that you have a comprehensive introduction to modernizing infrastructure and applications on Google Cloud, you can move on to the next course in the series. Trust and Security with Google Cloud, where you'll learn about fundamental cloud security concepts, google's multi layered approach to infrastructure security, and how Google Cloud strives to earn and maintain customer trust in the cloud. We'll see you next time.

### Your Next Steps

## 05: Trust and Security with Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/945

### Course Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487300

At Google Cloud, we understand the responsibility that comes with hosting, serving, and safeguarding our customers' valuable data. As organizations increasingly migrate their data and applications to the cloud, it becomes crucial to address the emerging security challenges. Trust and security lie at the heart of our product design and development philosophy. We firmly believe that our customers own their data and have complete control over its usage. Although we’ve implemented robust security measures to defend against potential breaches, we also acknowledge that security is a dynamic and ongoing process that demands constant attention and investment. To support you in this journey, we provide a range of security products and services that enable you to detect, investigate, and mitigate cyber threats while aligning with your policy, regulatory, and business objectives. The objective of this course, “Trust and Security with Google Cloud,” is to equip you with the knowledge and skills necessary to discuss fundamental cloud security concepts, explain the business value of Google’s multilayered approach to infrastructure security, and describe how Google Cloud earns and maintains customer trust in the cloud. Throughout the course, you’re presented with graded knowledge assessments. You must pass these assessments to receive course credit. OK, let's get started!

### Trust and Security in the Cloud

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487301

In recent years, the rise of cloud computing has transformed the way that organizations store, process, and manage their data. However, with this increased reliance on the cloud, the need for robust security measures has become essential. Securing data, applications, and infrastructure in the cloud is a complex and ever-evolving challenge. As new threats and vulnerabilities emerge, organizations must stay ahead of the curve and adapt their security strategies to mitigate risks effectively. In this section of the course you’ll define key security terms and concepts, describe the importance of confidentiality, integrity, availability, control, and compliance in a cloud security model, differentiate between cloud security and traditional on-premises security, and describe today's top cybersecurity threats and business implications.

#### Key security terms and concepts

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487302

In the field of cloud security, understanding the terminology is crucial to navigating the landscape effectively. In this lesson, we introduce essential security terms and concepts that are commonly encountered when discussing cloud security. Let's explore these terms and their significance. The first three concepts relate to reducing the risk of unauthorized access to sensitive data. The privileged access security model grants specific users access to a broader set of resources than ordinary users. For example, a system administrator may have privileged access to perform tasks such as troubleshooting and data restoration. However, the misuse of privileged access can pose risks, so it’s essential to manage and monitor such access carefully. The least privilege security principle advocates granting users only the access they need to perform their job responsibilities. By providing the minimum required access, organizations can reduce the risk of unauthorized access to sensitive data. For instance, a sales representative might only need access to a customer relationship management (CRM) system without requiring access to other systems like payroll or finance. The zero-trust architecture security model assumes that no user or device can be trusted by default. Every user and device must be authenticated and authorized before accessing resources. Zero-trust architecture helps ensure robust security by implementing strict access controls and continuously verifying user identities. These next three concepts relate to how an organization can protect itself from cyber threats. Security by default is a principle that emphasizes integrating security measures into systems and applications from the initial stages of development. By prioritizing security throughout the entire process, organizations can establish a strong security foundation in their cloud environments. Security posture refers to the overall security status of a cloud environment. It indicates how well an organization is prepared to defend against cyber attacks by evaluating their security controls, policies, and practices. Cyber resilience refers to an organization's ability to withstand and recover quickly from cyber attacks. It involves identifying, assessing, and mitigating risks, responding to incidents effectively, and recovering from disruptions quickly. Finally, let's explore essential security measures to protect cloud resources from unauthorized access. A firewall is a network device that regulates traffic based on predefined security rules. You can think of a firewall like a security guard for a network. It follows certain rules to decide which traffic is allowed to enter or leave a network. These rules help keep unauthorized people or harmful things away from important cloud resources, such as servers, databases, and applications. Following our previous analogy, a security guard checks everyone who wants to enter and only lets in those who have permission. Similarly, a firewall checks the incoming and outgoing traffic in a network and only allows the ones that are safe and authorized. Encryption is the process of converting data into an unreadable format by using an encryption algorithm. Decryption, however, is the reverse process that uses an encryption key to restore encrypted data back to its original form. Safeguarding the encryption key is crucial, because it holds the secret algorithm necessary for decrypting the data. Another way to think about encryption and decryption is writing a message in a secret language that only you and the person you want to send it to can understand. This way, even if someone intercepts the message, they won't be able to read it, because they don't know the secret language.

#### Cloud security components

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487303

In this lesson, we learn about the components that make up a cloud security model and discover how they contribute to a robust security posture in today's digital landscape. We’ll first explore three essential aspects of security: Confidentiality, Integrity, and Availability. These three principles form the foundation of the “CIA Triad”, a widely used model for developing effective security systems. The CIA triad emphasizes the importance of protecting sensitive information, ensuring data accuracy and trustworthiness, and maintaining uninterrupted access to resources and services. By understanding and implementing measures to address these aspects, organizations can establish a strong security framework to safeguard their digital assets. Confidentiality is about keeping important information safe and secret. It ensures that only authorized people can access sensitive data, no matter where it's stored or sent. Confidentiality is of utmost importance in the cloud, as sensitive information stored and transmitted across cloud environments must be protected from unauthorized access or disclosure. Encryption plays a crucial role in ensuring confidentiality in the cloud. By using encryption techniques and safeguarding encryption keys, organizations can ensure that only authorized individuals can access and decrypt sensitive data, effectively mitigating the risk of data breaches in the cloud. Integrity means keeping data accurate and trustworthy. It ensures that information doesn't get changed or corrupted, no matter where it's stored or how it's moved around. You can think of it like making sure a message doesn't get altered during delivery. Integrity in the cloud involves ensuring the accuracy and trustworthiness of data throughout its lifecycle. Implementing data integrity controls, such as checksums or digital signatures, enables organizations to verify the authenticity and reliability of their data in the cloud. This helps prevent unauthorized modifications or tampering, ensuring the integrity of critical information stored and processed in cloud environments. Availability is all about making sure that cloud systems and services are always accessible and ready for use by the right people when needed. It's like having a reliable electricity supply that never goes out. Cloud environments must be designed with redundancy, failover mechanisms, and disaster recovery plans to maximize availability and minimize downtime. By implementing these measures, organizations can ensure that their systems and applications in the cloud remain accessible whenever needed, promoting business continuity even in the face of potential disruptions. Control refers to the measures and processes implemented to manage and mitigate security risks. It involves establishing policies, procedures, and technical safeguards to protect against unauthorized access, misuse, and potential threats. Control measures in the cloud include implementing robust authentication mechanisms, access restrictions, and security awareness training. These measures help organizations manage and mitigate security risks associated with cloud-based systems. By ensuring that only authorized individuals have access to sensitive data and systems in the cloud, organizations can reduce the risk of data breaches and unauthorized activities. Finally, compliance relates to adhering to industry regulations, legal requirements, and organizational policies. It involves ensuring that security practices and measures align with established standards and guidelines. Meeting compliance standards in the cloud demonstrates an organization's commitment to data privacy and security, building trust with stakeholders, and minimizing legal and financial risks. Cloud providers often offer compliance frameworks and certifications that organizations can leverage to meet their regulatory obligations. By integrating these principles into a comprehensive cloud security model, organizations can establish a strong foundation for protecting their data, maintaining data integrity, and ensuring continuous access to critical resources.

#### Cloud security versus traditional on-premises security

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487304

In the past, businesses heavily relied on their own infrastructure and local data centers to manage and protect their digital assets. They had complete control over their hardware, software, and network components, fostering a sense of trust within their premises. However, as organizations now connect digitally with customers, partners, and employees worldwide, new risks have emerged that require enhanced security measures. This is where cloud security comes into play by offering a different approach compared to traditional on-premises security. Let's explore these important differences. The first is location. Cloud security involves hosting and managing data and applications in off-site data centers operated by cloud service providers. The responsibility for securing the infrastructure and underlying hardware lies with the cloud provider. Conversely, traditional on-premises security involves hosting and managing data and applications locally on an organization's own servers and infrastructure, granting direct control and responsibility for securing the physical and virtual environment. Next is responsibility. In a cloud model, the cloud service provider is responsible for securing the infrastructure, network, and physical facilities. The customer is typically responsible for securing their data, applications, user access, and configurations. On the other hand, in an on-premises setup, the organization is responsible for securing the entire infrastructure, including hardware, network, operating systems, applications, and data. The next difference is scalability. Cloud security offers scalability and elasticity, which allows organizations to easily scale their resources up or down based on demand. This flexibility is suitable for dynamic workloads and rapid growth. In contrast, on-premises security requires organizations to provision and maintain their own infrastructure, which can be more time-consuming and costly when they scale up or down. Next is maintenance and updates. Cloud service providers handle infrastructure maintenance, including security updates, patching, and software upgrades. Customers can focus on managing their applications and data without worrying about the underlying infrastructure. On-premises environments require organizations to maintain and update their own infrastructure, involving regular tasks such as patching, software updates, and hardware upgrades. The final difference is capital expenditure. Cloud security follows an operational expenditure (OpEx) model, where organizations pay for the services they consume on a subscription basis. This eliminates the need for large upfront capital investments in physical security infrastructure. Traditional on-premises security models involve significant capital expenditure (CapEx), because organizations must purchase and maintain their own security infrastructure. Understanding these differences between cloud security and traditional on-premises security helps organizations make informed decisions about the most suitable approach for their specific needs. Cloud security offers benefits such as offloading infrastructure management, scalability, and cost flexibility. However, traditional on-premises security provides direct control over the entire infrastructure. Organizations must carefully evaluate their requirements and consider factors such as data sensitivity, compliance regulations, and scalability to determine the most effective security strategy for their business.

#### Cybersecurity threats

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487305

In today's fast-paced digital world, we’re bombarded with attention-grabbing headlines. "CEOs Beware: The Perils of Career-Ending Cyberattacks" "Retailer Pays a Hefty $179 Million Due to Data Breach Fallout" "Credit Agency Settles US Data Breach, Facing Up to $700 Million in Penalties" The realm of cyberattacks is evolving rapidly, and these threats can emerge from unexpected sources, even disguised as government entities. So, what are some common cybersecurity threats faced by organizations? First is deceptive social engineering. Imagine that a skilled manipulator is seeking to extract confidential system information from unsuspecting individuals. These cybercriminals employ “phishing attacks,” which collect personal details about you, your employees, or your students. They skillfully craft tailored emails and mimic authenticity to deceive their targets. Therefore, anyone within your organization can be tricked into inadvertently downloading malicious attachments, divulging passwords, or compromising sensitive data. Next is physical damage. Whether it be damage to hardware components, power disruptions, or natural disasters such as floods, fires, and earthquakes, organizations are responsible for safeguarding data even in the face of physical adversity. You can think of this as protecting precious assets amidst nature's unforgiving forces. Another threat is malware, viruses, and ransomware. These digital adversaries architect chaos within the cyber domain. Employing malicious software, they aim to disrupt operations, inflict damage, or gain unauthorized access to computer systems. The most insidious of these is ransomware, where crucial files are held hostage until a considerable ransom is paid. It's like witnessing the digital equivalent of a calculated extortion scheme. The next cybersecurity threat is vulnerable third-party systems. Imagine inviting a trusted ally into your domain, only to discover that they inadvertently compromise your security. Many organizations rely on third-party systems for essential functions such as finance, inventory management, or account operations. However, without adequate security measures and regular evaluations, these systems can transform into potential threats, leaving data security vulnerable. It's like using a tool that unwittingly introduces risks to your own treasured possessions. The final threat is configuration mishaps. Even the most seasoned experts make mistakes. Misconfiguration occurs when errors arise during the setup or configuration of resources, which inadvertently exposes sensitive data and systems to unauthorized access. Surveys consistently identify misconfiguration as the most prominent threat to cloud security. In turn, adopting principles of least privilege and privileged access are imperative, because they allow resource access only when explicitly required and authorized. This is like granting access only to those who have earned your trust. As technology continues to advance at an astonishing pace, organizations must invest in the right expertise to assess, develop, implement, and maintain robust data security plans.

### Google’s Trusted Infrastructure

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487307

At Google Cloud, we believe in going beyond reliance on a single technology for security. Our multilayered strategy builds progressive security layers, combining global data centers, purpose-built servers, custom security hardware and software, and two-step authentication. This approach provides true defense-in-depth. In this section of the course, you’ll learn about how Google designs and builds its own data centers by using purpose-built servers, networking, and custom security hardware and software, the role that encryption plays in securing an organization’s data and the ways that it can protect data exposed to risks in different states, the differences between authentication, authorization, and auditing, the benefits of using two-step verification and Identity and Access Management, or IAM, how an organization can protect against network attacks by using Google products, and security operations in the cloud and its related business benefits.

#### Data centers

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487308

Data centers are more than just facilities filled with computers. They’re the backbone of round-the-clock operations for Google's services, including Search, Gmail, and YouTube. Moreover, they play a crucial role in storing and processing data for all the services provided on Google Cloud. At present, Google operates over 30 state-of-the-art data centers worldwide, with some still under construction. These advanced facilities are meticulously designed to deliver exceptional reliability, top-notch security, and outstanding efficiency, and they ensure that Google's services are always available when you need them. But it doesn't stop there. Google is committed to minimizing the environmental impact of data centers. By using cutting-edge technologies and renewable energy sources, we strive to reduce our ecological footprint. Let's explore the benefits of Google designing and building its own data centers, using purpose-built servers, advanced networking solutions, and custom security hardware and software. One of the greatest advantages of Google's data centers is the implementation of a zero-trust architecture, which ensures enhanced security at every level. Our custom hardware and software are purpose-built with features like tamper-evident hardware, secure boot, and hardware-based encryption, which establish a strong security posture within the data center environment. Physical security is paramount as well, with robust access control measures and biometric authentication in place. By adopting the principle of least privilege, only authorized personnel have access to the data centers, which minimizes the risk of physical breaches and maintains a privileged access framework. Furthermore, our data centers embody the concept of security by default. From the moment you step into a Google data center, you can trust that every aspect has been designed and implemented with your security in mind. With cyber resilience as a core principle, our data centers are equipped to withstand and recover from potential security incidents, and ensure the continuity and integrity of your data. Efficiency is another important aspect of our data center design. Purpose-built servers are optimized for specific tasks, which allows them to perform at great speed and with exceptional efficiency. This reduces energy consumption, cuts down on operating costs, and saves resources and the environment. In fact, we measure our success through the Power Usage Effectiveness (PUE) score. By continually striving for the lowest PUE scores, we ensure maximum efficiency in our data centers, leading to significant cost savings and a reduced carbon footprint. For instance, our data center in Hamina, Finland, stands out as one of the most advanced and efficient facilities in our fleet. Its innovative cooling system, which uses sea water from the Bay of Finland, sets a new standard for energy efficiency worldwide. Scalability is another benefit. Our data centers can quickly and seamlessly accommodate new hardware and servers, which allows us to scale up computing resources on demand. This flexibility is critical for Google to handle massive data volumes and traffic without any disruptions to services. Furthermore, managing our own servers and network provides us with unparalleled customization capabilities. This level of flexibility empowers us to deliver unique services and capabilities that are not available from other providers, giving you access to exclusive features and innovations. Although designing and building data centers requires significant upfront investment, the long-term benefits are substantial. By optimizing resources for efficiency and scalability, Google can significantly reduce energy consumption and operating costs, which results in remarkable savings over time.

#### Secure storage

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487309

Previously, we learned that encryption is like a secret code that transforms data into an unreadable format using special algorithms. This process ensures that only those with the right key or password can make sense of the data. It's like using a secret language to protect your information. By encrypting your data, you can protect it from various risks, such as unauthorized access, loss, or damage. Imagine your data is locked away in a safe. Without the right key, no one can steal, tamper with, or even understand the information inside. Let's take a closer look at how encryption protects your data in different states. When data is at rest, it's stored on physical devices like computers or servers. By encrypting data at rest, even if someone gains physical access to the device, they won't be able to decipher the data without the encryption key. At Google Cloud, we automatically encrypt all customer content at rest, without any effort required from you. It's a free and built-in feature that adds an extra layer of protection to your valuable data. And if you prefer to manage your encryption keys yourself, you can use our Cloud Key Management Service (Cloud KMS) for added control. When data is in transit, it's moving over networks or the internet. Encryption plays a crucial role here by shielding your data from interception by cybercriminals or unauthorized parties. It's like sending your information in a locked box that only the intended recipient can open. At Google Cloud, we employ robust security measures to ensure the authenticity, integrity, and privacy of your data during transit. We encrypt and authenticate data at multiple network layers, especially when it travels outside the physical boundaries we control. This way, your information remains safe and secure as it journeys through the digital world. Data in use refers to data being actively processed by a computer. Encrypting data in use adds another layer of protection, especially against unauthorized users who might physically access the computer. We use a technique called memory encryption, which locks your data inside the computer's memory, making it nearly impossible for unauthorized users to gain access to it. When it comes to encryption algorithms, the Advanced Encryption Standard (AES) takes center stage. AES is a powerful encryption algorithm trusted by governments and businesses worldwide. It's like having a top-secret encryption method that keeps your data safe from prying eyes. So, whether your data is resting, traveling, or actively in use, encryption acts as your loyal guardian, because it ensures its confidentiality and protection. At Google Cloud, we take encryption seriously to provide you with a secure storage solution you can trust.

#### Identity

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487310

Often referred to as the three A’s, authentication, authorization, and auditing are important aspects of cloud identity management used to ensure secure access, manage user privileges, and monitor resource usage. Let's begin with the first A, authentication. It serves as the gatekeeper, because it verifies the identity of users or systems that seek access. Authentication involves presenting unique credentials, such as passwords, physical tokens, or biometric data like fingerprints or voice recognition. Think of it as presenting your identification card before entering a restricted area. By validating the credentials provided, the server confirms that you are who you claim to be. Two-step verification, which you may also hear being referred to as two-factor authentication or multi-factor authentication, is a security feature that adds an extra layer of protection to cloud-based systems. With 2SV enabled, users need to provide two different pieces of information to log in. For example, it could be a combination of a password and a code sent to their phone through text message, voice call, or an app like Google Authenticator. This powerful feature makes unauthorized access more difficult, even if someone manages to obtain your password. The second A is authorization. After a user's identity is authenticated, authorization steps in to determine what that user or system is allowed to do within the system. Think of it as the access control mechanism. Different permissions are assigned to individuals or groups based on their roles, responsibilities, and organizational hierarchy. For example, a system administrator might have the authority to create and remove user accounts, whereas a standard user might only be able to view a list of other users. This fine-grained control ensures that each user has the appropriate level of access to perform their tasks while preventing unauthorized actions. The third A, auditing (sometimes referred to as accounting), plays a critical role in monitoring and tracking user activities within a system. By collecting and analyzing logs of user activity, system events, and other data, auditing helps organizations detect anomalies, security breaches, and policy violations. It provides a comprehensive record of actions taken on a system or resource, which proves invaluable during security incident investigations, compliance tracking, and system performance evaluation. Just like the surveillance cameras in a shopping mall, auditing keeps a watchful eye on activities happening within your system. To provide granular control over who has access to Google Cloud resources and what they can do with those resources, organizations can use Identity and Access Management (IAM). With IAM, you can create and manage user accounts, assign roles to users, grant and revoke permissions to resources, audit user activity, and monitor your security position. It provides a centralized and efficient approach to managing access control within your Google Cloud environment. Imagine IAM as your organization's security headquarters, equipped with robust tools to manage and safeguard your digital assets. By integrating IAM into your Google Cloud security strategy, you can ensure fine-grained access control, enhanced visibility, and centralized resource management. This empowers you to protect your organization's sensitive data and resources effectively.

#### Network security

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487311

When you expand your network to include cloud environments, security considerations take on a whole new dimension. Unlike traditional on-premises setups with clear perimeters, the cloud brings new possibilities and challenges. Let's explore some strategies to secure your organization's network and ensure the safety of your valuable data and workloads in Google Cloud. Embrace the power of zero trust networks. In the world of security, trust shouldn't be given freely. With Google Cloud's BeyondCorp Enterprise, you can implement a zero trust security model. It means that every access request is thoroughly verified, and both the user's identity and context are considered. This way, you maintain strict control over who can access your network and resources, both inside and outside your organization. Secure your connections to on-premises and multi-cloud environments. Many organizations have a mix of cloud and on-premises workloads, or they use multiple cloud providers for resiliency. Ensuring secure connectivity across these environments is crucial. Google Cloud provides private access methods through services like Cloud VPN and Cloud Interconnect, which let you establish secure connections between your on-premises networks and Google Cloud resources. Protect your perimeter with Google Cloud's powerful tools. Google Cloud offers various methods to help secure your perimeter, including firewalls and Virtual Private Cloud (VPC) Service Controls, which help you divide your cloud into different sections and keep them secure. You can also utilize Shared VPC, which is like having a large fence that separates each Google Cloud Project, so they can work independently and safely. With these tools, you can keep your cloud environment protected and give different teams their own space to work in. Stay ahead with a web application firewall. External web applications and services are often targeted by cyber threats, including DDoS attacks. DDoS, which stands for distributed denial-of-service, is a cyber attack that uses multiple compromised computer systems to flood a target with more traffic than it can handle, which causes a denial of service to legitimate users. Google Cloud Armor comes to the rescue by providing robust DDoS protection. It’s like a force field that stops harmful attacks and keeps your website or application safe from things that could make it stop working properly. Automate infrastructure provisioning for enhanced security. By adopting automation tools, you can create immutable infrastructure, which means that it can't be changed after provisioning. Think of infrastructure provisioning tools as your personal assistants for setting up and maintaining your cloud environment. When you use tools like Terraform, Jenkins, and Cloud Build, they handle all the behind-the-scenes work to create a secure and reliable cloud environment. It's like having a team of efficient workers who build and organize everything you need to run your environment smoothly. With these tools, your cloud environment becomes like a well-designed workspace where everything has its place and functions perfectly. And the best part is, when it's set up, it stays that way. No unexpected changes or disruptions. If anything does go wrong, these tools are there to quickly identify and fix any issue and ensure that your cloud environment keeps running smoothly. These examples illustrate just a few of the ways organizations use Google Cloud to fortify their networks against attacks. Your specific network setup and security measures will depend on your unique business requirements and risk tolerance.

#### Security operations

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487312

SecOps—short for Security Operations—is all about protecting your organization's data and systems in the cloud. It involves a combination of processes and technologies that help reduce the risk of data breaches, system outages, and other security incidents. Think of it as your secret weapon for keeping your valuable data safe. Let's explore some of the essential activities involved in SecOps. Vulnerability management is the process of identifying and fixing security vulnerabilities in cloud infrastructure and applications. It’s like regularly checking your castle walls for weak spots. Google Cloud's Security Command Center (SCC) provides a centralized view of your security posture. It helps to identify and fix vulnerabilities, and it ensures that your infrastructure remains solid and protected. Another crucial activity is log management. It's like having a watchful eye on your castle grounds, looking out for any suspicious activity. Google Cloud offers Cloud Logging, a service to collect and analyze security logs from your entire Google Cloud environment. It helps you detect and respond to any signs of trouble and ensures that you anticipate any potential threats. Of course, being prepared for security incidents is equally important. This is where incident response comes in. Imagine having a team of knights ready to defend your castle at a moment's notice. Google Cloud has expert incident responders across various domains, who are equipped with the knowledge and tools to tackle any security incident swiftly and effectively. Another crucial aspect of SecOps is educating your employees on security best practices. Just like teaching everyone in the castle to be vigilant and lock the gates, security awareness training helps prevent incidents by raising awareness and empowering employees to protect themselves and the organization. Now, you might be wondering, why should your organization implement SecOps? Well, here are the benefits. Reduced risk of data breaches: SecOps helps identify and fix vulnerabilities, which significantly reduces the risk of data breaches. Increased uptime: A swift and effective incident response minimizes the impact of outages on your business operations, which ensures smoother and uninterrupted services. Improved compliance: SecOps helps with meeting security regulations, such as the General Data Protection Regulation (GDPR), and keeps your organization in good standing. Enhanced employee productivity: By educating employees on security best practices, SecOps minimizes the risk of human error and promotes a more secure and productive work environment. SecOps is an integral part of your organization's security strategy. By implementing SecOps practices, you can fortify your defenses, reduce security risks, and protect your data in the ever-changing landscape of cloud security.

### Google Cloud’s Trust Principles and Compliance

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487314

At Google, we know that privacy plays a critical role in earning and maintaining trust. Customers need to be sure that their data and applications are safe and secure, and so Google Cloud has a strong set of trust principles and compliance programs in place, which are designed to protect customer data and meet the needs of a wide range of customers, from small businesses to large enterprises. In this final section of the course, you learn about Google’s seven trust principles, data residency and data sovereignty options with Google Cloud, and how the Google Cloud compliance resource center and Compliance Reports Manager support industry and regional compliance needs.

#### The Google Cloud Trust Principles and Transparency Reports

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487315

At Google, we believe in transparency and want you to have complete confidence in our services. Google Cloud’s trust principles are designed to empower you and ensure that the security and control of your business data is not compromised. Let’s review these principles. 1. You own your data, not Google. We prioritize your control and let you access, export, delete, and manage data permissions within Google Cloud. 2. Google does not sell customer data to third parties. We safeguard your data from being used for Google's marketing or advertising purposes. 3. Google Cloud does not use customer data for advertising. Your data remains confidential, because Google Cloud ensures that it’s never utilized to target ads. 4. All customer data is encrypted by default. Your data is protected with robust encryption, because Google Cloud safeguards it even in the unlikely event of unauthorized access. 5. We guard against insider access to your data. We implement stringent security measures to prevent unauthorized employee access to customer data. 6. We never give any government entity "backdoor" access. Your data remains secure, and no government entity can access it without proper authorization. And 7. Our privacy practices are audited against international standards. We undergo regular audits to ensure compliance with rigorous privacy standards. Transparency Reports and Independent Audits Transparency are a core element of our commitment to trust. We provide valuable insights and accountability through our transparency reports, which shed light on government and corporate actions that affect privacy, security, and access to information. These reports let you stay informed and maintain trust in our services. Additionally, Google Cloud undergoes independent, third-party audits and certifications. This verification process ensures that our data protection practices align with our commitments and industry standards. Our participation in initiatives like the EU Cloud Code of Conduct further reinforces our dedication to accountability, compliance support, and robust data protection principles.

#### Data residency and data sovereignty

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487316

When it comes to storing data and keeping it secure, data sovereignty and data residency are two important concepts to understand. Data sovereignty refers to the legal concept that data is subject to the laws and regulations of the country where it resides. For example, the General Data Protection Regulation (GDPR) in the European Union requires companies to comply with data protection laws when processing or storing personal data of EU citizens, regardless of their location. This ensures that individuals have control over their personal data and its usage. In contrast, data residency refers to the physical location where data is stored or processed. Some countries or regions have laws or regulations that require data to be stored within their borders. For instance, some countries mandate that personal data of its citizens must be stored on servers within the country. This ensures data remains within the jurisdiction of local laws. Now, let's explore how Google Cloud addresses data residency requirements. We offer a range of options to control the physical location of your data through regions. Each region consists of one or more data centers, which lets you choose where your data resides. For example, within the European Union, you can select regions located in various countries like the UK, Belgium, Germany, Finland, Switzerland, and the Netherlands. By configuring your resources in specific regions, Google ensures that your data is stored only within the selected region, as stated in our Service Specific Terms. Additionally, Google Cloud provides Organization Policy constraints, coupled with IAM configuration, to prevent accidental data storage in the wrong region. These controls offer peace of mind and reinforce your data residency requirements. Furthermore, Google Cloud offers features like VPC Service Controls, which let you restrict network access to data based on defined perimeters. You can limit user access through IP address filtering, even if they have authorization. Google Cloud Armor lets you restrict traffic locations for your external load balancer by adding an extra layer of protection. By using these capabilities, organizations can adhere to data residency and data sovereignty requirements, ensure compliance, and maintain control over their valuable data within the Google Cloud ecosystem.

#### Industry and regional compliance

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487317

As organizations migrate to the cloud, it becomes essential to protect sensitive workloads while ensuring compliance with diverse regulatory requirements and guidelines. Compliance is a critical aspect of the cloud journey, because not meeting regulatory obligations can have far-reaching consequences. To assist you in achieving compliance, Google Cloud offers robust resources and tools tailored to support your specific needs. First, let's explore the Google Cloud compliance resource center. This comprehensive hub provides detailed information on the certifications and compliance standards we satisfy. You can find mappings of our security, privacy, and compliance controls to global standards. This transparency lets you validate our adherence to industry-leading practices. The resource center also offers valuable documentation on regional and sector-specific regulations, and empowers you to navigate complex compliance landscapes. Imagine you're a healthcare organization subject to HIPAA regulations, which protect sensitive patient health information from being disclosed without the patient's consent or knowledge. The resource center equips you with the necessary insights and documentation to align your compliance efforts with HIPAA requirements. Similarly, if you operate within the financial sector, you'll find guidance on meeting regulations like PCI DSS, which stands for Payment Card Industry Data Security Standard. Google Cloud's compliance resource center is your go-to source for actionable information and support. In addition to the resource center, we provide the Compliance Reports Manager, a powerful tool at your disposal. This intuitive platform offers easy, on-demand access to critical compliance resources at no extra cost. Within the Compliance Reports Manager, you'll discover our latest ISO/IEC certificates, SOC reports, and self-assessments. These resources provide evidence of our adherence to rigorous compliance standards and help streamline your own reporting and compliance efforts. Imagine you're an enterprise seeking ISO/IEC 27001 certification. The Compliance Reports Manager lets you access the necessary documentation efficiently, and it saves you time and effort in the certification process. With this tool, we aim to simplify your compliance journey and empower you to meet your regulatory obligations effectively. By using the Google Cloud compliance resource center and the Compliance Reports Manager, you can navigate the complex realm of industry and regional compliance with confidence. Our dedicated teams of engineers and compliance experts work hand in hand with you to address your specific regulatory needs. Together, we create an integrated controls and governance framework, while we ensure a robust compliance posture. You can visit the compliance resource center at cloud.google.com/security/compliance and explore the Compliance Reports Manager at cloud.google.com/security/compliance/compliance-reports-manager.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/945/video/487319

This brings us to the end of the “Trust and Security with Google Cloud” course. Let’s do a quick recap. In the first section of the course titled, “Trust and security in the cloud,” you explored important security terms and concepts, the difference between cloud security and traditional on-premises security, today’s top cybersecurity threats and business implications, and the importance of control, compliance, confidentiality, integrity, and availability in a cloud security model. In the second section of the course, titled “Google’s trusted infrastructure,” you learned about how Google designs and builds data centers, the role encryption plays in securing an organization’s data, the differences between authentication, authorization, and auditing, the benefits of using two-step verification and IAM, ways to protect against network attacks by using Google products, and security operations in the cloud and its related business benefits. And finally, in the third section of the course, titled “Google Cloud’s trust principles and compliance,” you examined Google’s seven trust principles, data residency and data sovereignty options with Google Cloud, and how the Google Cloud compliance resource center and Compliance Reports Manager support industry and regional compliance needs. Now that you had a comprehensive introduction to trust and security on Google Cloud, you can move on to the final course in the Cloud Digital Leader series, “Scaling with Google Cloud Operations,” where you’ll learn about how Google Cloud supports an organization's financial governance and ability to control their cloud costs, the basic concepts of modern operations, reliability, and resilience in the cloud, and how Google Cloud helps organizations meet sustainability goals and reduce environmental impact. We’ll see you next time!

### Your Next Steps

## 06: Scaling with Google Cloud Operations

- https://www.cloudskillsboost.google/paths/9/course_templates/271

### Course Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487051

In today's digital landscape, organizations of all sizes are embracing the power and flexibility of the cloud to transform how they operate. However, managing and scaling cloud resources effectively can be a complex task. That's where cloud operations come in. Cloud operations refer to the set of practices and strategies employed to ensure the smooth functioning, optimization, and scalability of cloud-based systems. It involves managing and monitoring the infrastructure, applications, and services that run in the cloud, while adhering to best practices for reliability, performance, security, and cost optimization. Cloud operations play a pivotal role in enabling organizations to achieve digital transformation goals, because they ensure the availability, efficiency, and resilience of critical systems. So, with this in mind, let’s explore the goals of this course. “Scaling with Google Cloud Operations” was designed to help you learn how Google Cloud supports an organization's ability to control their cloud costs through financial governance, understand the fundamental concepts of modern operations, reliability, and resilience in the cloud, and explore how Google Cloud works to reduce our environmental impact and help organizations meet sustainability goals. Throughout the course, you’re presented with graded knowledge assessments. You must pass these assessments to receive course credit. OK, let's get started!

### Financial Governance and Managing Cloud Costs

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487052

Using cloud technology, either for business improvements or for large-scale transformation, can be challenging. In fact, one of the common pain points many organizations face, regardless of which cloud provider they use, is managing cloud costs. For large organizations especially, the transition from predictable capital expenditures for building and maintaining their IT infrastructure to agile operating expenditures using cloud resources requires process and organizational changes. Managing cloud costs requires vigilance and real-time monitoring in parallel. In fact, because almost anyone can now access cloud resources on demand, managing IT infrastructure costs no longer sits mainly with the finance team. Instead, it involves more people across multiple teams. So you might even be the person responsible for IT budgeting. Whatever your role, understanding how using cloud technology affects the business from a cost perspective will help you maximize the value your organization gains from using the cloud. In this first section of the course, you’ll explore the fundamentals of cloud cost management, cloud financial governance best practices, ways to control access by using the resource hierarchy, and ways to control cloud consumption.

#### Fundamentals of cloud financial governance

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487053

Easy access to cloud resources presents a need for precise, real-time control of what’s being consumed. Having cloud financial governance, which is in part a set of processes and controls that organizations use to manage cloud spend, can mean the difference between peace of mind and spiraling costs that lead to budget overruns. As an organization adapts, it'll need a core team across technology, finance, and business functions to work together to stay on top of cloud costs and make decisions in real time. The variable nature of cloud costs impacts people, process, and technology. Let’s explore these three areas, starting with people. People refers to the different roles involved in managing cloud costs. For small organizations, one person might fulfill multiple roles and be responsible for managing all aspects of a cloud infrastructure and associated finance. From budgeting to procurement, tracking optimization, and more. Large organizations, however, will likely look to a finance team to take on a financial planning and advisory role. Using business priorities, a finance team is expected to make data-driven decisions on cloud spending, but they might struggle to understand or monitor cloud spend on a daily, weekly, or monthly basis. Then there are members of technology and line of business teams. They can advise on how cloud resources are being used to meet the organization's overall business strategy and what additional resources might be needed throughout the upcoming year. However, they don’t necessarily factor costs into their decision making. To manage cloud costs effectively, a partnership across finance, technology, and business functions is required. This partnership might already exist, or it may take the form of a centralized hub, such as a cloud center of excellence. The central team would consist of several experts who ensure that best practices are in place across the organization and that there's visibility into the ongoing cloud spend. The centralized group would also be able to make real-time decisions and discuss trade-offs when spending is higher than planned. Now let’s transition from people to process. On a daily or weekly basis, organizations should monitor and analyze their cloud usage and costs. Then, on a weekly or monthly basis, the finance team should analyze the results, charge back the costs through the appropriate teams, and determine whether any changes are needed to ensure that the organization's cloud spend is optimized. Having a culture of accountability in place across teams helps organizations recognize waste, quickly act to eliminate it, and ensure they're maximizing their cloud investment. It will also help drive cross-group collaboration across technology, finance, and business teams to ensure that their cloud spend aligns with broader business objectives. And finally, there’s technology. Google Cloud provides built-in tools to help organizations monitor and manage costs. These tools help organizations gain greater visibility, drive a culture of accountability for cloud spending across the organization, control costs to reduce risks of overspending, and provide intelligent recommendations to optimize cost and usage. You’ll explore some of these tools later in this section.

#### Cloud financial governance best practices

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487054

Let’s explore some cloud financial governance best practices that organizations can adopt to increase the predictability and control of their cloud resources. The first best practice is to identify who manages cloud costs. If it's a team, it should ideally be a mix of IT managers and financial controllers. Because Cloud spending is decentralized and variable, it's important to establish a culture of accountability for costs across the organization. Defining clear ownership for projects and sharing cost views with the departments and teams that are using cloud resources helps establish this accountability culture and more responsible spending. As well as making teams accountable for their spending, Google Cloud financial governance policies and permissions make it easy to control who can spend and view costs across your organization. In addition, Google Cloud offers flexible options to organize resources and allocate costs to individual departments and teams. For example, budgets notify key stakeholders based on your actual or forecasted cloud costs. Creating multiple budgets with meaningful alerts is an important practice for staying on top of your cloud costs. The second best practice is to understand what kind of information can be found in an invoice versus cost management tools. They’re not the same concept. An invoice is a document that is sent by a cloud service provider to a customer to request payment for the services that were used. However, a cost management tool is software to help track, analyze, and optimize cloud spend. An organization is rarely only interested in how much they spend. They also want to know why they spent that much. Cost management tools, like those built into the Google Cloud console, are effective for answering the why. They can provide granular data, uncover trends, and identify actions to take to control or optimize costs. And this brings us to the third best practice for increasing the predictability and control of cloud resources: use Google Cloud’s cost management tools. Google Cloud believes in supporting organizations by providing strong financial governance tools that make it easier for customers to align their strategic priorities with their cloud usage. Before organizations can optimize their cloud costs, they first need to understand what they're spending, whether there are any trends, and what their forecasted costs are. So, how can this be done? Start by capturing what cloud resources are being used, by whom, for what purpose, and at what cost. From there, determine who will be responsible for monitoring that information, who will be involved in managing costs, and how the spending information will be reported on an ongoing basis. It's also important to set up the cadence and format for ongoing communication with main cloud stakeholders. Having this plan outlined up front helps ensure that managing costs isn't an afterthought. And how can you monitor current cost trends and identify areas of waste that could be improved? Google Cloud provides built-in reporting capabilities, which can help your team gain visibility into costs. Ideally, reports should be reviewed weekly, at a minimum. One powerful tool is the Google Cloud Pricing Calculator. The Pricing Calculator lets you estimate how changes to cloud usage will affect costs. The calculator is available at cloud.google.com/products/calculator. Now that you’ve had a chance to explore some cloud financial governance best practices, the next step is to implement them. If this doesn’t fall into your scope of responsibility, be sure to pass on those best practices to the relevant stakeholders within your organization.

#### Using the resource hierarchy to control access

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487055

One important cloud computing consideration involves controlling access to resources. With on-premises infrastructure, physical access controls were used. This method, however, is not as effective with resources stored in the cloud. The Google Cloud resource hierarchy is a powerful tool that can be used to control access to cloud resources. Much like the folder structure you use to organize and control access to your own files, this resource hierarchy is a tree-like structure that organizes resources into logical groups. This makes it easier to manage resources and control. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. The first level, resources, represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. A policy is a set of rules that define who can access a resource and what they can do with it. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services can also apply policies to individual resources. The third level of the Google Cloud resource hierarchy is folders. Folders let you assign policies to resources at the level of granularity that you choose. The resources in a folder inherit policies and permissions assigned to that folder. A folder can contain projects, other folders, or a combination of both. Now that you understand the structure of the Google Cloud resource hierarchy, let’s explore some additional benefits of using it to control access to cloud resources. First, the resource hierarchy provides granular access control, meaning you can assign roles and permissions at different levels of the hierarchy, such as at the folder, project, or individual resource level. Second, because the resource hierarchy follows inheritance and propagation rules, permissions set at higher levels of the resource hierarchy are automatically inherited by lower-level resources. For example, if you grant a user access at the folder level, all projects and resources within that folder inherit those permissions by default. This inheritance simplifies access management and reduces the need for manual configuration at each individual resource level. Third, the resource hierarchy enhances security and compliance through least privilege principles. By assigning access permissions at the appropriate level in the hierarchy, you can ensure that users only have the necessary privileges to perform their tasks. This reduces the risk of unauthorized access and helps maintain regulatory compliance. Finally, the resource hierarchy provides strong visibility and auditing capabilities. You can track access permissions and changes across different levels of the hierarchy, which makes it easier to monitor and review access controls. This improves accountability and helps identify and address potential security issues.

#### Controlling cloud consumption

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487056

Organizations want to control cloud consumption for many reasons. It could be about cost savings by ensuring they’re not overspending on unnecessary resources, increased visibility by providing a better understanding of how resources are being used and identifying areas to reduce costs, or improved compliance by ensuring your cloud environment is compliant with industry regulations. Google Cloud offers several tools to help control cloud consumption, including resource quota policies, budget threshold rules, and Cloud Billing reports. Let’s define each of these terms. Resource quota policies let you set limits on the amount of resources that can be used by a project or user. They can help prevent overspending on cloud resources; therefore, they help you ensure that your cloud usage is within your budget. Then there are budget threshold rules, which let you set alerts to be informed when your cloud costs exceed a certain threshold. They can act as an early warning for potential cost overruns, and let you take corrective action before costs get out of control. Both resource quota policies and budget threshold rules are set in the Google Cloud console. And then there are Cloud Billing reports. Whereas resource quota policies and budget threshold rules provide proactive means to control cloud consumption, Cloud Billing reports offer a reactive method to help you track and understand what you’ve already spent on Google Cloud resources and provide ways to help optimize your costs. You can use Cloud Billing reports to monitor costs by exporting billing data to BigQuery. This means exporting usage and cost data to a BigQuery dataset, and then using the dataset for detailed analyses. You can also visualize data with tools like Looker Studio. After analyzing how you're spending on cloud resources, you might realize that your organization can optimize costs through committed use discounts (CUDs). If your workloads have predictable resource needs, you can purchase a Google Cloud commitment, which gives you discounted prices in exchange for your commitment to use a minimum level of resources for a specific term.

### Operational Excellence and Reliability at Scale

#### Introduction

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487058

In today's rapidly evolving digital landscape, organizations use cloud technology increasingly to drive innovation, agility, and efficiency. However, harnessing the true power of the cloud requires a comprehensive understanding of operational excellence and reliability at scale. Operational excellence and reliability refers to the ability of organizations to optimize their operations and ensure uninterrupted service delivery, even as they handle increasing workloads and complexities in the cloud. This includes designing robust infrastructure, establishing resilient processes, and employing proactive monitoring and response mechanisms. Imagine a global ecommerce platform that experiences a sudden surge in traffic during a major sale event. To meet the increased demand, the platform needs to scale its resources rapidly while ensuring uninterrupted service availability. Operational excellence here involves efficiently scaling the underlying infrastructure, automating resource provisioning, and implementing load balancing mechanisms. Reliability focuses on minimizing downtime, employing fault-tolerant systems, and employing disaster recovery strategies. By excelling in these areas, the ecommerce platform can handle the increased load seamlessly, deliver a consistently positive user experience, and avoid revenue loss or reputational damage. In this section of the course, you explore modernizing operations by using Google Cloud, designing resilient infrastructure and processes, the fundamentals of cloud reliability, Google Cloud Customer Care, and the life of a support case.

#### Fundamentals of cloud reliability

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487059

Within any IT team, developers are responsible for writing code for systems and applications, and operators are responsible for ensuring that those systems and applications operate reliably. Developers are expected to be agile and are often pushed to write and deploy code quickly. Their aim is to release new functions frequently, increase core business value with new features, and release fixes fast for an overall better user experience. In contrast, operators are expected to keep the system stable, and so they often prefer to work more slowly to ensure reliability and consistency. Traditionally, developers pushed their code to operators who often had little understanding of how the code would run in a production or live environment. When problems arise, it can be very difficult for either group to identify the source of the problem and resolve it quickly. Worse, accountability between the teams isn’t always clear. DevOps is a software development approach that emphasizes collaboration and communication between development and operations teams to enhance the efficiency, speed, and reliability of software delivery. It aims to break down silos between these teams and foster a culture of shared responsibility, automation, and continuous improvement. One particular concept within the DevOps framework is Site Reliability Engineering, or SRE, which ensures the reliability, availability, and efficiency of software systems and services deployed in the cloud. SRE combines aspects of software engineering and operations to design, build, and maintain scalable and reliable infrastructure. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning and generally help improve an application client's experience and lessen their pain. There are “Four Golden Signals” that measure a system’s performance and reliability. They are latency, traffic, saturation, and errors. Latency measures how long it takes for a particular part of a system to return a result. Latency is important because it directly affects the user experience, changes could indicate emerging issues, its values might be tied to capacity demands, and it can be used to measure system improvements. Traffic measures how many requests reach your system. Traffic is important because it’s an indicator of current system demand, its historical trends are used for capacity planning, and it’s a core measure when calculating infrastructure spend. Saturation measures how close to capacity a system is. It’s important to note, though, that capacity is often a subjective measure that depends on the underlying service or application. Saturation is important because it's an indicator of how full the service is, it focuses on the most constrained resources, and it’s frequently tied to degrading performance as capacity is reached. And errors are events that measure system failures or other issues. Errors are often raised when a flaw, failure, or fault in a computer program or system causes it to produce incorrect or unexpected results, or behave in unintended ways. Errors are important because they can indicate something is failing, configuration or capacity issues, service level objective violations, or that it's time to send an alert. Three main concepts in site reliability engineering are service-level indicators (SLIs), service-level objectives (SLOs), and service-level agreements (SLAs). They are all types of targets set for a system’s Four Golden Signal metrics. Service level indicators are measurements that show how well a system or service is performing. They’re specific metrics like response time, error rate, or percentage uptime–which is the amount of time a system is available for use–that help us understand the system's behavior and performance. Service level objectives are the goals that we set for a system's performance based on SLIs. They define what level of reliability or performance that we want to achieve. For example, an SLO might state that the system should be available for 99.9% of the time in a month. Service level agreements are agreements between a cloud service provider and its customers. They outline the promises and guarantees regarding the quality of service. SLAs include the agreed-upon SLOs, performance metrics, uptime guarantees, and any penalties or remedies if the provider fails to meet those commitments. This might include refunds or credits when the service has an outage that’s longer than this agreement allows.

#### Designing resilient infrastructure and processes

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487060

When infrastructure and processes in a cloud environment are designed, they need to be resilient, fault-tolerant, and scalable, for high availability and disaster recovery. High availability refers to the ability of a system to remain operational and accessible for users even if hardware or software failures occur, whereas disaster recovery refers to the process of restoring a system to a functional state after a major disruption or disaster. Let's explore some of the key design considerations and their significance in more detail. Redundancy refers to duplicating critical components or resources to provide backup alternatives. Redundancy can be implemented at various levels, such as hardware, network, or application layers. For example, having redundant power supplies, network switches, or load balancers ensures that if one fails, the redundant component takes over seamlessly. Redundancy enhances system reliability and mitigates the impact of single points of failure. Replication involves creating multiple copies of data or services and distributing them across different servers or locations. It ensures redundancy and fault tolerance by allowing systems to continue functioning even if certain components or servers fail. By replicating data across multiple servers, the impact of hardware failures or outages is minimized, and the availability of services is improved. Cloud service providers offer multiple regions or data center locations spread across different geographic areas. By distributing resources across regions, businesses can ensure that if an entire region becomes unavailable due to natural disasters, network issues, or other incidents, their services can continue running from another region. This approach improves resilience and reduces the risk of prolonged service interruptions. Building a scalable infrastructure allows organizations to handle varying workloads and accommodate increased demand without compromising performance or availability. Cloud technologies enable the dynamic allocation and deallocation of resources based on workload fluctuations. Autoscaling mechanisms can automatically adjust resource capacity to match demand, ensuring that services remain available and responsive during peak periods or sudden spikes in traffic. Regular backups of critical data and configurations are crucial to ensure that if data loss, hardware failures, or cyber-attacks occur, organizations can restore their systems to a previous state. Cloud providers often offer backup services, and they let organizations automate backups, store them securely, and easily restore data when needed. Backups should be stored in geographically separate locations to protect against regional outages or disasters. These measures improve high availability, allow for rapid recovery from disasters or failures, and minimize downtime and data loss. It’s important to regularly test and validate these processes to ensure that they function as expected during real-world incidents. Also, monitoring, alerting, and incident response mechanisms should be implemented to identify and address issues promptly, further enhancing the overall resilience and availability of the cloud infrastructure.

#### Modernizing operations by using Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487061

If you've ever worked with on-premises environments, you know that you can physically touch the servers. If an application becomes unresponsive, someone can physically determine why that happened. In the cloud though, the servers aren't yours—they belong to the cloud provider—and you can’t physically inspect them. So the question becomes: how do you know what's happening with your server, database, or application? The answer is: by using Google’s integrated observability tools. Observability involves collecting, analyzing, and visualizing data from various sources within a system to gain insights into its performance, health, and behavior. To achieve this, Google Cloud offers Google Cloud Observability, which is a comprehensive set of monitoring, logging, and diagnostics tools. It offers a unified platform for managing and gaining insights into the performance, availability, and health of applications and infrastructure deployed on Google Cloud. Let's look at some of the managed services that constitute Google Cloud Observability. Cloud Monitoring provides a comprehensive view of your cloud infrastructure and applications. It collects metrics, logs, and traces from your applications and infrastructure, and provides you with insights into their performance, health, and availability. It also lets you create alerting policies to notify you when metrics, health check results, and uptime check results meet specified criteria. Cloud Logging collects and stores all application and infrastructure logs. With real-time insights, you can use Cloud Logging to troubleshoot issues, identify trends, and comply with regulations. Cloud Trace helps identify performance bottlenecks in applications. It collects latency data from applications, and provides insights into how they’re performing. Cloud Profiler identifies how much CPU power, memory, and other resources an application uses. It continuously gathers CPU usage and memory-allocation information from production applications and provides insights into how applications are using resources. Error Reporting counts, analyzes, and aggregates the crashes in running cloud services in real-time. A centralized error management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details: time chart, occurrences, affected user count, first- and last-seen dates, and a cleaned exception stack trace. Error Reporting supports email and mobile alerts notification through its API. Google's integrated observability tools provided by Google Cloud Observability offer valuable insights into the performance and health of applications and infrastructure in the cloud.

#### Google Cloud Customer Care

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487062

Any cloud adoption program can encounter challenges, so it's important to have an effective and efficient support plan from your cloud provider. Google Cloud Customer Care can simplify and streamline your support experience with scalable and flexible services built with your business needs at the center. There are four different service levels, which lets you choose the one that’s right for your organization. Basic Support is free and is included for all Google Cloud customers. It provides access to documentation, community support, Cloud Billing Support, and Active Assist recommendations. Active Assist is the portfolio of tools used in Google Cloud to generate insights and recommendations to help you optimize your cloud projects. Standard Support is recommended for workloads under development. You can kickstart your cloud journey with unlimited access to tech support, which lets you troubleshoot, test, and explore. It offers unlimited individual access to English-speaking support representatives during working hours, 5 days a week. Standard support also provides access to the Cloud Support API, which lets you integrate Cloud Customer Care with your organization's customer relationship management (CRM) system. Enhanced Support is designed for workloads in production, with fast response times and additional services to optimize your experience with high-quality, robust support. Support is available 24/7 in a selection of languages, and initial response times are quicker than those provided by Standard Support. Enhanced Support also offers technical support escalations and third-party technology support to help you resolve multi-vendor issues. Premium Support is designed for enterprises with critical workloads. It features the fastest response time, Customer Aware Support, and a dedicated Technical Account Manager. Our Premium Support level also offers credit for the Google Cloud Skills Boost training platform, an event management service for planned peak events, such as a product launch or major sales events, operational health reviews to help you measure your progress and proactively address blockers to your goals with Google Cloud, and customer aware support, where Customer Care learns and maintains information about your architecture, partners, and Google Cloud projects. This information ensures that our support experts can resolve your cases promptly and efficiently. Both the Enhanced and Premium support plans offer Value-Add Services that are available for additional purchase. You can learn more about the value-add services and all Google Cloud Customer Care support offerings at cloud.google.com/support.

#### The life of a support case

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487063

Any Google Cloud customer on the Standard, Enhanced, or Premium Support plan can use the Google Cloud console to create and manage support cases. Outside of filing a support case through the Google Cloud console, Customer Care Support also offers other contact options for live interactions with Support staff such as phone and video call support. The life of a support case during the Google Cloud Customer Care process typically involves several stages and interactions between the customer and the support team. Here's an overview of the typical journey of a support case. First, the customer initiates the support request by creating a case in the Google Cloud Console. Only users who were assigned the Tech Support Editor role within an organization can do this. The customer provides relevant details about the issue they are experiencing, including any error messages, logs, or steps to reproduce the problem. It’s important for the user to select a priority from P4, which means low impact, up to P1, which means critical impact, because this will influence response times from the Customer Care team. After the case is created, it goes through a triage process. The team reviews the information provided by the customer to understand the problem and determine its severity and impact on the customer's business operations. The team might request additional information or clarification from the customer at this stage. In many cases, the Customer Care representative will resolve the case, but for more complex issues, the case is assigned to a support engineer with the appropriate level of expertise. After the case is assigned, the team starts the troubleshooting and investigation process. They analyze the provided information, review system logs, and conduct various diagnostic tests to identify the root cause of the issue. Depending on the complexity of the problem, this stage might involve collaboration with other internal teams or experts. Throughout the investigation, the Customer Care team maintains regular communication with the customer. They provide updates on the progress, share findings, and request additional information or actions from the customer when needed. Escalation is meant for flagging process breaks or for the rare occasion that a case is stuck because a customer and the Customer Care team aren’t fully in sync, despite actively communicating the issue to determine the next steps. However, it’s important to note that escalation isn’t always the best solution, and with high-impact issues, escalation might not make the case go faster. This is because escalation can disrupt the workflow of the Customer Care team and lead to delays in other cases. The best solution for high-impact issues is to ensure that the case is set to the appropriate priority, ensuring that the case is assigned to the right resources as quickly as possible. Escalation is a tool that can be used to regain traction on a stuck case. However, it’s important to use escalation sparingly and only when it’s absolutely necessary. When the root cause is identified, the team works on resolving the issue or providing a mitigation plan. They might provide the customer with step-by-step instructions, configuration changes, or workaround suggestions to address the problem. In some cases, they might consult the issue with higher-level support or engineering teams for further assistance. The Customer Care team might also need to submit a feature request to the Google Cloud engineering team. After implementing the resolution or mitigation plan, the Customer Care team collaborates with the customer to validate the effectiveness of the solution. They might request the customer to perform specific tests or provide feedback on the outcome. This step ensures that the problem is fully resolved and meets the customer's expectations. When the customer confirms that the issue is resolved, the support case is closed. The team provides a summary of the resolution, documents the steps taken, and ensures that the customer is satisfied with the outcome. If needed, they might also offer recommendations for preventive measures or future best practices to avoid similar issues. The customer also receives a feedback survey, so the support team can learn what they did well and what needs improvement. Throughout the entire lifecycle of the support case, Google Cloud’s Customer Care team aims to provide timely and effective assistance to the customer. They prioritize customer satisfaction, responsiveness, and strive to address the possible technical challenges faced by customers when they use Google Cloud services.

### Sustainability with Google Cloud

#### Sustainability with Google Cloud

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487065

As we get closer to the end of this Cloud Digital Leader training, where you’ve explored how cloud computing can help transform the way you do business, it’s important that we underscore our technology efforts at Google with our commitment to the environment and sustainability. The virtual world, which includes Google Cloud’s network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy. Altogether, existing data centers use nearly 2% of the world’s electricity. With this in mind, Google works to make our data centers run as efficiently as possible. Just like our customers, Google is trying to look after the planet. We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals. Therefore, it’s useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a standard that outlines a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste. As an example of how this is being done, here’s Google’s data center in Hamina, Finland. This facility is one of the most advanced and efficient data centers in the Google fleet. Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world. In our founding decade, Google became the first major company to be carbon neutral. In our second decade, we were the first company to achieve 100% renewable energy. And by 2030, we aim to be the first major company to operate completely carbon free. We meet the challenges posed by climate change and the need for resource efficiency by working to empower everyone—businesses, governments, nonprofit organizations, communities, and individuals—to use Google technology to create a more sustainable world. So, what does that look like in practice? Let’s explore an example of how one customer, Kaluza, uses Google Cloud technology to launch smart electric vehicle charging programs that help customers save money while it reduces their carbon footprint. Electric vehicles already account for one in seven car sales globally, and with new gas and diesel cars being phased out across the world, global sales are forecast to reach 73 million units by 2040. But with power grids becoming increasingly dependent on variable energy sources such as wind and solar, rising demand from electric vehicles risks overstraining grids at peak times, which can potentially lead to power outages. Launched by OVO Energy in 2019, Kaluza has taken its deep understanding of the energy market to partner with some of the world’s major energy suppliers and vehicle manufacturers. With a program called Charge Anytime, customers use Kaluza to smart-charge their electric vehicle, and they pay just about a third of their household electricity rate to do so. This means that if the customer plugs in their vehicle to charge when they get home from work at, say, 6:00 p.m.—a time when both demand and the carbon intensity on the grid are at their highest—their vehicle will then be smartly charged at the lowest cost and greenest periods throughout the night, which leaves it ready for when they need it in the morning. Behind Kaluza’s smart charging solution lies some sophisticated technology, all of which is built on Google Cloud. Their core optimization engine gathers real-time data from a wide range of sources, including battery and charging data from the electric vehicles, and data from the energy suppliers and grid operators, such as the carbon intensity, and price forecasts. That data is stored in BigQuery where it’s used to train and validate the smart charging optimization models. These models are then deployed with Google Kubernetes Engine so that whenever a customer plugs in an electric vehicle, data from that vehicle passes in real-time through their optimization engine to calculate the ideal charging schedule for that vehicle, which ensures that it uses the cheapest, least carbon-intensive energy available. And as for the grid operators and energy companies, the Kaluza platform lets them visualize how many participating electric vehicles are plugged into the network at any one time. BigQuery and Looker Studio dashboards provide granular insights, such as how many vehicles are idle, how many are charging, and how well our optimization engine is working. At Google, we remain committed to sustainability and continue to lead and encourage others, like Kaluza, to join us in improving the health of our planet.

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/9/course_templates/271/video/487067

This brings us to the end of the “Scaling with Google Cloud Operations” course. Let’s do a quick recap. In the first section of the course titled “Financial governance and managing cloud costs,” you explored the fundamentals of cloud cost management, cloud financial governance best practices, ways to control access by using the resource hierarchy, and ways to control cloud consumption. In the second section of the course, titled “Operational excellence and reliability at scale,” you learned about modernizing operations by using Google Cloud, designing resilient infrastructure and processes, the fundamentals of cloud reliability, Google Cloud Customer Care, and the life of a support case. And finally, in the third section of the course, titled “Sustainability with Google Cloud,” you examined how Google Cloud works to reduce our environmental impact and help organizations meet sustainability goals. Completing this course also concludes the Cloud Digital Leader learning path. If you’re looking to demonstrate your knowledge of the topics from these six courses, you’re encouraged to take the Cloud Digital Leader certification exam. For more information about the exam, including additional resources to help continue your preparation, please visit https://cloud.google.com/learn/certification/cloud-digital-leader. And if you’re looking to further expand your knowledge of Google Cloud products and services, please explore the entire catalog at cloud.google.com/training. We’ll see you next time!

### Your Next Steps


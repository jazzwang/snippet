# Google Cloud Applied AI Summit Learning Path 

- https://www.cloudskillsboost.google/paths/280

[TOC]

## 02: Gemini for Application Developers

- https://www.cloudskillsboost.google/paths/280/course_templates/881

### Gemini for Application Developers

#### Introducing Gemini for application developers

- https://www.cloudskillsboost.google/paths/280/course_templates/881/video/475233

This is a developer. Let's learn how developers can use Gemini assistance to develop applications. This developer needs to migrate their code to a different language, and they need to generate code that is tailored to their company's proprietary database. They've been given this direction from their manager with a tight timeline. The developer knows how long exhausting and costly the process of modernizing code can be. But they suddenly remember they've heard about Gemini for Google Cloud and decide to use it to help them write better code fast, debug errors, and create reliable and scalable functionality for their company's website. Let's observe this developer as they use Gemini to simplify the task at hand. First, the developer opens VS code and finds the database connection function that connects their front end website to their back end product database. It's written in C++, and they hope to migrate it to go. We now see the developer open Gemini Code is within VS code and type their conversion request in the chat window. The developer is thrilled to make this request in natural language. By typing in the phrase, convert this function to go and use Cloud SQL, Gemini can understand the context and function that is being referenced, all without leaving the IDE and without having to context switch. Just like that, a converted function appears. The function referenced has now been converted from C++ to go. Our developer is in awe. Gemini converted the code to go and inferred that their Postgres connection needed to be converted to a Cloud SQL database. Because Gemini is trained on Google Cloud specific products and best practices, the developer reviews the generated code and feels confident with this conversion. Next, our developer wants to generate some code by using Gemini. They write a comment and natural language where they explain the function they want to create. After typing out the comment, they click the magic wand, and just like that, Gemini generated the function for them, pulled in appropriate product related API calls, and used functions and methods from their company's SDK and code base. Now our developer is celebrating. They are in awe of all the time, money, resourcing and training hours they've saved without sacrificing quality, and all thanks to Gemini. What's this? They get a new email to start working on a new project that requires them to develop a new product filtering feature to improve customer experience. They are excited to start the next project. Now that they know about Gemini, they open VS code, and instead of writing the function themselves, they simply write a comment in natural language, click on the Magic Wand, and Gemini generates the code for them. Gemini generated code. Here, you can also see that Gemini pulled in very specific methods and functions from their company's code base. This is huge. Typically, writing such code would require them to look up these functions, and that would take a lot of time. The developer then asked Gemini to explain code. Write a test plan and test cases and provide commands to deploy the app to production. Great. Our developer is ready to push their changes to production and ask Gemini how to deploy their application? Success, they got an endpoint, and it worked. Time to celebrate. Our developer quickly generated code deployed and generated tests all without leaving their development environment. With all that time saved, they can now finally get around to finishing their crocheting project. Thanks, Gemini. What will you build?

#### Develop an app with Gemini assistance

- https://www.cloudskillsboost.google/paths/280/course_templates/881/video/475234

I'm a developer who needs to build and deploy a simple inventory app to Google Cloud. So let's see how I can use Gemini to be more productive. The first thing I notice is that chatting with Gemini is available right here in my local IDE VS Code, which helps reduce context switching. Now I don't know much about Google Cloud but I do know that my team has standardized on containers. So let's see if Gemini can help me figure out how to build and deploy this app in a container. I love that Gemini's response breaks down tooling and platforms for building and deploying containers packed with just enough information for me to be able to make a decision. Cloud run jumps off the page for me in Gemini's response as I don't want to have to manage any infrastructure so let's go with that. Since I don't know cloud run, I'd actually love an example to use to bootstrap and to stay in context as much as possible. Let's see if Gemini can tell me how to get a Cloud Run app running using Google Cloud's IDE extension Cloud Code. We get step-by-step instructions back from Gemini. So let's step through them. [MUSIC] Okay, looks like I got the sample cloned to my environment and I've landed on a readme page that talks about deployment. But first lets see if Gemini can help me quickly understand the contents of the example. I know containers are often built using a Docker file, and I see one here. So let's actually ask Gemini to break it down. [MUSIC] By highlighting the contents of the Docker file, right-clicking, and selecting Explain this. It looks like Gemini gave me a layer by layer breakdown of what is in the Docker file and I see in Gemini's response that our entry point is actually app py. So let's jump over to see that file and see if Gemini can help us understand the application code. Now, I know there are already comments in the example Docker file, but I was still pretty happy with Gemini's explanation and breakdown. So lets see if it can actually shed some light on the hello function in this flask app. Sweet, with Gemini's explanation, I'm able to quickly learn about the code using natural language, including learning about some useful info that's stored in environment variables that my app can access in Cloud Run. Now, I want to start experimenting with adding inventory functionality to this template. And since I know I'll need to write logic to access inventory data, let's ask Gemini for some sample data. I gave Gemini two specifications that it should be JSON with two specific attributes. And not only was Gemini able to generate this data, but I'm also able to take that data from the Gemini interface and add it directly to this new file in the spirit of speeding up my workflow. Now I'm wondering if Gemini can actually help me write a function to access this data directly from my Flask app. Ill start by modifying my import statements in the flask app for what I think we might need. I read that I can use comments to prompt Gemini to generate code for me, so im going to paste in my requirements, highlighting them and select generate code. Ok I think this is what I wanted. The route looks good, were specifying the get method and were returning the inventory data using jsonify. I guess the next thing to do is test the app out. I'll test it locally first. And since there are some Cloud Run environment variables that we're accessing, I actually want to see if there's a local emulator for Cloud Run. Lets ask Gemini. Yep there is. Now can Gemini tell me how to use it? Just what I needed. Gemini responded with a couple of clear steps to follow to deploy to the cloud run local emulator. So lets follow them. Seems to have completed I think the app is now running and exposed on my local machine at port 8080. Let's open this up in a simple browser, and the app is up and running. I can see that the emulator has specified the cloud run environment variable to be local which makes sense. Now let's see if the code that Gemini gave us to return inventory data is working and it looks good to me. The last thing I want to do is figure out how I can actually deploy this to the cloud in cloud run. And now let's follow Gemini's instructions on how to do this. [MUSIC] Looks like I now have a public URL where I can access the app running in the cloud on cloud run and it looks good to me. To recap, with Gemini I was able to be incredibly productive going from idea to deployment in a platform I was not all too familiar with, all in a matter of minutes and without ever leaving my ide.

#### Develop an app with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/881/labs/475235

#### Deploying Node.js with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/881/video/475236

As a software engineer, I like solving problems and creating code. However, the need for boilerplate configurations and remembering deployment commands I don't use often enough, slows me down and takes some of the joy out of the work. Today, we're going to build, test, and deploy a simple Node.js app, and we'll use Gemini Code Assist to help us get up and running quickly, get testing quickly, and deploy our app to the Cloud. We'll be doing this inside of our IDE of choice, which will be VS code for this video. Make sure you have the Google Cloud code extension installed and Gemini enabled in your project to follow along. Let's get started. First things first, we know we're going to need an app.js and a test.js file for running and testing our application. We'll use the terminal to create a folder for our application and those two files to show how Gemini can help us create a project from scratch. We'll be making our app using the express framework. We'll need to install the express and EJS packages. Let's ask Gemini to remind us of the command. After a moment, Gemini gives us information on installing both packages, including a way to only install them for a single project. Since we're just using them for this demo, let's use the save argument to install them locally. Now that those packages are installed, we need to add them to our projects app.js file. Let's use Gemini to make it quick and correct. Gemini provides the code to require the packages and make them available in our file. Let's quickly copy this into our app.js file. Gemini isn't just about chatting, it can also recommend code in line. Let's use this feature to get the recommended boilerplate for initializing the EJS templating engine. All we have to do is write our request in a comment. When we hit Control plus Enter, Gemini will show us its recommendation, sometimes even multiple recommendations. All we have to do is click ''Accept'' and the code is immediately inserted. This is kind of fun. Basically, we can program in pseudocode for these configurations. Let's keep adding some more code, starting with middleware to parse form data. And we'll need to set up the home route. We want a post route called greeting with a parameter for the name of the person to greet. We don't want to forget that the server needs to be started. Finally, let's create a route for greeting that accepts a name parameter. Now that we've got app.js configured, we need to create a package.json file based on it. Let's go back to chat and ask Gemini if it can create one for our existing file. A few moments later, we get a response with the contents for our package.json file. We also get a description of the code so we can understand enough to make any changes. Let's just update the file and copy in that code. In order for our app to work well, we'll need to create our views. Let's create our index and greeting templates inside our views directory. Our index page will be a simple form asking for a name. Our greeting page will use the data it gets from the form to say, hello. Now that we have all our files, we can use NPM start to test that it works. Going to localhost port 3,000 in our browser opens the index page. I'll put in my name and, Hello, Daryl. We've tested that it runs. But what about unit testing? The easier it is to write tests, the more likely they are to get written. Let's see how well Gemini can help us. First, let's make sure our tests will be able to see our app by adding the line module.exports equals app to the end of app.js. Now, let's ask Gemini to generate a unit test for our home route and ensure that the response code is 200. We get a response with all the needed requirements in the code and an explanation of what it does. Not surprisingly, it does what we asked it to do. Let's copy the code into our test.js file we created earlier. From the requirements, it uses the Super Test package, and we'll use the MOCA test framework with that. So let's make sure we have those installed. Now, let's also update our package.json file with our test script and packages in mind. We could ask Gemini how to create our file contents again. But thanks to the previous response we read, we understand that we just need to update the scripts and dependencies sections. We can now run our test with the npm-test command. The test is passing, and we could continue to use Gemini to aid us in writing more tests. Once you have your app working and your tests passing, you'd probably like to know how you can deploy your app to Google Cloud's Cloud Run. Once again, we turn to Gemini to let us know if we can deploy directly and how to do it. We'll run the sample command. When asked, we'll use the US-Central one region and allow unauthenticated invocations, since this is only a test application. After a few minutes, we will be shown a link to where our app is publicly available. And here's the link that we could follow. But we already know what that page looks like. Gemini helps with many different aspects of development so that you can focus on solving more interesting and more difficult problems.

#### Quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/881/quizzes/475237

#### Next Steps

- https://www.cloudskillsboost.google/paths/280/course_templates/881/documents/475238

### Your Next Steps

## 03: Gemini for Cloud Architects

- https://www.cloudskillsboost.google/paths/280/course_templates/878

### Gemini for Cloud Architects

#### Introducing Gemini for cloud architects

- https://www.cloudskillsboost.google/paths/280/course_templates/878/video/475984

[MUSIC] This is a cloud architect. Let's learn how to use Gemini to build and deploy web apps in Google Cloud. To do this, we learned to enable Gemini in the Google Cloud console. Create a series of prompts that describe the infrastructure that you want to deploy. Create a series of prompts that helps you select the correct storage option for your infrastructure. Refine our prompts to fine tune the specific settings in our deployment. To do that, the cloud architect asks Gemini for help and looks to pick the right compute option that will be fully managed. After looking at the options suggested, they decide to focus on Cloud Run. But what's the best way to make sure it can scale? Gemini comes to the rescue once more. Being new to Cloud Run, the architect gets more help from Gemini on networking and authentication help. They will also need caching to help with scale, so Gemini can give advice on caching options and manage services like Memorystore. The web app will also need a database layer, and for that there's a few options in cloud. The architect asks for recommendations and settles on Cloud SQL. Finally, it's time to deploy all three tiers. Gemini shares a sample architecture that includes Cloud Run, Memorystore, and Cloud SQL, all working happily together. Delighted with her progress, our cloud architect is ready to test it out, hooray! With all the time they saved getting this architecture spun up, the cloud architect has more time to practice their ballet moves. What will you build? [MUSIC]

#### Provision Cloud Infrastructure with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/878/labs/475985

#### Creating GKE clusters with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/878/video/475986

If someone asks you if you know about Kubernetes, you have no problem saying yes. If they ask you to set up and manage a GKE cluster on Google Cloud, perhaps it's been a while or you are new to Google Cloud. You may not be clear on some of the actions you need to take. Gemini is here to help. Let's start by opening up Gemini from the Google Cloud console toolbar. If you don't see this icon, make sure you've got Gemini activated on your project, and you have the permissions to use it. You could also use Gemini from within your IDE of choice. Let's jump right in and ask how we can run Kubernetes on Google Cloud without having to own management of nodes and the control pane. Here we have Gemini's response. To run Kubernetes on Google Cloud, you can use Google Kubernetes Engine Autopilot mode. You can also use GKE-managed nodes, which are fully managed Kubernetes that are provisioned and maintained by Google. Let's continue our conversation. We'd love to standardize our cluster creation in autopilot using Terraform, so let's ask if we can do that. From the response, we see that we can. We even get a link or three to dive deeper. We could bookmark these for now, as I'd really like to test GKE cluster creation. Gemini chat is an ongoing conversation as long as it stays in this session. We could also reset our chat by clicking the reset icon. For now, we'll continue on. Gemini has recommended GKE in Autopilot mode for running Kubernetes in Google Cloud. However, I'm unfamiliar with how this managed Kubernetes service works, so let's first test GKE in Autopilot mode and have Gemini help us along. We get a response, but there are some placeholder values for cluster name, region, and project ID. Since I'm already in the project and I know the cluster name in region, I'll be more specific with Gemini, so I can just copy and paste the response. Now let's copy this gcloud command into Cloud Shell and get started deploying a sample application. This will take some time. After our cluster has finished provisioning, we can use a test application for further experimentation. Google already has one called hello-app, and we can use Gemini to help us see how it behaves in GKE Autopilot. Let's use Google's test container image to create a deployment called hello-server. After a few moments, Gemini responds and includes the command. Let's run it in Cloud Shell. Now, the web server is created, but in order for anyone to get to it, we'll need to provision a load balancer for it. Let's ask Gemini how to expose this deployment on Port 80 with a load balancer. Look at that. I didn't even tell the deployment name, but it understood from context that I wanted hello-server when I said to expose this deployment. Finally, if we want to see this service in action, we'll need to point our browser to an IP address. Let's ask Gemini to help us find it. We should get a response with a command to run in our terminal. Let's run it, and in a few minutes, we should have an IP address. Plugging the IP address into our browser shows us the hello-server deployment in action. Once again, let's head into chat and ask Gemini how we can configure time periods where GKE Autopilot will not perform maintenance or upgrades. Gemini responds with information on maintenance windows and exclusions. After reading this, you might realize the off hours of 10:00 PM to 2:00 AM are a great time for maintenance to be run, so let's ask how to create a daily maintenance window on our cluster at that time. After a few moments of thinking, voila, Gemini has given us a response. We could input this command in our shell to set this up and feel free, but now you are ready to add Gemini to your infrastructure administration tool belt. You will also use it to aid in deploying and maintaining your Kubernetes clusters on Google Cloud.

#### Architect web apps with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/878/video/475987

As a Cloud architect, you might find yourself designing an organization wide golden path for your developers, one in which they can quickly build a web app in Google Cloud. I'll show you a few ways you can use Gemini as your partner to understand the options better. First, we'll open up Gemini using the icon in the Cloud Console toolbar. Here, we'll chat with Gemini as we would another human being in any chat application. You could also use Gemini chat in some popular IDEs like VS Code with our Google Cloud extensions. We'll start telling Gemini that we are designing a multi tier web app on Google Cloud and that we first want information on what to use for the compute tier. It is important to provide as many details as we would provide a subject matter expert. Let's make it clear we are serving traffic over the public Internet and that we would like fully managed services. In just a few moments, Gemini has provided us with quite a bit of information. Let's say that we are creating the fastest path to running smaller web apps in our organization. The description given for Cloud Run seems to meet our requirements. Let's just confirm this by checking which metrics Cloud Run uses for auto scaling. After a moment, Gemini gives us an answer about incoming requests and also provides a link to follow for more information. Since Gemini told us that Cloud Run uses the number of incoming requests for auto scaling. Let's see where those requests might come from by asking how it is exposed to users over the Internet. The response we get back talks about a public load balancer, which is probably fine for our applications. But some apps might need to be available only to users with the correct permissions. Let's ask how easy it is to only allow authenticated users access to an application. Great. It looks like there's an easy way to accomplish that. Gemini also provides a bit of description as to how to accomplish this and what to expect plus, more information is available if we want to read further. Finally, our goal is to make this as simple as possible for our developers. We prefer they didn't need to write any configuration related to infrastructure like Docker files. Also, we've already standardized on Django as our web framework. Let's ask Gemini if we could deploy these types of apps without the need to containerize anything. Again, Gemini responds in the positive and provides even more context to allow us to use it as we need. It looks like Cloud Run is the way we'll go for the compute layer. Now, onto the caching layer, Gemini will use our previous discussion and answer with that discussion in mind. Let's ask how we can introduce a caching layer and ask to prioritize fully managed services for this tier as well. Our response recommends Cloud Memory Store, Cloud CDN and Redis. Cloud Memory store sounds the most reasonable, but a good architect has security concerns. Let's ensure communication between Cloud Run and Cloud memory store is private. Here is the response, and there is a step to take in ensuring private communication, a serverless VPC access connector. I'm glad we asked that question. I'm glad Gemini was able to help us with the caching tier. Apps of any complexity deal with storing and using data. A relational database might be the way you'd like your developers to go. Continuing our conversation with Gemini, let's ask what it recommends and let it know we'd like to continue prioritizing fully managed services. A few moments later, the response lets us know there are a few options for us. But given our previous discussion, tells us about Cloud SQL first. Given our goals and developer expertise with SQL, this seems like a perfect solution. With our three tiers settled upon, how would we go about deploying an app that uses Cloud Run, Cloud Memory Store, and Cloud SQL together. Let's ask Gemini one more time. Of course, Gemini responds with a solution that has already been developed. In a short conversation, we've come up with a multi tiered web app architecture that fits our needs, and we can move forward with testing it to finalize any thoughts we have on using it. Gemini is a great tool to use when you would normally think out loud, but would also like to get answers to those thoughts. As an architect, you are likely to have many questions when deciding on the best solutions, and Gemini can be there for you.

#### Quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/878/quizzes/475988

#### Next Steps

- https://www.cloudskillsboost.google/paths/280/course_templates/878/documents/475989

### Your Next Steps

## 04: Gemini for Security Engineers

- https://www.cloudskillsboost.google/paths/280/course_templates/886

### Gemini for Security Engineers

#### Introducing Gemini for security engineers

- https://www.cloudskillsboost.google/paths/280/course_templates/886/video/475990

[MUSIC] This is a security engineer. Let's learn how security engineers can identify and mitigate security threats with Gemini. They need to understand the newest vulnerabilities and find out if they have any mitigations to do. Based on their cloud infrastructure and logging. They need to know what their threat exposure is and are hoping to use Gemini for Google Cloud to help them take corrective action if needed. Lets observe this security engineer and their natural habitat as they use Gemini to simplify the task at hand. First our security engineer wonders what are the latest threats they should be aware of? Let's start with the view of the latest threat intelligence from Mandiant, helpfully gathered all in one place for consumption. Whoa, thousands and thousands of results just for one threat actor. Our engineer doesnt have time to read all that. Luckily they can get a quick summary thanks to the AI helper. Picking one prominent threat the engineer wants to dig deeper, but this is just showing a big mess of metadata which is confusing and overwhelming. They start with the Gemini generated summary on the sidebar to give them a faster way to understand what this threat actor is trying to do and where. The security engineer needs to know if they're vulnerable to any of these attack patterns. Luckily they can do a plain language search in chronicle to find out if any of these attacks are hitting their infrastructure. Well this is both good and bad. The good part is that they've got some results without having to navigate the arcane UDM syntax. The bad part is that it appears that they had some hits, so some crypto mining attacks are going on. Let's explore this further. Once again, we see our security engineer overwhelmed with information. Lots of cases, lots of windows, lots of statistics. Help us Gemini summary you're our only hope. Hooray, it's right here and it shows them exactly what they needed. It's time to act now. They go directly to the security command center for actions to respond to the detected threats. No, they found more mountains of metadata. Luckily they've got Gemini here to summarize and give them a quick guide towards mitigation actions. Summary read and digested it's time for the engineer to unwind with watering their plants. What will you build. [MUSIC]

#### Navigate Security Decisions with Gemini

- https://www.cloudskillsboost.google/paths/280/course_templates/886/labs/475991

#### Gemini for Navigating Security Decisions

- https://www.cloudskillsboost.google/paths/280/course_templates/886/video/475992

>> I'm a security engineer working on my company's infrastructure. I need to make sure I stay on top of any issues with our security posture or deployments. Let's see if Gemini for Google Cloud can make that faster and easier. Today I'm digging into our Google Kubernetes engine clusters and looking for configuration errors that could increase our risk. To start, I'll deploy a new cluster and clone a simple web app demo from this GitHub repo that models an online store using microservices. Once thats all set you can see the web app is running and available exposed to the Internet. For this exercise pretend im new at monitoring and securing GKE so I turn to Gemini for some help. I can ask for advice on how to secure microservices running in a GKE cluster and get some helpful options back. I decide that today, I'm going to pursue the first one listed, security command center, sounds authoritative. I'll begin at the overview. Then it's time for more advice from Gemini. After I read through some of the helpful documentation that Gemini gives, it's time to return to sec to check on its findings. I see a bunch of medium severity findings and I'll click into them one at a time. Wow. Gemini is right there with me, helping out with a plain language summary of the various metadata and array of fields. Below. I can repeat this on each of the findings that I want to learn about and check on the summaries plus the recommended mitigations. Now that I know what's wrong with this deployment, I'd like to begin remediation. Gemini pointed me to the network policy and I'll follow its instructions. [MUSIC] First, I'll enable network policy for this cluster. While that's processing time to learn a bit more about what I'm actually doing right now, asking Gemini about network policies in GKE gives me a bunch of helpful info and a sample policy to examine. I'd like to make some policies so I'll ask Gemini for help in understanding my pods better via their labels. Hooray, another helpful command that I can use to learn about my microservices. With all the info I've gotten thanks to Gemini, I can work on fixing my clusters, securing my microservices, and replacing my labels. These tools enabled me to learn more about what Google Cloud can do to help me understand security vulnerabilities and how to mitigate them. Gemini was able to give me cloud shell code as well as helpful summaries speeding up my work.

#### Quiz - Gemini for Security Engineers

- https://www.cloudskillsboost.google/paths/280/course_templates/886/quizzes/475993

#### Next Steps

- https://www.cloudskillsboost.google/paths/280/course_templates/886/documents/475994

### Your Next Steps

## 05: Conversational AI on Vertex AI and Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425551

Welcome to the course Conversational AI on Vertex AI and Dialogflow CX. This course is intended for developers and conversational designers. Please be aware that this course content builds upon concepts taught in the course: Customer Experiences with Contact Center AI. You can find this course in Cloud Skills Boost. If you haven't already, please review that course and then come back to learn the new capabilities that generative AI and conversational AI is bringing to CCAI and Dialogflow CX. If you are familiar with Dialogflow CX, but need a refresher, there’s a short review in this module. It is also helpful if you have a good understanding of the fundamentals of Google Cloud, there is also a course on this in Cloud Skills Boost. In this course, you’ll learn: [...] About Vertex AI Conversation, a single powerful platform for building conversational AI solutions that use Generative AI, and the key benefits of using Vertex AI Conversation features in Dialogflow CX. Dialogflow CX, is now supercharged with generative AI features. The core components of these new features are generative AI Agents, Data Stores, Generators and Generative Fallback. You’ll explore how the new generative AI features help virtual agents handle specific customer interactions. And, where in your current Dialogflow CX solution, these generative features can be integrated. You’ll learn about Generators and the various capabilities that they can bring to your Dialogflow CX agents. Generators can be customized and configured to generate dynamic responses or text that can be used during fulfillment. And you’ll explore examples of how to write prompts to generate various responses for your customers. And, you’ll find out how to configure and deploy generators in your Dialogflow CX solution. You’ll learn how Generative Fallback is a mechanism for handling points in the user conversation where the the conversation moves away from the intended flow. And, how the quality of your virtual agents interactions can be enhanced by more natural and conversational responses. You’ll discover that you can enable generative fallback on no-match event handlers, which can be used at three levels: in flows, pages, or during parameter filling. And you’ll learn how to configure and enable generative fallback at all the necessary levels of you Dialogflow CX solution. Finally, you’ll learn more about Generative AI Agents, hybrid agents and data stores. Including when to use intent-based flows, generative AI, or a mix of intents-based flows and generative AI for certain use cases. You’ll explore the two methods for enabling generative AI features for your virtual agent, either by creating a generative AI agent, or by adding generative capabilities to your existing agent with data store handlers. And you’ll look at the costs of enabling Conversational AI in your Dialogflow CX solution, for both standard and enterprise customers.

#### Dialogflow CX recap

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425552

This lesson provides a recap of Dialogflow CX, which is covered in full in the Customer Experiences with Contact Center AI course. Dialogflow CX enables you to create virtual agents for dynamic and personalized 24/7 self-service. This is achieved through: An easy to understand interface using a state-machine based model and flow-based modules optimized for large enterprise customers. Handling detours, by answering customer follow-up questions at any point, then seamlessly transition the customer back to the dynamic flow. Build and deploy agents in multiple languages, Automatically run tests and experiments on the agent before deployment and A/B testing during production, with integrated CI/CD. Analyze conversational paths, assess agent performance, and improve the agent, Native Interactive Voice Response, or IVR features such as DTMF, to capture parameters from different input such as pin codes, Timeouts for the agent to react if the user didn’t respond for some time, and Barge-In, to be able to interact the user. All of that with a selected voice Additionally, it includes pre-built components, for one-click import of conversational building blocks. You can transform the user experience and optimize contact centers through easy-to-build, conversational agents, by: Building faster; Start training with only a few examples, and leverage over 40 pre-built agents, to go-to-market with a simple bot in hours. Engage your audience more efficiently with built-in, world-class natural language understanding, and utilize multiple fulfillment options, with natural, conversational responses through generative AI. Training and analytics are also accessible across all platforms, to upskill and reap the benefits of your investment. And maximize reach, by building once, and deploying everywhere, with more than 100 supported languages. There are also 14 single-click platform integrations and 7 SDKs to help you spread the reach even further. You should have encountered the Dialogflow CX user interface already, but let’s very briefly recap on how it works. The Dialogflow CX UI offers a state diagram overview of your virtual agent logical flows. This is where you select the project and agent you want to work on. Note that, before you reach this screen, a project and agent have been created already; you would be prompted earlier. The Build and Manage tabs. Build: While working on your agent, you’ll have the Build tab enabled much of the time. Under the FLOWS section, you will notice the “Default Start Flow,” which is all you need unless you have a complex virtual agent. In that case, you’ll create multiple flows and need to select the one you want to work on at any given time. We’ll talk about that later in this module. And, Manage: While working on other resources such as intents, entities, and webhooks, you’ll have the Manage tab enabled. You’ll use the Graph settings to change your view of the agent configuration. The Graph panel shows you the current relationships and flow via Pages that you’ve created. You can click on any of these pages to go to the configuration details. When you’ve finished, click “x” to close it and view the main graph again. The task indicator will show you whether any agent is training or another task is in progress. The Agent settings and test agent buttons. Agent settings is where you configure the name of your agent, timezone, language, ML, speech, and version settings. Clicking on Test agent opens the Simulator panel. Type a user prompt into the text box that says “Talk to agent” and press ENTER to get the response from the agent. The contents of this right-side panel change depending on what you have selected. For example, you may have clicked on “Test agent” to enable the Simulator to be shown in this panel. You can expand or close it as appropriate.

#### Dialogflow CX recap: Key terms

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425553

Let’s recap on some key terms used to describe certain concepts, functionality, and objects in Dialogflow CX. First, let’s look at a brief definition. Head intent, is the main topic of conversation. Flows, are the overall sequence or block of instructions. Pages, specify what to do in specific states of the conversation. And Routes, are where to go next when the state of the conversation changes. A head intent is the main topic of conversation. In other words, you refer to an intent specifically as a head intent when you want to indicate it as the main topic of a conversation; minor topics may occur within it. For example, in the Cloudio scenario, the intents include update_first_name, update_last_name, and update_account. update_account would be the head intent, where the others are more granular. In another example, the car rental scenario, the intents include: confirm_pickup_location, confirm_dropoff_location, and rent_a_car. rent_a_car would be the head intent, and the others are supporting intents to accomplish the overall need. Note that a sub intent could be something a little more generic and could support more than one head intent. Some pages represent a state the agent will be in for more than a single scenario. For example, in the Cloudio scenario, you need to prompt the user for their phone number whenever you need to uniquely identify their account in our database. Whether the user wants to update their first name, last name, or PIN, you need to know their phone number. If your agent was broken into multiple flows, you would need to create the same phone number gathering page in each flow. This is tedious and error-prone. That’s where route groups are useful. You define an intent requirement, and optionally a condition requirement, and assign the route to a group. Then you can reference that route from any page. Head intents are configured in the Start page. You’ll return to that after discussing the meaning of a page in Dialogflow CX. Pages in Dialogflow CX represent the possible states of a conversation. There’s one active state and thus one currently active page at any given moment of a virtual agent session. A page in the Dialogflow CX user interface is also where you configure everything related to a given state of the conversation. How does a page differ from a flow? Because a page represents a specific state of the conversation, it’s where you configure the parameters, prompts, and fulfillment, that should be processed for the given state. A flow is made up of multiple pages and represents an overall process or conversation topic rather than a specific moment in time. One example that illustrates this concept is a state diagram for what you can do with a door. The diagram represents the possible current states of a door, Opened or Closed, the entry actions, the door has been opened or closed, that define when the door can transition to each state, and the transition conditions, the trigger expressing when to change the door’s state from Opened to Closed or from Closed to Opened. Using this example, you might have a virtual agent defined as follows: Intents: open-door, close-door Flow: Door Pages: Open, Closed Transition condition: open = true, open = false And, Condition route: from Open page to Closed page, from Closed page to Open page. The virtual agent may need to transition to a new page or a flow, depending on design. At least one of the following must be provided: Intent route or Condition route. Each virtual agent in Dialogflow CX must have a main flow, which is called the Default Start Flow. A special page in this Default Start Flow is the entry point to all other functionality the virtual agent will handle. Although the Start page doesn’t have the same configuration options as all other pages, it always has the ability to: Propagate intents, Route using a simple condition evaluating to true, and, propagate custom events. In the Start page you define which intents, when matched by Dialogflow, will be addressed by the flow. For example, the Default Start Flow with reference to list_most_played tells Dialogflow to invoke the flow and tells it which page or flow to route to next. when the virtual agent consists of a single flow, the Default Start Flow page is the only start page in the virtual agent. When the virtual agent contains other flows, the Default Start Flow is the “mega agent” that triggers other flows. Each of the other flows will contain their own start page but none of those are the entry point into the conversation. So how does Dialogflow CX know what to do when? Is it AI based on the intent matched, or based on configuration of pages (more like an IVR system)? The answer is that it’s a bit of both. The start page is where you define all the intents you anticipate handling within this flow. No matter which state or page the virtual agent is in the conversation, it’s possible for Dialogflow to match a new intent and direct the conversation flow over to a different page, based on what you’ve defined in the start page. What happens if an intent is defined within the start page of a separate flow from the Default Start Flow? Of course the answer depends on the configuration, but as a general rule, if the intent is matched to a specific start page, that’s the flow it will execute next. Let’s explore an example. Imagine that the conversation topic was about a user’s subscription and involved several activities to complete the conversation, such as getting the user’s phone number to identify them, updating the tier on their account, confirming that the tier was updated to the tier the user requested, and asking the user whether they needed anything else. The following could be a reasonable way to structure the virtual agent: Flow is the Subscription Inquiry conversation. Page 1 is the Start page. Every flow must have one. Here, the subscribe intent is defined. When the user inquires about their subscription, Dialogflow matches the intent and triggers this flow. The first route is to the Identify User page. Page 2 is Identify User. Here, every prompt and response, parameter, fulfillment, etc. required to complete the task of identifying the user is defined. In the case of the Cloudio virtual agent, we would identify the user by their phone number. When the phone number is obtained, the route goes to the Update Tier page. Page 3 is Update Tier. Here, everything required to complete the task of determining which tier the user wanted and making that change is defined. When the tier is obtained, the route goes to the Confirmation page. Page 4 is Confirmation. Here, everything required to inform the user of the change made and get feedback that it was correct is defined. The next route is to the Anything Else page. Page 5 is Anything Else. Here, the “Can I help you with anything else?” prompt is played. If the user’s utterance at that point matches a completely different intent, another flow could be invoked. Otherwise, the route could be defined to simply end the session. Let’s explore an example of how pages are defined to carry the user through the conversation. Beginning with the Start page, also known as the “Default Start Flow”. State: This is the beginning of the conversation. The user asks “What’s the most popular song in Rock,” and Dialogflow matches the list_most_played intent. This page is configured to transition to the “Get Most Played” page after all conditions of the page are satisfied. The Get Most Played page. State change: Now this page is active. This page is configured to capture the music genre parameter so it can do a lookup in a database. After it has the genre parameter, it uses a webhook to do the lookup. This page is configured to transition to the “List Most Played” page. Next, the List Most Played page State change: Now this page is active. Tell the user the song title. This page is configured to transition to the “Anything Else” page. Then, the Anything Else page State change: Now this page is active. Ask user “anything else?” When the user says “No,” this page is satisfied and Dialogflow transitions to end the session. If matched to an intent, the “Start” page becomes active again. Why have a separate page for “Anything Else”? Mostly to keep a clean design where all page transitions end here before going to the final end session state. Otherwise, you’d have the text saying “Anything else?” to the user inside every page. And finally, the End Session page or start a new flow if a new intent has matched. Each page is composed of elements related to the state of the conversation. It’s where you tell Dialogflow CX what to say and do and where to go next. Entry dialogue, also known as an agent prompt, is used to say something to the user. What to say to the user and when depends on what state the agent is currently in. For example, you might need to ask the user, “What’s the phone number on the account?” You may not need a dialog in every page. Parameters are used when something should be collected from or played back to the user. For example, you may need a value set for the music-genre parameter to use in your webhook that looks up the most played song. During each virtual agent conversation, Dialogflow evaluates the conditions you specify. You use this to control whether Dialogflow should follow the configured route. Any expression that evaluates to true can be used to trigger a transitional route. For example, if all the required parameters defined on the page have been identified and assigned values, the page status is considered final. When this condition is evaluated as true, Dialogflow looks at the configuration of the transition route to determine what to do next. Routes used for transitions to pages or Flows based on either a matched intent or condition that evaluates to true. Transition routes are used to specify where to take the conversation next. When everything else for the page is successfully completed, you will probably want to transition to another flow or page. For example, if it’s the last state of the conversation, you’d at least have a courtesy “Anything Else” page. If the conversation should jump to another topic, you may need to specify another Flow (which you would have already defined). Transition routes are also sometimes referred to as state handlers. Event handlers are meant for scenarios where the user did not give the agent information for it to move to the next state. For example, the virtual agent prompts the user, “What is the phone number on the account?,” but the user does not respond. Dialogflow can handle this no-input scenario, and it re-prompts the user for input accordingly. And Fulfillment, when Dialogflow needs to read from or write to an external source, you’ll use a webhook to perform this service. Note that in Dialogflow CX, the term fulfillment has broader scope than in Dialogflow ES. It also includes prompts and parameters. You may hear references to event handlers and state handlers. So, what’s really the difference between state and event handlers? Consider it this way: State handlers, or transition routes, address your virtual agent’s expected behavior to cover all the services offered by your virtual agent. And, Event handlers address how your virtual agent will behave when anticipated routes aren’t followed. The virtual agent needs ways to course correct so the user isn’t abandoned.

#### Dialogflow CX recap: Entities and Intents

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425554

Let’s now recap, that a Virtual Agent conversation is made up of representative words and phrases, these pieces include: Intents, Entities and Parameters. You can use these to dissect and interpret the customer’s need. You'll need to configure your list of entities, train the agent to recognize the words and phrases that users say as those entities, and then match the phrases to an intent. This process is still a big part of Virtual Agents in Dialogflow CX, however new features are being added to help you support certain aspects of the customer experience with Virtual Agents. When creating entities, most will need to be configured following a process similar to this: In the main Dialogflow CX menu, click on the Manage tab. Under Resources, click on Entity Types, and click Create. Give your new custom entity type a name, for instance “tier” for the choice of service level your customers are able to subscribe to. Then, add some values to your entity type, such as “silver”, “gold”, and “platinum” if that’s what your business calls your service levels. Next, provide each entity value with some synonyms. This part is important because your customers may not already know what you call your different tiers. By adding synonyms you’re doing two things: Providing more phrases or terms for Dialogflow’s Natural Language Understanding, to map what a customer says, to what you internally call your tier. This way, your virtual agent takes on more human-like intelligence to help the customer. And, ensuring a customer does not have to know the precise term you use internally, in order to recognize the tier they want, to have their request fulfilled. Finally, click “Save” to complete the entity type. When creating intents, most will need to be configured similarly to the following: In the main Dialogflow CX menu, click on the “Manage” tab. Under Resources, click on “Intents”. Click “+ Create”. Give your new custom intent a name. For instance, “change-tier” for the action the user wants performed. Give your intent some training phrases. This is important because this helps Dialogflow start the Natural Language Understanding process. By giving five to ten representative phrases that your customers frequently use to describe that this is the action they want from you, Dialogflow can intelligently learn from those and match other phrases which are similar to the training phrases defined in this intent. For instance, enter some training phrases such as the following: “Can I get the platinum tier” “Please give me your medium-priced package” Notice that the entities in your phrases are highlighted. This is referred to as form filling or slot filling. Dialogflow recognized these terms because you already defined your entity type before creating the intent. Click Save. At the bottom of your intent definition, you may have noticed the section for Parameter ID. This is used for when you will reference a parameter, for example in a response to the customer or in a webhook call. Parameter ID, “tier”, refers to the parameter value. Entity type, “@tier”, refers to the type of entity. Is list would be enabled if the entity value could be in the form of a list of items, for instance if someone wanted to order a shirt that has the colors red, blue, and yellow. In our Cloudio example, all entities are single values. Redact in log would be enabled if the information is sensitive and the value should not be stored in plain text in logging. For instance, if your entity was for a PIN, you might enable Redact in log.

### Generative AI for Dialogflow CX

#### Introduction to Generative AI for Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425555

Dialogflow CX, is now supercharged with generative AI features. With generative AI you can help users find the exact answers they're looking for quickly using natural language queries. Provide rich, multimodal experiences using natural language, and allow users to communicate expressing the intent in natural language, instead of following a set of rigid questions as to fill up a questionnaire. Dialogflow with generative AI is Informational, because it gets information from your companies documents and websites to provide information, or gives generic answers or summarizations from the LLM capabilities. And Dialogflow with generative AI is responsibly controlled. You can define controls with flows and business logic, helping reduce risk and improve security, generate answers grounded in your content, and choose the degree of generated content that is presented to your users. At Google Cloud, our vision is for Vertex AI Conversation to provide a single powerful platform for building conversational AI solutions that use Generative AI. This combination of technologies allows you to create virtual agents that can understand and respond to natural language in a human-like way, even for complex or difficult requests but remain controlled by a graph and your conversational design. The goal is simple but important: Your virtual agent is able to bring the conversation back on track. One of the key benefits of using the Vertex AI Conversation features in Dialogflow CX is that a developer controls the user experience, unlike using a pure LLM approach. This means that you can ensure that your chatbots provide accurate and helpful information, conversations remain on track and follow a known path, and generated content is restricted to what you want it for. Vertex AI Search and Conversation can create virtual agents that are able to handle a wide range of tasks, from simple FAQs to complex transactions and blend these Generative AI features within the user experience. In addition to this, our broader AI ecosystem of speech-to-text, text-to-speech, along with implementation and product partners, means that you can tailor the user experience that you want for customers. In addition to providing control over the user experience, Google Cloud and Vertex AI Conversation also offer a number of other benefits, including: Scalability: Vertex AI Conversation is deployed on our cloud platform and scales elastically, so you can easily add more users or traffic to your chatbots without having to worry about performance or managing underlying infrastructure. Security: Our managed Vertex AI Conversation solution is deployed following our standard security and data residency standards - so you do not have to manage this manually if you deploy your own solution and can be confident that your virtual agents meet your serving requirements. All with Reliability and resiliency: Vertex AI Conversation is served with a SLA. A Conversational AI Solution is the combination of a Virtual Agent type and End User Channel. This combination determines the complexity of the solution and increases the effort required to build a high quality user experience. Google Cloud products are designed to reduce that complexity for customers so that building high quality Virtual Agents does not imply a much higher order of magnitude of implementation effort. The Dialogflow CX platform is designed to accommodate three main types of Virtual Agents: The Dialogflow CX platform is designed to accommodate three main types of Virtual Agents: Informational: These virtual agents are designed to provide information to users. They can answer FAQs, provide product or service information. Transactional: These virtual agents are for completing transactions with users. They can help users with tasks such as booking appointments or making reservations, process orders, provide customer support, or handle other types of business-to-customer interactions. And, companions are a class of virtual agents that are designed to anticipate end user needs within context and provide intelligent and useful propositions to help. For example, a customer asking for information about their phone bill, which might be higher than normal, a companion might detect this situation and propose another plan. A customer looking to make a hotel reservation might also be proposed other related information. In order to build the end user experience of the Virtual Agent, you need to expose it via an End User channel: Text or chatbot: A user interacts over text on a website or other text based medium, like sms, or WhatsApp. Rich Text: In addition to simple text responses, Rich Text responses like chips, buttons, URL links, or answer summaries can also be provided. Voice: A user interacts with their voice in real-time. This is typically over a telephony network subject to line noise and compression artefacts and typical disruptions from the end user side. Custom or Multi-Modal: For more complex user experiences, customers may build a custom mobile application, a rich web application, combining text, voice, and perhaps even image input to the virtual agent. This kind of user experience may be required to implement more complex transactional functionality, such as processing documents. Finally, on Google Cloud’s Conversational AI platform you have the choice of four different underlying Natural Language Understanding or NLU technologies to implement your Virtual Agent based on your needs: Intent based: Highly deterministic intent based matching based on training phrases. State machine and Intent based: As per intent based, but now also the ability to control different conversational paths deterministically through a state machine. State machine and Intent based with Generative AI: As per state machine and Intent based, but with generative AI features to handle complex natural language tasks where generative or summarisation language features are required. And, Instruction-based Control LLM and LLM-based NLU: Using a framework such as ReAct, a virtual agent can be implemented by using instructions to achieve a task.

#### Generative AI additions to Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425556

There are two ways to add generative AI capabilities to virtual agents in Dialogflow CX. One way is to create a new generative AI Agent using Vertex AI Conversations. Within Vertex AI Conversation you will create a data store containing the content that the virtual agent will search. Your new virtual agent will then be able to answer questions about the data it indexes. The second way is to upskill your existing Dialogflow CX agents by adding Vertex AI Search and Conversation data stores to your agent. This allows you to add another layer of question coverage to your intent-based routing. Generative AI has really supercharged the capabilities of Dialogflow CX. The first Conversational AI offering in this space is the “Generative AI Agent”. These new Bots add value where it’s too expensive and cumbersome to use the intent-driven design of existing Dialogflow CX solutions. With Generative AI Agent, you can point the bot at a URL, have the bot crawl that content, index it, and then after the indexing process is complete, it will be able to start responding to questions about the content. You can auto-create bots from your content, starting with as little as your website or other business documents. Your bot is created in a few clicks and can find or generate answers to complex questions using that content. But there's more! To power those generative AI agents, you will use Vertex AI Search & Conversation to create data stores from websites, structured, or unstructured data. Generators allows you to use Google's latest generative large language models during Dialogflow CX fulfillment. You can use generators to generate agent responses at runtime. A generator can handle responses that involve knowledge from the large textual dataset it was trained on or from the context of the conversation, such when generating a conversation summary. You can also use generated text as an input to webhooks. For example it can can generate the text of an email that the webhook will send. And, Generative fallback responses: This feature can be configured with a text prompt that instructs the LLM how to respond. You can use a predefined text prompt or add your own prompts. With the predefined prompt, the virtual agent can handle basic conversational situations. For example: Greet and say goodbye to the user, Repeat what the agent said in case the user didn't understand, Hold the line when the user asks, And summarize the conversation. Let's explore how the new generative AI features help virtual agents handle specific customer interactions. Let's assume you have an agent that understands multiple customer intents. If a customer poses a question that matches an intent route that is product related, your Dialogflow virtual agent will use webhooks to source the answer from your core systems, such as existing Retail Search, or from Vertex AI and Langchain. But what if the customer's question doesn't match any of the intent routes? Or perhaps you haven't created any intent routes for certain customer enquiries? Well this is where the new generative AI solutions can bolster your Virtual Agents. You can use Vertex AI Search and Conversation to create and configure data stores for your organization's documents or websites. And you can create a Generative AI Agent to access the relevant domain or documents to provide the answer, for example the Generative AI Agent could search the existing FAQs on the company website to generate a response to the customer through Dialogflow CX. Generators and generative fallback both leverage LLMs to provide answers to the customer. Generators can generate text for emails or conversation summaries and Generative Fallback Responses are used to generate communicative responses when the customer conversation deviates from the intended route, such as if a customer stops communicating or does not follow a direct line of enquiry with designed intents. Your Dialogflow CX Agent uses intent detection to determine how to direct a customer conversation through a defined route to reach a desired outcome. The first route is the simplest step by step parameter input while form filling. A second route is through parameter or conditional route evaluation. A third route is a knowledge handler that can source answers to particular questions from internal data. And a fourth route is a no match or no input situation, during a customer conversation. All of these routes would need to be manually constructed for every possible outcome during a customer conversation. This is where generative AI comes in. With the second route for parameter or conditional route evaluation a Generator can be used to create a Fulfillment response, such as content or conversation summarization, sentiment analysis and question answering. The state handler route can be replaced by a Generative AI Agent that uses data stores to search for and return relevant information to the customer. These data stores can be configured for web, structured or unstructured data, for example specific domains, internal documents or FAQs. And with a no match or no input route a Generative Fallback response can be provided to handle a customer conversation that has gone off-line or off-track in a friendly and natural way. Generators and Generative Fallback are both LLM based answers, calling on the natural language capabilities and large amounts of data that the LLM would have been trained on.

#### Generative AI for Dialogflow CX Quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425557

#### How to build better conversational experiences with generative AI

- https://www.cloudskillsboost.google/paths/280/course_templates/892/documents/425558

### Generators

#### Generators on Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425559

The generator feature is a Dialogflow CX feature that allows developers to use Google's latest generative large language models and custom prompts to generate agent responses at runtime. A generator can handle generic responses that involve general knowledge from a large textual dataset it was trained on or context from the conversation. Generators enable you to inject dynamic, LLM-based responses into Dialogflow CX agents. Generators are a powerful tool because they: Use generative large language models during Dialogflow CX fulfillment, Pull context from a conversation, for example, a conversation summary. Add generated, personalized text, or use generated text as input to a webhook, such as a generated email, or to preset parameters. And, chain LLM calls, enabling you to build fully generative virtual agents in Dialogflow CX. Also, Responsible AI is critical, so you’ll be able to control what content is used to ground the bot in factual relevant data, as well as put up guardrails on how the Generative AI Agent responds to users. Generators allow you to call an LLM inline within Dialogflow CX. This can perform a multitude of capabilities like: Article Summarization, Conversation Summarization, Real-time customer sentiment analysis, Updating Structured Data Formats, such as JSON, And, a generator can use Google Cloud's large language models to dynamically make an informed decision based on the context of the conversation and the knowledge base. For example, an LLM can assess the outcome of an eligibility quiz to make a decision on a users progress, and then handle the conversation appropriately. …and many more Now let’s explore some of the use cases that generators can be used to support.

#### Tunable prompt, parameters, and model selection

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425560

Generators can be customized and configured to generate dynamic responses or text that can be used during fulfillment. A text prompt is sent to the generative model during fulfillment at runtime. It should be a clear question or request in order for the model to generate a satisfactory response. You can also select which language model you would like to use for your Generator, such as text-bison or code-bison, and adjust the LLM parameters for variants such as Temperature, Top P and Top K. A prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, examples, and partial input for the model to complete or continue. The text prompts are fully customizable, you can configure the text prompt that is sent to the generative model. For example, in this prompt, you are instructing the LLM to be an expert at Text Summarization. Given an input text, summarize the information in 1 to 2 sentences. Think about the input content and do your best to provide a concise summary of the content for the user. Do not simply repeat the input content. You can make the prompt contextual by marking words as placeholders by adding a $ before the word. You can later associate these generator prompt placeholders with session parameters in fulfillment and they are replaced by the session parameter values during execution. These placeholders usually hold a position in the prompt that will be substituted for user input data at runtime. For example, here we are highlighting the text. We could then replace that with the whole conversation, the last utterance, or any of the other session parameters we might have collected. Here’s an example of linking the prompt placeholder with a session parameter in the fulfillment section. Notice how the session parameters are highlighted in the screenshot on the right. Remember that Parameters are used to capture and reference values that have been supplied by the end-user during a session. Each parameter has a name and an entity type. Unlike raw end-user input, parameters are structured data that can easily be used to perform some logic or generate responses. There are special generator prompt placeholders that do not need to be associated with session parameters. These built-in generator prompt placeholders are: $conversation - The conversation between the agent and the user, excluding the very last user utterance. And, $last-user-utterance -The last text input submitted by the user. If you are familiar with the Vertex AI PaLM 2 for Text using the text-bison model, you may notice that the Dialogflow CX Generator feature is essentially identical and use the same backend. You might notice that the user interface has almost all of the same features: both PaLM 2 for Text and Generators allow you to: define a prompt, choose a model, and adjust LLM attributes. If you were to use either approach for conversation summarization, you would end up with similar results. However, there are some differences, the main difference is that in Dialogflow CX the parameters can be used and are automatically replaced within the tool, while in Vertex AI you would have to parse out and send the complete response. Also, Vertex AI PaLM APIs currently have many more integrated features, such as Max responses, stop sequences, streaming responses, and safety filters. These features may be integrated into Dialogflow CX Generators in the future. This is a high-level overview of how the new generative AI features slot into existing processes to help virtual agents handle specific customer interactions. This diagram shows how Dialogflow CX leverages Vertex AI Foundation models, specifically PaLM2 for Text, text-bison and code-bison, to power the Generators feature.

#### Generator examples

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425561

This example shows how you might write a prompt to summarize content. Here you first provide an instruction for the goal of the generator. Then, you specify the Text prefix, with a prompt placeholder for any given text input. And then describe the format of the desired output, for example: a concise summary of the text in 1 or 2 sentences. And here is an example of how to write a prompt to generate a conversation summary between a human and an AI. Notice how you can explicitly specify the prefixes of the conversation turns in the conversation history, and add the $last-user-utterance because it is not included in $conversation. And, here is an example of what the conversation could be in the resolved prompt, that is sent to the generative model to summarize. The conversation reads: AI: Good day! What can I do for you today? Human: Hi, which models can I use in Dialogflow CX’s generators? AI: You can use all models that Vertex AI provides! Human: Thanks, thats amazing! The generator is tasked to provide a concise summary of this conversation in 1 or 2 sentences. Now, let's explore an example of a prompt for question answering with self-knowledge. In this example you are simply asking the generator to answer any question that a human may ask, politely. First, you can simply rely on the internal knowledge of the generative model to answer the question. However, the model will simply provide an answer based on information that was part of its training data. There is no guarantee that the answer is true or up-to-date. Second, you can add information to the prompt. Often, the information you want the model to base its answer on is too much to simply be pasted into the prompt. In this case you can connect the generator to an information retrieval system like a database or a search engine, to dynamically retrieve the information based on a query. In this example the prompt specifies that the goal is to politely answer questions based on the provided information. If the question cannot be answered given the provided information, then politely decline to answer. You can simply save the output of that system into a parameter and connect it to a placeholder in the prompt. The output request is framed as a Question Answer format, using the last user utterance as input. This example shows how to write a prompt to handle escalation to a human agent. The Generator is instructed to act as a polite customer service agent that handles requests from users to speak with an operator. Based on the $last-user-utterance, the generator should respond to the user appropriately about their request to speak with an operator. And it should always be polite and assure the user that it will do its best to help their situation. The final two instructions in the prompt prevent the model from being too verbose. Do not ask the user any questions. And, do not ask the user if there is anything you can do to help them. And finally, here is an example that shows how to optimize a Google Search query provided by the user. The generator is instructed to act as an expert at Google Search and use “Google Fu” to build concise search terms that provide the highest quality results. A user will provide an example query, and the generator will attempt to optimize this to be the best Google Search query possible. An example of a user query input and a concise Google Search query output is provided, to show what is expected from the interaction. The task allows any user input text to be processed, and asks for the Agent response.

#### Define, configure and deploy Generators

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425562

Now let’s explore how to get started defining, configuring and deploying generators for your virtual agents. In the Dialogflow CX Console, select your Google Cloud project, and the agent. Select Generators in the Manage tab, and click Create new. Now, you can enter a descriptive display name for the generator, and configure the text prompt, model, and controls. Once you have saved your generator, it will become available for selection in the Route settings panel. hen configuring the route for any page in a flow, you can enable Generators in the Fulfillment section, by selecting the generator that you created. After selecting a generator, you need to associate the generator prompt placeholders of the prompt with session parameters. If you have only used built-in placeholders in your generator prompt you will not see the Input parameters section here. You can add several generators in one fulfillment, which would be executed sequentially, one taking the output from the other as input. That way you can chain multiple LLMs, which might make it easier for debugging specific steps. One generator could provide information about destinations and the other could format or summarize that destination output to the user. That way you only have one concise response, but you have 2 separate prompts with clear instructions. That can help the LLM perform better. Notice that you are outputting the response of the Generator in the variable called request.generative.destination. Basically you are giving a Generator a unique output name, which can be used later as input for other generators or for Agent responses. For example, here you see the request.generative.destination being used as the agent response. Additionally, the request generative output parameter could be used like any other Session Parameter in Agent Says, Webhooks, or Preset Parameters.

#### Generators demo

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425563

in this next section we'll talk about a new dialogue feature called generators uh so in your dialogue FX agent you can go to manage and you'll see generators on the Le hand side uh you get the option to create a new generator so basically a generator is a single uh llm that allows you to provide some text prompt for what you want it to do um and then you can apply a generator uh anywhere throughout your agent on a fulfillment uh so for example here we'll we'll create a sample generator and we'll say that you know you're an expert summarization agent uh and based on the current conversation you know provide a summary and one to two sentences uh to the user uh you know do not tell any jokes always be polite Etc and So based on the prompt that you provide your generator uh when you get to a fulfillment inside of your bot then you can uh the llm will follow those particular instructions to do whatever you want it to do so here I have several other generators setup we'll take a look at um for example my operator Handler um so like in my operator Handler generator I'm basically describing that this is a customer service agent that is going to take the last user utterance and then you know uh provide an appropriate response to the user uh so after you have your generators built the way you apply them is you uh select a page and you find a fulfillment that you want to apply the generator on you'll see that we have a new generator section um you select the generator uh based on the ones that you have built and then you'll see the prompt for the generator there and then you have two things that you need to fill out uh one is the input uh so uh based on the input from your your prompt you can put a clear text prompt there or you can use a session parameter uh and you also needed to provide an output and so you can here see here I do request generative do summary and then I'm using that output on this next route so that when the llm is finished summarizing all the information um it will actually just respond with that summary back to um the user so here what I'm going to do is um use my search intent uh and then I'm going to send the query are the Lakers still in the playoffs this is going to the Ser API and then the results come back and they're summarized you can see here so if I look at the details here all of this information comes back from the Google search results that's is actually what's passed to the llm uh and then the llm will follow the instructions to summarize that information and provide this a short snippet of text so generative actions are really great at summarization but they can be used for other tasks like Dynamic responses uh so on my start page here I have an operator intent uh and you can see I've set up a generator called operator Handler now you'll notice uh Below in the agent say section I don't have any hardcoded responses I only have my generator generative do operator response uh so this generator is actually looking at the last user utterance and then based on that utterance it's going to respond appropriately to the user you know whether they're irate or set or whatever uh so if I were to say something like this is crazy I want to speak to a manager now this is actually going to hit my operator Handler generator and you can see that it responds dynamically uh based on what I told the llm to do uh so that's not a hard-coded response it's actually coming directly from the llm so this shows you a couple of different ways that you can use generators for summarization and dynamic responses

#### Code-based responses with Generators

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425564

The Dialogflow CX generators feature also enables you to generate code based on natural language descriptions in agent responses at runtime. Generators are powered by PaLM 2 for Text, with the text-bison language model, and the code-bison code generation model. Depending on the type of answers that you want to be able to support your users with you can configure generators for specific purposes. And it makes sense to use a generative model that was specifically trained to generate code! Code generation models will have been trained on a large corpus of code data to be fully equipped to answer user questions and provide generated code as a response. However, you should always inform users that: This does not replace human is involvement. Code output by a generator should be comprehensively tested before the solutions are used by customers in production. Generated code is not intended or designed to be a replacement for code development. And, you shouldn’t use generated code to implement solutions for sensitive industries, such as cybersecurity and hacking prevention. Generators can answer code-based questions, specifically for: Code generation, Unit test generation, Code fixing, Code optimization, And, Code translation. Code generation models support multiple coding languages, including Go, Google Standard SQL, Java, Javascript, Python, and Typescript. To create a generator for code generation, you simply select the code-bison model when configuring the model for your generator. You may have noticed that the Text prompt is written in a very similar way to how you would write a prompt for conversation. This example shows how to use a generator to write code. As part of the instructions, you first state the goal of the prompt, to write code in a given programming language to solve a problem. Next, you specify the problem to solve by using a placeholder for the problem parameter. And then specify the programming language by using a programming language parameter placeholder. Before asking for the solution from the generator. This way whatever problem and programming language that the user specifies in their natural language description input, the generator will be able to provide an answer in code, that attempts to solve the problem.

#### Generators quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425565

### Generative Fallback

#### Generative Fallback responses

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425566

The generative fallback feature uses LLMs with a custom prompt to respond to users when your chatbot doesn't know the answer. Let's look at an example conversation topic where we can use generative fallback. Think about a user inquiring about their subscription to a service. The dialog could lead to several activities such as identifying the user, updating the tier on their account, getting confirmation that the tier was updated as requested, and then asking the user whether they needed anything else. At every stage in the flow there's a chance that the Virtual Agent will not be able to match the user's intent to an appropriate response. This is where generative fallback responses come in to play. Generative Fallback is a mechanism for handling points in the conversation where the user moves away from the intended flow and doesn't trigger another action or flow. In between key use cases there are a number of somewhat common user requests, like repeating what the agent said in case the user didn't understand, holding the line when the user asks for it, summarizing the conversation, and greet and say goodbye to the user. Even with robust intents, there is still room for error. Users may go off script by saying something unexpected, Or making a mistake while filling in a form. These types of errors are known as No-Match errors. While preventing errors from occurring is better than handling errors after they occur, errors cannot be totally avoided. Generic responses like “Sorry I'm not sure how to help” are often not good enough, and can frustrate users. Error prompts should be inspired by the Cooperative Principle according to which, efficient communication relies on the assumption that there's an undercurrent of cooperation between conversational participants. The Cooperative Principle, is the foundation of conversation design, and can be understood in terms of four rules, called Grice’s Maxims. It says that humans instinctively cooperate in terms of: The truth of what we say, the Maxim of Quality. The quantity of information that we provide, the Maxim of Quantity. The relevance of what we contribute, the Maxim of Relevance. And, the way we strive to communicate clearly, without obscurity or ambiguity, the Maxim of Manner. Research has shown that people respond to technology as they would to another human. This means users rely on their existing mental model of human-to-human conversation, following the Cooperative Principle, even when interacting with the persona of a machine. Furthermore, users expect that machine persona to follow these rules as well. Advances in automatic speech recognition means that the machine persona almost always know exactly what users have said. However, determining what the users meant is still a challenge. Utterances often can't be understood in isolation; they can only be understood in context. For example, your LLM needs to keep track of context in order to understand the user's utterances. In this example, our user is asking for tickets to a concert. The agent asks which concert the user is referring to. Our user then asks “When’s he coming to the city?”, but the agent didn't understand, and instead simply repeats its question to the user. Responses containing pronouns or generic references can quickly derail the conversation without generative fallback. However, with generative fallback, the agent understands the pronoun ‘he’ in the context of the conversation, and is able to progress the conversation along the desired path. Your LLM needs to keep track of context in order to understand follow-up intents. Just like in human-to-human interactions, unless the user changes the subject, you can assume that the thread of conversation continues. Therefore, it's likely that ambiguities in the current utterance can be resolved by referring to previous utterances. Each page in a Dialogflow CX agent flow is composed of elements related to the state of the conversation, such as Entry dialogue, Parameters, Conditions, Routes, and Fulfillment. This is where you tell Dialogflow CX what to say and do, and where to go next. Generative fallback can help you manage the Event handlers element without requiring someone to configure logic to handle each and every scenario that could occur in a customer conversation. Event handlers are meant for scenarios where the user did not give the agent the appropriate information for it to move to the next state. Event handlers address how your virtual agent will behave when anticipated routes aren’t followed. The virtual agent needs ways to course correct so the user isn’t abandoned.

#### Enablement levels

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425567

Generative Fallback, increases intent coverage and helps handle errors gracefully. You can enable generative fallback on no-match event handlers, which can be used in flows, pages, or during parameter filling. When generative fallback is enabled for a no-match event, whenever that event triggers, Dialogflow will attempt to produce a generated response that will be said back to the user. If the response generation is unsuccessful, the regular prescribed agent response will be issued instead. If you enable generative fallback for an entire flow’s no-match events on the Start Page of the flow, it will catch any no-match event that isn't explicitly handled in the flow. You can use generative responses for no-match events for the entire flow by enabling it for the event handlers in the Agent response section of the Start Page of the flow. If your text prompt has been properly configured to use the $flow-description parameter, it will leverage the flow description to generate an appropriate message. Here is an example of a flow description. Note that this description, “Search, find, and book liveaboards”, contains important information about the overall purpose of the Agent. Keep this in mind when designing your own description of flows. If you enable generative fallback at page level and the user input doesn't match any active intents in scope for the page, then Dialogflow will trigger generative fallback. You can use generative responses for no-match events for specific pages, by enabling it in the Agent response section of the event handler for a particular page. If your text prompt has been properly configured to use $route-descriptions, it will leverage the page active intents descriptions to generate an appropriate message. Here is an example of an intent description. The description reads: Assist users with group or full charter reservations. Initially collect travel details including departure period, destination, number of guests (min 4 max 15 people), contact details. The destination must be one of the following in the Pacific: Costa Rica, Mexico, Galapagos islands. Note that the description contains important information to complete the form, such as the available destinations, the minimum and maximum number of passengers allowed on a boat, the departure period and contact details. Keep this in mind when designing your own description of intents. And, If you enable generative fallback at parameter-level, you are specifically handling a no-match raised by a violation of the entity type of the parameter. When configuring an Entity type you can specify reference values and synonyms, or a numeric value range, that can be used to determine intents and ensure the customer can progress through to an endpoint by providing the correct information. In this example a regular expression is used to define the range for the parameter: number-of-guests. The regular expression evaluates to a number between 4 and 15. If the user enters an invalid number when making a booking, the generative fallback will be able to identify this, and then prompt the user to input a number in the correct range. You can enable generative fallback to handle invalid inputs when the agent asks for the number of guests: First, open the page that contains the form parameters, in this example the number-of-guests parameter. Scroll down to the Reprompt event handlers section, then select the relevant No-match event handler. Within the event handler settings, you can enable generative fallback in the Agent responses.

#### Configure a text prompt

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425568

The generative fallback feature passes a request to a large language model to produce the generated response. The request takes the form of a text prompt that is a mix of natural language and information about the current state of the agent and of the conversation. You've seen that the feature can be configured for greeting and saying goodbye to the user, repeating what the agent said in case the user didn't understand, holding the line when the user asks for it, and summarizing the conversation. However, to match your tone, make sure you give context to your agent. In the ML tab of Agent Settings, select the Generative AI sub-tab, you can then either select a predefined prompt template, or define a custom prompt. You can select one of the predefined prompts by clicking the Template dropdown, Initially you will have only two choices: Default and Example. Later, as you build more templates, they will appear in this list. You can define your own prompt, by selecting the “new template” option in the Template dropdown. Or you can select an existing template. The Default template is not directly modifiable, but you can influence the agent’s responses by adding details to the Data store prompt, which is also found on this page. The Example template is modifiable and can serve as a guide for writing your own prompts. Let's walkthrough editing the Example prompt. Clicking edit opens the new/edit template window. Here you can enter a Template name, and add a Text prompt. In the Text Prompt you can add context to the generative fallback. This is important because it helps you set the tone of voice. For example, a customer support application generally doesn't use the same tone of voice as you would hear in a game for kids. In this example, the text prompt instructs the generative model to be a friendly AI assistant that helps humans with a certain task. The prompt specifies that the generative model should assist with particular aspect of this task. The conversation between the AI Agent and human is included in the prompt. Notice how you can explicitly specify the prefixes of the conversation turns in the conversation history, and add the $last-user-utterance because it is not included in $conversation. In addition to the natural language prompt to provide instructions and context to the Agent, the following placeholders have been used: $conversation - is the conversation between the agent and the user, excluding the very last user utterance. $last-user-utterance - is the last text input submitted by the user. $flow-description -is the flow description, in the Start page of the active flow. And, $route-descriptions - is the intent descriptions of the active intents. So make sure to have good flow and intent descriptions when calling these parameters. Let’s explore an example of the type of information that the placeholders bring into the text prompt. In this example, a custom text prompt has been configured in Dialogflow CX with all four placeholders used throughout. It reads ‘You are a friendly agent that likes helping traveling divers. You are under development and you can only help $flow-description placeholder At the moment you can't help customers with land-based diving and courses. You cannot recommend local dive shops and diving resorts. Currently you can $route-descriptions placeholder. The conversation between the human and you so far was: ${conversation placeholder USER:”Human:” AGENT:”AI”} Then the human asked: $last-user-utterance placeholder And here is the same text prompt with the placeholders replaced with the text from the various relevant sources. The $flow-description placeholder, has extracted text from the Flow description on the start page and now reads ‘You are under development and you can only help search, find and book liveaboards.’ The $route-description placeholder has extracted text from the Intent description on the associated page-level and now reads ‘Currently you can assist users who are looking for a group reservation or a full charter. Initially collect travel details including departure period, destination, number of guests (min 4 max 15 people), contact details. The destination must be one of the following in the Pacific: Costa Rica, Mexico, Galapagos Islands.’ The $conversation placeholder has extracted the conversation history, excluding the last user utterance, and has been formatted as instructed, and now reads: ‘Human: Hi, my name's Alessia AI: Hi Alessia, what can I help you with today? Human: Can you help me find a nice boat for myself and my family? AI: To assist you with that I need to collect the details of your travel and then we'll get back to you with an offer shortly. Where would you like to go? We can organize a charter in Costa Rica, Galapagos Islands and several locations around Mexico.’ And, the $last-user-utterance has extracted the last message submitted by the user, and now reads ‘The kids want to go to the Maldives’. All of this information pulled into the text prompt ensures that the generative fallback feature is equipped with the full context required to provide the ongoing necessary responses in the customer conversation. Finally, the LLM falls back on the Flow and Page level descriptions to generate an answer to the customer, the output reads ‘I'm sorry Alessia, we can only help you with liveaboards in Costa Rica, Galapagos Islands and several locations around Mexico.’

#### Handling customer interactions

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425569

The first location in a Dialogflow CX solution where generative fallback can be enabled is at the flow level. Flow level Generative Fallbacks enable you to have a more natural conversation before you enter into specific well defined tasks. For instance, here you are asking the chatbot a generic question, “how are you doing” and the chatbot uses a generative response to say it’s doing great, and then refocuses the conversation on diving trips. It’s possible to say the right thing thanks to the Flow Description, which is included in the prompt to the LLM to set up some context. Another example is a generic question such as “how can you help me?”. Since everyone is used to interfaces where you choose from a range of options, you might have users that are not sure how to formulate a request. Here the chatbot response with a generative answer explaining its purpose, which includes help with searching, finding and booking liveaboards. Finally, it can also serve for explaining terms without leaving the conversation. In this case, you are asking a generic question, “what’s a liveaboard?”, which the LLM can answer using its pre-trained data. It then provides an answer explaining that a liveaboard is a boat used for diving trips. ext you have page level generative fallbacks. Here the idea is that you will have specific intents, where the chatbot will be able to help with specific use cases. For instance, you have an intent for booking a trip, which will direct us to the Collect further info page. This route has a description explaining that the chatbot should assist users and collect information about the trip including departure period, destination, number of guests and contact details. So if you send a message that is not merely I want to book a trip, or here are my details. But rather, you ask for information about the trip itself or clarifications about it, the chatbot will use the Route description as context in the prompt together with the original question to query the LLM about a response. In this case, it generates a sentence explaining that diving trips can accommodate groups of 4 to 15 people. Next down the line are parameter-level generative fallbacks. Those take over when you send an invalid response. For example, here you answered that you wanted to travel to the Canary Islands. However, that destination is not in the list of available destinations. So the LLM guides you with a custom answer explaining the locations that are available. One you are done with the form filling or if you ask other questions outside of the specified intents, the no-match event handler in the page will be invoked. In this case, since you have Generative Fallbacks enabled at the page level, where you configured a prompt to include the conversation as context. That way, when you ask to summarize the booking, the LLM will have the context of your conversation It will be able to give you an overview of the booking you created. You can also add use case or customer specific safety checks, by adding a list of banned phrases for your generative fallback response. If you were to add the words, dangerous, high crime-rate, narcotics, to the banned phrases list and save the settings, you would now have some words and phrases that would cause the generative fallback to fail. In this example the end-user has used to term dangerous as part of their query, and the the generative fallback has not picked up the response. If the end-user input includes banned phrases, or phrases deemed unsafe, generation will be unsuccessful, and the regular prescribed response, under Agent says in the same fulfillment, will be issued instead. Generative Fallback is an awesome feature, however it doesn’t solve all your problems. The best practice is still to prevent a no-match event from being triggered by providing good, robust, and varied training phrases to your intents, to increase intent matching accuracy and keep your conversations on an intended route. This is a high-level overview of how the new generative AI features slot into existing processes to help virtual agents handle specific customer interactions. This diagram shows how Dialogflow CX leverages Vertex AI Foundation models, specifically PaLM2 for Text, text-bison, to power the Generative Fallback feature.

#### Apply built-in hacks of conversation to your Voice UI

- https://www.cloudskillsboost.google/paths/280/course_templates/892/documents/425570

#### Generative Fallback quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425571

### Generative AI for Virtual Agents

#### Generative AI Agents and hybrid agents

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425572

Next, let’s explore how Generative AI Agents leverage the best of Dialogflow CX and generative AI, so that you can choose your path when implementing your Virtual Agent solution. Vertex AI Search and Conversation provides the backend technology for Dialogflow CX to build Generative AI Agents. There are two components, data stores and Generative AI Agents. A data store is where your company data will be parsed and indexed. Data can be ingested from website links, documents or FAQs, and Bigquery. The generative agent, which is powered by a Large Language Model, queries the data inside those data stores. This is accomplished by creating special state handlers called data store handlers. Using these data store handlers, your agent can have conversations about the content with your end-users. Here are some examples of the types of Generative AI Agent that you can create: Website agent: an agent that answers questions based on your website contents, FAQ agent: an agent that answers frequently asked questions, And, Employee-facing agent: for answering help and support questions using internal helpdesk or HR support documents. In addition to handling questions about the content you provide, the agent can handle special intents, such as: Agent identification: questions like “Who are you?” or “Are you human?”. And escalating to a human agent: questions like “I want to talk to a human” or “I want to talk to a real person”. This is accomplished by automatically generated intents and intent routes. If you have an existing Dialogflow CX agent, you can upgrade this agent to a hybrid agent, which combines the power of precise conversation controls, such as flows, parameters, intents, conditions, and transitions, with data store handlers for generative features. As part of this upgrade, you may wish to delete or temporarily disable intent routes, while testing data store handlers, for certain conversation scenarios from your agent, because the data store handlers can handle those scenarios more simply. When thinking about adding new generative AI capabilities to your Virtual Agents in Dialogflow CX, it’s important to remember that generative AI is not a solution to everything. Your approach should rather be to keep what is working well in Dialogflow CX, and incrementally add capabilities, or build new use cases. With Dialogflow CX you can have the best of both worlds, you can implement Hybrid Agents, which are a “mix-and-match” design option enabling you to use the appropriate approach for each user journey. It’s essential to understand when to use intent routes and when to use generative AI. Some questions require more deterministic agent responses, or actions that must be taken by the agent. You can use the traditional intent-based routing for when the content may not have the answer, or questions that require database lookups or server requests. Intent-based routing is also the best choice for scenarios that require data redaction. For use cases such as questions that can be answered by your organization's documents or website, or FAQs that do not require database lookups, you can use Generative AI Agents and data stores. So, why add Generative AI Agent features to your Dialogflow CX agent? Well, Generative AI Agent enables you to extend coverage at low cost with answers from your content. For example, Generative AI Agent can handle questions that can be answered from organizational knowledge documents or website content, for which you have not had time or budget to build intents. And as a fallback in case existing intents cannot answer a user query. Generative AI Agent also enables you to replace certain intents with generative answers to increase quality and reduce maintenance. For example, you can replace existing intents that could find a better response through organizational knowledge documents or website content. And replace intent route groups that were solely built to answer FAQs, such as route groups you may have built to answer FAQs that exist on your website. And, you can leverage your existing knowledge base and make it available through a conversational interface. Dialogflow evaluates end-user input in the following order for hybrid agents: First, parameter input while form filling, Next, Intent matches for routes in scope, Then, the data store handler with FAQ data store content, And finally, the data store handler with unstructured data store content.

#### Intent-driven and Generative AI use cases

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425573

The new Generative AI with LLM approach, can now address use cases that were too costly or too complex, in a cost effective way. Both Intent-driven and Generative use cases are useful and valuable. You should always review the requirements of your specific use cases to determine the appropriate option. This table shows that you would use intent-driven flows when you require fully deterministic responses, an auditable step-by-step process, transactional requests, or strict data security and compliance. This is implemented through manual intents, training phrases, routes, and conversation transitions, to support specific, high volume intents with very specific business logic. Intent-driven flows obviously require a longer build time. Whereas, you would use generative AI with LLMs when you want to allow some variance in responses to offer dynamic, broad question coverage, with lower data security and compliance needs. This is achieved by allowing the bot to scan content or by describing a bot task, and the AI model designs the conversation flow. Generative AI with LLMs work with broad unstructured data, such as websites and documents, to enable dynamic flows and extend intent bots. Google Cloud’s approach allows the customer to design the appropriate user journeys, using both Intent-based flows and Generative AI in the same bot design, all within Dialogflow CX. As previously mentioned, Intent-based use cases are best for fully deterministic responses, auditable step-by-step processes, or existing bots delivering high containment. Whereas, Generative AI use cases are best for extracting answers from unstructured data and chaining LLM calls to build fully generative virtual agent flows. With Hybrid Agents, you can use a mix of intents-based flows and generative AI for certain use cases, such as adding question coverage from unstructured data to an intent-based bot, adding generative fallback coverage when intent-based flows hit a no-match outcome or an invalid parameter value is provided, and adding summarization or generated personal responses to intent-based flows. So how does Dialogflow leverage Vertex AI Search and Conversation data stores? You have your Dialogflow CX Agent configured for your business needs. And, you can create and manage your own data stores in Vertex AI Search and Conversation, covering website domains, unstructured documents, and structured FAQs. A Virtual Agent uses intent detection to route customer questions to the relevant information. Most Intents for your business use cases will require definition and specification based on matching intents to customer queries. However, the state handler route can be replaced by a Generative AI Agent that uses data stores to search for and return relevant information to the customer. Other intent routes, such as conditional route evaluation, or no match events can be supported by LLM based generative responses during fulfillment. This is a high-level overview of how the new generative AI features slot into existing processes to help virtual agents handle specific customer interactions. Customer input to Dialogflow Agents can be resolved with a query to Vertex AI Search and Conversation for non-deterministic responses. For example, if a customer uses an enterprise chat app, and asks a natural language question relating to the companies operational manuals for a product. A Dialogflow Agent will direct this question to Vertex AI Search and Conversation, to access the relevant data store to answer the question. Vertex AI Large Language Models are leveraged to generate the answers, providing the results of the search, snippets and summarizations of the relevant documents or information, and enables multi-turn conversation with the customer, in order to facilitate further support along this line of enquiry. The information is returned as a natural language response to the customer, powered by the PaLM 2 for Text foundation model and text-bison language model. The customer continues their conversation comfortably through the chat app, unaware of what is going on behind the scenes.

#### Creating a Generative AI Agent for use in Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425574

Now, let’s explore how you create and configure a Generative AI Agent for use in Dialogflow CX. You can create a Hybrid Agent for Dialogflow CX in two ways: From the Create a New Agent window, you can select either Auto-generate or Build your own. Auto-generate takes you to Vertex AI Search and Conversation, where you can create a Generative AI Agent and data store, to use in Dialogflow CX, to answer domain and document focused customer questions. And Build your own, takes you to Dialogflow CX to create a virtual agent, but providing you the ability to take control over the experience and leverage powerful AI tools, such as Generators, and Generative Fallbacks, through a simple user interface. Let’s start with the Auto-generate route for creating a Generative AI Agent. The first step is to create a chat application in Vertex AI Search and Conversation. You also need to provide a company name, time zone and language, as well as define an agent, it’s name and the location for your agent. You can now add existing data stores to your Generative AI Agent, Or create a new data store If you create a new data store for your application, select the type of data your application will index; either website, unstructured or structured data. Later in the module, you’ll explore how to ingest data into your data store, in more detail. Finally, you can connect your data stores to an existing agent to source of answers from specific intents. Now, let’s explore the Build-your own route. To add Generative AI Agent features to an existing agent, you first need to enable the Data Stores state handler. Start by selecting your flow from the FLOWS section of the Build tab, Then, select a page of the flow, to open the configuration window. Then click Add state handler. Finally, click the checkbox next to Data stores, to enable the knowledge handler. Then click Apply. Once the Data stores have been enabled, you can edit which data stores will be used for each page in the conversation flow. On the properties of a page, click Edit data stores. A menu will appear on the right side of your screen showing a list of the currently configured data stores. Here you can add three types of data store; a website, unstructured documents and FAQ documents, For each field you can either locate an existing data store that you have set up with your own data previously, or you can create a new data store, If you create a new data store, you will be directed to Vertex AI Search and Conversation, where you will need to specify the type of app you want to create, and provide a name for the data store. Once your data store has propagated through to your Dialogflow CX Agent, you will be able to leverage the generative AI capabilities associated with Vertex AI Search and Conversation. You can also move conveniently from Dialogflow CX to Vertex AI Conversation by clicking on the Data stores link under the Manage tab in Dialoflow CX. You can use the output of the Vertex AI Conversation request to the data store and combine it with some additional text in the Agent says text box. In the Agent responses section, you can provide custom responses that reference generative answers. Use $request.knowledge.questions[0] in the Agent says section to provide the generative answer. Each response generated from the content of your connected data stores is given a score of how likely it is to be grounded and, as a correlation, accurate. You can customize which types of scores to allow. If a response comes back with a score you have not allowed, it will not be shown. That way you can customize the range of answers you provide to a user based on context. Depending on the service you are building, you might want to provide answer that are strictly based on your data. For example, if you are providing legal advice, you probably want to be as accurate as possible. However, if the service is more informational, such as a travel companion providing data from locations, you might want to allow more freely created responses. Providing data store prompt information can improve the quality of answers generated from data store content and make them feel more your brand. The same way as when using generators, you can give a tone of voice and an identity to your Generative AI Virtual Agent. Here’s an example of a data store prompt: “Your name is the ACME Virtual Assistant, and you are a helpful and polite AI Assistant at ACME Co., a fictional e-commerce site. Your task is to assist humans on the company website.” This feature currently supports a range of languages, including: Danish, Dutch, English, French, German, Hindi, Italian, Brazilian Portuguese, Spanish, and, Swedish. Only the global region is supported and customer managed encryption keys is not supported.

#### Creating a data store

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425575

welcome back to the course vertex AI search this is the managing data in vertex AI search in conversation module in this module you'll explore creating an app engine ingesting data into a data store and managing data in a data store let's start by outlining how to create a data store one of the greatest features of vertex AI search and conversation is its ease of use for developers you can index and search a publicly accessible website and you can create a data store based on certain data types such as structured data that is organized in a specific format and follows a schema such as bigquery Json a provider directory and product catalogs or you can use unstructured data which is data that doesn't follow a schema it may be stored in the form of HTML PDFs with embedded text plain text files slides or other documents let's go through the process of setting up a basic search engine for the supported data types first choose the type of data that you want to import whether it is website data structured or unstructured data any data needs to reside in bigquery or cloud storage or it needs to be a publicly accessible website second prepare the data to be ingested data located in cloud storage can either be structured or unstructured structured data must be in ND Json or Json lines format unstructured data can be ingested automatically or you can add metadata the metadata can be stored in a file in cloud storage or as a table in bigquery the goal of the metadata is to describe the contents of the data and therefore get better search results data located in bigquery is structured and can be imported into your data store without any further modifications for public websites vertex AI search and conversation utilizes the existing indexed information for Google Search so you'll notice that an engine built on external sites is immediately searchable third when importing structured data you can choose to Auto detect the schema provide a custom schema or a combination of both options Auto detecting a schema is the recommended method for your first engine because it's the most straightforward however it will provide the worst quality results to improve your search results you can choose to edit the schema once it's been Auto detected however your data will have to be re-indexed which will take a long time to speed up the process you can ingest a small but representative subset of your data review the schema suggested by Auto detect provide any edits required and then ingest the rest of your data finally providing the schema as a Json object is the fastest and most efficient option the downside of this approach is that you have to create the Json schema file manually without cloud console support if you choose to provide a Json schema file or edit the auto detected schema you should focus on the following fields the key property mapping Maps predefined keywords to critical fields in your documents helping to clarify their semantic meaning for example you have the title description URI and category all properties have a primitive type which can be Boolean object array number string or an integer additionally Fields have options that Define the accessibility of using the datastore indexable defines whether a field can be filtered faceted boosted or sorted in the servings config.search method you'll explore these options in more detail in an upcoming module of this course searchable defines whether a field can be reverse indexed to match unstructured text queries which can provide more accurate search results if the field is relevant retrievable indicates whether a field can be returned in a search response Dynamic facetable defines whether it can be used to present information organized in suction headers and finally completable indicates whether the field can be directly used as suggestions fields in Auto detected schemas are indexable searchable and retrievable by default as long as they follow the type requirements and stay within the maximum limits here's an example of an auto detected schema in the vertex AI search and conversation console finally import data as documents into a data store create an engine of type search or conversation and link to the data stores data stores can be shared across engines so that the same data can power a search and a conversation solution also an engine can have multiple data stores that is specifically useful if you want to query data of different types

#### Ingesting data into a data store

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425576

in this lesson you'll learn how to ingest data into a data store the process to ingest website data into a data store is currently only available by using the Google Cloud console you can enter the URLs of the websites to include in your data store feel free to include any website you want not just the websites that you own you can also exclude any websites that you don't want to be searched at the moment only public websites can be ingested for private sites create an unstructured data store and upload the website's HTML files you can use the Google Cloud console or code to create a data store and ingest structured or unstructured data from a cloud storage bucket or bigquery here's an example written in Python about how to import documents both from cloud storage and bigquery notice that you can do either a full or incremental import of the data on both cloud storage and bigquery a full import is a replacement of the data in the data store and an incremental import is adding additional data it's important to be aware that there are some limitations in the vertex AI search in conversation data stores in terms of support website search only supports public websites for unstructured search we can import at most 100 000 files each time with a maximum of 40 million files and a single file size cannot exceed 100 megabytes for structured search we can import at most 100 files each time and a single file size cannot exceed two gigabytes the ingestion throughput is around 200 rows per second so for 100 million rows it would take about 1.4 hours user events can help you improve your output sending user events can help improve the quality of your recommendations as well as the ranking of your search results search results with higher click-through rates are boosted while results with lower click-through rates are buried you can send real-time events using the API or using a JavaScript pixel Additionally you can import historical events from cloud storage bigquery or inline you can record real-time user events using the vertex AI search and conversation sdks or JavaScript pixel this example shows you a python code snippet that performs the recording as you see there are a few straightforward instructions to import the vertex AI search in conversation libraries create a client initialize the request and make the request to the vertex AI backend you can also import user event data from past events in bulk this can help improve the quality of your recommendations as well as the ranking of your search results this example shows how to import historical user events using the discovery engine API

#### Managing data in a data store

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425577

in this lesson you'll learn how to manage data in a data store these are the stages in the life cycle of a data store first you start by creating a data store depending on the type of data store you choose you might start by creating a schema for the data to be processed correctly when it's imported or you might choose to import the data and optionally update the schema afterwards furthermore you can always refresh data by either adding incremental data or redoing an import of the data in case you change the origin of the data you might have to delete the existing Data before doing a new import finally deleting an existing schema or a full data store will require you to delete the data first here's a python example that shows how to update your schema there are some requirements and limitations when updating schemas the new schema should be backward compatible with the schema you are updating non-backward compatible schemas need to be deleted and recreated and some changes are not supported including changing a field type for example a field map to an integer cannot be changed to a string and removing a field you can continue adding new Fields but you cannot remove an existing field here's an example in Python showing how to purge documents the same is also available from the UI you can Purge a data store when you want to completely delete its contents and re-adjust fresh data purging deletes only the documents from the data store however it leaves your engine schema and configurations intact and you can Purge data from a structured data store or an unstructured data store there is no option to purge data from a website data store

#### Generative AI for virtual agents quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425578

### Vertex AI Search and Conversation Architecture and Security

#### Vertex AI Search and Conversation architecture

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425579

let's start by exploring the vertex AI search and conversation architecture vertex AI search and conversation provides the same security controls as many other Google Cloud perimeter products such as big query customer data is always stored with encryption using customer managed encryption Keys when provided also in the very unlikely event that a member of the Google operations team must access customer data know that all such access is logged and is traceable by using access trans transparency data residency and VPC security controls continue to be respected when considering the security controls of the llm specifically it's important to be aware that llms are stateless meaning the weights are frozen and cannot be reconfigured and they do not store customer data including customer specific parameters you can also use alternative cloud data with our llms but the customer will need to process the data into the J B on format that they need on the alternative Cloud before moving it over to cloud storage additionally it's important to remember that data in Flight is always encrypted and your data is not used to benefit another customer vertex AI search and conversation provides the security expected from Google Cloud perimeter to enable the best of AI so whether you're deploying a large language model or running a batch prediction job on structured data Google Cloud's entire stack is optimized for AI workloads in three main ways first is scale and speed Google's AI infrastructure makes it fast and costeffective for organizations to scale their end-to-end machine learning workloads from data preparation and model training to feature engineering and deployment second is price performance and efficiency Google is a leader in Hardware optimized for running large models and has created the tensor Processing Unit or TPU which is a highly specialized processing unit for training large models with Google Cloud you can choose from gpus through the partnership with Nvidia Google's own tpus or CPUs to support a variety of use cases including high performance training lowcost inferencing and large scale data processing third is sustainability it's important to have a low carbon footprint when training your models and Google's TPU infrastructure runs on 90% clean energy and is the most sustainable Cloud option for Enterprise customers

#### Authentication to Vertex AI Search and Conversation apps

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425580

in this lesson you'll learn how to gain programmatic access to vertex AI search in conversation and authenticate your application vertex AI search and conversation client libraries are available for many of the most popular programming languages supported languages include python nodejs Java go PHP Ruby and the.net languages including c-sharp if your application uses any of these languages check out the corresponding client library now let's examine an example of using the python client library to perform a search in search engine first you import the package for the Google Cloud client library for vertex AI search next create a client connection to the search engine and configure the client this must include the full resource name of the search engine serving config once the client has been configured you're ready to perform a search and send the results back the client libraries let you easily manage your vertex AI search and conversation Resources by using the natural style of the programming language you've chosen you can use a service account to authenticate your application to do this you'll need to First create the servers account by using the Google Cloud console then you generate and download your credentials file next you can set an environment variable with the path to your downloaded credentials the example shows commands to set the environment variable in both Linux or OS X and windows finally you can make an authenticated API request in your code the example code is in Python and shows that if you don't explicitly specify the credentials when constructing a client the client Library will look for credentials in the environment

#### Access control with IAM

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425581

in this lesson you'll explore the types of Access Control with identity and access management or IM it's extremely important to Grant the right level of access and permissions to the right people when it comes to your vertex AI search and conversation resources to do this you'll need to understand the levels of access that are available within IM and the types of roles and responsibilities they might apply to IM provides three main levels of access for vertex AI search in conversation discovery engine viewer discovery engine editor and discovery engine admin these predefined IM roles can be used to provide granular access to vertex AI search and conversation resources the discovery engine viewer role provides read-only access to all vertex AI search in conversation resources for example you might assign this role to Auditors of the infrastructure the discovery engine editor role can read all vertex AI search and conversation resources and write products events and other resources this role is used for importing updating and deleting documents in the search engine creating updating and tuning the model as well as creating user events and updating widgets the discovery engine admin role has full control over all vertex AI search and conversation resources those permissions include the management of data stores controls serving configurations and Target sites

#### Data governance and compliance

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425582

in this lesson you'll learn more about data governance and compliance with Google Cloud Google Cloud products Empower Enterprises to embrace generative AI with the confidence that there is data governance built in this is especially important and highly regulated Industries like financial services and Healthcare where you need to guarantee that no one can access confidential or sensitive data you also want to be certain that none of your data or your customers data is being used for any other purpose than determined by you so let's outline Google Cloud's approach to governance of customer data for cloud llms and generative AI Google cloud is always committed to transparency compliance with regulations like gdpr and HIPAA and privacy best practices by default Google Cloud does not use customer data to train llms in accordance with the Google Cloud terms and the cloud data processing addendum and Google Cloud will obtain customer permission before using any customer data to train llms

#### Vertex AI Search and Conversation architecture and security quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425583

### Generative Playbooks

#### Introduction to Generative Playbooks on Dialogflow CX

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425584

Generative Playbooks is a DialogFlow feature that allows developers to build chat experiences with natural language by leveraging LLMs within DialogFlow CX. Rather than defining flows, pages, intents, and transitions. You provide natural language instructions and structured data in the form of playbooks. Generative Playbooks enable you to define end to end workflows for your DialogFlow CX Agents. Generative Playbooks are a powerful tool because they leverage LLMs for instruction driven flow development. Enable LLMs to access APIs and tools can therefore retrieve information and submit transactions and simplify development and maintenance. The platform features tooling to make building and maintenance easier. You can develop production grade high quality bots out of a prototype. Monitor production traffic and improve the bot over time. There's a variety of use cases that are ten times easier to implement with Generative Playbooks compared to traditional state machine and intent based development frameworks. But you can still combine DialogFlow flows with Generative Playbooks for more complex implementations that require both generative capabilities and strict business rules that must be followed precisely. Here are some examples of what you can build with Generative Playbooks. Customer Service Bots. To answer customer questions, troubleshoot issues and provide information. Sales and marketing bots to generate leads, qualify prospects and answer questions. Productivity bots to schedule appointments. , create tasks, and find information. Education and training bots to assess a student's level, answer questions, and give feedback. And research bots to collect data, conduct surveys, and analyze data. Generative Playbooks implement the ReAct pattern: reasoning and action. This prompt engineering technique originated at Google, and Generative Playbooks are a first class, first party implementation of the technique. The work of encapsulating the technique has been done so the user doesn't have to write ReAct prompt engineering themselves. ReAct combines reasoning and acting advances to enable LLMs to solve various language reasoning and decision making tasks. The ReAct pattern systematically outperforms reasoning-only and acting-only models when prompting bigger language models and fine tuning smaller language models. ReAct enables LLMs to generate both verbal reasoning traces and text actions in an interleaved manner, while actions lead to observation feedback from an external environment. Reasoning traces do not affect the external environment. Instead, they affect the internal state of the model by reasoning over the context and updating it with useful information to support future reasoning and acting. After a prompt, the LLM responds with a thought and an action which is used to generate an observation. The thoughts are the reasoning about how to act. The actions are used to formulate calls to an external system which can be Cloud Functions or external APIs, and the observations are the response from the external system. Through these interleaved thoughts, actions, and observations, the LLM eventually arrives at an answer. This pattern of thought, to action, to observation is repeated, and is called the core ReAct loop or ReAct cycle. The ReAct cycle is repeated until the LLM arrives at an answer. Let's walk through the ReAct cycle visually. So, you have an LLM. You prompt the LLM by asking it to answer a question. For example, “explain the features of a Google Pixel 8 phone to me like I'm a five year old”. The LLM then responds with a thought and an action. In our case, it will think about whether it possesses the information about the Google Pixel 8, and since it doesn't, it will trigger an action to search for that information elsewhere. The action is then used to call an external system such as an API or a Cloud Function. That external system could contain a database of phones with their technical features. The LLM is then prompted again with the response from the external system as an observation. The LLM will then formulate a thought again. In our example, the LLM now has the technical features, but they're not in a state they would be understandable for a five year old. This cycle will then repeat. The LLM will formulate an action in addition to the thought. Another external call will be made and then the LLM will be prompted again with an observation. In our example, the LLM acts by writing text that is adequate for a five year old with the technical specifications provided. Eventually, the LLM will have a thought that constitutes an answer and will no longer formulate an action in DialogFlow CX. The LLM that we see in the middle of the diagram is implemented by Generative Playbooks. To connect to an external system, you need to define a Generative Playbooks tool. The Generative Playbook tool is defined using the OpenAPI format. In it you define the structure of the external system's API endpoint. For example, the external system might be implemented by a Cloud Function or Cloud Run, which accesses a database such as FireStore to look up information.

#### Creating a Generative Playbook

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425585

You can create a DialogFlow agent as usual, either by creating an auto generated generative agent or by building your own. When creating an agent, you can either start with a default flow or select the generative agent type to begin with a flow that automatically uses Generative Playbooks. After you create a generative agent. You'll go straight into the new playbooks view. A playbook is a set of natural language instructions that the virtual agent will carry out. You'll begin by creating a list of step by step execution instructions to accomplish your target goal. The help text tells you how to format the text of your instructions. Note that even though these steps are an ordered list, when you enter the text into the steps text box, you must use the markdown syntax for an unordered list, meaning each entry begins with a dash. You can nest steps within other steps, and use the dollar sign followed by a tool name in order to make external calls or perform actions. For example, the $PLAYBOOK parameter allows you to refer to another playbook by name. This is an example of a call routing playbook that may serve as the front door for your users. You'll notice that the steps include tool references using the $PLAYBOOK parameter. These refer to two other playbooks called Renew_License and Book_Appointment. After you have created your playbook, it will be added to the list of playbooks in your project. You can choose different playbooks to start with in the same way you would conventionally choose a flow to start. And as playbooks can be versioned, you can choose a version of a playbook to start with. Let's look at another example. This is the Renew_License Playbook. It's the analog of a traditional DialogFlow CX flow where a particular group of tasks happen. In this case, renewing a driver's license. If we take a closer look at the steps of this playbook, you may notice that the $TOOL variable is used in order to invoke an external tool. This is another aspect of a playbook that implements the ReAct (Reason and Act) prompting pattern. The renew license playbook says: “Ask the customer to provide their most recent driver's license number and expiry date and verify them using the $TOOL: drivers_license tool. If the API responds with an error, ask the customer to clarify. ” “Ask the customer to provide their address and send them using the driver's license tool. ” “Check if the customer is eligible to renew their driver's license online using the driver's license tool. If true, send the customer's application. If the customer is not eligible for online renewal, tell the customer that the application is being submitted and further information will be mailed to their address.” You can also add examples. This is an example for the Renew License playbook. Examples contribute to the LLM’s understanding of the conversation by providing in-context learning, also called few-shot prompting. This allows you to define what a good conversation should look like and to specify the input and output for your tools. Playbooks can also have input and output parameters. Input parameters contain values passed into a playbook from a flow. Output parameters contain values sent back to a flow from a playbook. First, you define the parameters, then define an example using the parameters. The Moniker key relates to the flows parameter named moniker. And the poem_state key and done value is what's sent back to the flow with the output. As you can imagine, tools are one of the most important concepts in Generative Playbooks. Tools represent a collection of functions that are available to the playbook and the dialog manager. You may hear the term tool use in literature. This refers to advanced prompting techniques like Chain of Thought or ReAct in order to provide external context to an algorithm. In other words, tools allow the conversational agent to combine external data and LLM information. You can think about a tool as a collection of functions grouped by topic that work together to solve a particular customer problem. In the context of a playbook, a tool is defined by a schema in the OpenAPI 3.0 format. OpenAPI is a common standard for defining an HTTP ReST endpoint and can be an either YAML or JSON format. The example here is shown in YAML format. After you define a tool in DialogFlow CX, you can use it in your steps to access external data. The way you do it is by specifying the $TOOL: “tool name” reference. The example here shows an OpenAPI definition for renewing a driver's license. The server is the backend that you are accessing and you may notice that it is hosted in Cloud Run. There is a POST request to the verify last driver's license endpoint, including all the parameters for the request. The structure of the body as well as the expected response. You can test a playbook in the DialogFlow simulator jJust like a conventional flow. Conversation history shows when a Generative Playbook has been invoked and when a tool has been used.

#### Generative Playbooks - limits and tips

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425586

Generative Playbooks do have some limitations, such as only being able to make five calls per conversation turn, a limit of 15,000 input tokens, and 500 output tokens summed across a conversation turn. There are also limitations when it comes to HTTP API calls. For example, except for the session ID, query parameters are not supported. The body of requests and responses must be empty or in JSON format. Advanced schema features like “oneOf” and the “multipleOf” data type are not supported. And parameter validation has to be handled with web hooks. Generative Playbook tool schemas also have some limitations, such as currently only the application JSON format is supported. Plain text and other formats are not supported. A summary field is required in all API paths. Currently, only the specification of one path is supported and nesting using keywords like “allOf” or “anyOf” is not supported. Prompt engineering is a new and very sensitive virtual agent development technique. Generative agents and flows are still in the very early stages of developing best practices, so it's important to remember that LLMs don't always perform in the most predictable way, and the current model performance is far from perfect. It is important, and a requirement, to write clear and descriptive instructional steps. However, the quality and quantity of your training examples will determine the accuracy of the agent's behavior. Generally speaking, if the agent is not responding or behaving in the manner you expect or want, it's almost certainly due to inadequate or missing training examples for that particular path or permutation of your dialog with the agent. You cannot solely depend on instructions to dictate the way the agent responds. Examples are not a requirement for Generative Playbooks. However, it is highly recommended to include examples with your instructions. Instructions and examples are the natural language way of building a flow. In contrast to the standard graph based approach of flows and pages, examples should complement, enhance and help steer the instructions to provide a clear template for how to construct a response aligned to your expectations. This helps you achieve more predictable behavior and provide a more natural way to create conversational flows. Few-shot examples contribute greatly to the performance of model prediction. It's highly recommended to generate four or more representative few-shot examples for each of the playbooks and cover all the happy path scenarios with few-shot examples. Keep this in mind as you're writing your instructions. There's no need to painstakingly obsess over the exact language of your steps if you're not providing examples. Instead put more effort into writing clear instructions combined with thorough training examples.

#### Hybrid flows with Generative Playbooks

- https://www.cloudskillsboost.google/paths/280/course_templates/892/video/425587

Generative Playbooks can be used with Virtual and Generative AI Agents. You can invoke a Dialogflow CX flow within a Playbook provided that the flow already exists. Additionally, you can pass parameters between flows and playbooks. To invoke a Dialogflow CX flow within a Playbook, you can use the variable FLOW and specify a flow name. For example, the following instruction says that “If the customer booked an appointment, use the make payment flow to help them pay for the appointment”. If the condition matches that statement, the conversation will be directed to that flow. Please note that attempting to call a flow that doesn’t exist will result in an error. You can also connect a Generative Playbook with a Generative AI Agent data store. These data stores can be configured for web, structured or unstructured data, for example specific domains, internal documents or FAQs. And a Generative AI Agent can access relevant domains or documents to provide an answer to a customer with that data. For example the Generative AI Agent could search the existing FAQs on the company website to generate a response to the customer through Dialogflow CX. To connect a Generative Playbook to a data store, you need to: Create a Playbook tool for the Generative AI Agent, Select the data store in the Tool user interface and save the tool, Then, reference the tool in the Playbook instructions to call upon that data, And finally, test the new Generative Playbook, to ensure that it is performing as you would expect. Let’s explore how to do this in the user interface. First create a tool for the Generative AI Agent. You will need to provide a name for the tool, select Data store as the tool type, and then provide a description of what the data store will be used for. Next, select the data store that you want to use of the appropriate data store type. You can select up to one of each type of a data store to connect to your tool. Then save the tool. Now, you need to reference the tool in the Playbook instructions, by using the $TOOL: FAQ parameter. In this example, both steps include a reference to the tool. Step 1 reads: If a user asks a question, trigger the FAQ tool. And Step 2 states that, If the FAQ tool has a valid answer, share it with the user. If the answer contains a link or there is no answer, respond that you don’t have an answer and ask the user if you can help with anything else. Finally, test the new Generative Playbook in the Simulator to ensure that the natural language response provided by the Generative AI Agent contains the relevant data from the data store. Here, the test user asks what tests they need to get their driver’s license, and the Generative AI Agent uses the data store to generate an answer with the relevant information to answer the question. The test user then asks if they can book an appointment in San Francisco, and the Playbook again utilizes the Generative AI Agent and data stores to provide a relevant response with information about the available field offices where you are able to book a driving test. Generative Playbooks cannot be used with Generators or Generative Fallback, these are mutually exclusive features. If you're inside of a Playbook, the LLM is in full control, and you’ll be able to provide instructions that act as a Generative Fallback would. You will need to provide more instructions or examples to have it do what you want to do. Generators are a prompt for an LLM that can execute on a single turn of a conversation, such as LLMs for Dialogflow CX fulfillment. With Generative Playbooks, the LLM is orchestrating the entire conversation state. In other words, Generators are LLMs applied to the Dialogflow CX state machine, whereas Generative Playbooks are LLMs applied to everything including orchestrating the conversation state. Generative Playbooks can be used with traditional Flows. You can route to a Playbook seamlessly from a normal Dialogflow CX Flow by choosing the playbook option in the transition menu. And remember, you can also route from a playbook to a flow by referencing the Flow name within a Playbook using the $FLOW variable followed by the name of the flow.

#### Generative Playbooks quiz

- https://www.cloudskillsboost.google/paths/280/course_templates/892/quizzes/425588

### Your Next Steps


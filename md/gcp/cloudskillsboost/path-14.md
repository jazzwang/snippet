# Network Engineer Learning Path

- https://www.cloudskillsboost.google/paths/14

[TOC]

## 01: Preparing for Your Professional Cloud Network Engineer Journey 

- https://www.cloudskillsboost.google/paths/14/course_templates/383

### Introduction

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509702

Person: Welcome to Preparing for Your Professional Cloud Network Engineer Journey. In this course you'll learn more about the skills covered on the Professional Cloud Network Engineer certification exam. Each module points to one section of the exam guide. However, it's important to clarify that this course by itself will not prepare you to take the certification exam. This course is not a cram session. The exam is purposely calibrated to test your ability to apply the knowledge required of a Professional Cloud Network Engineer, not merely repeat it. Cram sessions have minimal impact on your ability to pass the exam. Instead, the goal of this course is to help you better structure your preparation time for the exam. You'll learn about the scope of each exam section, assess your current knowledge and skills through diagnostic questions and review where to find additional tools and resources to include in your study plan. Module zero, Introduction, describes what will be covered in the course. It also introduces Cymbal Bank, a fictitious company that is used in the network setup and configuration scenarios throughout this course. You're watching this module right now. Module one, Designing, Planning and Prototyping a Google Cloud Network, provides an overview of the Google Cloud features that Cymbal Bank will use. You will edit your study plan to note the skills that you must develop to implement these features. Module two, Implementing a Virtual Private Cloud, or VPC, reviews Cymbal Bank's functional requirements which you need to know to implement their VPC network. In particular, this module focuses on routing, GKE clusters, firewalls and VPC service controls. You will edit your study plan to note the skills that you must develop to design and implement the VPC network. Module three, Configuring Network Services, reviews Cymbal Bank's functional requirements regarding network services including load balancing, Cloud CDN, Cloud Armor, Cloud NAT, Cloud DNS and packet mirroring. You will edit your study plan to note the skills that you must develop to design and implement these services. Module four, Implementing Hybrid Interconnectivity, reviews Cymbal Bank's function requirements regarding hybrid interconnectivity. The focus is on Cloud Interconnect, Cloud VPN and Cloud Router. You will edit your study plan to note the skills that you must develop to implement hybrid interconnectivity. Module five, Managing, Monitoring and Optimizing Network Operations, reviews Cymbal Bank's logging and monitoring requirements. The focus is on troubleshooting and debugging, especially with regard to network traffic, latency and connectivity. You will edit your study plan to note the skills that you must develop to design and implement logging and monitoring, perform effective troubleshooting and manage and optimize network operations. Module six, Your Next Steps, helps you finish your study plan. It also provides some information about registering for the Professional Cloud Network Engineer certification exam. In this introductory module, you'll learn about the role of a Professional Cloud Network Engineer, the types of resources available to support your study and how you will use the workbook in this course to create your study plan.

#### Introduction to the Professional Cloud Network Engineer role

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509703

So you're preparing for the Professional Cloud Network Engineer certification. But what exactly is the role of a Professional Cloud Network Engineer? Let's review the job role description. A Professional Cloud Network Engineer is responsible for the design, implementation, and management of Google Cloud network infrastructure. This includes designing network architectures for high availability, scalability, resiliency, and security. This individual is skilled in configuring and managing Virtual Private Clouds (VPCs), routing, network security services, load balancing, and Cloud DNS. Additionally, they are proficient in setting up hybrid connectivity through Cloud Interconnect and Cloud VPN. Their expertise extends to diagnosing, monitoring, and troubleshooting network operations by using Google Cloud Observability and the Network Intelligence Center. For more information, see the Professional Cloud Network Engineer Certification page. In this course, you'll examine the role of a Professional Cloud Network Engineer by putting yourself in the shoes of a Professional Cloud Network Engineer at Cymbal Bank, a fictional company that is digitally transforming and preparing to migrate some of its on-premises network architecture to Google Cloud. As part of its digital transformation, Cymbal Bank wants to explore a hybrid cloud model. Cymbal Bank plans to extend its on-premises data center infrastructure to connect into Google Cloud. As a Professional Cloud Network Engineer at Cymbal Bank, your role involves working with cloud architects to design and plan the network architecture. You're also involved in implementing VPCs, hybrid connectivity, and network services. You need to be familiar with application and container networking. You also need a solid understanding of network security. As you continue through this course, you’ll explore the role of a Professional Cloud Network Engineer as Cymbal Bank designs and implements a hybrid cloud model. We'll use this scenario to illustrate the types of considerations and tasks that correspond to each section of the exam guide. Cymbal Bank's network will also provide context for many of the diagnostic questions you'll encounter along the way.

#### Certification value and benefits

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509704

>> Why become a Google Cloud certified Professional Cloud Network Engineer? Certification value has skyrocketed. Becoming Google certified gives you industry recognition. It validates your technical experience and can be the starting point to take your career to the next level. You may be curious about what differentiates a Professional Cloud certification. The professional level certification expects the exam taker to know how to evaluate case studies and design solutions to meet business requirements, in addition to knowing about technical requirements and customer implementation. The Professional Cloud Network Engineer certification exam is based on the exam guide. In the following modules, you'll take diagnostic questions to assess your knowledge of each section of the exam guide. The exam guide is divided into five sections. Each section has several objectives. We'll focus on where you can find resources at the section objective level. You can find the exam guide on the certification page at the URL noted on screen.

#### Certification process

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509705

Throughout this course, you'll be pointed to specific resources and documentation that can help you fill the gaps you identify through the diagnostic questions. Let's go over the types of resources you may want to include in your study plan. Google provides resources to help you develop your skills and experience with Google Cloud solutions. The learning path for this certification includes in-person or online courses, online practice labs and skill badges, and practice questions. The courses recommended for the Professional Cloud Network Engineer certification include Google Cloud Fundamentals: Core Infrastructure, Networking in Google Cloud, and Logging, Monitoring and Observability in Google Cloud. You'll learn more about how these courses relate to the sections of the exam guide as you complete the modules in this course. Keep in mind that Networking in Google Cloud is available in an on-demand format as a six-course series. You should take all six courses: Fundamentals, Routing and Addressing, Network Architecture, Network Security, Load Balancing, and Hybrid and Multicloud for equivalent content to the 3-day instructor led course. The same applies to Logging, Monitoring, and Observability in Google Cloud. You should take both courses, Logging and Monitoring in Google Cloud and Observability in Google Cloud for equivalent content to the 2-day instructor led course. Skill badges provide hands-on experience working in Google Cloud. Skill badges are learning paths made up of labs that give you hands-on practice with Google Cloud services or solutions. Pass the challenge lab at the end, and you'll receive a shareable credential that recognizes your ability to solve real-world problems with your cloud knowledge. As we review the diagnostic questions in this course, you'll also get recommendations for skill badges to include in your study plan. Sample questions are another resource you can use to prepare. The diagnostic questions in this course are designed to help you identify your knowledge gaps. On the certification page, Google provides a different set of sample questions that can help you familiarize yourself with the format of the exam questions. Once you complete the question set, you will receive feedback describing the rationale for the correct answers. The sample questions provide a good opportunity to practice taking the type of scenario-based, application-level questions on the exam. The exam questions present you with a scenario, explain the goal or what you're trying to achieve, and ask you what you would do in this situation. Remember these tips for multiple choice questions. Read the question stem carefully. Make sure you understand exactly what the question is asking. Try to anticipate the correct answer before looking at the options. You should be able to come up with the correct answer just from reading the question stem. You may find that more than one answer may be possible on multiple choice tests. Take questions at face value. If certain details are omitted, then they are unlikely to contribute to the selection of the best answer. Pay attention to qualifiers such as ‘usually’, ‘all’, ‘never’, ‘none’, and keywords like ‘the best’, ‘the least’, ‘except’. Google also supplies official public documentation for its products and services. This documentation can be found at the URL noted on the screen. In each of the following modules, you'll learn about specific documentation resources to help you study that section in preparation for the exam.

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509706

One of the primary goals for this course is to help you devise a study strategy that focuses on areas you need to work on. Let’s quickly explore how the course is set up. The course - and your course workbook - focuses on each section of the exam guide in turn. To help you craft a study strategy, you’ll take diagnostic questions as part of each module. Many of these questions relate to our Cymbal Bank scenario and ask you to apply concepts you will need to be familiar with as a Professional Cloud Network Engineer. Keep in mind that these diagnostic questions are meant to help you identify gaps in your knowledge, but they don’t represent all possible topics on the exam. Remember, we don’t expect that you’ll answer all these questions correctly right now. This is meant to be a course that you take toward the beginning of your Professional Cloud Network Engineer journey, and many of you may not be networking experts yet. We’ll review the answers to the questions related to each section objective. As we cover each objective, you’ll learn more about where the key concepts appear in Google Cloud documentation, specific courses and modules, and/or specific Skill Badges. At the end of each section objective, you’ll find a list of related resources. Mark or highlight the specific resources you need to add to your study plan. In the final part of your workbook, you’ll find a template to help you identify weekly goals and study activities. We’ll talk more about putting together weekly goals at the end of this course. Now that you know about the overall setup of this course and how to use the workbook, let’s get started by exploring section 1 of the exam guide. Section 1 of the exam guide is covered in the next module.

#### Workbook

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509707

### Designing and planning a Google Cloud Network

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509708

Welcome to Module 1: Designing and Planning a Google Cloud network. In this module, you'll explore the scope of tasks involved in designing and planning Cymbal Bank's cloud network, which corresponds to the first section of the professional Cloud Network Engineer Exam Guide. We'll start by discussing some different aspects of Cymbal Bank's network design. Next, you'll assess your skills in this section through ten diagnostic questions. Then, we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Designing Cymbal Bank's cloud network

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509709

Let's start by exploring the breadth of considerations involved in the design of a Google Cloud network, and the role of the Professional Cloud Network Engineer at Cymbal Bank. Cymbal Bank plans to extend its on-premises data center infrastructure to connect into Google Cloud to support a hybrid cloud model. As a Professional Cloud Network Engineer, you play an integral role in designing and planning the network infrastructure. Cymbal Bank plans to continue deploying some workloads in your on-premises data center environments various locations, while moving others to Google Cloud for deployment into cloud virtual infrastructure. You need to design a hybrid cloud environment to connect on-premises, branch, office, and data center environments to the new cloud environment. These workloads, running on-premises and in the cloud, will communicate and exchange significant amounts of data, both for real-time transactional workloads as well as streaming or batch analytics workloads. Your network design will require secure high bandwidth, low latency communication, connecting Cymbal Bank’s, physical and virtual data center networks with the virtual private cloud (VPC) networks in Google Cloud. To give you a better understanding of the types of considerations involved in designing and planning the network, let's review Cymbal Bank’s existing infrastructure and the changes you will make with the upcoming cloud migration. Cymbal Bank has data centers in New York, London, and Singapore. It also has branches and offices distributed across the U.S., western Europe, and southeast Asia. Cymbal Bank deploys a mixture of monolithic and some microservices-based workloads, including both transactional and analytical workloads. Most workloads reside in data centers, but some run in branch and office environments. You want to deploy Google Cloud resources close to your data centers. Cymbal Bank plans to use Compute Engine, Google Kubernetes Engine, or GKE, Cloud Storage, Dataflow, Dataproc, and BigQuery in its cloud solutions. The closest Google Cloud regions to Cymbal Bank’s data centers all support these required features. You also decide to utilize other nearby regions in Iowa, Belgium, and Jakarta as secondary deployment locations. This will provide higher capacity and lower latency for users outside the primary regions. These secondary regions will also provide fallback in case of regional cloud failures in the primary regions. The deployments will utilize at least two zones in each region to provide higher availability. As a Professional Cloud Network Engineer, your starting task is to design the network architecture for Cymbal Bank. You determine that Cymbal Bank will have four primary shared VPC networks in four different host projects corresponding to development, test, staging, and production environments. Each VPC will have subnets in the six primary and secondary regions. You will configure appropriate routes and firewall rules for the expected traffic profiles. Cymbal Bank will have a large number of service projects using those four shared VPCs. Service projects will be deployed as quadruplets with development, testing, staging, and production projects provided per team, department, or product. Each component service project connects to its respective host project. You will assign access to network resources using IAM predefined roles: Shared VPC Admin, Network Admin, Security Admin, and Network User roles. You decide that Cymbal Bank will also deploy standalone VPC networks for ephemeral analytics workloads. You will use VPC peering to connect them to the shared VPC networks as needed. You decide to use Dedicated Interconnect connections to Google Cloud colocation facilities. You will also use Layer 2 and 3 Partner Interconnect connections. These connections will have a mix of regular and high availability configurations. Cymbal Bank will also utilize both Classic and HA Cloud VPN with a mix of static and dynamic routing. You will use Cloud Router for private connectivity between Google Cloud and the satellite branches and offices. You plan to use Cloud Load Balancing with global external and regional internal Application Load Balancers. This configuration will serve static and dynamic content with low latency. You will use Cloud CDN to provide caching of static resources to increase capacity and reduce latency. Cloud CDN improves serving capacity because most users are served resource requests from edge locations, rather than having the requests enter the VPC and be served from the origin. Cymbal Bank will use Google Cloud Armor to provide DDoS and other attack protections to their public endpoints, which will all be exposed via Cloud Load Balancing. They will use Cloud NAT to provide internet access to resources with no public or external IP address. You'll use Network Intelligence features for monitoring, troubleshooting, and debugging. Network Topology lets you visualize VPCs, interconnects, and VPN links and associated metrics. Connectivity Tests lets you troubleshoot connectivity issues. Performance Dashboard provides high-level interzone metrics. Firewall Insights lets you troubleshoot firewall traffic control. Google Cloud Observability lets you perform general network monitoring and debugging using VPC Flow Logs and firewall logs. You'll use it for incident detection and response, latency tracking, network traffic flow or firewall debugging, and forensics. Logs may also be exported to Pub/Sub, Cloud Storage, or BigQuery for integration with external systems, long-term storage, and/or SQL analysis. Some of Cymbal Bank’s legacy monoliths cannot, or will not, be converted to microservices. These will be deployed into regional managed instance groups of VMs. Cymbal Bank will deploy all its microservices to GKE. Some existing monoliths will be converted to microservices that will also be deployed into GKE. You decide to utilize private VPC-native GKE clusters in standard mode to ensure security and maximum infrastructure flexibility. Learning about Cymbal Bank’s network design should give you a sense of the scope of considerations involved in planning a Google Cloud network. This is a vital part of your role as a Professional Cloud Network Engineer, so let's explore further with diagnostic questions focusing on this area.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509710

>> It's your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509711

#### Your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509712

You’ll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under Where to look and Content mapping to build a study plan that meets your learning needs.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509713

#### Knowedge Check

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509714

### Implementing Virtual Private Cloud (VPC) networks

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509715

Welcome to Module 2: Implementing Virtual Private Cloud networks. In this module, you'll explore the scope of tasks involved in implementing a VPC for Cymbal Bank which corresponds to the first section of the Professional Cloud Network Engineer exam guide. Similar to the last module, we'll start by discussing some considerations for this section. Next, you'll assess your skills in this section through 10 diagnostic questions. Then, we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Considerations for Cymbal Bank’s VPC

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509716

Let's start by exploring the breadth of considerations involved as a Professional Cloud Network Engineer at Cymbal Bank, tasked with implementing a VPC. In module one, you learned about the Professional Cloud Network Engineer's role in Cymbal Bank's Google Cloud network design. Now, you must start implementing this design. You need to decide on a private IP address plan to integrate Cymbal Bank's VPCs and let them communicate with the on-premises infrastructure. This includes devising an IP plan for Cymbal Bank's GKE clusters. You should also assign IP address ranges to subnets and create those subnets in the appropriate regions. Your implementation should consider where the various managed instance groups and GKE clusters will reside. You complete the implementation of Cymbal Bank's VPC with routing rules to provide the communication paths between the on-premises resources and VPCs. You also set firewall rules that restrict traffic flow on the VPCs to only the sources, destinations, ports, and protocols required for the workload traffic. As a Professional Cloud Network Engineer, you begin with implementing an IP addressing plan for Cymbal Bank's Google Cloud VPCs. For its existing on-premises solutions, Cymbal Bank uses RFC 1918 IP blocks for internal networking. The three data centers use the 172.28.0.0/14, 172.24.0.0/14, and 172.20.0.0/14 blocks. Each branch reuses the 192.168.0.0/16 block. Because it will not overlap with the on-premises RFC 1918 usage, you decide to use the 10.0.0.0/8 block for all Cymbal Bank’s Google Cloud VPC IP addressing. You assign subnet primary and secondary ranges for the four shared VPCs from the 10.128.0.0/9 block, reusing that block for each shared VPC. The overlap among shared VPCs is not a concern because they are separate environments and should not communicate using internal IP addresses. You give access to specific subnets in a shared VPC to each team that uses the service project. You use the 10.0.0.0/9 block for standalone VPCs that host ephemeral analytics or ML workloads. These may be peered to the shared VPCs whenever you require internal IP address communication. You give access to several subnets in different regions to each team. Teams will deploy their managed instance group of VMs or Kubernetes Engine clusters in the subnets for their service project. You choose a primary IP address range that is just large enough to support the current expected number of VM instances, or Google Kubernetes Engine nodes. As workload needs change and more VM instances or nodes are required, you can expand the primary ranges without disruption of workloads. You select secondary ranges based on the expected number of pods and services for GKE clusters. All GKE clusters will be private clusters deployed in VPC-native mode. Although secondary ranges can be deleted and recreated as the pod and service accounts change, this requires deletion and recreation of the cluster, which could cause disruption to workloads. When VPC networks are created, a set of default routes is also created. For every subnet that routes traffic to an associated subnet, a default route is created for each primary range and each secondary range. Also, an internet gateway route (0.0.0.0/0) is created that routes all traffic not matching any subnet primary or secondary range to the internet. Cloud Router can automatically add dynamic routes based on BGP communication with on-premises VPN gateways, routers across Cloud VPN, or Interconnect links. This allows traffic to be routed between VPCs and on-premises networks. You add custom routes for specialized routing scenarios, applying some routes strictly to specific VMs as required. Custom routes let you route through NAT/Proxy, WAF or traffic scanning, an internal load balancer, or a VPN gateway. When peering standalone VPCs to the shared VPC, you decide whether to exchange custom and dynamic routes across the peering link. You've now defined the IP plan, VPC networks and subnets, and routing rules for Cymbal Bank’s network. Next, you need to implement firewall rules that only allow traffic required for the operation and maintenance of Cymbal Bank’s workloads. The default behavior of VPCs is to allow any requests sent from resources, but to block any request arriving at resources. These are referred to as the implicit ‘deny all ingress’ and ‘allow all egress’ rules. They can be overridden by higher-priority firewall rules specified for the VPC. Cymbal Bank has identified the ports and protocols used for workload, operational, and maintenance traffic. You create ingress and egress allow and deny firewall rules to ensure that valid traffic can flow and invalid traffic is blocked. You simplify the design and implementation of rules by setting priority values and having the rules applied in priority order. The rules can also be applied generally to all resources in the VPC, or selectively to only certain resources by network tag or workload service account. You opt to use workload service accounts and have firewall rules applied to ensure that workloads only communicate with other valid workloads using intended ports and protocols. Cymbal Bank will use the same firewall rules across all their shared VPC host projects. You leverage hierarchical firewall rules and bind the rules at the Organization or Folder level to avoid duplication of rules across all four projects. After configuring the firewall rules, you run system testing on Cymbal Bank’s workloads to determine whether they function normally. As a Professional Cloud Network Engineer, you monitor and troubleshoot the firewall configuration using firewall logs and insights, and fix any detected problems. Remember that Cymbal Bank will only run private VPC-native Kubernetes Engine clusters. Private clusters include four control plane template options, which are the combinations of public access enabled or disabled, and authorized networks enabled or disabled. These options determine where the control plane can be accessed from. You decide to configure your clusters with public access enabled and authorized networks enabled. This configuration enables access to the control plane from other locations, in addition to the on-premises private network and the VPCs. This decision lets administrators administer the cluster from the Cloud Shell or other locations that can only reach the cluster publicly. You will only add authorized networks on a temporary basis, and update the cluster to add or remove authorized networks as needed. You want Cymbal Bank's GKE workloads to stay fully portable and maintain the same in-cluster firewalling when deployed in non-Google Cloud environments. You leave intra-node visibility disabled and use built-in network policies for in-cluster firewalling. You want Cymbal Bank to rely on Network Connectivity Center to streamline its network operations and ensure secure, reliable connectivity for its branches and services. Cymbal Bank will use Network Connectivity Center to: Centrally manage its VPC networks. Network Connectivity Center will provide Cymbal Bank with a single pane of glass to manage its complex network infrastructure. This centralized approach simplifies network management, improves visibility, and reduces operational overhead. Secure branch office connectivity. Cymbal Bank will use Network Connectivity Center to establish secure VPN connections between its branches and its central Google Cloud environment. This approach ensures that sensitive customer data remains protected while enabling seamless communication and access to critical resources. Scale its VPC network infrastructure. As Cymbal Bank expands its operations and opens new branches, Network Connectivity Center will enable it to scale its network infrastructure effortlessly. New connections can be provisioned quickly and efficiently, ensuring minimal disruption and maintaining high performance. By leveraging Network Connectivity Center, Cymbal Bank will achieve a more agile, secure, and scalable network environment, empowering it to deliver exceptional financial services to its customers.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509717

Chris: Now it's your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, the purpose of these questions is to help you better understand what is involved in this section of the exam guide and identify which areas you want to focus on in your study plan.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509718

#### Your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509719

You’ll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under Where to look and Content mapping to build a study plan that meets your learning needs.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509720

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509721

### Configuring managed network services

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509722

Welcome to Module 3: Configuring managed network services. In this module we explore the scope of tasks involved in configuring network services for Cymbal, which corresponds to the third section of the Professional Cloud Network Engineer Exam guide. We'll start by examining some considerations with our example at Cymbal. Next, you'll assess your skills in this section through ten diagnostic questions. Then we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Considerations for Cymbal Bank’s VPC

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509723

Let's start by examining how a Professional Cloud Network Engineer at Cymbal would configure network services for Cymbal and the types of considerations involved. The next step in the network design and implementation for Cymbal Bank is to configure the network services you will use. Cymbal Bank will have many user-facing applications deployed to Google Cloud. Users will interact with them via web or mobile apps. You want to provide these applications with optimal performance for users distributed over large geographies. To provide the lowest possible latency with the highest capacity and highest availability, you deploy the applications behind Application Load Balancers. The load balancers provide both security and performance by protecting against layer 3 and 4 attacks. To provide additional layer 7 protection, you also use configurable security policies in Google Cloud Armor. To further reduce latency and increase capacity for serving static resources, you leverage Google's Cloud CDN, to catch all those static assets in all appropriate Google Cloud edge locations. You use Cloud DNS to map Cymbal Bank's domain names to the load balancer IP addresses. You also protect compute resources from the internet by ensuring that they only have private internal IP addresses, and by setting firewall rules to allow access only from the load balancers. Many of these private compute resources will still require outbound internet access to make requests to partner APIs, and that capability will be provided by Cloud NAT. Finally, to understand network performance in more detail and to provide for real-time scanning of network traffic, you leverage VMs with multiple network interfaces and packet mirroring. Each global external Application Load Balancer can have a single anycast IP address, or a single IPv4 address and a single IPv6 address, that can be used by users around the world. It automatically routes to the backend service that is closest to the user. Cymbal Bank will deploy at least two target proxies and URL maps per Application Load Balancer. The HTTP target proxy redirects to HTTPS. The HTTPS target proxy uses the URL map to direct static resource requests to a backend bucket and dynamic requests to one of several backend services. Some backend services can be connected to more powerful backends with more CPU, more RAM, or GPUs and TPUs, to serve requests that require more processing power, such as machine learning inference at /api/ml or video or graphics rendering at /api/video or /api/graphics. Other backends can be less powerful for basic CRUD-type REST APIs connected to a DB, such as /api/db. Different hosts or paths in the request indicate which backend service should be used for that request. This is known as content-based load balancing and will be used by Cymbal Bank. Each backend connected to a backend service can also autoscale to provide high capacity in a cost-effective fashion by scaling the compute resources up or down based on demand. Cymbal Bank will also use other protocols in their web and mobile apps to improve performance. You leverage WebSockets and gRPC for 2-way communication in chat interfaces. You leverage HTTP/3 (running over QUIC) where the best possible communication performance is required. You run these protocols through the Google Cloud global external Application Load Balancer, and can add or update target proxies to support them. As a Professional Cloud Network Engineer at Cymbal Bank, you also need to choose and configure the right load balancers for other scenarios. For load balancing private internal requests, you would use either the internal regional Application Load Balancer or the internal regional Network Load Balancer. These can load balance between tiers of an N-tier architecture or between microservices in a microservices architecture. The internal Application Load Balancer can provide full layer 7 capabilities, such as content-based load balancing. The internal Network Load Balancer can support other non-HTTP based protocols and provide lower latency because it is a passthrough load balancer. Cymbal Bank will primarily be using protocols in the HTTP family. However, there are some scenarios where you deal with other protocols. External proxy Network Load Balancers can provide global load balancing with anycast IP. They can also support IPv6, such as the global external Application Load Balancer. They are intended for non-HTTP protocols running over TCP or TLS. You can use regional external Network Load Balancers for minimum latency passthrough or UDP-based protocols such as RTP. Cymbal Bank expects attacks against any endpoints that are accessible by the public internet. You want to provide as much protection as possible. Google Cloud Armor gives you a configurable managed service that is integrated with the Application Load Balancer. Google Cloud Armor protects against SQL injection, XSS injection, and similar attacks. You can also configure it to filter traffic based on request properties. Traffic through Google Cloud Armor can also be throttled with configurable rate limits or challenged with reCAPTCHA for bot management, or to further protect against DDoS. Cymbal Bank will use a content delivery network (CDN) to cache static resources at many edge locations worldwide to offload those requests from the origin. Using a CDN dramatically increases capacity and also serves requests with much lower latency from the nearest edge location to the user. Finally, a CDN protects static resources from DDoS attacks because the massively distributed cache is designed to support worldwide user request rates far beyond the levels seen in DDoS attacks. You decide to use a CDN to serve a number of static resources, including Cymbal’s website and web application files, such as HTML, JavaScript and CSS, and photos and other images, video, logos and documents. Google Cloud integrates with many popular CDN providers. You decide to use Cloud CDN, which offers best-in-class performance, ease-of-use, and tight integration with Application Load Balancers. Cloud CDN supports standard cache control headers, customizable cache keys, configurable TTL overrides, cache invalidation, and signed URLs and cookies for authorization - among many other features. Cymbal can also use Cloud CDN to cache on-premises or other cloud origin static resources. These sources can also be placed behind the Application Load Balancer, so that they share a common IP address and domain name and support offload to Google Cloud, or vice versa. You want to continue to use Cymbal Bank's existing on-premises private DNS. You create a hybrid DNS system using Cloud DNS to provide private DNS for Cymbal Bank’s Google Cloud resources. The performance and ease-of-use then leads Cymbal Bank to also migrate the public DNS over to Google Cloud. The migration process is exceedingly simple, and Cymbal Bank completes the migration with a move to Google Domains as its domain registrar. You map Cymbal Bank's global Application Load Balancer IP addresses to the public domain names. With Cloud DNS, you quickly and simply configure private DNS across Cymbal’s shared and standalone VPCs. You integrate this with Cymbal’s on-premises DNS into a hybrid DNS via forwarding zones, peering zones, and inbound and outbound DNS server policies. To maintain maximum security, Cymbal Bank will not allow any resources with external IP addresses to be used in Google Cloud aside from Google managed services such as global external Application Load Balancers. However, Cymbal Bank has resources such as VMs that need to make requests to the internet to call partner or other APIs, or to download software. Cloud NAT provides secure internet-bound requests using managed external IP addresses. You define a small pool of external IP addresses, and Cloud NAT allows this pool to be used efficiently among a large number of private VMs, making periodic internet connections and requests by enabling static or dynamic port allocation and configurable port re-use timeouts. You distribute the pool of external IP addresses among a set of Cloud NAT gateways and bind these managed gateways to their VMs by assigning subnet ranges. When VMs with internal IP addresses (either primary or alias IP) in these ranges make requests, they use the associated Cloud NAT gateway. For Cymbal Bank's most sensitive systems, you set aside isolated internal VPCs with extra control on the network traffic in or out. These most trusted VPCs will be separated from all other VPCs, and will only allow traffic that has been scanned and filtered. To enable this controlled communication, Cymbal Bank uses VMs that run scanning and filtering software with multiple network interfaces. One of those network interfaces connects to the most trusted VPC, and the other interfaces connect to the other, less trusted VPCs. Cymbal Bank will also do periodic security, forensic, and performance analysis of traffic on the less trusted VPCs, which requires packet captures. You will use packet mirroring to efficiently collect these packet captures for real-time or batch analysis. As we just covered, as a Professional Cloud Network Engineer, you have a lot to consider when configuring network services for Cymbal Bank. As you prepare for the exam, you should develop a solid understanding of the different aspects.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509724

Lisa: Now it's your turn to assess your experience [Indistinct] related to this section with some diagnostic questions. Remember, the purpose of these questions is to help you better understand what is involved in this section of the exam and identify which areas you'll want to focus on in your study plan.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509725

#### Your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509726

You’ll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under Where to look and Content mapping to build a study plan that meets your learning needs.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509727

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509728

### Implementing hybrid network interconnectivity

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509729

Welcome to Module 4: Implementing hybrid network interconnectivity. In this module, you'll continue to examine the Professional Cloud Network Engineer's role at Cymbal Bank, this time focusing on tasks and considerations involved in implementing hybrid interconnectivity. This corresponds to the fourth section of the Professional Cloud Network Engineer Exam Guide. As in previous modules, we'll start by exploring the Cymbal Bank scenario. Next, you'll assess your skills in this section through 10 diagnostic questions. Then, we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Configuring network services for Cymbal Bank

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509730

Let's go over how you would implement hybrid connectivity as a Professional Cloud Network Engineer at Cymbal Bank. In module 3, you explored how a Professional Cloud Network Engineer would configure Cymbal Bank's network services. This module examines the considerations involved in implementing hybrid connectivity, including configuring Google Cloud Interconnect, site-to-site IPsec VPN, and Cloud Router. You decide to use Dedicated Interconnect connections to Google Cloud co-location facilities. You will also use layer 2 and 3 Partner Interconnect connections. These connections will have a mix of regular and high availability configurations. They will also utilize both Classic and HA Cloud VPN with a mix of static and dynamic routing with a Cloud Router for private connectivity between Google Cloud and the satellite branches and offices. Cymbal Bank will use Dedicated Interconnect connections to connect their most critical data center workloads to the nearest Google Cloud region. Dedicated Interconnect can provide the highest bandwidth and lowest latency connectivity, and Cymbal Bank's data center is sufficiently close to a Google Cloud co-location facility to make it cost-effective. Cymbal Bank will create a configuration with 99.9% availability. This configuration will also utilize Partner Interconnect and Cloud VPN links in other locations that can serve as alternative routes, if the Dedicated Interconnect link is temporarily down. The implementation for Dedicated Interconnect involves installing a router in a Google Cloud co-location facility, as well as configuration and connection in that facility to a Google Cloud edge router. You will also need to configure the connections, VLAN attachments, and Cloud Routers in the connected Google Cloud projects and VPCs. Some data centers are not close enough to a Google co-location facility to cost-effectively use Dedicated Interconnect. Cymbal Bank will use Partner Interconnect to connect these data centers to the nearest Google Cloud region. Partner Interconnect bandwidths and latencies can vary by partner, but some partners can offer performance that approaches that provided by Dedicated Interconnect. Partners may provide layer 2 and/or layer 3 connectivity, which have similar configuration. For layer 3, the BGP configuration is performed in the partner router, and not in the on-premises router. Cymbal Bank will utilize both approaches in different locations with different partners, based on the capabilities of the partner in those locations. You will create configurations with 99.9% availability by utilizing other Partner and Dedicated Interconnect and Cloud VPN links in other locations that can serve as alternative routes if the Partner Interconnect link is temporarily down. The implementation for Partner Interconnect involves establishing layer 2 or 3 connectivity to a Google Cloud partner. It also involves configuring of the connections, VLAN attachments, and Cloud Routers in connected Google Cloud projects and VPCs. Cymbal Bank will utilize Google Cloud VPN: HA connections to connect satellite offices or branches to Google Cloud when the office or branch VPN gateway in those locations supports BGP. Cloud VPN HA can provide 99.99% availability when configured with two or four VPN tunnels. It only supports dynamic routing with BGP, and the Cloud Router can only be used with on-premises VPN gateways that also support BGP. Cloud VPN provides 1.5 to 3 Gbps per tunnel with standard internet latency. It is suitable for connecting hybrid workloads that don't require the low latencies and high bandwidth offered by Interconnect. It also can be much faster to set up and less expensive to operate, compared to Interconnect. When the VPN gateway in satellite offices or branches does not support BGP, Cymbal Bank will utilize Cloud VPN Classic. This does not provide the 99.99% availability that can be achieved with Cloud VPN HA, but can offer 99.9% availability, and can be used with static routing. Cloud VPN Classic provides the same bandwidth and latency per tunnel as Cloud VPN HA; however, Cloud VPN HA is the recommended approach when the on-premises peer VPN gateway can support BGP. Cymbal Bank will utilize Cloud Routers to provide dynamic routing via BGP for their interconnecting Cloud VPN HA connections. You will configure the Cloud Routers with the default subnet advertisement mode. You also set your VPC's dynamic routing mode to global so that all subnets and all regions are advertised by each Cloud Router. Any subnet can be reached by any of the VPN or Interconnect links. This ensures connectivity, even when there are simultaneous connection failures, as long as one of the VPN or Interconnect links for the VPC is available. Network Connectivity Center lets you create VPC spokes to connect VPC networks together for full mesh connectivity. It also includes the Router Appliance feature, which lets you use a third-party network virtual appliance to establish site-to-site or site-to-cloud connectivity. Cymbal Bank will utilize Network Connectivity Center to use Google's network as part of a wide area network, including external sites using site-to-site data transfer. After completing the required configuration, you will be able to move data between the different sites. Data transfer provides IPv4 connectivity between external networks using a Google Cloud VPC network and hybrid spokes. You can transfer data between multiple on-premises networks or to other cloud networks. You can create spokes that rely on connectivity resources such as Cloud VPN, Cloud Interconnect, Router Appliance, and Cross-Cloud Interconnect.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509731

Person: Now it's your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, the purpose of these questions is to help you better understand what is involved in this section of the exam guide and identify which areas you'll want to focus on in your study plan.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509732

#### Your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509733

You’ll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under Where to look and Content mapping to build a study plan that meets your learning needs.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509734

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509735

### Managing, monitoring, and troubleshooting network operations

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509736

Welcome to Module 5: Managing, monitoring and troubleshooting network operations. In this module, you'll learn about the final area of the Professional Cloud Network Engineer’s role at Cymbal Bank. Once the network has been configured, as a Professional Cloud Network Engineer you play an integral role in ensuring the effectiveness of ongoing network operations. This corresponds to the fifth and final section of the Professional Cloud Network Engineer Exam Guide. As in previous modules, we'll begin by exploring what this aspect of your role looks like at Cymbal Bank. Next, you'll assess your skills in this section through eight diagnostic questions. Then we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Module scenario

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509737

Let's explore how a Professional Cloud Network Engineer at Cymbal manages, monitors, and optimizes network operations at Cymbal Bank. Now that Cymbal Bank has completed the implementation of its hybrid network, your role as a Professional Cloud Network Engineer shifts focus to ensuring effective, ongoing operations. You play an integral role in monitoring network performance with Google Cloud Observability, and maintain and troubleshoot any connectivity issues that arise. You also use Network Intelligence Center to monitor and troubleshoot common networking issues. Cymbal Bank will use Cloud Logging to capture and monitor logs from their networking resources. Many of these network resource logs are captured automatically and do not require any setup or configuration. However, you need to enable some resource logs, including those for Application Load Balancers, Firewall Rules Logging, and VPC Flow Logs. Cymbal Bank will use the Cloud Logging page in the Google Cloud console for viewing, filtering, and searching these logs. You'll export the logs into BigQuery for SQL filtering and analysis. Cymbal Bank will also use Cloud Monitoring to visualize network metrics in real time for monitoring network activity. You configure alerts to trigger when metric thresholds approach service level objectives, or indicate connectivity or performance issues, so that support staff can be notified or appropriate incident response automation can be initiated. Cymbal Bank will use VPC Flow Logs and Network Intelligence Center features such as Connectivity Tests, Network Topology, Firewall Insights, and Performance Dashboard to monitor traffic flow and troubleshoot connectivity issues. VPC Flow Logs provide details of traffic being sent or received by IP addresses in subnets. Network Topology lets you visualize network topology as well as metrics about traffic flow. Connectivity Tests lets you verify connectivity between endpoints and provides details of impeding configurations. Performance Dashboard shows packet loss and latency metrics between zones. Firewall Insights lets you monitor activity of firewall rules and identify misconfiguration that would block authorized traffic or allow unauthorized traffic. You will also use Cloud Router, Cloud VPN, and Cloud Interconnect logs and metrics provided by Cloud Logging and Cloud Monitoring to detect and troubleshoot connectivity problems with hybrid connections from on-premise networks to Google Cloud. Similarly, Cymbal Bank will use these same Network Intelligence Center features to troubleshoot latency and traffic flow issues. These features also let you visualize network topology and routing. Use the Google originated open-source tool PerfKit Benchmarker to test, verify, and monitor network performance.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509738

Lisa: Now it's your turn to assess your skills and experience related to this section with some diagnostic questions. Remember, the purpose of these questions is to help you better understand what is involved in this section of the exam and to identify which areas you'll want to focus on in your study plan.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509739

#### Your study plan

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509740

You’ll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under Where to look and Content mapping to build a study plan that meets your learning needs.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/14/course_templates/383/documents/509741

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/14/course_templates/383/quizzes/509742

### Your next steps

#### Your next steps

- https://www.cloudskillsboost.google/paths/14/course_templates/383/video/509743

>> Welcome to Module 6: Your Next Steps. In this module, you'll focus on creating your individualized study plan. In this module, you'll use the notes you've been taking throughout this course to put together a study plan for each week in your Professional Cloud Network Engineer journey. Now that you've explored all five sections of the exam guide, consider what you've learned about your knowledge and skills through the diagnostic questions in this course. You should have a better understanding of what areas you need to focus on and what resources are available. Think about the answers to these questions: When will you take the exam? How many weeks does that give you to prepare? How many hours can you realistically spend preparing for the exam each week? How many total hours will you prepare? Be sure to leave enough time at the end of your plan to retake the diagnostic questions and the sample questions and fill in any gaps in your knowledge that may remain. Take a few minutes to think about how much time you will allocate to preparing for the exam and note your answers in the workbook. The number of weeks in your preparation journey will depend on a variety of factors, such as your prior experience with Google Cloud and how much time you have available to studying each week. You might choose to focus on specific courses or skill badges each week, such as in the sample study plan or instead focus your study on a specific topic, such as VPN configuration. Once you have a high level idea of how many weeks you have to study and how you want to determine your weekly focus, you'll want to build out a plan with weekly goals and study activities. Use the template in your workbook to plan your study goals for each week. Consider: What exam guide sections or topic areas will you focus on? What courses or specific modules will help you learn more? What skill badges or labs will you work on for hands on practice? What documentation links will you review? What additional resources will you use, such as sample questions? You may do some or all of these study activities each week. Let's review an example. If you've identified configuring VPCs as a particular area you need to study, you might choose to structure your study for a week to include targeted modules from the on-demand training, a related skill badge for hands on practice and documentation. Ultimately, you might choose one week to complete an entire course and another week to focus on a skill badge. You can determine the approach that fits your existing skillset. Find the weekly study template at the end of your workbook. Duplicate the weekly template for the number of weeks in your individual preparation journey. Remember, you may need to adjust your plans based on the areas where you need to learn more. For more information about the resources we've discussed in this course, refer to your notes in the student copy of the slides. To register for the exam, follow the link on the Professional Cloud Network Engineer certification information page using the URL shown on the screen. Thank you for attending the preparing for your Professional Cloud Network Engineer journey. Good luck as you begin your journey to study for the Professional Cloud Network Engineer certification.

### Your Next Steps

## 02: Google Cloud Fundamentals: Core Infrastructure

- https://www.cloudskillsboost.google/paths/14/course_templates/60

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531558

Hi, and welcome to the Google Cloud Fundamentals: Core Infrastructure training course. The goal of this course is to provide you with an overview of Google Cloud. Google Cloud offerings can be broadly categorised as compute, storage, big data, machine learning, and application services for web, mobile, analytics, and back-end solutions. Through a combination of videos, quizzes, and hands-on labs, you’ll learn the value of Google Cloud and how cloud solutions factor into business strategies. The intended target audience of today’s course consists of solutions developers, systems operations professionals, and solution architects planning to deploy applications and create application environments on Google Cloud. The course will also be useful for business decision makers evaluating Google Cloud. While you should be happy to hear that we’ll be finding out about services and concepts that are specific to Google Cloud in this course, do keep in mind that, as a ‘fundamentals’ level course, some content will be geared towards learners who are entirely new to cloud technologies. This course has no prerequisites, although it’s helpful be familiar with application development, Linux operating systems, systems operations, and data analytics/machine learning to best understand the technologies covered. There are seven key learning objectives that we’re hoping to achieve. By the end of this course, you should be able to: Identify the purpose and value of Google Cloud products and services. Define how infrastructure is organized and controlled in Google Cloud. Explain how to create a basic infrastructure in Google Cloud. Select and use Google Cloud storage options. Describe the purpose and value of Google Kubernetes Engine. Identify the use cases for serverless Google Cloud services. And combine Google Cloud knowledge with prompt engineering to improve Gemini responses. OK, all set? Let’s begin!

### Introducing Google Cloud

#### Cloud computing overview

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531559

Let’s start at the beginning with an overview of cloud computing. The cloud is a hot topic these days, but what exactly is it? The US National Institute of Standards and Technology created the term cloud computing, although there is nothing US-specific about it. Cloud computing is a way of using information technology (IT) that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they require without the need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they’re flexible, so customers can be. If customers need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. And that's it, that's the definition of cloud. But why is the cloud model so compelling nowadays? To understand why, we need to look at some history. The trend towards cloud computing started with a first wave known as colocation. Colocation gave users the financial efficiency of renting physical space, instead of investing in data center real estate. Virtualized data centers of today, which are the second wave, share similarities with the private data centers and colocation facilities of decades past. The components of virtualized data centers match the physical building blocks of hosted computing—servers, CPUs, disks, load balancers, and so on—but now they’re virtual devices. With virtualization, enterprises still maintain the infrastructure; but it also remains a user-controlled and user-configured environment. Several years ago, Google realized that its business couldn’t move fast enough within the confines of the virtualization model. So Google switched to a container-based architecture— a fully automated, elastic third-wave cloud that consists of a combination of automated services and scalable data. Services automatically provision and configure the infrastructure used to run applications. Today, Google Cloud makes this third-wave cloud available to Google customers. Google believes that, in the future, every company, regardless of size or industry, will differentiate itself from its competitors through technology. Increasingly, that technology will be in the form of software. Great software is based on high-quality data. This means that every company is, or will eventually become, a data company.

#### IaaS and PaaS

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531560

The move to virtualized data centers introduced customers to two new types of offerings: infrastructure as a service, commonly referred to as IaaS, and platform as a service, or PaaS. IaaS offerings provide raw compute, storage, and network capabilities, organized virtually into resources that are similar to physical data centers. Compute Engine is an example of a Google Cloud IaaS service. PaaS offerings, in contrast, bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. App Engine is an example of a Google Cloud PaaS service. In the IaaS model, customers pay for the resources they allocate ahead of time; in the PaaS model, customers pay for the resources they actually use. As cloud computing has evolved, the momentum has shifted toward managed infrastructure and managed services. Leveraging managed resources and services allows companies to concentrate more on their business goals and spend less time and money on creating and maintaining their technical infrastructure. It allows companies to deliver products and services to their customers more quickly and reliably. Serverless is yet another step in the evolution of cloud computing. It allows developers to concentrate on their code, rather than on server configuration, by eliminating the need for any infrastructure management. Serverless technologies offered by Google include Cloud Run, which allows customers to deploy their containerized microservices based application in a fully-managed environment. and Cloud Run functions, which manages event-driven code as a pay-as-you-go service. While it’s outside the scope of this course, you might have heard about software as a service, SaaS, and wondered what it is and how it fits into the Cloud ecosphere. SaaS provides the entire application stack, delivering an entire cloud-based application that customers can access and use. Software as a Service applications are not installed on your local computer. Instead, they run in the cloud as a service and are consumed directly over the internet by end users. Popular Google applications such as Gmail, Docs, and Drive, that are a part of Google Workspace, are all examples of SaaS.

#### The Google Cloud network

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531561

Google Cloud runs on Google’s own global network. It’s the largest network of its kind, and Google has invested billions of dollars over many years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by leveraging more than 100 content caching nodes worldwide. These are locations where high demand content is cached for quicker access, allowing applications to respond to user requests from the location that will provide the quickest response time. Google Cloud’s locations underpin all of the important work we do for our customers. From redundant cloud regions to high- bandwidth connectivity via subsea cables, every aspect of our infrastructure is designed to deliver your services to your users, no matter where they are around the world. Google Cloud’s infrastructure is based in seven major geographic locations: North America, South America, Europe, Africa, the Middle East, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency, the latter of which measures the time a packet of information takes to travel from its source to its destination. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy. You can run also resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, such as a natural disaster. Some of Google Cloud’s services support placing resources in what we call a multi-region. For example, Spanner multi-region configurations allow you to replicate the database's data not just in multiple zones, but in multiple zones across multiple regions, as defined by the instance configuration. These additional replicas enable you to read data with low latency from multiple locations close to or within the regions in the configuration, like The Netherlands, and Belgium. The number of zones and regions Google Cloud supports is increasing all the time. You can find the most up-to-date numbers at cloud.google.com/about/locations.

#### Environmental impact

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531562

The virtual world, which includes Google Cloud’s network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy. Altogether, existing data centers use roughly 2% of the world’s electricity. With this in mind, Google works to make their data centers run as efficiently as possible. Just like our customers, Google is trying to do the right things for the planet. We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals. Therefore, it’s useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a standard that maps out a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste. As an example of how this is being done, here’s Google’s data center in Hamina, Finland. This facility is one of the most advanced and efficient data centers in the Google fleet. Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world. In our founding decade, Google became the first major company to be carbon neutral. In our second decade, we were the first company to achieve 100% renewable energy. By 2030, we aim to be the first major company to operate completely carbon free.

#### Security

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531563

Nine of Google’s services have more than one billion users each, and so you can be assured that security is always on the minds of Google's employees. Design for security is prevalent throughout the infrastructure that Google Cloud and Google services run on. Let's talk about a few ways Google works to keep customers' data safe. The security infrastructure can be explained in progressive layers, starting from the physical security of our data centers, continuing on to how the hardware and software that underlie the infrastructure are secured, and finally, describing the technical constraints and processes in place to support operational security. We begin with the Hardware infrastructure layer which comprises three key security features: The first is hardware design and provenance. Both the server boards and the networking equipment in Google data centers are custom-designed by Google. Google also designs custom chips, including a hardware security chip that's currently being deployed on both servers and peripherals. The next feature is a secure boot stack. Google server machines use a variety of technologies to ensure that they are booting the correct software stack, such as cryptographic signatures over the BIOS, bootloader, kernel, and base operating system image. This layer's final feature is premises security. Google designs and builds its own data centers, which incorporate multiple layers of physical security protections. Access to these data centers is limited to only a very small number of Google employees. Google additionally hosts some servers in third-party data centers, where we ensure that there are Google-controlled physical security measures on top of the security layers provided by the data center operator. Next is the Service deployment layer, where the key feature is encryption of inter-service communication. Google’s infrastructure provides cryptographic privacy and integrity for remote procedure call (“RPC”) data on the network. Google’s services communicate with each other using RPC calls. The infrastructure automatically encrypts all infrastructure RPC traffic that goes between data centers. Google has started to deploy hardware cryptographic accelerators that will allow it to extend this default encryption to all infrastructure RPC traffic inside Google data centers. Then we have the User identity layer. Google’s central identity service, which usually manifests to end users as the Google login page, goes beyond asking for a simple username and password. The service also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also employ secondary factors when signing in, including devices based on the Universal 2nd Factor (U2F) open standard. On the Storage services layer we find the encryption at rest security feature. Most applications at Google access physical storage (in other words, “file storage”) indirectly via storage services, and encryption using centrally managed keys is applied at the layer of these storage services. Google also enables hardware encryption support in hard drives and SSDs. The next layer up is the Internet communication layer, and this comprises two key security features. Google services that are being made available on the internet, register themselves with an infrastructure service called the Google Front End, which ensures that all TLS connections are ended using a public-private key pair and an X.509 certificate from a Certified Authority (CA), as well as following best practices such as supporting perfect forward secrecy. The GFE additionally applies protections against Denial of Service attacks. Also provided is Denial of Service (“DoS”) protection. The sheer scale of its infrastructure enables Google to simply absorb many DoS attacks. Google also has multi-tier, multi-layer DoS protections that further reduce the risk of any DoS impact on a service running behind a GFE. The final layer is Google's Operational security layer which provides four key features. First is intrusion detection. Rules and machine intelligence give Google’s operational security teams warnings of possible incidents. Google conducts Red Team exercises to measure and improve the effectiveness of its detection and response mechanisms. Next is reducing insider risk. Google aggressively limits and actively monitors the activities of employees who have been granted administrative access to the infrastructure. Then there’s employee U2F use. To guard against phishing attacks against Google employees, employee accounts require use of U2F-compatible Security Keys. Finally, there are stringent software development practices. Google employs central source control and requires two-party review of new code. Google also provides its developers libraries that prevent them from introducing certain classes of security bugs. Additionally, Google runs a Vulnerability Rewards Program where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications. You can learn more about Google’s technical-infrastructure security at cloud.google.com/security/security-design.

#### Open source ecosystems

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531564

Some organizations are afraid to bring their workloads to the cloud because they're afraid they'll get locked into a particular vendor. However, if, for whatever reason, a customer decides that Google is no longer the best provider for their needs, we provide them with the ability to run their applications elsewhere. Google publishes key elements of technology using open source licenses to create ecosystems that provide customers with options other than Google. For example, TensorFlow, an open source software library for machine learning developed inside Google, is at the heart of a strong open source ecosystem. Google provides interoperability at multiple layers of the stack. Kubernetes and Google Kubernetes Engine give customers the ability to mix and match microservices running across different clouds, while Google Cloud Observability lets customers monitor workloads across multiple cloud providers.

#### Pricing and billing

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531565

To round off this section of the course, let’s take a brief look at Google Cloud’s pricing structure. Google was the first major cloud provider to deliver per-second billing for its infrastructure-as-a-service compute offering, Compute Engine. In addition, per-second billing is now also offered for users of Google Kubernetes Engine (our container infrastructure as a service), Dataproc (which is the equivalent of the big data system Hadoop, but operating as a service), and App Engine flexible environment VMs (a platform as a service). Compute Engine offers automatically applied sustained-use discounts, which are automatic discounts that you get for running a virtual machine instance for a significant portion of the billing month. Specifically, when you run an instance for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. Custom virtual machine types allow Compute Engine virtual machines to be fine-tuned with optimal amounts of vCPU and memory for their applications so that you can tailor your pricing for your workloads. Our online pricing calculator can help estimate your costs. Visit cloud.google.com/products/calculator to try it out. Now, you’re probably thinking, “How can I make sure I don’t accidentally run up a big Google Cloud bill?” You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month’s spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you’ll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud Console that allows you to monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack, protecting both account owners and the Google Cloud community as a whole. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources you can have in your projects. For example, by default, each Google Cloud project has a quota allowing it no more than 15 Virtual Private Cloud networks. Although projects all start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531566

### Resources and Access in the Cloud

#### Google Cloud resource hierarchy

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531567

In this section of the course we’ll look at the functional structure of Google Cloud. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. At the first level are resources. These represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are also inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let’s take a look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they’re billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can’t be changed after creation. They’re what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don’t have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It’s helpful to know that these Google-generated numbers exist, but we won’t explore them much in this course. They’re mainly used internally by Google Cloud to keep track of resources. Google Cloud’s Resource Manager tool is designed to programmatically help you manage projects. It’s an API that can gather a list of all the projects associated with an account, create new projects, update existing projects, and delete projects. It can even recover projects that were previously deleted,and can be accessed through the RPC API and the REST API. The third level of the Google Cloud resource hierarchy is folders. Folders let you assign policies to resources at a level of granularity you choose. The resources in a folder inherit policies and permissions assigned to that folder. A folder can contain projects, other folders, or a combination of both. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. As previously mentioned, the resources in a folder inherit policies and permissions from that folder. For example, if you have two different projects that are administered by the same team, you can put policies into a common folder so they have the same permissions. Doing it the other way--putting duplicate copies of those policies on both projects–could be tedious and error-prone. if you needed to change permissions on both resources, you would now have to do that in two places instead of just one. To use folders, you must have an organization node, which is the very topmost resource in the Google Cloud hierarchy. Everything else attached to that account goes under this node, which includes folders, projects, and other resources. There are some special roles associated with this top-level organization node. For example, you can designate an organization policy administrator so that only people with privilege can change policies. You can also assign a project creator role, which is a great way to control who can create projects and, therefore, who can spend money. How a new organization node is created depends on whether your company is also a Google Workspace customer. If you have a Workspace domain, Google Cloud projects will automatically belong to your organization node. Otherwise, you can use Cloud Identity, Google’s identity, access, application, and endpoint management platform, to generate one. Once created, a new organization node will let anyone in the domain create projects and billing accounts, just as they could before. folders underneath it and put projects into it. Both folders and projects are considered to be “children” of the organization node.

#### Identity and Access Management (IAM)

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531568

When an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The “who” part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A “who” is also called a “principal.” Each principal has its own identifier, usually an email address. The “can do what” part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. When a principal is given a role on a specific element of the resource hierarchy, the resulting policy applies to both the chosen element and all the elements below it in the hierarchy. You can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. Deny policies, like allow policies, are inherited through the resource hierarchy. There are three kinds of roles in IAM: basic, predefined, and custom. The first role type is basic. Basic roles are quite broad in scope. When applied to a Google Cloud project, they affect all resources in that project. Basic roles include owner, editor, viewer, and billing administrator. Let’s look at these basic roles in a bit more detail. Project viewers can access resources but can’t make changes. Project editors can access and make changes to a resource. And project owners can also access and make changes to a resource. In addition, project owners can manage the associated roles and permissions and set up billing. Often companies want someone to control the billing for a project but not be able to change the resources in the project. This is possible through a billing administrator role. A word of caution: If several people are working together on a project that contains sensitive data, basic roles are probably too broad. Fortunately, IAM provides other ways to assign permissions that are more specifically tailored to meet the needs of typical job roles. This brings us to the second type of role, predefined roles. Specific Google Cloud services offer sets of predefined roles, and they even define where those roles can be applied. Let’s look at Compute Engine, for example, a Google Cloud product that offers virtual machines as a service. With Compute Engine, you can apply specific predefined roles—such as “instanceAdmin”—to Compute Engine resources in a given project, a given folder, or an entire organization. This then allows whoever has these roles to perform a specific set of predefined actions. But what if you need to assign a role that has even more specific permissions? That’s when you’d use a custom role. Many companies use a “least-privilege” model in which each person in your organization is given the minimal amount of privilege needed to do their job. So, for example, maybe you want to define an “instanceOperator” role to allow some users to stop and start Compute Engine virtual machines, but not reconfigure them. Custom roles will allow you to define those exact permissions. Before you start creating custom roles, please note two important details. First, you’ll need to manage the permissions that define the custom role you’ve created. Because of this, some organizations decide they’d rather use the predefined roles. And second, custom roles can only be applied to either the project level or organization level. They can’t be applied to the folder level.

#### Service accounts

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531569

Imagine you have a Compute Engine virtual machine running a program that needs to access other cloud services regularly. Instead of requiring a person to manually grant access each time the program runs, you can give the virtual machine itself the necessary permissions. This is where service accounts come in. Service accounts allow you to assign specific permissions to a virtual machine, so it can interact with other cloud services without human intervention. Let’s say you have an application running in a virtual machine that needs to store data in Cloud Storage, but you don’t want anyone on the internet to have access to that data - just that particular virtual machine. You can create a service account to authenticate that VM to Cloud Storage. Service accounts are named with an email address, but instead of passwords they use cryptographic keys to access resources. So, if a service account has been granted Compute Engine’s Instance Admin role, this would allow an application running in a VM with that service account to create, modify, and delete other VMs. Service accounts do need to be managed. For example, maybe Alice needs to manage which Google accounts can act as service accounts, while Bob just needs to be able to view a list of service accounts. Fortunately, in addition to being an identity, a service account is also a resource, so it can have IAM policies of its own attached to it. This means that Alice can have the editor role on a service account, and Bob can have the viewer role. This is just like granting roles for any other Google Cloud resource.

#### Cloud Identity

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531570

When new Google Cloud customers start using the platform, it’s common to log in to the Google Cloud Console with a Gmail account and then use Google Groups to collaborate with teammates who are in similar roles. Although this approach is easy to start with, it can present challenges later because the team’s identities are not centrally managed. This can be problematic if, for example, someone leaves the organization. With this setup, there’s no easy way to immediately remove a user’s access to the team’s cloud resources. With a tool called Cloud Identity, organizations can define policies and manage their users and groups using the Google Admin Console. Admins can log in and manage Google Cloud resources using the same usernames and passwords they already use in existing Active Directory or LDAP systems. Using Cloud Identity also means that when someone leaves an organization, an administrator can use the Google Admin Console to disable their account and remove them from groups. Cloud Identity is available in a free edition and also in a premium edition that provides capabilities to manage mobile devices. If you’re a Google Cloud customer who is also a Google Workspace customer, this functionality is already available to you in the Google Admin Console.

#### Interacting with Google Cloud

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531571

There are four ways to access and interact with Google Cloud. The Google Cloud console, the Google Cloud SDK and Cloud Shell, the APIs, and the Google Cloud app. Let’s explore each of those now. First is the Google Cloud console, which is Google Cloud’s graphical user interface, or GUI, that helps you deploy, scale, and diagnose production issues in a simple web-based interface. With the Google Cloud console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Google Cloud console also provides a search facility to quickly find resources and connect to instances via SSH in the browser. Second is through the Google Cloud SDK and Cloud Shell. The Google Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These include the Google Cloud CLI, which provides the main command-line interface for Google Cloud products and services, and bq, a command-line tool for BigQuery. When installed, all of the tools within the Google Cloud SDK are located under the bin directory. Cloud Shell provides command-line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent 5 gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. With Cloud Shell, the Google Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The third way to access Google Cloud is through application programming interfaces, or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Google Cloud console includes a tool called the Google APIs Explorer that shows which APIs are available, and in which versions. You can try these APIs interactively, even those that require user authentication. So, suppose you’ve explored an API, and you’re ready to build an application that uses it. Do you have to start coding from scratch? No. Google provides Cloud Client libraries and Google API Client libraries in many popular languages to take a lot of the drudgery out of the task of calling Google Cloud from your code. Languages currently represented in these libraries are Java, Python, PHP, C#, Go, Node.js, Ruby, and C++. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app provides up-to-date billing information for your projects and billing alerts for projects that are going over budget. You can set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app also offers alerts and incident management. You can download the Google Cloud app at cloud.google.com/app.

#### Google Cloud Fundamentals: Getting Started with Cloud Marketplace

- https://www.cloudskillsboost.google/paths/14/course_templates/60/labs/531572

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531573

### Virtual Machines and Networks in the Cloud

#### Virtual Private Cloud networking

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531574

In this section of the course, we’re going to explore how Google Compute Engine works with a focus on virtual networking. Many users start with Google Cloud by defining their own virtual private cloud inside their first Google Cloud project or by starting with the default virtual private cloud. So, what is a virtual private cloud? A virtual private cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud – like Google Cloud! On a VPC, customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but this private cloud is hosted remotely by a public cloud provider. This means that VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing. VPC networks connect Google Cloud resources to each other and to the internet. This includes segmenting networks, using firewall rules to restrict access to instances, and creating static routes to forward traffic to specific destinations. Here's something that tends to surprise a lot of new Google Cloud users: Google VPC networks are global. They can also have subnets, which is a segmented piece of the larger network, in any Google Cloud region worldwide. Subnets can span the zones that make up a region. This architecture makes it easy to define network layouts with global scope. Resources can even be in different zones on the same subnet. The size of a subnet can be increased by expanding the range of IP addresses allocated to it, and doing so won’t affect virtual machines that are already configured. For example, let’s take a VPC network named vpc1 that has two subnets defined in the asia-east1 and us-east1 regions. If the VPC has three Compute Engine VMs attached to it, it means they’re neighbors on the same subnet even though they’re in different zones. This capability can be used to build solutions that are resilient to disruptions yet retain a simple network layout.

#### Compute Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531575

Earlier in the course, we explored infrastructure as a service, or IaaS. Now let’s explore Google Cloud’s IaaS solution: Compute Engine. With Compute Engine, users can create and run virtual machines on Google infrastructure. There are no upfront investments, and thousands of virtual CPUs can run on a system that’s designed to be fast and to offer consistent performance. Each virtual machine contains the power and functionality of a full-fledged operating system. This means a virtual machine can be configured much like a physical server: by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system. A virtual machine instance can be created via the Google Cloud console, which is a web-based tool to manage Google Cloud projects and resources, the Google Cloud CLI, or the Compute Engine API. The instance can run Linux and Windows Server images provided by Google or any customized versions of these images. You can also build and run images of other operating systems and flexibly reconfigure virtual machines. A quick way to get started with Google Cloud is through the Cloud Marketplace, which offers solutions from both Google and third-party vendors. With these solutions, there’s no need to manually configure the software, virtual machine instances, storage, or network settings, although many of them can be modified before launch if that’s required. Most software packages in Cloud Marketplace are available at no additional charge beyond the normal usage fees for Google Cloud resources. Some Cloud Marketplace images charge usage fees, particularly those published by third parties, with commercially licensed software, but they all show estimates of their monthly charges before they’re launched. At this point, you might be wondering about Compute Engine’s pricing and billing structure. For the use of virtual machines, Compute Engine bills by the second with a one-minute minimum, and sustained-use discounts start to apply automatically to virtual machines the longer they run. So, for each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every additional minute. Compute Engine also offers committed-use discounts. This means that for stable and predictable workloads, a specific amount of vCPUs and memory can be purchased for up to a 57% discount off of normal prices in return for committing to a usage term of one year or three years. And then there are Preemptible and Spot VMs. Let’s say you have a workload that doesn’t require a human to sit and wait for it to finish–such as a batch job analyzing a large dataset. You can save money, in some cases up to 90%, by choosing Preemptible or Spot VMs to run the job. A Preemptible or Spot VM is different from an ordinary Compute Engine VM in only one respect: Compute Engine has permission to terminate a job if its resources are needed elsewhere. Although savings are possible with preemptible or spot VMs, you'll need to ensure that your job can be stopped and restarted. Spot VMs differ from Preemptible VMs by offering more features. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. However, the pricing is, currently the same for both. In terms of storage, Compute Engine doesn’t require a particular option or machine type to get high throughput between processing and persistent disks. That’s the default, and it comes to you at no extra cost. And finally, you’ll only pay for what you need with custom machine types. Compute Engine lets you choose the machine properties of your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types or by creating your own custom machine types.

#### Scaling virtual machines

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531576

As we’ve just seen, with Compute Engine, you can choose the most appropriate machine properties for your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types, or by creating custom machine types. To do this, Compute Engine has a feature called Autoscaling, where VMs can be added to or subtracted from an application based on load metrics. The other part of making that work is balancing the incoming traffic among the VMs. Google’s Virtual Private Cloud (VPC) supports several different kinds of load balancing, which we’ll explore shortly. With Compute Engine, you can in fact configure very large VMs, which are great for workloads such as in-memory databases and CPU-intensive analytics, but most Google Cloud customers start off with scaling out, not up. The maximum number of CPUs per VM is tied to its “machine family” and is also constrained by the quota available to the user, which is zone-dependent. Specifications for currently available VM machine types can be found at cloud.google.com/compute/docs/machine-types

#### Important VPC compatibilities

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531577

Now let’s explore some of the most important Virtual Private Cloud compatibility features. Much like physical networks, VPCs have routing tables. VPC routing tables are built-in so you don’t have to provision or manage a router. They’re used to forward traffic from one instance to another within the same network, across subnetworks, or even between Google Cloud zones, without requiring an external IP address. Another thing you don’t have to provision or manage for Google Cloud is a firewall. VPCs provide a global distributed firewall, which can be controlled to restrict access to instances through both incoming and outgoing traffic. Firewall rules can be defined through network tags on Compute Engine instances, which is really convenient. For example, you can tag all your web servers with, say, “WEB,” and write a firewall rule saying that traffic on ports 80 or 443 is allowed into all VMs with the “WEB” tag, no matter what their IP address happens to be. You’ll remember that VPCs belong to Google Cloud projects, but what if your company has several Google Cloud projects, and the VPCs need to talk to each other? With VPC Peering, a relationship between two VPCs can be established to exchange traffic. Alternatively, to use the full power of Identity Access Management (IAM) to control who and what in one project can interact with a VPC in another, you can configu

#### Cloud Load Balancing

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531578

Previously, we explored how virtual machines can autoscale to respond to changing loads. But how do your customers get to your application when it might be provided by four VMs one moment, and by 40 VMs at another? That’s done through Cloud Load Balancing. The job of a load balancer is to distribute user traffic across multiple instances of an application. By spreading the load, load balancing reduces the risk that applications experience performance issues. Cloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. And because the load balancers don’t run in VMs that you have to manage, you don’t have to worry about scaling or managing them. You can put Cloud Load Balancing in front of all of your traffic: HTTP or HTTPS, other TCP and SSL traffic, and UDP traffic too. Cloud Load Balancing provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if backends become unhealthy. Cloud Load Balancing reacts quickly to changes in users, traffic, network, backend health, and other related conditions. And what if you anticipate a huge spike in demand? Say, your online game is already a hit; do you need to file a support ticket to warn Google of the incoming load? No. No so-called “pre-warming” is required. Google Cloud offers a range of load balancing solutions that can be classified based on the OSI model layer they operate at and their specific functionalities. Application Load Balancers operate at the application layer and are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content-based routing and SSL/TLS termination. Application Load Balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules you define. They are highly flexible and can be configured for both internet-facing (external) and internal applications. Network Load Balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types: Proxy Network Load Balancers also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments. Unlike proxy Network Load Balancers, passthrough Network Load Balancers do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well-suited for applications that require direct server return or need to handle a wider range of IP protocols.

#### Cloud DNS and Cloud CDN

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531579

One of the most famous free Google services is 8.8.8.8, which provides a public Domain Name Service to the world. DNS is what translates internet hostnames to addresses, and as you might imagine, Google has a highly developed DNS infrastructure. It makes 8.8.8.8 available so that everyone can take advantage of it. But what about the internet hostnames and addresses of applications built in Google Cloud? Google Cloud offers Cloud DNS to help the world find them. It’s a managed DNS service that runs on the same infrastructure as Google. It has low latency and high availability, and it’s a cost-effective way to make your applications and services available to your users. The DNS information you publish is served from redundant locations around the world. Cloud DNS is also programmable. You can publish and manage millions of DNS zones and records using the Cloud console, the command-line interface, or the API. Google also has a global system of edge caches. Edge caching refers to the use of caching servers to store content closer to end users. You can use this system to accelerate content delivery in your application by using Cloud CDN - Content Delivery Network. This means your customers will experience lower network latency, the origins of your content will experience reduced load, and you can even save money. After an Application Load Balancer is set up, Cloud CDN can be enabled with a single checkbox. There are many other CDNs available out there, of course. If you are already using one, chances are, it’s a part of Google Cloud’s CDN Interconnect partner program, and you can continue to use it.

#### Connecting networks to Google VPC

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531580

Many Google Cloud customers want to connect their Google Virtual Private Cloud networks to other networks in their system, such as on-premises networks or networks in other clouds. There are several effective ways to accomplish this. One option is to start with a Virtual Private Network connection over the internet and use Cloud VPN to create a “tunnel” connection. To make the connection dynamic, a Google Cloud feature called Cloud Router can be used. Cloud Router lets other networks and Google VPC, exchange route information over the VPN using the Border Gateway Protocol. Using this method, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But using the internet to connect networks isn't always the best option for everyone, either because of security concerns or because of bandwidth reliability. So, a second option is to consider “peering” with Google using Direct Peering. Peering means putting a router in the same public data center as a Google point of presence and using it to exchange traffic between networks. Google has more than 100 points of presence around the world. Customers who aren’t already in a point of presence can work with a partner in the Carrier Peering program to get connected. Carrier peering gives you direct access from your on-premises network through a service provider's network to Google Workspace and to Google Cloud products that can be exposed through one or more public IP addresses. One downside of peering, though, is that it isn’t covered by a Google Service Level Agreement. If getting the highest uptimes for interconnection is important, using Dedicated Interconnect would be a good solution. This option allows for one or more direct, private connections to Google. If these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%. Also, these connections can be backed up by a VPN for even greater reliability. Another option we’ll explore is Partner Interconnect, which provides connectivity between an on-premises network and a VPC network through a supported service provider. A Partner Interconnect connection is useful if a data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if the data needs don’t warrant an entire 10 GigaBytes per second connection. Depending on availability needs, Partner Interconnect can be configured to support mission-critical services or applications that can tolerate some downtime. As with Dedicated Interconnect, if these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%, but note that Google isn’t responsible for any aspects of Partner Interconnect provided by the third-party service provider, nor any issues outside of Google's network. And the final option is Cross-Cloud Interconnect. Cross-Cloud Interconnect helps you establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud network with your network that's hosted by a supported cloud service provider. Cross-Cloud Interconnect supports your adoption of an integrated multicloud strategy. In addition to supporting various cloud service providers, Cross-Cloud Interconnect offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps.

#### Getting Started with VPC Networking and Google Compute Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/60/labs/531581

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531582

### Storage in the Cloud

#### Google Cloud storage options

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531583

Every application needs to store data, like media to be streamed or perhaps even sensor data from devices, and different applications and workloads require different storage database solutions. Google Cloud has storage options for structured, unstructured, transactional, and relational data. In this section of the course, we’ll explore Google Cloud’s five core storage products: Cloud Storage, Cloud SQL, Spanner, Firestore, and Bigtable. Depending on your application, you might use one or several of these services to do the job.

#### Cloud Storage

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531584

Let’s begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as “objects” and not as a file and folder hierarchy (file storage), or as chunks of a disk (block storage). These objects are stored in a packaged format which contains the binary form of the actual data itself, as well as relevant associated meta-data (such as date created, author, resource type, and permissions), and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. Cloud Storage is Google’s object storage product. It allows customers to store any amount of data, and to retrieve it as often as needed. It’s a fully managed scalable service that has a wide variety of uses. A few examples include serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users via Direct Download. Cloud Storage’s primary use is whenever binary large-object storage (also known as a “BLOB”) is needed for online content such as videos and photos, for backup and archived data and for storage of intermediate results in processing workflows. Cloud Storage files are organized into buckets. A bucket needs a globally unique name and a specific geographic location for where it should be stored, and an ideal location for a bucket is where latency is minimized. For example, if most of your users are in Europe, you probably want to pick a European location, so either a specific Google Cloud region in Europe, or else the EU multi-region. The storage objects offered by Cloud Storage are immutable, which means that you do not edit them, but instead a new version is created with every change made. Administrators have the option to either allow each new version to completely overwrite the older one, or to keep track of each change made to a particular object by enabling “versioning” within a bucket. If you choose to use versioning, Cloud Storage will keep a detailed history of modifications -- that is, overwrites or deletes -- of all objects contained in that bucket. If you don’t turn on object versioning, by default new versions will always overwrite older versions. With object versioning enabled, you can list the archived versions of an object, restore an object to an older state, or permanently delete a version of an object, as needed. In many cases, personally identifiable information may be contained in data objects, so controlling access to stored data is essential to ensuring security and privacy are maintained. Using IAM roles and, where needed, access control lists (ACLs), organizations can conform to security best practices, which require each user to have access and permissions to only the resources they need to do their jobs, and no more than that. There are a couple of options to control user access to objects and buckets. For most purposes, IAM is sufficient. Roles are inherited from project to bucket to object. If you need finer control, you can create access control lists. Each access control list consists of two pieces of information. The first is a scope, which defines who can access and perform an action. This can be a specific user or group of users. The second is a permission, which defines what actions can be performed, like read or write. Because storing and retrieving large amounts of object data can quickly become expensive, Cloud Storage also offers lifecycle management policies. For example, you could tell Cloud Storage to delete objects older than 365 days; or to delete objects created before January 1, 2013; or to keep only the 3 most recent versions of each object in a bucket that has versioning enabled. Having this control ensures that you’re not paying for more than you actually need.

#### Cloud Storage: Storage classes and data transfer

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531585

There are four primary storage classes in Cloud Storage. The first is Standard Storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline Storage. This is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline Storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline Storage, Coldline Storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive Storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes has differences, it’s worth noting there are several characteristics that apply across all of these storage classes. These include: Unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience, which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data. Cloud Storage has no minimum fee because you pay only for what you use, and prior provisioning of capacity isn’t necessary. And from a security perspective, Cloud Storage always encrypts data on the server side, before it’s written to disk, at no additional charge. Data traveling between a customer’s device and Google is encrypted by default using HTTPS/TLS, which is Transport Layer Security. Regardless of which storage class you choose, there are several ways to bring data into Cloud Storage. Many customers simply carry out their own online transfer using gcloud storage, which is the Cloud Storage command from the Cloud SDK. Data can also be moved in by using a drag and drop option in the Cloud Console, if accessed through the Google Chrome web browser. But what if you have to upload terabytes or even petabytes of data? Storage Transfer Service enables you to import large amounts of online data into Cloud Storage quickly and cost-effectively. The Storage Transfer Service lets you schedule and manage batch transfers to Cloud Storage from another cloud provider, from a different Cloud Storage region, or from an HTTP(S) endpoint. And then there is the Transfer Appliance, which is a rackable, high-capacity storage server that you lease from Google Cloud. You connect it to your network, load it with data, and then ship it to an upload facility where the data is uploaded to Cloud Storage. You can transfer up to a petabyte of data on a single appliance. Cloud Storage’s tight integration with other Google Cloud products and services means that there are many additional ways to move data into the service. For example, you can import and export tables to and from both BigQuery and Cloud SQL. You can also store App Engine logs, Firestore backups, and objects used by App Engine applications, like images. Cloud Storage can also store instance startup scripts, Compute Engine images, and objects used by Compute Engine applications.

#### Cloud SQL

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531586

Google Cloud’s second core storage option is Cloud SQL. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It’s designed to hand off mundane, but necessary and often time-consuming, tasks to Google—like applying patches and updates managing backups, and configuring replications—so your focus can be on building great applications. Cloud SQL doesn't require any software installation or maintenance. It can scale up to 128 processor cores, 864 GB of RAM, and 64 TB of storage. It supports automatic replication scenarios, such as from a Cloud SQL primary instance, an external primary instance, and external MySQL instances. Cloud SQL supports managed backups, so backed-up data is securely stored and accessible if a restore is required. The cost of an instance covers seven backups. Cloud SQL encrypts customer data when on Google’s internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. A benefit of Cloud SQL instances is that they are accessible by other Google Cloud services, and even external services. Cloud SQL can be used with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. Compute Engine instances can be authorized to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might use, like SQL Workbench, Toad, and other external applications using standard MySQL drivers.

#### Spanner

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531587

The third core storage option offered by Google Cloud is Spanner. Spanner is a fully managed relational database service that scales horizontally, is strongly consistent, and speaks SQL. Battle tested by Google’s own mission-critical applications and services, Spanner is the service that powers Google’s $80 billion business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, strong global consistency, and high numbers of input and output operations per second. We’re talking tens of thousands of reads and writes per second or more.

#### Firestore

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531588

Google Cloud’s fourth core storage option is Firestore. Firestore is a flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development. With Firestore, data is stored in documents and then organized into collections. Documents can contain complex nested objects in addition to subcollections. Each document contains a set of key-value pairs. For example, a document to represent a user has the keys for the firstname and lastname with the associated values. Firestore’s NoSQL queries can then be used to retrieve individual, specific documents or to retrieve all the documents in a collection that match your query parameters. Queries can include multiple, chained filters and combine filtering and sorting options. They're also indexed by default, so query performance is proportional to the size of the result set, not the dataset. Firestore uses data synchronization to update data on any connected device. However, it's also designed to make simple, one-time fetch queries efficiently. It caches data that an app is actively using, so the app can write, read, listen to, and query data even if the device is offline. When the device comes back online, Firestore synchronizes any local changes back to Firestore. Firestore leverages Google Cloud’s powerful infrastructure: automatic multi-region data replication, strong consistency guarantees, atomic batch operations, and real transaction support.

#### Bigtable

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531589

The last of Google Cloud’s core storage options we’re going to explore is Bigtable. Bigtable is Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding which storage option is best, customers often choose Bigtable if: They’re working with more than 1TB of semi-structured or structured data. Data is fast with high throughput, or it’s rapidly changing. They’re working with NoSQL data. This usually means transactions where strong relational semantics are not required. Data is a time-series or has natural semantic ordering. They’re working with big data, running asynchronous batch or synchronous real-time processing on the data. Or they’re running machine learning algorithms on the data. Bigtable can interact with other Google Cloud services and third-party clients. Using APIs, data can be read from and written to Bigtable through a data service layer like Managed VMs, the HBase REST Server, or a Java Server using the HBase client. Typically this is used to serve data to applications, dashboards, and data services. Data can also be streamed in through a variety of popular stream processing frameworks like Dataflow Streaming, Spark Streaming, and Storm. And if streaming is not an option, data can also be read from and written to Bigtable through batch processes like Hadoop MapReduce, Dataflow, or Spark. Often, summarized or newly calculated data is written back to Bigtable or to a downstream database.

#### Comparing storage options

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531590

Now that we’ve covered Google Cloud’s core storage options, let’s do a comparison to help highlight the most suitable service for a specific application or workflow. Consider using Cloud Storage if you need to store immutable blobs larger than 10 megabytes, such as large images or movies. This storage service provides petabytes of capacity with a maximum unit size of 5 terabytes per object. Consider using Cloud SQL or Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides up to 64 terabytes, depending on machine type, and Spanner provides petabytes. Cloud SQL is best for web frameworks and existing applications, like storing user credentials and customer orders. If Cloud SQL doesn’t fit your requirements because you need horizontal scalability, not just through read replicas, consider using Spanner. Consider Firestore if you need massive scaling and predictability together with real time query results and offline query support. This storage service provides terabytes of capacity with a maximum unit size of 1 megabyte per entity. Firestore is best for storing, syncing, and querying data for mobile and web apps. Finally, consider using Bigtable if you need to store a large number of structured objects. Bigtable doesn’t support SQL queries, nor does it support multi-row transactions. This storage service provides petabytes of capacity with a maximum unit size of 10 megabytes per cell and 100 megabytes per row. Bigtable is best for analytical data with heavy read and write events, like AdTech, financial, or IoT data. Depending on your application, it’s possible that you might use one, or several, of these services to do the job. You may have noticed that BigQuery hasn’t been mentioned in this section of the course. This is because it sits on the edge between data storage and data processing, and is covered in more depth in other courses. The usual reason to store data in BigQuery is so you can use its big data analysis and interactive querying capabilities, but it’s not purely a data storage product.

#### Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL

- https://www.cloudskillsboost.google/paths/14/course_templates/60/labs/531591

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531592

### Containers in the Cloud

#### Introduction to containers

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531593

In this section of the course we’ll explore containers and help you understand how they are used. Infrastructure as a service, or IaaS, allows you to share compute resources with other developers by using virtual machines to virtualize the hardware. This lets each developer deploy their own operating system (OS), access the hardware, and build their applications in a self-contained environment with access to RAM, file systems, networking interfaces, etc. This is where containers come in. The idea of a container is to give the independent scalability of workloads in PaaS and an abstraction layer of the OS and hardware in IaaS. A configurable system lets you install your favorite runtime, web server, database, or middleware, configure the underlying system resources, such as disk space, disk I/O, or networking, and build as you like. But flexibility comes with a cost. The smallest unit of compute is an app with its VM. The guest OS might be large, even gigabytes in size, and take minutes to boot. As demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly. A container is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware. It only requires a few system calls to create and it starts as quickly as a process. All that’s needed on each host is an OS kernel that supports containers and a container runtime. In essence, the OS is being virtualized. It scales like PaaS but gives you nearly the same flexibility as IaaS. This makes code ultra portable, and the OS and hardware can be treated as a black box. So you can go from development, to staging, to production, or from your laptop to the cloud, without changing or rebuilding anything. As an example, let’s say you want to scale a web server. With a container, you can do this in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host. That's just a simple example of scaling one container running the whole application on a single host. However, you'll probably want to build your applications using lots of containers, each performing their own function like microservices. If you build them this way and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts. The hosts can scale up and down and start and stop containers as demand for your app changes or as hosts fail.

#### Kubernetes

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531594

A product that helps manage and scale containerized applications is Kubernetes. So to save time and effort when scaling applications and workloads, Kubernetes can be bootstrapped using Google Kubernetes Engine (GKE). So, what is Kubernetes? Kubernetes is an open-source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud which is a virtual machine running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Deploying containers on nodes by using a wrapper around one or more containers is what defines a Pod. A Pod is the smallest unit in Kubernetes that you can create or deploy. It represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per Pod, but if you have multiple containers with a hard dependency, you can package them into a single Pod and share networking and storage resources between them. The Pod provides a unique network IP and set of ports for your containers and configurable options that govern how your containers should run. One way to run a container in a Pod in Kubernetes is to use the kubectl run command, which starts a Deployment with a container running inside a Pod. A Deployment represents a group of replicas of the same Pod and keeps your Pods running even when the nodes they run on fail. A Deployment could represent a component of an application or even an entire app. To see a list of the running Pods in your project, run the command: $ kubectl get pods. Kubernetes creates a Service with a fixed IP address for your Pods, and a controller says "I need to attach an external load balancer with a public IP address to that Service so others outside the cluster can access it." In GKE, the load balancer is created as a network load balancer. Any client that reaches that IP address will be routed to a Pod behind the Service. A Service is an abstraction which defines a logical set of Pods and a policy by which to access them. As Deployments create and destroy Pods, Pods will be assigned their own IP addresses, but those addresses don't remain stable over time. A Service group is a set of Pods and provides a stable endpoint (or fixed IP address) for them. For example, if you create two sets of Pods called frontend and backend and put them behind their own Services, the backend Pods might change, but frontend Pods are not aware of this. They simply refer to the backend Service. To scale a Deployment, run the kubectl scale command. In this example, three Pods are created in your Deployment, and they're placed behind the Service and share one fixed IP address. You could also use autoscaling with other kinds of parameters. For example, you can specify that the number of Pods should increase when CPU utilization reaches a certain limit. So far, we’ve seen how to run imperative commands like expose and scale. This works well to learn and test Kubernetes step-by-step. But the real strength of Kubernetes comes when you work in a declarative way. Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like, and Kubernetes determines how to do it. You accomplish this by using a Deployment config file. You can check your Deployment to make sure the proper number of replicas is running by using either kubectl get deployments or kubectl describe deployments. To run five replicas instead of three, all you do is update the Deployment config file and run the kubectl apply command to use the updated config file. You can still reach your endpoint as before by using kubectl get services to get the external IP of the Service and reach the public IP address from a client. The last question is, what happens when you want to update a new version of your app? Well, you want to update your container to get new code in front of users, but rolling out all those changes at one time would be risky. So in this case, you would use kubectl rollout or change your deployment configuration file and then apply the change using kubectl apply. New Pods will then be created according to your new update strategy. Here’s an example configuration that will create new version Pods individually and wait for a new Pod to be available before destroying one of the old Pods.

#### Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531595

So now that we have a basic understanding of containers and Kubernetes, let’s talk about Google Kubernetes Engine, or GKE. GKE is a Google-hosted managed Kubernetes service in the cloud. The GKE environment consists of multiple machines, specifically Compute Engine instances, grouped together to form a cluster. You can create a Kubernetes cluster with Kubernetes Engine, but how is GKE different from Kubernetes? From the user’s perspective, it’s a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes responsibility for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need of a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes. Let’s examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. Autopilot also helps produce a strong security posture. And Autopilot also promotes operational efficiency. The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode. You can create a Kubernetes cluster with Kubernetes Engine by using the Google Cloud console or the gcloud command that's provided by the Cloud software development kit. GKE clusters can be customized, and they support different machine types, number of nodes, and network settings. Kubernetes provides the mechanisms through which you interact with your cluster. Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks, set policies, and monitor the health of deployed workloads. Running a GKE cluster comes with the benefit of advanced cluster management features that Google Cloud provides. These include: Google Cloud's load-balancing for Compute Engine instances, Node pools to designate subsets of nodes within a cluster for additional flexibility, Automatic scaling of your cluster's node instance count, Automatic upgrades for your cluster's node software, Node auto-repair to maintain node health and availability, And logging and monitoring with Google Cloud Observability for visibility into your cluster. To start up Kubernetes on a cluster in GKE, all you do is run this command: $> gcloud container clusters create k1

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531596

### Applications in the Cloud

#### Cloud Run

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531597

So far in this course, we’ve provided an introduction to Google Cloud and explored the options and benefits related to using virtual machines, networks, storage, and containers in the Cloud. In the final section of the course, we’ll turn our attention to developing applications in the Cloud. We’ll begin with Cloud Run, which is a managed compute platform that runs stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It’s built on Knative, an open API and runtime environment built on Kubernetes. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you‘ll never pay for over-provisioned resources. The Cloud Run developer workflow is a straightforward three-step process. First, you write your application using your favorite programming language. This application should start a server that listens for web requests. Second, you build and package your application into a container image. And third, the container image is pushed to Artifact Registry, where Cloud Run will deploy it. Once you’ve deployed your container image, you’ll get a unique HTTPS URL back. Cloud Run then starts your container on demand to handle requests, and ensures that all incoming requests are handled by dynamically adding and removing containers. Because Cloud Run is serverless, it means that you, as a developer, can focus on building your application and not on building and maintaining the infrastructure that powers it. For some use cases, a container-based workflow is great, because it gives you a great amount of transparency and flexibility. Sometimes, you’re just looking for a way to turn source code into an HTTPS endpoint, and you want your vendor to make sure your container image is secure, well-configured and built in a consistent way. With Cloud Run, you can do both. You can use a container-based workflow, as well as a source-based workflow. The source-based approach will deploy source code instead of a container image. Cloud Run then builds the source and packages the application into a container image. Cloud Run does this using Buildpacks - an open source project. Cloud Run handles HTTPS serving for you. That means you only have to worry about handling web requests, and you can let Cloud Run take care of adding the encryption. The pricing model on Cloud Run is unique; as you only pay for the system resources you use while a container is handling web requests, with a granularity of 100ms, and when it’s starting or shutting down. You don’t pay for anything if your container doesn’t handle requests. Additionally, there is a small fee for every one million requests you serve. The price of container time increases with CPU and memory. A container with more vCPU and memory is more expensive. You can use Cloud Run to run any binary, as long as it’s compiled for Linux sixty-four bit. Now, this means you can use Cloud Run to run web applications written using popular languages, such as: Java, Python, Node.js, PHP, Go, and C++. You can also run code written in less popular languages, such as: Cobol, Haskell, and Perl. As long as your app handles web requests, you’re good to go.

#### Development in the cloud

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531598

Many applications contain event-driven parts. For example, an application that lets users upload images. When that event takes place, the image might need to be processed in a few different ways, like converting the image to a standard format, converting a thumbnail into different sizes, and storing each new file in a repository. This function could be integrated into the application, but then you’d have to provide compute resources for it–whether it happens once a millisecond or once a day. With Cloud Run functions, you write a single-purpose function that completes the necessary image manipulations and then arrange for it to automatically run whenever a new image is uploaded. Cloud Run functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events, without the need to manage a server or a runtime environment. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also connect and extend cloud services. You’re billed to the nearest 100 milliseconds, but only while your code is running. Cloud Run functions supports writing source code in a number of programming languages. These include Node.js, Python, Go, Java, . Net Core, Ruby, and PHP. For more information about the supported specific version, refer to the runtimes documentation. Events from Cloud Storage and Pub/Sub can trigger Cloud Run functions asynchronously, or you can use HTTP invocation for synchronous execution.

#### Hello Cloud Run

- https://www.cloudskillsboost.google/paths/14/course_templates/60/labs/531599

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531600

### Prompt Engineering

#### Prompt Engineering

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531601

Generative AI and large language models are proving to be powerful tools, but to leverage their capabilities, it's important to understand their architecture. It's also important to consider recommended practices when implementing these technologies. The goal of this module, titled Google Cloud: Prompt Engineer Guide, is to help with these important steps. In this guide to prompt engineering, you’ll get answers to the questions: What is generative AI? What is a large language model? What is prompt engineering? You’ll also explore prompt engineering best practices. Before we delve into this lesson, let's define the interchangeably used terms such as 'generative AI' and 'Large Language Model' (LLM). While both terms describe AI models capable of generating human-like responses based on input prompts in many references, it's important to note they're not identical. Generative AI encompasses a broader range of models capable of generating various types of content beyond just text, while LLM specifically refers to a subset of generative AI models focusing on language tasks. We'll thoroughly explore each term in this lesson. So let's begin with an important question: What is generative AI? Generative artificial intelligence, which is commonly referred to as gen AI, is a subset of artificial intelligence that is capable of creating text, images, or other data using generative models, often in response to prompts. It has grown in popularity hugely since 2021 but artificial intelligence has been around since the mid 1950s. By the way, a prompt is a specific instruction, question, or cue given to a computer program or user to initiate a specific action or response, but we examine this more later. In its current format, gen AI models are like conversational programs that can generate content based on the inputs supplied. Gen AI models learn the patterns and structure from input training data and then create new data with similar characteristics. Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, and sales. However, rather than exploring all generative AI applications, this training will specifically focus on articulating prompts to harness the power of gen AI effectively. Let me introduce a scenario that we’ll make reference to throughout this section of the training. We’ve included it to help put some of this theory into practice. Meet Sasha, a cloud architect, who needs to create a prototype design of Google Cloud VPC network architecture for Cymbal Bank. Sasha wants to save time by combining her existing knowledge of cloud architecture and generative AI tools to create a usable prototype design. Sasha was excited to learn about Gemini, since having the tool inside the Google Cloud Console means that she can access it without any additional installs. We’ll check back in with Sasha later. Let’s spend some time exploring large language models, which are a highly sophisticated computer programs trained on gigantic amounts of data that can be text or images. But how are they trained? And why do they need training at all? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. In this context, large refers to: The size of the training dataset, which can sometimes be at the petabyte scale. And the number of parameters. Parameters are the memories and knowledge that the machine has learned during model training. They determine the ability of a model to solve a problem, such as predicting text, and can reach billions or even trillions in size. General-purpose means that the models can sufficiently solve common problems. This is thanks to the commonality of a human language, regardless of the specific tasks. Saying LLMs are pre-trained and fine-tuned, means… …that they have been pre-trained for a general purpose with a large dataset… ...and then fine-tuned for specific goals with a much smaller dataset. But how are LLMs trained? When you submit a prompt to an LLM, it calculates the probability of the correct answer from its pre-trained model. The probability is determined through a task called pre-training. Pre-training an LLM involves feeding a massive dataset of text, images, and code to the model so that it can learn the underlying structure and patterns of the language. This process helps the model to understand and generate human language more effectively. In this way, the LLM works like a fancy autocomplete, suggesting the most common correct response to the prompt. But sometimes the LLM gives a completely wrong answer. This is called a hallucination. Hallucinations are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. This happens because LLMs can only understand the information they were trained on. This means that they might not be aware of your business's proprietary or domain-specific data. Also, they do not have access to real-time information. To make matters worse, LLMs only understand the information that is explicitly given to them in the prompt. In other words, they often assume that the prompt is true. They also do not have the ability to ask for more context information. Ultimately, an LLM does not know anything outside of what it was trained on, and it cannot truly know if that information is accurate. But what causes a hallucination. Hallucinations can be caused by a number of factors, including: The model is not trained on enough data. The model is trained on noisy or dirty data. The model is not given enough context. The model is not given enough constraints. Hallucinations can be a problem for LLMs because they can make the output text difficult to understand. They can also make the model more likely to generate incorrect or misleading information. But we will see in the Prompt Engineering section that there are things we can do to minimize this problem. OK, but knowing where the sun is will not help Sasha with her current task. Lucky for Sasha, Google Cloud offers a generative AI model called Gemini, [[Pause here for 5 seconds]] which can act as an always-on collaborator. This gen AI-powered assistant can help a wide range of Google Cloud users, including developers, data scientists, and operators. To provide an integrated assistance experience, Gemini is embedded in many Google Cloud products. Gemini has access to a massive range of data, including Google Cloud documentation, tutorials, and samples. With the right prompts, it can produce detailed suggestions and guides on what resources will best suit Sasha’s current challenge and their configuration. Gemini can even create detailed gcloud commands and insert them into Cloud Shell for her. She just needs to articulate her needs in a way that gets the best response from Gemini. For example, if she uses the prompt “How can I create a network that uses IPv4 and IPv6 addresses?”, she will get a response that details how to do just that. You’ve learned that a large language model is a huge object model containing a massive dataset of text. But how can you extract the information you need from this dataset? This is where prompt engineering comes in. A prompt is the text that you feed to the model, and prompt engineering is a way of articulating your prompts to get the best response from the model. The better structured a prompt is, the better the output from the model will be. Let’s explore what this means. Prompts can be in the form of a question, and are categorized into four categories: zero-shot, one-shot, few-shot, and role prompts. Zero-shot prompts do not contain any context or examples to assist the model. For example, the prompt “What’s the capital of France?” does not provide any examples of what a capital is. Clearly, that is not too important for this example. But for more specific and technical prompts, an example would help refine the scope of the response from Gemini. One-shot prompts, however, provide one example to the model for context. Here, we ask for the capital of France again, but we provide Italy and Rome as an example. And few-shot prompts provide at least two examples to the model for context. Here, our prompt is updated to also include Japan and Tokyo in our examples. And then, there are role prompts which require a frame of reference for the model to work from as it answers the questions. In our example, we state “I want you to act as a business professor. I’ll give you a term, and you will correctly explain its meaning. Make sure your answers are always right. What is ROI? “ For Sasha’s needs, using role prompts might be the best solution. She can define what is required and in what context. This means that the LLM will have a clear point of reference when supplying an answer. Now that you’ve seen the types of prompts you can create, let’s explore the two elements of a prompt: the preamble and the input. The preamble refers to the introductory text you provide to give the model context and instructions before your main question or request. Think of it as setting the stage for the LLM to better understand what you want. It can include the context for the task, the task itself, and some examples to guide the model. The input is the central request you're making to the LLM. It’s what the instruction or task will act upon, for example “Comment: I don’t know what to think about the video. The review is:” Based on the preamble, Gemini reviews the input and suggests if the review is positive, neutral, or negative. It is worth noting that not all the components are required for a prompt, and the format can change depending on the task at hand. The element order can also change. Let's amend Sasha’s original prompt “How can I create a network that uses IPv4 and IPv6 addresses?” and add a role context to the input fed into Gemini. She also adds the detail of needing a dual stack subnet. The new prompt is “I want you to act as a cloud architect in Google Cloud. How can I use gcloud to create a network that uses IPv4 and IPv6 subnets?” But since Gemini maintains its own interaction context, she could have just asked “I want you to act as a cloud architect in Google Cloud. How can I adjust the gcloud command provided to create a subnet to ensure the subnet is dual stack?” Now that you’ve had a chance to explore what Gen AI is, what large language models are and how they’re trained, and what prompt engineering is, it’s time to explore some prompt engineering best practices. The first best practice is to write detailed and explicit instructions. The more vague the prompt, the more chance that the model will produce a result that is not usable. Be clear and concise in the prompts that you feed into the model. Next, be sure to define boundaries for the prompt. It’s better to instruct the model on what to do rather than what not to do. If the model gets stuck, give it a few 'fallback' outputs that work in various situations. For example, something like "I'm still learning about that" to use when unsure. Another best practice is to adopt a persona for your input. Adding a persona for the model can provide meaningful context to help it focus on related questions, which can help improve accuracy. This prompt would help Sasha, the cloud architect, get started with prototyping a network architecture for Cymbal Bank. And finally, it’s a recommended practice to keep each sentence concise. Longer sentences can sometimes produce suboptimal results. It’s best to break long sentences in a prompt into a series of shorter sentences and simpler tasks. So, let’s return to Sasha, and use what we have learned so far. Sasha updates her prompt to: “You're a cloud architect. You want to build a Google Cloud VPC network that can be centrally managed. You also connect to other VPC networks in your company's other regions. You don't want to have many different sets of firewall policies to maintain. What sort of network architecture would you recommend?” With this new prompt, Gemini proposes a hub-and-spoke network architecture, which fits Sasha’s needs exactly. By refining and amending her prompts, Sasha has articulated her requirements in a way that Gemini can respond with the correct focus and level of detail.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/60/quizzes/531602

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/14/course_templates/60/video/531603

Congratulations on completing the Google Cloud Fundamentals: Core Infrastructure training course. Before you go, let’s take a few minutes to review what we’ve covered. In module 1, you were introduced to Google Cloud and cloud computing. Specifically, you explored: The concept of managed infrastructure and managed services, through IaaS, infrastructure as a service, and PaaS, platform as a service. The Google Cloud network. Google Cloud’s focus on security throughout our infrastructure. How Google publishes key elements of technology using open source licenses. And Google Cloud’s pricing structure and billing tools. In module 2, you learned about the Google Cloud Resource Hierarchy, which is made up of four levels: resources, projects, folders, and an organization node. You also learned about: Defining policies and their downward inheritance. When to use Identity and Access Management, or IAM, And the four ways to access and interact with Google Cloud: through the Google Cloud console, the Cloud SDK and Cloud Shell, APIs, and the Google Cloud App. In module 3, you explored how Compute Engine works, with a focus on virtual machines and virtual networking. You were introduced to: The VPC, or virtual private cloud. Compute Engine’s Autoscaling feature. And important Google Virtual Private Cloud compatibility features, like routing tables, firewalls, VPC peering, and shared VPC, all of which result in the need for less network management. You also explored Cloud Load Balancing, a fully distributed, software-defined, managed service for all your traffic. Finally, you compared how on-premises or other-cloud networks can be interconnected with a Google VPC. In module 4, you explored Google Cloud's five core storage options: Cloud Storage, Bigtable, Cloud SQL, Spanner, and Firestore. You also examined the four storage classes that make up Cloud Storage: Standard Storage, which is used for frequently accessed hot data, Nearline Storage and Coldline Storage, which are used for less-frequently accessed cool data, and Archive Storage. In module 5, you learned about containers, which are invisible boxes around your code and its dependencies. You were introduced to containers, along with: Kubernetes, an open-source platform for managing containerized workloads and services. And Google Kubernetes Engine (GKE), a Google-hosted managed Kubernetes service in the cloud. In module 6, the focus was on developing applications in the cloud. You explored: Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. And Cloud Run functions, a lightweight, event-based, asynchronous compute solution to create single-purpose functions. Finally, in module 7, you explored how to combine Google Cloud knowledge with prompt engineering to improve Gemini responses. You discovered answers to the following questions: What is generative AI? What is a large language model? And what is prompt engineering? You ended the module by identifying prompt engineering best practices. We hope that this course is just the beginning of your Google Cloud journey. For more training and hands-on practice, explore the different learning paths available at cloud.google.com/training. And if you’re interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you might consider working toward a Google Cloud certification. You can learn more about Google Cloud’s certification offerings at cloud.google.com/certification. Thanks for completing this course. We’ll see you next time!

#### Course resources

- https://www.cloudskillsboost.google/paths/14/course_templates/60/documents/531604

### Your Next Steps

## 03: Networking in Google Cloud: Fundamentals

- https://www.cloudskillsboost.google/paths/14/course_templates/35

### Welcome to Networking in Google Cloud

#### Networking in Google Cloud Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568656

Google Cloud makes it easy to manage, scale, and secure your networks. In the networking and Google Cloud course series, learn how to implement load balancing and content delivery, and optimize your network for performance and costs. Hi, I'm Barry, a certified trainer at Google, and today I'm here to teach you about the wide variety of networking options and design patterns on Google Cloud. In this series of courses, you'll explore and deploy Google Cloud networking technologies, including VPC networks, subnets, and Firewalls. You'll learn about load balancing, cloud DNS, cloud CDN, and cloud NAT. This series of courses is designed for network engineers and administrators who are either using or planning to use Google Cloud and any other individuals who are interested in software-defined networking solutions in the cloud. Before enrolling in this series, it's recommended that you complete Google Cloud fundamentals core infrastructure, and the networking fundamentals in Google Cloud quests. This series also requires you to have prior understanding of the seven layer OSI model, IPv4 addressing, and prior experience managing IPv4 routes. Through a series of courses featuring videos, quizzes, and hands-on labs, you'll gain the knowledge you need to configure, maintain, and troubleshoot network components. You'll configure Google VPC network, subnets, and routers, control administrative access to VPC objects and network access to endpoints in VPCs. In addition, you'll learn to interconnect networks among Google Cloud projects, interconnect networks among Google Cloud VPC networks, and on-premises or other cloud networks. Configure traffic management among load balancer backend services, use cloud CDN to reduce latency and save money, optimize network spend using network tiers. Configure private connection options to provide access to external resources and services from internal networks. Design networks to meet common customer requirements, and configure monitoring and logging to troubleshoot network problems. Ready? Let's get started.

### VPC Networking Fundamentals

#### VPC networks

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568657

A virtual Private Cloud VPC network is a virtual version of a physical network that provides connectivity for your Compute Engine virtual machine VM instances, including Google Kubernetes Engine GKE clusters, App Engine Flexible environment instances, and other Google Cloud products built on Compute Engine VMs. Offers built-in internal pass-through network load balancers and proxy systems for internal application load balancers. A VPC network connects to on-premises networks by using Cloud VPN tunnels and Cloud interconnect attachments. It distributes traffic from Google Cloud external load balancers to back ends. By default, every network has routes that let instances in a network send traffic directly to each other, even across subnets. In addition, every network has a default route that directs packets to destinations that are outside the network. Although these routes cover most of your normal routing needs, you can also create special routes that override these routes. Just creating a route does not ensure that your packets will be received by the specified next hop. Firewall rules must also allow the packet. The default network has preconfigured firewall rules that allow all instances in the network to talk with each other. Manually created networks do not have such rules, so you must create them. Projects can contain multiple VPC networks. Unless you create an organizational policy that prohibits it, new projects start with a default network, and auto mode VPC network that has one subnetwork, subnet in each region. An auto-mode VPC network can be useful when you start learning about Google Cloud. However, it's a best practice to create a custom mode network and include subnetworks only in desired regions.

#### Multiple network interfaces

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568658

In conventional networking, devices can use multiple network interfaces to communicate with multiple networks. Next, let's discuss using multiple network interfaces in a Google Cloud VPC network. Sasha is a network engineer at Symbol Corporation. Symbol is rolling out a new web-based customer portal. This portal will give access to sensitive account data and the capability to initiate transactions. The environment also has a management network that connects to the on-premises environment using a VPN. For compliance and security purposes, Sasha is looking to filter control plane traffic from data plane traffic. She is thinking of using a security virtual appliance to route traffic between VPC networks to an on-premises environment and to the Internet. This means she needs a mechanism to interconnect multiple networks to a single virtual appliance. Let's look into how we can solve this use case. VPC networks are isolated private networking domains by default. As we mentioned earlier, VM instances within a VPC network can communicate among themselves by using internal IP addresses as long as firewall rules allow it. However, no internal IP address communication is allowed between networks unless you set up mechanisms such as VPC peering or VPN. Every VM instance within a VPC network starts with a built-in default network interface. When you add more interfaces, you must choose a VPC network and a subnet within it for each new interface. Importantly, each additional interface has to connect to a different VPC network than the others. This multi-interface setup lets you establish configurations where a single instance directly connects to multiple VPC networks. Use multiple network interfaces when you need a single instance to act as a network appliance for tasks like load balancing, intrusion detection, prevention, IDS, IPS, and web application firewalls, WAF. In the diagram, VM1 communicates with VM2 through an internal IP address. VM1 communicates with VM3 through an external IP address. VM appliance communicates with VM1 and VM3 through NIC. Multiple network interfaces let you create configurations in which an instance connects directly to several VPC networks. Each of the interfaces must have an internal IP address and each interface can also have an external IP address. For example, in this diagram, you have two VM instances. Each instance has network interfaces to a subnet within VPC1, VPC2, VPC3, and VPC4. For some situations, you might require multiple interfaces. For example, to configure an instance as a network appliance for load balancing. Multiple network interfaces are also useful when applications running in an instance require traffic separation such as separation of data plane traffic from management plane traffic. Also, you cannot add or remove NIC to an instance once the instance is created so make sure to add the required NIC when you create the instance. Going back to the use case, the simplest way Sasha can connect multiple networks to a VM appliance is by using multiple network interfaces. This diagram shows multiple VPC networks connecting through a virtual appliance using multiple network interfaces. Each interface connects to one of the VPC networks. The diagram also shows Internet and on-premises connections over separate network interfaces including an Internet connection through an untrusted interface. By configuring multiple interfaces, you can apply separate firewall rules and access controls to each interface separately and enforce security functions in communications from the public to a private domain. When creating VM instances with multiple network interfaces, note these caveats. You can only configure a network interface when you create an instance. Each network interface configured in a single instance must be attached to a different VPC network. Each interface must belong to a subnet whose IP range does not overlap with the subnets of any other interfaces. The additional VPC networks that the multiple interfaces will attach to must exist before you create the instance. You cannot delete a network interface without deleting the instance. When an internal DNS, domain name system, query is made with the instance host name, it resolves to the primary interface nic0 of the instance. If the nic0 interface of the instance belongs to a different VPC network than the instance that issues the internal DNS query, the query will fail. You will explore this in the upcoming lab. The maximum number of network interfaces per instance is eight, but this depends on the instances machine type as shown in this table. Instances with less than are equal to two VCPU can have up to two virtual NICs. Examples include the f1-micro, g1-small, n1- standard-1, and any other custom VMs with one or two VCPUs. Instances with more than two VCPU can have one NIC per VCPU with a maximum of eight virtual NICs.

#### Lab Intro: Working with Multiple VPC Networks

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568659

In this lab, you create several VPC networks and VM instances and test connectivity across networks. The lab tasks are to: create custom mode VPC networks with firewall rules, create VM instances by using Compute Engine, explore the connectivity for VM instances across VPC networks, create a VM instance with multiple network interfaces.

#### Working with multiple VPC networks

- https://www.cloudskillsboost.google/paths/14/course_templates/35/labs/568660

#### Network Service Tiers

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568661

Next, let's talk about how network service tiers help boost your network performance. To illustrate this, consider the following scenario. Sarah, a network engineer at Symbol Corporation, is facing a challenge. Their video streaming platform is experiencing high latency and inconsistent performance, especially during peak hours. The current network infrastructure struggles to keep up with the growing number of global users. The user request traverses the Internet. Finally, the request arrives at the load balancer in Google Cloud. Google Cloud cannot control the user experience on the Internet, so the provider has no way to deliver low latency and great user experience. What can Sarah do to reduce latency and optimize performance? Network service tiers enable Sarah to optimize your Cloud network for performance by choosing premium tier or for cost with the new standard tier. What is the difference between these two tiers? The premium tier delivers traffic on Google's global network, providing high performance routing. If Sarah uses Google Cloud today, Sarah already uses the powerful premium tier. The standard tier alternatively offers an attractively priced network with a performance comparable to that of other major public Clouds. There are other differences between the two tiers. The SLA is 99.99% for premium and 99.9% for standard tier. Premium tier also supports regional and global external IPv4 addresses and standard tier supports regional external IPv4 addresses. Premium also allows for Google Cloud networking features and the standard tier provides a wide variety of foundational feature sets such as Cloud NAT, external application load balancer, and external pass through network load balancer. Why would Sarah choose the standard tier? Well, it all comes back to optimizing the Cloud network for performance by choosing premium tier or for cost with the standard tier. In other words, network service tiers allows Sarah to design the Cloud network her way. Let's explore each feature in network service tiers to better understand network performance and cost differences. Refer to the documentation for a detailed list. Premium tier delivers traffic on Google's premium backbone, while standard tier uses regular ISP networks. As you can see on this map, this network consists of an extensive global private fiber network with over 100 points of presence across the globe. Let's explore each network service tier to better understand network performance and cost differences. This information is subject to change. For more accurate and current details, please visit the link. See www.gcpn.com for information on latency displayed by region. In premium tier, inbound traffic from the end user to the video streaming application in Google Cloud enters Google's private high performance network at the pop closest to your end user, and Google Cloud delivers this traffic to your application over this network. Similarly, Google Cloud delivers outbound traffic from the application to end users on Google's network and exits at the pop closest to them, wherever the end users are across the globe. This means that most of this traffic will reach its destination with a single hop to the end users ISP, so it enjoys minimum congestion and maximum performance. Sarah could also use standard tier. Standard tier provides network quality that is comparable to other public Cloud providers, but lower than premium tier. Also, regional network services such as regional load balancing have one VIP per region. Standard tier is priced lower than premium because your traffic between Google Cloud and your end user is delivered over ISP networks instead of Google's Network. Now that you understand the differences in performance, let's get into cost. Premium tier pricing is based on both source and destination of traffic. This is because the cost of network traffic varies depending on the distance your traffic travels over Google's network. In contrast, standard tier traffic is source-based because it does not travel much over Google's network. This map illustrates that network service tiers categorizes all countries and continents into the listed geolocations. Depending on the origin and destination of traffic, costs will vary. We have gone over both the performance and cost differences between network service tiers. The decision tree will help Sarah decide on the tier that best meets the organization needs. For your specific workload or resource, what holds higher priority? Exceptional performance or cost optimization? The premium tier is the clear choice for performance. If cost is the main consideration, remember that the standard tier has other restrictions in addition to network performance. If you want to deploy your back ends, have users in multiple regions, but don't want to use the public Internet instead of Google's network for intercontinental and cross-regional traffic, you want to choose the premium tier. Also, if you want global load balancing or Cloud CDN, you need to use the premium tier. The standard tier is a great choice if you don't need any of those services and are okay using the public Internet instead of Google's network.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568662

If you're still not sure where to start and how, you can always lean on Gemini, an AI-powered collaborator in Google Cloud for some help. Wondering how? Ensure that Gemini is set up for your Google Cloud user account and project, and then you are all set to simply chat with Gemini to get help. Using the Gemini pin, you enter prompts, which are questions or statements that describe the help you want, and Gemini returns responses. In the Google Cloud console toolbar, click chat_spark, open Gemini, and type, 'how can I use gcloud to create my first VPC network in Google Cloud?' This response will cover instructions to create a simple VPC and a subnet. You can adjust to ask advanced steps by refining the prompt. For example try, 'how can I adjust the gcloud command provided to create a subnet to ensure the subnet is dual stack?' In this module, you learned about some fundamental Google Cloud VPC networking concepts. We began with an overview of Google Cloud VPC networks. Then we discussed using multiple network interfaces on compute engine VMs, as well as some important caveats. After that, we discussed network service tiers options, and a use case. We concluded the module with a lab exercise and a brief quiz to test your knowledge of what you've learned. Thank you.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/35/quizzes/568663

### Sharing VPC Networks

#### VPC Network Peering

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568664

Welcome to the Sharing VPC Networks module of the Networking in Google Cloud course. This covers more details on sharing a VPC network. You will learn how to connect isolated Virtual Private Clouds (VPCs) using VPC Network Peering, enable centralized network management with Shared VPC, and migrate virtual machines (VMs) between networks. Through hands-on labs and practical examples, you'll gain a understanding of these essential networking concepts, ensuring optimal performance, security, and resource utilization. Let us start with a usecase. Lucian, a network engineer at Cymbal Corporation, faces a new challenge: seamlessly connecting and establishing a direct connection between their two organizations by consumer and producer. Public internet peering introduces unacceptable latency and security risks, while dedicated VPNs are proving cost-prohibitive. Lucian needs a solution that's private, secure, and scales with their evolving needs. Lucian opts to use VPC Network Peering. VPC Network Peering allows private connectivity across two VPC networks. Even if both VPC networks belong to the same project or the same organization, you can still peer them. Remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the peered networks. For example, in this diagram, two organizations represent a consumer and a producer, respectively. Each organization has its own organization node, VPC network, VM instances, Network Admin, and Instance Admin. To successfully establish VPC Network Peering, two peering relationships must be created. The producer network administrator must peer the producer network with the consumer network. The consumer network administrator must peer the consumer network with the producer network. When both peering connections are created, the VPC Network Peering session becomes active and routes are exchanged. The peering relationships let the VM instance use their internal IP addresses to communicate privately. VPC Network Peering is a decentralized or distributed approach to multi-project networking. Each VPC network may remain in the control of separate administrator groups and maintains its own global firewall and routing tables. Historically, such projects would consider external IP addresses or VPNs to facilitate private communication between VPC networks. However, VPC Network Peering does not incur the network latency, security, and cost drawbacks that are present when you use external IP addresses or VPNs. Let’s talk about a few points to remember when using VPC Network Peering. First of all, VPC Network Peering works with Compute Engine, Google Kubernetes Engine, and App Engine flexible environments. You need to have the network administrator role to perform peering. Peered VPC networks remain administratively separate. In other words, routes, firewalls, VPNs, and other traffic management tools are administered and applied separately in each of the VPC networks. These tools are not managed centrally for all network peers. Each side of a VPC Network Peering association is set up independently. Peering will be active only when the configuration from both sides matches. This arrangement allows either side to delete the peering association at any time. A subnet CIDR prefix in one peered VPC network cannot overlap with a subnet CIDR prefix in another peered network. For example, two auto mode VPC networks that only have the default subnets cannot peer. Google Cloud returns an error if any of the following operations result in an overlap. Only directly peered networks can communicate, which means that transitive peering is not supported. Let me explain this with an example. Suppose VPC network CymbalBank1 is peered with CymbalBank2 and CymbalBank3. In that scenario, CymbalBank2 and CymbalBank3 are not directly connected. VPC network CymbalBank2 cannot communicate with VPC network CymbalBank3 over a peered connection. If CymbalBank1 offered SaaS services to CymbalBank2 and CymbalBank3, this situation could be critical. Next, let's explore the steps involved in initiating a VPC peering connection. Before you begin, you must have the name of the VPC network to which you will peer with. If that VPC network is located in another project, you must also have the project ID and network name. A peering configuration establishes the intent to connect to another VPC network. The VPC networks are not connected until each one has a peering configuration for the other. After the other network has a corresponding configuration to peer with your network, the peering state changes to active in both networks. At that point, the VPC networks are connected through VPC Network Peering. After peering is established, the two networks automatically exchange subnet routes. The peering state remains inactive. An inactive peering state indicates that there is not yet a full VPC Network Peering connection. Suppose you want to peer the two VPC networks shown on the slide. Let’s begin with the Cymbal-Rahway VPC network. You must know the name of the other VPC network, which is Cymbal-Kampala. Because Cymbal-Kampala is not in the same project as Cymbal-Rahway, you must also have the project ID. Cymbal-Kampala is in the Cymbal-bank2 project, whose project ID is cymbal-10101. Custom routes help achieve a desired granularity that cannot be achieved with a default route. Sharing custom routes and dynamic routes with peered VPC networks let networks learn routes directly from their peered networks. For example, if a custom route in a peered network is updated, your VPC network automatically learns and uses the updated custom route. You are not required to configure any additional settings in your VPC network. When you create or modify a peering configuration, you can choose to import routes, export routes, or both. The peer network administrator must similarly configure their peering configuration before routes are exchanged. This process ensures that both network administrators explicitly agree to exchange custom routes before they are exchanged. Let’s consider a sample scenario. Consider you are connecting your on-premises environment through a VPN tunnel or a Cloud Interconnect connection. Cloud Interconnect establishes direct, private connections between your on-premises network and your Google Cloud Virtual Private Cloud (VPC) network. VPN on the other hand provides a secure remote access that establishes an encrypted connection over the public internet. When using either of these connections, you can share dynamic routes. Sharing these routes enables peered networks to reach your on-premises network, as shown in the graphic. For dynamic routes, you must add Cloud Router custom route advertisements in your VPC network. These advertisements announce peered network subnets to your on-premises network. Local routes are always preferred over dynamic routes that are learned by using VPC Network Peering.

#### Shared VPC

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568665

In this module, we are going to cover two configurations for sharing VPC networks across Google Cloud Projects. First, we will go over shared VPC, which allows you to share a network across several projects in your Google Cloud organization. Then we will go over VPC Network Peering, which allows you to configure private communication across projects in the same or different organizations. Let's start by talking about shared VPC. Kim is a network administrator at Symbol Corporation. Symbol has four networks that belong to the Web application servers project, namely the recommendation, personalization, analytics, and web. The recommendation service, the personalization service, and the analytics service are each hosted on a different VPC. In this model, Kim is facing challenges such as time consuming management of network, difficulty scaling, and limitation communicating between networks. In addition to this, it is a constant turn to set up access policy, new projects, billing, etc. Kim would like to centrally manage the VPC networks while still segregating workloads that use these networks. What should Kim do? Kim can benefit by implementing shared VPC among these networks. When you use shared VPC, you designate a project as a host project and attach one or more other service projects to it. In this case, the web application servers project is the host project, and the three other projects are the service projects. The overall VPC network is called the Shared VPC network. Shared VPC makes use of Cloud IAM roles for delegated administration. Let me walk through how to provision shared VPC by focusing on the required administrative roles. The first required role is the organization administrator. The organization resource represents an organization, for example, a company, and is the root node in the Google Cloud Resource hierarchy. The Google Workplace or Cloud Identity Super administrators are the first users who can access the organization, and they assign the organization Admin role to users. The organization Admins role in provisioning shared VPC is to nominate shared VPC admins by granting them appropriate project creation and deletion roles, and the compute.xpnAdmin role for the organization. Note that shared VPC is also referred to as XPN in the API and command line interface. Next, the shared VPC administrator performs various tasks necessary to set up shared VPC, such as enabling shared VPC on the host project, attaching service projects to the host project, and delegating access to some or all of the subnets in shared VPC networks to service project Admins by granting the compute.networkUser role. Typically, a shared VPC administrator is also the project owner for a given host project. In addition to being a network user, service project administrators also maintain ownership and control over resources defined in their service projects. The service project administrators must at least have the compute.instanceAdmin role to the corresponding service project. However, typically, the service project administrators are project owners of their service projects. This allows them to create and manage resources in the shared VPC. These resources could be VM instances, instance templates and groups, static internal IP addresses, and load balancers. Let's come back to our original example that had one host project and three service projects. In this diagram, the Shared VPC administrator, which was nominated by an organization administrator, configured the web application project to be a host project with subnet-level permissions. Doing so allowed the shared VPC administrator to selectively share subnets from the VPC network. Next, the shared VPC administrator attached the three service projects to the host project and gave each project owner the network user role for the corresponding subnets. Each project owner then created VM instances from their service projects in the shared subnets. By the way, billing for those VM instances is attributed to the project where the resources are created, which are the service projects. Shared VPC administrators have full control over the resources in the host project, including administration of the shared VPC network. They can optionally delegate the network administrator and security administrator roles for the host project. Overall, shared VPC is a centralized approach to multi-project networking because security and network policy occurs in a single designated VPC network.

#### Demo: Shared VPC

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568666

Next, let's watch a demo that describes how to set up SharedVPC. We'll see what must be done to configure the host project and service projects. At the organization level, my login Stephanie. Wong currently has org admin and SharedVPC admin privileges. I can also assign the SharedVPC admin role to somebody else in my org. Switching over to my host project, I created a custom VPC called vpc1 and two subnets development and production. I also created two fireball rules, one to allow SSH to all instances in the network and one to allow ICMP traffic between instances in these subnets. In the SharedVPC page, first enable host project to be the host project by clicking set up SharedVPC. I'm going to select sharing individual subnets. This option allows you to share specific subnets in the host project VPC with your service project. Then select the vpc1 dev and prod subnets. Let's attach two service projects, development and production. Make sure the compute engine API has been enabled in each service project before attaching them. Here, you can edit the default permissions given to this service project. Then click save. We can now see host project is the SharedVPC host with two subnets and it has two attached service projects. This is also where you can add additional host project users and admins. Now I'm going to log into my development project owner account, John, who only has permission to that project. Switching to the development project, create a new VM. And add a development tag. Then select network shared with me from the host project so that you can select the dev subnet. Once it's done creating, copy the internal IP of the instance since you'll need it just in a second. Next, I'll switch to Mary's account, who is the owner of the production project. Create a new VM in the production subnet and add a production tag. Making sure to change the zone to the one in the production subnet region, select the prod subnet. When both VMs are done creating, you can see I can SSH into the prod instance and ping the dev instance's internal IP address since I've created the right firewall rules in the host project SharedVPC.

#### Shared VPC versus VPC Network Peering

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568667

Let's compare both of these configurations to help you decide which one is appropriate for a given situation. If you want to configure private communication between VPC networks in different organizations, you must use VPC network peering shared VPC only works within the same organization. If you want to configure private communication between VPC networks in the same project, you must use VPC network peering. The VPC networks can be in the same project, but it's not required. Shared VPC only works across projects. In our opinion, the biggest difference between the two configurations is the network administration models. Shared VPC is a centralized approach to multi project networking because security and network policy occurs in a single designated VPC network. In contrast, VPC network peering is a decentralized approach. Each VPC network is controlled by administrator groups in that VPC networks organization. Each VPC network can maintain its own global firewall and routing tables. For more information about the limits of VM instances per VPC network, see per network on the quotas and limits page of the Google Cloud documentation. Now that we've compared both of these configurations to share VPC networks across Google cloud projects, lets look at one last use case. In Google Cloud you can peer with a shared VPC network. Here you can see an example of a shared VPC network called network SVPC. Network SVPC is in host project P One. Service projects P3 and P4 can attach VM instances to network SVPC, which enables private communication between VMs 1, 2, and 4. If we establish a peering session between Network A and network SVPC, all VM instances will have private internal IP connectivity. Each VPC network has firewall rules that define which traffic is allowed or denied between the networks. You can also set up VPC network peering between two shared VPC networks.

#### Let's ask Gemini

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568668

If you're still not sure where to start and how, you can always lean on Gemini, an AI-powered collaborator in Google Cloud, for some help. In the Google Cloud console toolbar, click Chat_Spark. Open Gemini, and type your prompt. In this case, let's ask a peering question. I need VM instances in Network A Org1, to access services from two different external organizations, Org2 and Org3, using internal IP addresses. Is this possible with peering? Gemini will guide you through the process and also provide steps. How cool is that?

#### Lab Intro: Configuring VPC Network Peering

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568669

In this lab, you will learn how to perform the following tasks. Explore connectivity between non-peered VPC networks. Configure VPC network peering. Verify private communication between peered VPC networks. Delete VPC network peering. On the screen, you can see a general topology of my network and private net. Once VPC network peering is implemented, VMs in each VPC network will be able to communicate using internal IP addresses.

#### Configuring VPC Network Peering

- https://www.cloudskillsboost.google/paths/14/course_templates/35/labs/568670

#### Migrating a VM between networks

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568671

Finally, we discuss how to migrate a VM instance from one network to another. When a VM is connected to more than one network using multiple interfaces, the migration process updates one of the interfaces and leaves the rest in place. Luca needs to migrate a VM from one VPC network to another. What can she do? Before she migrates it, it is important that she is aware of the supported migration requirements and limitations, so let's go over them. Before you migrate a VM, you must meet the following requirements. The migration is a cold migration. The VM must be stopped before it can be migrated. The VM must not be in an instance group or network endpoint group NEG. If the VM is in an unmanaged instance group or NEG, you must take it out of the group before migrating it. VMs in managed instance groups cannot be migrated. Instead, you must copy your instance template to the new network and use it to rebuild the managed instance group. A target pool is a group of Google compute engine instances that receive incoming traffic from a load balancer. You can move instances and target pools without removing them first. The target pool expands to cover both networks. The migrations supported from Legacy network to a VPC network in the same project, from one VPC network to another VPC network in the same project. From one subnet of a VPC network to another subnet of the same network, from a service project network to the shared network of a shared VPC host project. In all cases, the VM stays in the region and zone where it was before only the attached network changes. There are limitations to migrating that you should consider. You cannot migrate a VM interface to a legacy network. The MAC address allocated to the network interface will change during the migration. This could have an impact on services tightly coupled with MAC addresses such as third-party license agreements. If you are migrating the VM to a network or subnet with a different IP range, the internal IP address of your instance must change. If you are migrating to a subnet with the same IP range, you can keep the old IP address as long as it is not already in use at the destination by specifying it during the migration. You can keep the VMS existing external IP address in the new location. However, to do this, you must have the compute.subnetworks.use external IP permission on the target network. And the target network cannot have external IP addresses disabled by the constraints/compute.vm externalip access constraint.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568672

In this module, we looked at shared VPC and VPC network peering, which are two configurations for sharing VPC networks across Google Cloud projects. You got to explore VPC network peering in a lab, and we compared both configurations and their network administration models to help you decide when to choose which. Google Cloud's flexibility to support multiple approaches to network administration allows organizations like yours to more carefully map resource policies, administrative controls, and related accounting to existing structures. In addition, administrators can carefully control the manner in which environments interact with each other, on premises networks, and the public Internet. Thank you.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/35/quizzes/568673

### Network Monitoring and Logging

#### Module intro

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568674

Welcome to the Network Monitoring and Logging module of the Networking in Google Cloud Fundamentals course. In this module, we will cover Google Cloud network monitoring and logging features that can help you troubleshoot your Google Cloud networking services. You implement monitoring and logging in two separate lab exercises. At the end of the module, you will test your knowledge by taking a brief quiz.

#### Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568675

Before we dive deep into monitoring, let's take a look at a scenario. Tal is a Cloud network engineer at Cymbal, where networks are incredibly large and complex. There are multiple VPCs and multiple interconnections used and monitored by various teams. Tal faces challenges with maintaining visibility into the network, ensuring optimal network performance, identifying bottlenecks and connectivity issues, analyzing and viewing network insights. With growing complexity, Tal can benefit from a solution to streamline network monitoring and gain actionable insights. Let's explore the comprehensive set of tools that Tal can utilize to monitor the network. To have visibility into the network, Tal can use Google Cloud monitoring to create custom dashboards that contain charts of the metrics that he wants to monitor. For example, Tal can create charts that display instances CPU utilization, the packets or bytes sent and received by those instances, and the packets or bytes dropped by the firewall of those instances. In other words, charts provide visibility into the utilization and network traffic of your VM instances, as shown on this slide. These charts can be customized with filters to remove noise, groups to reduce the number of time series and aggregates to group multiple time series together. For a full list of supported metrics, please refer to the documentation. Now, although charts are extremely useful, they can only provide insight while someone is looking at them, but what if Tal server goes down in the middle of the night or over the weekend? Tal is not always available to look at dashboards and determine whether the servers are available, or of enough capacity or bandwidth. To solve that, Tal can create alerting policies that notify when specific conditions are met. An alerting policy describes the circumstances under which you want to be alerted and how you want to be notified about an incident. The alerting policy can monitor time series data stored by Cloud monitoring, or log stored by Cloud logging. When that data meets the alerting policy condition, Cloud monitoring creates an incident and sends the notifications. Tal can create an alerting policy when the network egress of your VM instance goes above a certain threshold for a specific time frame. When this condition is met, you or someone else can be automatically notified through email, SMS, or other channels in order to troubleshoot this issue. There are also uptime checks that Tal can configure to test the availability of public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource to be checked can be an app engine application, a compute engine instance, a URL of a host, or an AWS instance or load balancer. For each up time check, you can create an alerting policy and view the latency of each global location. Checking up time time helps in monitoring and maintaining service level indicator, service level agreement, and service level objective for availability. Seventy five percent of network outages happen due to misconfiguration. More often than not, these misconfigurations are discovered in production. Not knowing the impact of making a configuration change in firewall rules or routing rules, makes network monitoring reactive rather than proactive, introducing risk and extending mean time to resolution. Network intelligence center enables Tal to prevent networking outages and performance issues before they happen. Centralized monitoring cuts down troubleshooting time and effort, increases network security, and improves the overall user experience. Network intelligent center modules offer network topology, visualization, network connectivity tests, a performance dashboard, and firewall insights. When a virtual machine is unreachable, Tal may need to diagnose the connectivity issue quickly to prevent any issues. For example, there may be an issue between source and destination endpoints in your VPC network. Using the network intelligence center connectivity tests, Tal can self diagnose connectivity issues within Google Cloud or Google Cloud to an external IP address, which could be on premises or on another Cloud, helping to isolate whether the issue is in Google Cloud or not. Tal can create, save, and run tests to help verify the impact of configuration changes and ensure that network intent captured by these tests is not violated proactively preventing network outages. These tests also help assure network security and compliance. Connectivity tests have been used internally by the Google Cloud Support team to resolve customer issues. How can Tal diagnose if the application or the underlying network is the root cause of the issues? Network intelligence centers performance dashboard can show you real-time performance metrics, latency and packet loss between the zones where you have VMs, enabling you to quickly troubleshoot where the packet loss is happening, and indeed, if it's a networking issue at all. Performance dashboard now shows customer project information and also Google Cloud general information. Firewall configuration can be daunting. How can Tal verify that firewall rules are being used in the intended way? Firewall insights enables you to better understand and safely optimize firewall configurations. Firewall insights provides reports that contain information about firewall usage and the impact of various firewall rules on your virtual Private Cloud, VPC network. Make sure to enable firewall rules logging to view the reports. Firewall Insights uses Cloud monitoring metrics and recommender insights. Cloud monitoring collects measurements to help you understand how your applications and system services are performing. A collection of these measurements is generically called a metric. The applications and system services being monitored are called monitored resources. Measurements might include the latency of requests to a service, the amount of disk space available on a machine, the number of tables in your SQL database, the number of wig its sold, and so forth. Recommender is a service that provides recommendations and insights for using resources on Google Cloud. These recommendations and insights are per product or per service, and are generated based on heuristic methods, machine learning, and current resource usage. You can use insights independently from recommendations. Each insight has a specific insight type. Insight types are specific to a single Google Cloud product and resource type. Firewall insights metrics let Tal analyze the way firewall rules are being used. Firewall insights metrics are available through Cloud Monitoring and Google Cloud Console, metrics are derived through firewall rules logging. With firewall insights metrics, you can perform the following tasks. Analyze firewall rule usage, determine if firewall rules are functioning as expected. Track connection behavior. Verify that firewall rules are permitting or blocking the correct traffic over defined time intervals. Diagnose dropped connections. Investigate connections that may be unintentionally blocked by firewall rules. Identify potential threats. Detect anomalies in firewall rule hit counts, which could indicate malicious network activity. Network analyzer automatically monitors your VPC network configurations and detects misconfigurations and suboptimal configurations. It provides insights on network topology, firewall rules, routes, configuration dependencies, and connectivity to services and applications. It identifies network failures, provides root cause information, and suggests possible resolutions. Network analyzer runs continuously and triggers relevant analysis based on near real-time configuration updates in your network. If a network failure is detected, it tries to correlate the failure with recent configuration changes to identify root causes, whenever possible, it provides recommendations to fix the issues. In the example above, an insight of the type error, a GKE node to control plane connectivity is generated. The insight page also describes the following. The root cause. An ingress firewall rule is blocking the connection between the node and the plane. This indicated that the default firewall rules were modified, removed, or shadowed by another firewall rule. A solution. If the root of the problem is a deleted firewall, create a new firewall rule. If it's a shadowed firewall rule that increase the priority, you can use Gemini if you are unsure of where to find a particular performance metric. For example, you can ask Gemini how to find out the average latency between my VMs in US East 4 and US Central 1. Gemini assists you with the dashboard name and its purpose.

#### Lab Intro: Resource Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568676

In this lab you learn how to: explore Cloud monitoring, add charts to dashboards, create alerts with multiple conditions, create resource groups, create uptime checks.

#### Resource Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/35/labs/568677

#### Logging

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568678

Next, let’s talk about Logging. Now, you’ve already been exposed to Cloud Logging throughout this course. In this module, we’ll focus on VPC Flow Logs and exporting your logs to BigQuery and Looker Studio so that you can analyze and visualize your logs. VPC Flow Logs records a sample of network flows sent from and received by VM instances, as you can see in this animation. These logs can be used for network monitoring, forensics, real-time security analysis, and expense optimization. Google Cloud is unique for its near-real-time visibility, providing log updates every 5 seconds. Also, there is no extra delay and no performance penalty in routing the logged IP packets to their destination. DNS provides a lookup for sites on the internet. You can think of it as a phone book, but instead of using the name of an organization to look up its phone number, you use the name of an organization to find an IP address. A DNS service is provided by your ISP (internet service provider). For example, suppose a request comes from a client computer to access cymbal.com. To direct the client computer to the cymbal.com site, the internet service provider needs the IP address of cymbal.com. The ISP connects to get this information from its DNS service. The DNS service recursive resolver issues a request to look up the IP address of cymbal.com from one of its name servers. The name server responds with the ISP. You can enable or disable VPC Flow Logs per VPC subnet. Once enabled for a subnet, VPC Flow Logs collect data from all VM instances in that subnet. Each log entry contains a record of different fields. For example, this table illustrates the IP connection information that is recorded. This consists of the source IP address and port, the destination IP address and port, and the protocol number. This set is commonly referred to as 5-tuple. Other fields include the start and end time of the first and last observed packet, the bytes and packets sent, instance details, VPC details, and geographic details. For more information on all data recorded by VPC Flow Logs, please refer to the documentation. VPC Flow Logs capture traffic from both ends of a VM-to-VM conversation within the same VPC network. To use this feature, ensure both the communicating VMs reside in subnets with VPC Flow Logs enabled. In this example, VM 10.10.0.2 sends a request with 1,224 bytes to VM 10.50.0.2, which is also in a subnet that has logging enabled. In turn, 10.50.0.2 responds to the request with a reply containing 5,342 bytes. Both the request and reply are recorded from both the requesting and responding VMs. Flow Analyzer lets you quickly and efficiently understand your Virtual Private Cloud (VPC) traffic flows without the need to write complex SQL queries for analyzing VPC Flow Logs. Flow Analyzer lets you perform opinionated network traffic analysis with 5-tuple granularity (source IP, destination IP, source port, destination port, and protocol). Developed using Log Analytics and powered by BigQuery, Flow Analyzer enables in-depth analysis of inbound and outbound traffic of your VM instances. It lets you monitor, troubleshoot, and optimize your networking deployment for better performance and enhanced security which helps ensure compliance, and save on costs. Packet Mirroring clones the traffic of specific instances in your Virtual Private Cloud (VPC) network and forwards it for examination. It also captures all ingress and egress traffic and packet data, such as payloads and headers. The mirroring happens on the virtual machine (VM) instances, not on the network. Consequently, Packet Mirroring consumes additional bandwidth on the hosts. Packet Mirroring is useful when you need to monitor and analyze your security status. It exports all traffic, not only the traffic between sampling periods. For example, you can use security software that analyzes mirrored traffic to detect all threats or anomalies. Additionally, you can inspect the full traffic flow to detect application performance issues. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios: When a network connection using NAT is created. When a packet is dropped because no port was available for NAT. You can choose to log both kinds of events, or only one. Created logs are sent to Cloud Logging. Cloud NAT logging handles TCP and UDP traffic only. Cloud NAT logging only logs dropped packets if they are egress (outbound) TCP and UDP packets. It does not log dropped incoming packets, for example, if an inbound response to an outbound request is dropped for any reason, no error is logged. Although Tal can explore each log entry within Google Cloud Logging, we recommend exporting logs to BigQuery. BigQuery runs blazing-fast SQL queries on gigabytes to petabytes of data. This allows Tal to analyze network traffic to better understand traffic growth to forecast capacity, analyze network usage to optimize network traffic expenses, or analyze network forensics to examine incidents. For example, in this screenshot we queried my logs to identify the top IP addresses that have exchanged traffic with the web server. Depending on where these IP addresses are and who they belong to, we could relocate part of my infrastructure to save on networking costs or deny some of these IP addresses if we don’t want them to access my web server. If Tal wants to visualize logs, we recommend connecting BigQuery tables to Looker Studio. Looker Studio transforms raw data into the metrics and dimensions that Tal can use to create easy-to-understand reports and dashboards.

#### Lab Intro: Analyzing Network Traffic with VPC Flow Logs

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568679

In this lab you learn how to: configure a custom network with VPC flow logs, create an Apache web server, verify that network traffic is logged, export the network traffic to BigQuery, to further analyze the logs, set up VPC flow log aggregation.

#### Analyzing Network Traffic with VPC Flow Logs

- https://www.cloudskillsboost.google/paths/14/course_templates/35/labs/568680

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/35/video/568681

In this module, we covered Google Cloud network monitoring and logging features that can help you troubleshoot your Google Cloud networking services. Monitoring is important to Google because it is at the base of site reliability engineering or SRE. SRE is a discipline that incorporates aspects of software engineering and applies that to operations whose goals are to create ultrascalable and highly reliable software systems. This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world. Thank you.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/35/quizzes/568682

### Course Resources

#### Networking in Google Cloud: Fundamentals Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/35/documents/568683

### Your Next Steps

## 04: Networking in Google Cloud: Routing and Addressing

- https://www.cloudskillsboost.google/paths/14/course_templates/36

### Welcome to Networking in Google Cloud

#### Networking in Google Cloud Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568825

Welcome to the second course in the Networking in Google Cloud series, Routing and Addressing. In this course, we'll cover the essential routing and addressing concepts that are relevant to Google Cloud's networking capabilities. Module 1 will lay the foundation by exploring network routing and addressing in Google Cloud, covering key building blocks such as routing IPv4, bringing your own IP addresses, and setting up Cloud DNS. In module 2, we'll shift our focus to private connection options, exploring use cases, and methods for accessing Google and other services privately using internal IP addresses. By the end of this course, you'll have a solid grasp of how to effectively route and address your network traffic within Google Cloud.

### Network Routing and Addressing in Google Cloud

#### Routes and route preferences

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568826

Welcome to the Network Routing and Addressing module. In this module, we will explore foundational elements that guide traffic through the Google Cloud network. We'll start with the building blocks, routing, IPv4, and BYOIP. We'll also explore Cloud DNS. Before we cover routes, let's cover IP addresses. Speaking of IP addresses of a subnet, Google Cloud VPCs let you increase the IP address space of any subnets without any workload shutdown or downtime. This diagram illustrates a network with subnets that have different subnet masks, allowing for more instances in some subnets than others. This gives you flexibility and growth options to meet your needs. But there are some things to remember. The new subnet must not overlap with other subnets in the same VPC network in any region. Each IP range for all subnets in a VPC network must be a unique valid CIDR block. Also, the new subnet IP address ranges are regional internal IP addresses and have to fall within valid IP ranges. Subnet ranges cannot match, be narrower, or be broader than a restricted range. Subnet ranges cannot span a valid RFC range and a privately used public IP address range. Subnet ranges cannot span multiple RFC ranges. The new network range must be larger than the original, which means the prefix length value must be a smaller number. In other words, you cannot undo an expansion. Now, auto mode subnets start with a /20 IP range. They can be expanded to a /16 IP range, but no larger. Alternatively, you can convert the auto mode subnetwork to a custom mode subnetwork to increase the IP range further. A route is created when a network or subnet is created, enabling traffic delivery from anywhere. Routes define the paths that network traffic takes from a virtual machine, or VM, instance to other destinations. These destinations can be inside your Google Cloud Virtual Private Cloud (VPC) network for example, in another VM, or outside of it. Routes match packets by destination IP address. Forward traffic to highest priority or specific route. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, which enables traffic delivery from anywhere. Also, a route is created when a subnet is created. This is what allows VMs on the same network to communicate. Network tags fine-tune which route is picked. If a route has a network tag, it can be applied only to instances that have the same network tag. Routes without network tags can apply to all instances in the network. This slide shows a simplified routing table. A route can be of many types. There are system-generated, custom peering, NCC, and policy based routes. System-generated routes are simple and can be used by default. When they do not provide the desired granularity, create custom routes. For example, custom routes can be used to route traffic between subnets through a network virtual appliance. Peering routes are used for network peering. VPC Network Peering routes in a different VPC network connected using peering. NCC routes that represent a subnet IP range in a VPC spoke. Policy based routes apply to packets based on source IP, destination IP, protocol, or a combination thereof. Next, you'll learn more about system generated and custom route types. Peering Network Connectivity Center routes and policy-based routes are covered in another module. When you create a VPC network, it includes a system-generated IPv4 default route, 0.0.0.0/0. When you create a dual-stack subnet with an external IPv6 address range in a VPC network, a system generated IPv6 default route (::/0) is added. If the default route doesn't exist, it isn't added. The IPv4 and IPv6 default routes that serve these purposes define a path out of the VPC network to external IP addresses on the Internet. If you access Google APIs and services without using a private service connect endpoint, the default system-generated route can serve as the path to Google APIs and services. Private Service Connect enables you to publish and consume services by using the internal IP addresses that you define. You'll learn more about Private Service Connect later in this course. For more information, in the Google Cloud documentation, refer to configuring Private Google Access and accessing APIs from VMs with external IP addresses. Google Cloud only uses a default route if a route with a more specific destination does not apply to a packet. For information about how destination specificity and route priority influence route selection, see routing order in the Google Cloud documentation. To completely isolate your network from the Internet, or to replace the default route with the custom route, you can delete the default route. For IPv4 only, to route Internet traffic to a different next hop, you can replace the default route with a custom static or dynamic route. For example, you could replace it with a custom static route whose next hop is a proxy VM. If you delete the default route and you do not replace it, packets to IP ranges not covered by other routes are dropped. If you don't have custom static routes that meet the routing requirements for Private Google Access, deleting the default route might disable Private Google Access. Some organizations do not want a default route pointing to the Internet. Instead, they want the default route to point to an on-premises network. To do that, you can create a custom route. You will learn about custom routes later in this module. When you create a subnet, system-generated subnet routes are automatically created. Subnet routes always have the most specific destination and cannot be overridden by higher priority routes. Recall that lower priority number indicates higher priority, so 1 would have a higher priority than 10. Each subnet has at least one subnet route whose destination matches the primary IP range of the subnet. If the subnet has secondary IP ranges, each secondary IP address range has a corresponding subnet route. Custom static routes forward packets to a static route next hop and are useful for all topologies. Dynamic routing generally provides quicker routing performance. Unlike dynamic routing, no processing power is devoted to maintaining and modifying the routes, hence the quicker performance. Custom static routing is more secure than dynamic routing because there's no route advertisement. Note these custom static routing limitations. A custom static route cannot point to a VLAN attachment. It also requires more maintenance because routes are not dynamically updated. For example, a topology change on either network requires you to update static routes. Also, if a link fails, static routes can't reroute traffic automatically. Manually configured routes, which are called custom learned routes, can be used to overcome this limitation. For small, stable topologies, this is not always a significant concern. Here we have two VMs, Pekoe and Oolong, set up in two different VPC, VPC1 and VPC2. A VPN gateway is set up between these two VPCs and two IPSec tunnels have been created. A static route has been created for Pekoe to be able to ping Oolong and enable traffic to flow through the tunnel. A static tunnel forwards packets to a static route next hop and supports various destinations. The supportive static route next hop are instances, internal passthrough Network Load Balancers, and Classic VPN tunnel next hops. Each VM instance has a controller that is kept informed of all routes from the network's routing table. Route changes are propagated to the VM controllers. When you add or delete a route, a set of changes is propagated to the VM controllers. In this example, if you change any of the routes to the Oolong VM, Pekoe can still route packets to Oolong. You can create custom static routes either manually or automatically. To create custom static routes manually, use the Google Cloud app, the Google Cloud CLI compute routes create command, or the routes.insert API. When creating a Classic VPN tunnel without dynamic routing in the Google Cloud console, Cloud VPN may automatically generate static routes. To create the routes, you can also use the Google Cloud app to create a Classic VPN tunnel with policy-based routing, or as a route-based VPN. For more information, see Cloud VPN networks and tunnel routing. Another code-based approach would be to use an IaC system such as Terraform. For large organizations that turn up several networks and test them, static routes can be painful. In the above topology, VMs in the Google VPC route traffic to the VPN gateway through static routes. The VPN gateway encrypts traffic to and from the on-premises network. The on-premises environment has a firewall and a router that knows how to route traffic to the Cloud VPN gateway. As the on-premises network expands through the server network, routes have to be manually configured to route traffic to the resource in the server network. A topology change on either network requires you to manually update static routes. Also, static routes cannot automatically reroute traffic when there is a link failure. A solution is for a network to automatically and rapidly discover topology changes and then route traffic accordingly to minimize disruption. This is exactly the function of Cloud Router. Whenever a link fails, Cloud Router will automatically reroute traffic if another path is possible. Cloud Router peers with an on-premises VPN gateway or router. The router exchanges topology information through a border gateway protocol, or BGP. Cloud Router advertises subnets from its VPC network to the on-premises gateway via BGP. Then, topology changes automatically between your VPC and on-premises network. Dynamic routes are managed by Cloud Routers in the VPC network. Their destinations always represent IP address ranges outside of your VPC network, which are advertised from a BGP peer router. BGP peer routers are typically outside the Google network, like on-premises or on another cloud provider. Dynamic routes are used by: Dedicated Interconnect, Partner Interconnect, Cross-Cloud Interconnect, HA VPN tunnels, Classic VPN tunnels that use dynamic routing, and NCC Router appliances. Routes are added and removed automatically by Cloud Routers in your VPC network. The routes apply to VMs according to the VPC network's dynamic routing mode. This example shows a VPC network connected to an on-premises network that uses Dedicated Interconnect. Cloud Router handles the BGP advertisements and adds them as custom routes. Cloud Router creates a BGP session for the VLAN attachment and its corresponding on-premises peer router. The Cloud Router receives the routes that your on-premises router advertises. These routes are added as custom dynamic routes in your VPC network. The Cloud Router also advertises routes for Google Cloud resources to the on-premises peer router.

#### IPv6

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568827

In this module, we will discuss IPv6. VPC networks now support IPv6 addresses. Support for IPv6 addresses can vary per subnet. To support IPv6, Google Cloud has introduced the concept of a subnet stack. The subnet stack defines the type of address that can be assigned to objects in the subnet. Single and dual-stack subnets support IPv4 and IPv6. There's no subnet that only supports IPv6. IPv6 addresses can be assigned to objects in a subnet that supports IPv6. In other words, you can only assign IPv6 addresses to objects in a dual-stack subnet. You can configure the IPv6 access type to be internal or external. Internal IPv6 addresses are used for VM to VM communication within VPC networks. These use unique local addresses, ULAs, which can only be routed within VPC networks and cannot be routed to the Internet. External IPv6 addresses can be used for communication between VMs within VPC networks. These use global unicast addresses or GUAs, and are also routable on the Internet. Connected VMs inherit the IPv6 access type from the subnet. When configuring your VPC networks and subnets to use a IPv6 address, consider the caveat that dual-stack subnets are not supported on auto mode VPC networks or legacy networks. If you have an auto mode VPC network that you want to add dual-stack subnets to, you can convert the auto mode VPC network to custom mode.

#### BYOIP (bring your own IP)

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568828

The next topic we'll be discussing is BYOIP. Enterprises want to move their applications to the Cloud, but worry about having to swap their IP addresses for one's from their Cloud provider. We hear from our customers that managing the migration of IP addresses can be one of the most challenging aspects of a Cloud migration for network administrators. Here at Google Cloud, we now allow you to bring your own IP, or BYOIP addresses to Google's network infrastructure across all our 24 regions. By bringing over your own IP addresses, you can accelerate your migration while minimizing downtime, as well as significantly reducing your network infrastructure costs. With Google Cloud, your BYOIP prefixes can be broken into blocks as small as 16 addresses, or S/28. It can be distributed to any region, and can also be used for global load balancers, creating more flexibility with the resources you already have. You can also advertise the IP addresses you bring to Google Cloud globally to all peers. BYOIP enables customers to assign IP addresses from a public IP range that they own, to Google Cloud resources. With BYOIP, customers can route traffic directly from the Internet to their VMs without having to go through their own physical networks. After the IP addresses are imported, Google Cloud manages them in the same way as Google-provided IP addresses with these exceptions. The IP addresses are available only to the customer who brought them, and idle or in-use IP addresses incur no charges. The object that the IP address is assigned to can have a regional scope, like a VM or the forwarding rule of a network load balancer. It can also have a global scope, like the forwarding rule of a global external application load balancer. It must support an external address type, because BYOIP ranges will be advertised by Google to the public Internet. BYOIP, bring your own IP addresses, are a way to incorporate your existing static external IP addresses into Cloud environments. They are compatible with most resources that support static external IP addresses. You can leverage BYOIP addresses for Classic VPN gateway tunnels as PRIP addresses, external Google Kubernetes Engine forwarding rules, and configuring static IP addresses for VMs within state-full managed instance groups. However, there are certain limitations. BYOIP addresses cannot be used as PRIP addresses for HA VPN gateway tunnels, or as external IP addresses for VPN gateway tunnels in general. Additionally, shared VPC service projects, Google Kubernetes Engine nodes and pods, and managed instance groups with automatic IP allocation, do not support the use of BYOIP addresses. BYOIP prefixes cannot overlap with subnet or alias ranges in the VPC used by the customer. For BYOIP, the IP address must be IPv4. Importing IPv6 addresses is not supported. Overlapping BGP route announcements can be problematic. BGP is a routing protocol that picks the most efficient route to send a packet. If Google and another network advertise the same route with matching or mismatched prefix lengths, BGP can not work properly, you might experience unexpected routing and packet loss. For example, suppose you're advertising a 203.0.112.0/20 address block, and you're using BGP to route packets, you could bring a 203.0.112.0/23 address block that you own to Google, using BYOIP, and set it up to route externally. Because the /23 block is contained within the /20 block, BGP route announcements may overlap. If you're maintaining the routing registry correctly, BGP routing practices cause the more specific route to take precedence. Thus, the /23 block will take precedence over the /20 block. However, if the 23 route ever stopped being advertised, the /20 block could be used.

#### Cloud DNS

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568829

Next, let's talk about Cloud DNS. Before we talk about Cloud DNS, let's quickly review how DNS or domain name system works. DNS provides a lookup for sites on the Internet. You can think of it as a phonebook, but instead of using the name of an organization to look up its phone number, you use the name of an organization defined in IP address. A DNS service is provided by your ISP or Internet service provider. For example, suppose a request comes from a client computer to access cymbal.com. To direct the client computer to the cymbal.com site, the Internet service provider needs the IP address of cymbal.com. The ISP connects to get this information from its DNS service. The DNS service recursive resolver issues a request to look up the IP address of cymbal.com from one of its name servers. The name server responds with the ISP, and the recursive resolver sends the IP to the client. When the name server cannot satisfy the lookup, the DNS service might contact another DNS service for this information. Some organizations don't rely on their ISP to provide DNS service, so they create and maintain their own DNS servers. Organizations sometimes do this to limit or customize the information that is returned or because they can achieve better performance if they use their own DNS servers. Alternatively, they can purchase DNS services from another organization. Obviously, there's a lot more that can be said about DNS and its components, but that's not covered in this course. Various companies provide DNS services. Google Cloud is one of them. Cloud DNS lets you create and update millions of DNS records without the burden of managing your own DNS servers and software. Instead, you use a simple user interface, command line interface, or API. On Linux by default, the VM's metadata server 169.254.169.254 resolves internal DNS names. Now, Windows by default, subnet default gateway resolves internal DNS names. Google provides a monthly uptime percentage of serving DNS queries from at least one of the Google managed authoritative name servers to customers of 100% SLO. Important notes, exclusions. It's crucial to understand there are certain situations such as maintenance, force majeure events or actions on your part that void this SLA. Intermittent issues. The downtime definition specifically focuses on at least 60 consecutive seconds of unavailability. Intermittent issues, less than a minute might not count towards the SLA. Private zones are used to provide a namespace that is visible only inside the VPC or hybrid network environment. For example, an organization would use a private zone for a domain dev.gcp.example.com, which is reachable only from within the company Internet. Public zones are used to provide authoritative DNS resolution to clients on the public Internet. For example, a business would use a public zone for its external website cymbal.com, which is accessible directly from the Internet. Don't confuse the concept of a public zone with Google public DNS 8.8.8.8. Google public DNS is just a public recursive resolver. For more information, refer to the Cloud DNS documentation. Cloud DNS policies provide a flexible way to define how your organization uses DNS. After you create the DNS zones and artifacts needed for lookups, create Cloud DNS policies. Cloud DNS supports different types of policies. Server policies apply private DNS configuration to a VPC network. Response policies enable you to modify the behavior of the DNS resolver by using rules that you define. Routing policies steer traffic based on geolocation or round robin. Next, let's look at each of these types of policies. Use server policies to set up hybrid deployment for DNS resolution. You can configure DNS server policy for each virtual private cloud or VPC network. You can set up an inbound server policy depending on the direction of the DNS resolutions. If your workloads plan to use an on-premises DNS resolver, you can set up DNS forwarding zones by using an outbound server policy. If you want your on-premises workloads to resolve Cloud DNS private zones, you can set up an inbound server policy. The policy can specify inbound DNS forwarding, outbound DNS forwarding or both. In this section, inbound server policy refers to a policy that permits inbound DNS forwarding. Outbound server policy refers to one possible method for implementing outbound DNS forwarding. If a policy implements the features of both, it can be an inbound server policy and an outbound server policy. DNS server policies are not available for legacy networks. DNS server policies require VPC networks. For detailed information about server policies, see server policies overview in the Google Cloud documentation. To configure and apply DNS server policies, see apply Cloud DNS server policies in the Google Cloud documentation. A response policy is a Cloud DNS private zone concept that contains rules instead of records. These rules can be used to achieve effects similar to the DNS response policy zone, RPZ draft concept. In other words, you can use response policies to create a DNS firewall by returning modified DNS results to clients. For example, you can use response policies to block access to specified HTTP servers. The response policy feature lets you introduce customized rules in DNS servers within your network that the DNS resolver consults during lookups. If a rule in the response policy affects the incoming query, it's processed. Otherwise, the lookup proceeds normally. For more information, see manage response policies and rules in the Google Cloud documentation. A response policy is different from an RPZ or response policy zone. An RPZ is an otherwise normal DNS zone with specifically formatted data that causes compatible resolvers to do special things. Response policies are not DNS zones and are managed separately in the API. To create and modify response policies in Cloud DNS, use the response policies API. Response policies are separate from managed zones and cannot be managed by using either the managed zones API or the RR set API. DNS routing policies let you steer your traffic based on specific criteria. Google Cloud supports three types of DNS routing policies: weighted round robin, geolocation, geofencing and failover. A weighted round robin routing policy lets you specify different weights per DNS target, and Cloud DNS ensures that your traffic is distributed according to the weights. You can use this policy to support manual, active active, or active passive configurations. You can also split traffic between production and experimental versions of software. A geolocation routing policy let's you map traffic originating from source geographies, Google Cloud regions to specific DNS targets. Use this policy to distribute incoming requests to different service instances based on the traffic's origin. You can use this feature with the Internet, with external traffic, or with traffic originating within Google Cloud and bound for internal load balancers. Google Cloud uses the region where the queries enter Google Cloud as the source geography. A failover routing policy, lets you set up active backup configurations. This option is only available for private zones. Next, you will implement a geolocation routing policy as part of a lab exercise. An example is shown on the screen. Routing policies use geolocation to route requests to the closest load balancer. In a lab exercise, you will configure a routing policy that uses geolocation. To create, edit, or delete DNS routing policies, see manage DNS routing policies in the Google Cloud documentation. Let us look at a scenario before we move on to the lab exercise. Carl is a network engineer at Cymbal Corporation. Cymbal has a hybrid environment with an on-premises data center in Europe and a Google Cloud environment with VPC subnets in Asia and the US. Carl does not want to juggle managing multiple DNS providers for their global web presence. Fragmented setup leads to inconsistencies, manual configuration overhead, and potential for human error. Carl is looking for a solution that can provide a single endpoint for a hybrid application so that external clients in multiple regions resolve www.cymbal.com to the nearest region. Carl can leverage Cloud DNS for its global anycast network for low latency DNS resolution and high availability. The intuitive interface simplifies record management, offering features like health checks and traffic routing. Cloud DNS provides global traffic management for www.cymbal.com, routing users to the closest available server. Asia and US-based visitors are directed to network load balancers within their respective regions, which connect them to GKE instances. European users, a load balancer in European data center handles traffic, forwarding to GKE on VMware. This setup demonstrates flexibility in application hosting. Carl can create this configuration by using the following steps. Carl creates the network load balancers, and the on-premises load balancer in each region. Carl sets up a public Cloud DNS zone for his domain. Cloud DNS automatically assigns anycast name servers to handle domain record storage. These name servers are strategically located in a fully managed shared environment. Then they create a DNS routing policy. In the policy, you set the type to GEO, and you set the routing policy data value to a list of target regions that are mapped to the corresponding network load balancers. When configuring routing policies, consider these caveats. Only one type of routing policy can be applied to a resource record set at a time, and nesting or otherwise combining routing policies is not supported.

#### Lab intro: Traffic Steering Using Geolocation

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568830

Next, we will test our skills in a lab. In this lab, you will configure and test the geolocation routing policy. The geolocation routing policy applies to the nearest match for the source location when the traffic source location doesn't match any policy items exactly. The lab tasks are to: Launch client VMs, one in each region. Launch server VMs, one in each region except asia-south1. Create a private zone, like example.com. Create a geolocation routing policy using gcloud commands. And test the configuration.

#### Cloud DNS - Traffic Steering using Geolocation Policy

- https://www.cloudskillsboost.google/paths/14/course_templates/36/labs/568831

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/36/quizzes/568832

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568833

In this module, you learned about some fundamental Google Cloud VPC networking concepts. We began with an overview of Google Cloud VPC networks. We then discussed how to use IPv6 addressing and the configuration that must be done at the subnet level. After that, we discussed routes and route preferences, including system-generated routes, custom routes, and dynamic routes. We continued with information about bringing existing external IP addresses into Google Cloud, also known as BYOIP. Then we discussed using multiple network interfaces on compute engine VMs, as well as some important caveats. After that, we use Cloud DNS policies to refine how an organization uses Cloud DNS. We concluded the module with a lab exercise and a brief quiz to test your knowledge of what you've learned.

### Private Connection Options

#### Private access overview

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568834

Private connection options. In this module, we'll discuss some general methods of accessing Google and other services privately by using internal IP addresses then we will cover each of the methods, private Google access, private service connect, private services access, and Cloud NAT or network address translation. After that, you will try what you learned in a Cloud NAT lab exercise. Private access uses internal IP addresses therefore, consumers connect to supported APIs and services with an internal connection. Unless a consumer connects to Google Cloud by using an external connection, private access communication does not go through the public Internet. Access is quicker and more secure. Choose a private access option based on your needs. All Google Cloud APIs and services support private access. You can also set up private access to APIs and services that you publish. You can access these API and services from Google Cloud, other Public Clouds, or on-premises. Google APIs and services have public URLs and are accessible on the public Internet. Google provides several private access options. Each option allows VM instances with internal IP addresses to reach certain APIs and services. You could configure one or all of these options because they operate independently of each other. Private Google Access for on-premises hosts let's your on-premises hosts connect Google APIs and services through the default Internet gateway of the VPC network. Your on-premises hosts don't need external IP addresses. Instead, they use internal IP addresses. Private service connect lets you connect to a Google or third-party managed BPC network through a service attachment. As with private Google access, the connection is internal. Serverless VPC access connects serverless products to your VPC network to access Google, third party, or your own services with internal IP addresses. For example, Cloud Run, App Engine standard, and Cloud functions environments send packets to the internal IPV for address of the resource. Serverless VPC access is not covered in this module. For more information, refer to connect from serverless Google services to VPC networks in the Google Cloud documentation. Private Services access is a private connection between your VPC network and a service producer VPC network. This connection is implemented as a VPC network peering connection. The service producer network is created exclusively for you, and it's not shared with other customers. For more information, see private access options for services in the Google Cloud documentation.

#### Private Google Access

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568835

Next, let's discuss how to use private Google access to connect to Google APIs and services over an internal connection. Joe, Cymbol Corporation's network engineer faces a challenge. Cymbol runs a recommendation engine on Google Cloud VMs that analyzes customer data to provide tailored product suggestions. This data is highly sensitive, and they need to protect it from potential Internet based attacks and comply with strict data privacy regulations. They also run a large fleet of internal systems, for example, inventory management and point of sale systems. These need regular updates, but don't require direct Internet access for their core functionality. Challenge. The challenge is providing the recommendation engine access to Google Cloud APIs, such as BigQuery for data analysis and Cloud storage for model storage would traditionally require public IPs or a NAT setup. Using public IPs means exposing the VMs to the Internet, increasing the attack surface. Using complex NAT setups involves adding management overhead and potential bottlenecks. Joe wonders if there's a way for Google Cloud VMs and fleet of VMs to securely connect to Google Cloud API without exposing IPs or creating complex NAT. Private Google access enables securely connecting VMs without external IP addresses to essential Google APIs and services. You enable the private Google access feature on a subnet by subnet basis by editing the subnet in Google Cloud console or the Google Cloud CLI. If you disable private Google access for a subnet, VMs with internal IP addresses can only send traffic within the VPC network. Later in this module, you will learn about Cloud NAT, which can allow these BMs to send traffic outside of the VPC network. Private Google access has no effect on instances that have external IP addresses. For a list of services that are supported by private Google access, see private access options for services in the Google Cloud documentation. In the sample topology, the VPC network has two subnets, subnet A and subnet B. The network has been configured to meet the domain name system or DNS routing and firewall network requirements for Google Cloud APIs and services. Private Google access has been enabled on subnet A, but not on subnet B. VM A1 can access Google APIs and services, including Cloud storage, because its network interface is located in subnet A, which has private Google access enabled. Private Google access applies to the instance because it only has an internal IP address. VM B1 can't access Google APIs and services because it only has an internal IP address and private Google access is disabled for subnet B. VM A2 and VM B2 can both access Google APIs and services, including Cloud storage, because each of them has its own IP address. Private Google access has no effect on whether these instances can access Google APIs and services because both have external IP addresses. Cymbol Bank is expanding and wants to use internal IP addresses to access Cloud SQL and Cloud TPU from their Tokyo on premises network. This example shows how this access can be achieved. In the example, the on premises network is connected to a VPC network through a Cloud VPN tunnel. Traffic from on premises host to Google API is traveled through the tunnel to the VPC network. After traffic reaches the VPC network, it's sent through a route that uses the default Internet gateway as its next hub. This next hub allows traffic to leave the VPC network and be delivered to restricted.googleapis.com. 199.36.153.4/30. The on-premises DNS configuration maps [inaudible]. googleapis.com to restricted.googleapis.com, which resolves to the 199.36.153.4/30 address range. In this example, Cloud router uses a custom route advertisement for this IP address range. This route sends traffic through the Cloud VPN tunnel. The traffic that goes to Google APIs is routed through the tunnel to the VPC network. A custom static route was added to the VPC network. This route directs traffic with the destination 199.36.153.4/30 to the default Internet gateway as the next hub. Google then directs traffic to the appropriate API or service. In this example, network administrators at Cymbol Bank created a Cloud DNS managed private zone for.googleapis.com that maps to the 199.36. 153.4/30 address range. The network administrators authorize the BPC network to use that zone. Requests to the googleapis.com domain are sent to the IP addresses that are used by restricted.googleapis.com. Only the supported APIs are accessible to this configuration, which might cause other services to be unreachable. Cloud DNS doesn't support partial overrides. If you require partial overrides, use BIND a software that interacts with DNS. Private Google access has a few caveats. Because private Google access is enabled on a per subnet basis, you must use a VPC network. Legacy networks are not supported because they don't support subnets. Enable the Google Cloud APIs that you want to use. You enable these desired APIs on the APIs and services page in the Google Cloud console. Your VPC network must have appropriate routes and egress firewalls defined. This network must also have appropriate routes for the destination IP ranges that are used by Google APIs and services. If you use the private.googleapis.com or the restricted.googleapis.com domain names, you must create DNS records to direct traffic to the IP addresses that are associated with those domains. For more information, see network configuration on the configure private Google access page of the Google Cloud documentation. These domain names only offer IPv4 connectivity. If you want to use IPv6 to connect to Google APIs and services, your VM must be configured with the /96 IPv6 address range. The software on the VM must send packets whose sources match one of those IPv6 addresses from that range. You must send the packets to the IPv6 addresses or the default domains.

#### Private Service Connect

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568836

Next, let's discuss Private Service Connect, which lets you use internal IP addresses to consume, produce, and make services available. Neuer is a network engineer at Cymbal. A large financial institution offers a real-time transaction processing API for its partners. This API handles sensitive financial data and needs to be highly secure with restricted access, easily accessible to authorized partners over the Internet, and scalable to handle fluctuating traffic loads. The challenges. Security. Exposing the API directly to the public Internet poses security risks. Control. Traditional methods, IP whitelisting or VPNs, can be cumbersome for managing access and don't always scale well. Flexibility. The API should be consumable by partners who might be in different Cloud environments or have their own on-premises infrastructure. Private Service Connect provides a secure, scalable, and flexible way to expose services to specific partners or networks. It's ideal for scenarios involving sensitive APIs or the need for custom IP addressing. Private endpoint. The financial institution creates a Private Service Connect endpoint attached to their API within their VPC. This endpoint gets a private IP address within their network. Service publishing. The service transaction processing API is published, making it discoverable by authorized consumers. Controlled access. Partners create PSC consumer endpoints in their respective VPCs. These endpoints are assigned private IP addresses from a range the financial institution specifies, facilitating fine grain control. Secure, scalable consumption. Authorized partners can now consume the API using the private IP address of the published service. Traffic flows through Google's network. Load balancing is handled on the service producer's side. As with Private Google Access, you can use Private Service Connect to access Google APIs and services with a global internal IP address. Private Service Connect also lets you access third-party services and services provided within the organization with an internal IP address. To access resources with Private Service Connect, use a Private Service Connect endpoint or a back-end. Organizations can choose the internal IP address to associate with each endpoint. Private Service Connect has lined rate performance and scales to enterprise size networks. In other words, Private Service Connect is fast and grows with your organization. Look at the example shown on the right. VM 1 in the consumer network uses a Private Service Connect endpoint to connect to services that run in the producer network on VM 2. The Private Service Connect endpoint has an internal IP address, 10.0.20.10. VM 1 uses the internal address to access services on VM 2. Next, let's talk about other things you can do with Private Service Connect. The consumer contacts a producer service or VM by using the Private Service Connect endpoint in their VPC network. This endpoint has an internal IP address and maps to the service attachment in the producer VPC network. A service attachment refers to services from a producer. In this example, the service attachment receives requests redirected from the Private Service Connect endpoint and sends it to a forwarding rule. The forwarding rule sends the request to the appropriate VM or service. You can see this flow in the example with the red-dotted line. Refer to the documentation for more details. Using a load balancer provides some additional features. Private Service Connect back-ends let Google Cloud load balancers send traffic through Private Service Connect to reach published services or Google APIs. You can assign DNS names to these internal IP addresses, or even Google APIs and services with meaningful names for your organization. For example, if you have a service with the name spanner.example.com, you can map spanner.cymbal.com or some other name that makes sense for your organization. These names and IP addresses are internal to your VPC network. On-premises networks use Cloud VPN tunnels or VLAN attachments to connect to it. Also, you can control which traffic goes to which endpoint and demonstrate that the traffic stays within Google Cloud. You can configure the load balancer to log all requests to Cloud logging. With Private Service Connect and an application load balancer, you can use a URL map to choose which services are available to consumers. You could also use it to evaluate the request and route it to the correct VM or service. For added security between clients and the load balancer, you can use customer-managed transport layer security, or TLS, certificates. You can also enable data residency in transit by connecting to regional endpoints for Google APIs from workloads in that same region. In other words, you can be certain that data at rest is stored in the region you configure. This example shows an on-premises network that is connected to a subnet of a Google Cloud VPC network in the us-central-1 region. The requests that a VM or service in an on-premises network make to Cloud logging, Pub/Sub, or Cloud KMS are not routed on the public Internet. They remain within the Google Cloud backbone network. Requests to other Google Cloud services are routed over the public Internet. The internal application load balancer in the consumer network and the desired Google services are both located in the us-central-1 zone. You can see that after the request from the on-premises network is sent through the Cloud Interconnect connection to the load balancer, it remains in the us-central-1 zone. If desired, access through the load balancer can be sent to Cloud logging. With Private Service Connect and consumer HTTPS service controls that use a global external application load balancer, consumers connect to an external IP address. Private Service Connect uses a network endpoint group to route the request to the service producer. Let's look at a more detailed example of Private Service Connect that uses a global external application load balancer. This topology shows a Private Service Connect endpoint based on a global external application load balancer. This topology lets service consumers with Internet access connect to the load balancer. The load balancer then directs the request to the appropriate network endpoint group and a consumer network. Each of these endpoints is associated with a service attachment. A forwarding rule then routes the request to the appropriate VM instance or service. Private Service Connect lets you configure access to specific Google and third-party services using internal IP addresses. Except for the global external application load balancer use case, connections use internal IP addresses. Traffic stays on the Google backbone network. Connections are thus more secure and much faster than over the public Internet. Services that use Private Service Connect interact like services on a private network. Configuration is simple. Private Service Connect works with the internal IP address range that you provide and sets up the routing tables. Private Service Connect is highly scalable and supports thousands of consumers. Consumers use consumer VPC networks to access VMs and services from producer VPC networks. Consumers can control the internal IP address that is used to connect to a managed service. They don't need to reserve internal IP address ranges for back-end services that are consumed in their VPC network. Instead, consumers choose an IP address from their own subnet to connect to the producer services. For security purposes, all communications between the consumer VPC network and service producer VPC network must be initiated by the consumer. Service producers can't initiate this communication. This unidirectional connectivity drastically simplifies firewall configuration, but also reduces risk from rogue traffic originating from the service producer. Service producers make VMs and services available to consumers. Producers can choose to deploy a multi-tenant model, where your VPC network contains services that are used by multiple consumer VPCs. The consumer networks can have overlapping subnet ranges. Service producers can scale services to as many VM instances as required without asking consumers for more IP addresses. Service producers don't need to change firewall rules based on the subnet ranges in the consumer VPC networks. You can simply create firewall rules for the network address translation, or NAT, IP address range configured for your service. A Private Service Connect interface is a special type of network interface that refers to a network attachment. A Private Service Connect interface enables services in a producer VPC network to securely reach resources and destinations within a consumer VPC network. Producer and consumer networks can be in different projects and organizations. If the service consumer accepts the connection, Google Cloud allocates the interface and IP address from a subnet in the consumer VPC network that's specified by the network attachment. The VM of the Private Service Connect interface has a second standard network interface that connects to the producer's VPC network. A connection between a Private Service Connect interface and a network attachment is similar to the connection between a Private Service Connect endpoint and a service attachment, but it has two key differences. A Private Service Connect interface lets a producer network initiate connections to a consumer network, managed service egress, while an endpoint lets a consumer network initiate connections to a producer network, managed service ingress. A Private Service Connect interface connection is transitive. This means that a producer network can communicate with other networks that are connected to the consumer network. A common use case is when a managed service needs to securely access data within a customer's VPC network. Private Service Connect interface lets the service securely access the data, whether the data resides in the Cloud, on-premises, via VPN or Cloud Interconnect, or with a third-party service. This maintains privacy and isolation for sensitive information. There are many different types of Private Service Connect interface, so producers will provide documentation on how to use it with their services. For example, some producers will use an API, where others might develop a user interface. A consumer network administrator and a consumer service administrator are working together to get an easier way to configure Private Service Connect or the producer network to be able to initiate a connection to the consumer network. Satisfy both of these needs using service connection policies. A service connection policy is a regional Google Cloud resource. It lets a network administrator specify which producer services can be deployed and connected through service connectivity automation. If a service connection policy exists for a managed service, a consumer service administrator can deploy that service. Service connection policies have the following fields. Service class. This specifies the type of managed service that the policy is for. Each producer that supports service connection policies has its own globally unique service class. VPC network, specifies the VPC network that the policy is scoped for. Subnets, specifies the subnets that IP addresses for Private Service Connect endpoints are allocated from. Connection limit, specifies the maximum number of Private Service Connect connections that a producer can create in the policy's VPC network and region. Deploying an instance of a managed service by using service connection policies involves the following steps. A consumer network administrator creates a service connection policy for their VPC network. This policy lets Google automatically deploy Private Service Connect endpoints on behalf of a consumer service administrator. Consumer network administrators have more control over who can create and use Private Service Connect endpoints. The service connection policy references a service class, a globally unique resource that identifies a specific producer service. A single-service connection policy is scoped to a single-service class and a single-consumer VPC network, which delegates the ability to configure connectivity within that scope. A consumer service administrator deploys a managed service using the service administrative API or UI. Google producer of service attachments can be found using the UI or a described command. Self-hosted and third-party service attachment URLs may be shared programmatically or through email depending on the implementation. The producer receives the consumer's connectivity configuration and passes this information to a service connection. Private Service Connect service connectivity automation creates an endpoint in the consumer VPC network. This endpoint connects to a service attachment in the producer VPC network. Private Service Connect has a few caveats. You can't create a Private Service Connect endpoint in the same VPC network as the published service that you are accessing. The endpoint can only be used to access a published service in another VPC network. The address counts toward the project quota for global internal IP addresses. Private Service Connect endpoints are not accessible from peered VPC networks. Instead, create a Private Service Connect endpoint in the peered VPC network. You can then configure workloads to refer to that endpoint. Connections from on-premises environments to non-Google services must use Cloud VPN tunnels. These on-premises environments must be in the same region as the Private Service Connect endpoint. For information about accessing Private Service Connect endpoints from on-premises environments that are connected using Cloud VPN, see access the endpoint from on-premises hosts in the Google Cloud documentation.

#### Private services access

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568837

Now let's discuss how to use private services access to provide access to producer services. Consider a scenario, Mark, a Cymbal corporation's network architect. Within Cymbal, a data science team has developed a proprietary machine learning API deployed with a private VPC subnet on Google Cloud. These systems need to access essential Google Cloud services like Cloud SQL or Vertex AI. The challenge, ensuring connectivity to essential Google services while keeping the API inaccessible from the public Internet. PSA is the solution. Let us see how. Like with private service connect, private services access lets consumers use internal IPv4 addresses to consume producer services. You will locate an internal IP range within your own VPC for PSA. The connection between consumer and producer uses VPC network peering. Private services access automates much of the VPC network peering configuration. With private service connect, you had to import and export routes between the consumer and producer VPC networks. Because the connection between the consumer and the producer is made using VPC network peering, you don't need to import and export routes. Subnet routes that don't use privately used public IP addresses are always exchanged between peered VPC networks. Private services access is available only for supported producer services, like Apigee, Cloud SQL, and Cloud TPU. For a complete list of supported producer services, see private services access supported services in the Google Cloud documentation. To offer private connectivity, the service producer must complete a one-time onboarding process. To complete the onboarding process, contact your Google representative. For more information, see onboarding process on the enabling private services Access page of the Google Cloud documentation. After the onboarding process is complete, you can configure private services access. To use private services access, both service consumers and producers must activate the service networking API in their projects. The consumer and producer VPC networks require some configuration as well. Service producers must allocate an IPv4 address range in the VPC network that contains the service. This address range is used for each connection from a service consumer. Service consumers must also allocate an IPv4 address range in their VPC network for each service producer. For example, to use services from three different producer VPC networks, the consumer must allocate three IPv4 address ranges, one for each producer VPC network. After the service producer has completed the initial configuration, consumers can create a private services access connection to the producer VPC network. You can use the Google Cloud Console or the Google Cloud CLI to create this connection. Consumers and producers can use the Google Cloud Console to edit their VPC network to configure private services access. If a service producer offers multiple services, you only need one private connection. For example, if a consumer uses Cloud SQL and Cloud TPU, only one private connection is created. Google Cloud uses VPC network peering to implement the connection between the consumer and producer VPC networks. Consumers can disable the private services access connection between their VPC network and the producer VPC network. Consumers can also edit their VPC network settings to disable access. Disabling the private services access connection does not delete the VPC network peering to the producer VPC network. You can delete the VPC network peering connection by editing the VPC network. Likewise, disabling the private services access connection does not release the IPv4 address range. Consumers must edit the VPC network to release the IPv4 address range. This example shows a sample private services access topology. The customer of VPC network allocated the 10.240.0.0/16 address range for Google services and established a private connection that uses the allocated range. Google then creates a project for the customer. With that project, each Google service creates a subnet from the allocated block to provision new resources in a given region. In the example, you can see a Cloud SQL instance. Cloud SQL instance is assigned an IP of 10.240.0.2, which is within the 10.240.0.0/16 range. In the customer VPC network, requests with the destination of 10.240.0.2 are routed over the private connection to the producer VPC network. The request is then sent to the correct resource in the producer VPC network. If the service supports cross-region communication, VM instances in the customer network can access service resources in any region. Some services might not support cross-region communication. For more information, see the documentation of the relevant service. Private services access has a few caveats. For private services access to an on-premises network to work, you must export custom routes from the on-premises network to the producer VPC network. Not all Google services are supported. For a complete list of supported Google services, see private services access supported services in the Google Cloud documentation. The same quota and limits that apply to VPC network peering also apply to private services access. Private services access uses VPC network peering to implement connections and thus has the same restrictions as VPC network peering. This table outlines three Google Cloud networking features. Private Google Access, PGA, simplifies how VMs without public IPs connect to essential Google services. Private service connect, PSC, allows you to expose your own services or Google produced or third party services securely to external consumers, even outside of Google Cloud. Private services access, or PSA, enables private access to services provided by Google or third parties within your VPC.

#### Cloud NAT

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568838

Next, you will learn how to use Cloud NAT to provide access to the public Internet for resources without an external IP address. Let's start with a simple use case. Janet is a network engineer at Cymbal Corporation. Cymbal has several non-production environments, development, testing, and staging, on Google Cloud. These environments host various VMs that need occasional outbound Internet access for tasks such as downloading software updates and dependencies, and accessing external testing tools or resources. Cymbal needs a streamlined, cost-effective way to handle outbound traffic without burning through their limited pool of public IPs. The solution, Janet decides to use Cloud NAT. This managed service provides a pool of highly available IP addresses that Janet can dynamically assign to their servers on demand. Cloud NAT translates the private IP addresses of Cymbal servers to a public IP address for outbound communication, seamlessly masking the internal network from the outside world. Cloud NAT is the Google managed network address translation service. It lets you provision your application instances without public IP addresses, and it also lets them access the Internet in a controlled and efficient manner. With Cloud NAT, your private instances can access the Internet for updates, patching, configuration management and more. Cloud NAT, as shown on the right, offers several advantages when compared to other NAT offerings, as shown on the left. As a fully managed software-defined service, Cloud NAT differs from traditional NAT proxy solutions. There are no NAT middle proxies in the path from the instance to the destination. Instead, each instance is a located NAT IP address, along with a slice of the associated port range. This allocated IP address and port range are used by the instance to perform NAT. This design is free of checkpoints and is highly reliable, performant, and scalable. Cloud NAT lets you configure multiple NAT IP addresses per NAT gateway. You can scale based on the size of your network without having to add or manage another NAT gateway. NAT IP allocation has two modes: manual and auto. The manual mode provides full control when specifying IP addresses. If you want to allow NAT addresses on the receiving side, use the manual mode. The auto mode enables the NAT IP addresses to be allocated and scaled automatically based on the number of instances. For a full overview of Cloud NAT features, see Cloud NAT overview in the Google Cloud documentation. In this diagram, Cloud NAT enables two private instances to access and update server on the Internet, which is referred to as outbound NAT. However, Cloud NAT does not implement inbound NAT. In other words, hosts outside your VPC network can't directly access any of the private instances behind the Cloud NAT gateway. Your VPC networks remain isolated and secure. With Cloud NAT, VMs without external IP addresses can access destinations on the Internet. For example, you might have VMs that only need Internet access to download updates or complete provisioning. Cloud NAT allows you to configure these VMs with an internal IP address. Thus, your organization needs fewer external IP addresses. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses. Cloud NAT supports VMs that belong to managed instance groups, including those with auto scaling enabled. Cloud NAT is not dependent on a single physical gateway device. Cloud NAT is a distributed, software-defined, managed service. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT. Cloud Router contains the NAT configuration parameters. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. With Private Google Access, Cloud NAT never performs NAT for traffic sent to the selected external IP addresses of Google APIs and services. Google Cloud routes this traffic internally. When you configure a Cloud NAT gateway to apply to a subnet range, Google Cloud automatically enables Private Google Access for that range. Thus, any VMs in that subnet range use Private Google Access to connect to Google APIs and services. If the gateway provides NAT for a subnet range, Private Google Access is in effect for that range and can't be disabled manually. There are two types of Cloud NAT available in Google Cloud. First up, we have public NAT. Think of this as your gateway to the Internet. It enables the resources inside your virtual private cloud, those without public IP addresses, to securely access the online world. Here's how it works. When a resource needs to connect to the Internet, public NAT dynamically assigns it a public IP address from a shared pool. This way, your resource can browse, download updates, and do all the things it needs to online, while its internal IP address remains safely tucked away. Now, let's explore private NAT. This type of NAT you'll use for secure communication between different networks. One important subtype of private NAT is inter-VPC NAT. This comes in handy when you have multiple virtual private clouds that need to talk to each other. It acts like a translator, making sure communication flows smoothly between these different networks, all through a central hub like the Network Connectivity Center. With both public and private NAT options at your disposal, you have the flexibility to securely connect your resources in Google Cloud, whether they need to access the Internet or communicate with other private networks. Before we wrap up this module, let us take a look at a sample Gemini prompt related to private service access. Here's an example of an operational prompt. How can I use private service access to connect my VMs to third-party services? You'll be provided with a well-structured response that outlines the prerequisites, steps, and detailed commands that can help you configure private service access. The output structure makes it easy to follow and minimizes the risk of errors during implementation.

#### Lab Intro: Implement Private Google Access and Cloud NAT

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568839

Next, you will apply what you learned by completing a lab exercise. In this lab, you will complete the following tasks. Configure a VM instance that doesn't have an external IP address, connect to a VM instance using an identity-aware proxy tunnel, enable private Google access on a subnet, configure a Cloud NAT gateway, and verify access to public IP addresses of Google APIs and services and other connections to the Internet.

#### Implement Private Google Access and Cloud NAT

- https://www.cloudskillsboost.google/paths/14/course_templates/36/labs/568840

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/36/video/568841

In this module, you learned about several ways to connect privately from internal IP addresses to Google Cloud APIs, Google Services, and other resources. You can use Private Google Access to connect to Google APIs and Services. In addition to Google APIs and services, Private Service Connect lets you connect to other configured resources. Private Services Access simplifies creating a VPC network period connection between consumer and producer VPC networks. However, Private Services Access only works for some Google products and services. We then discussed using Cloud NAT to provide public Internet access for resources without public IP addresses. We finished the module with a lab exercise to implement Private Google Access with Cloud NAT, followed by a short quiz.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/36/quizzes/568842

### Course Resources

#### Networking in Google Cloud: Routing and Addressing Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/36/documents/568843

### Your Next Steps

## 05: Networking in Google Cloud: Load Balancing

- https://www.cloudskillsboost.google/paths/14/course_templates/1143

### Welcome to Networking in Google Cloud

#### Networking in Google Cloud Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577553

Welcome to the fifth course of the Networking in Google Cloud course series, Load Balancing. In this course, we will cover the topics listed on the screen. We’ll begin with an overview of Google Cloud load balancing, and continue with a discussion of hybrid load balancing - in other words, load balancing between Google Cloud, other public clouds, and on-premises environments. We’ll follow with a discussion of traffic management, which provides enhanced features to route traffic based on criteria that you specify. After that, you’ll apply what you’ve learned in a traffic management lab exercise. Next, we’ll discuss using internal Network Load Balancers as next hops, including benefits, caveats, and some use cases. We’ll continue by discussing how to use the Google global edge network to serve content closer to users with the Cloud Content Delivery Network (CDN), and discuss the benefit of using Google Cloud Armor with Cloud CDN to provide a secure and optimized web experience. We’ll follow that with a lab exercise. Finally, we’ll cover content caching and share some strategies to optimize load balancing, and end with a brief quiz. Let’s get started!

### Hybrid Load Balancing and Traffic Management

#### Load balancing

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577554

Welcome to the hybrid load balancing and traffic management module of the networking and Google Cloud load balancing course. In this module, we will cover the topics listed on the screen. We'll begin with an note review of cloud load balancing. We will continue with the discussion of hybrid load balancing. In other words, load balancing between Google Cloud, other public clouds, and on-premises environments. We will follow with a discussion on traffic management, which provides enhanced features to route traffic based on criteria that you specify. After that, you'll apply what you've learned in a traffic management lab exercise. Let's get started. A load balancer, as the name suggests, balances load across multiple instances of your applications. Cloud load balancing receives client traffic. This traffic can be external or internal depending on the load balancer you use. A backend configuration distributes requests to healthy backends. Some load balancers also support backend buckets. One or more back ends must be connected to the backend service or backend bucket. Backend configuration defines how traffic is distributed, which health check to use, if session affinity is used, and which other services are used, such as Cloud CDN or Identity-Aware Proxy. Cloud load balancing can route traffic to either a backend service or a backend bucket. The backend services define how to handle the traffic. For example, backend services define how the traffic is distributed, which health check to use, and if session affinity is used. Backend services also define which other Google Cloud services to use, such as Cloud CDN or Identity-Aware Proxy. On the other hand, backend buckets direct incoming traffic to Cloud Storage buckets. Backend buckets are useful in serving static content. We will discuss this in more detail in the upcoming section. Some of the backend services include a managed instance group or a network endpoint group or NEG. In this module, we are going to look at some special features related to network endpoint groups. Google Cloud Platform offers a range of load balancing solutions that can be classified based on the OSI model layer that they operate at and their specific functionalities. Application load balancers, these load balancers are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content based routing and SSL or TSL termination. Application load balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules that you define. They are highly flexible and can be configured for both Internet facing external and internal applications. Network load balancers. Network load balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types, proxy load balancers. These also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments and passthrough load balancers. Unlike proxy load balancers, these do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well suited for applications that require direct server return or need to handle a wide range of IP protocols. A network endpoint group or NEG is a configuration object that specifies a group of backend endpoints or services. A common use case for this configuration is deploying services and containers, as in Google Kubernetes Engine. The load balancer must be able to select a pod from the container, as shown in the example. You can also distribute traffic in a granular fashion to workloads and services that run on your backend hosts. You can use NEGs as backends for some load balancers with traffic director. There are five types of NEGs. Zonal NEGs define how end points should be reached, whether they are reachable and where they are located. A zonal NEG contains one or more endpoints that can be compute engine virtual machines, VMs, or services that run on VMs. Each endpoint is specified either by an IP address or an IP port combination. An Internet NEG contains a single endpoint that is hosted outside of Google Cloud. This endpoint is specified by host name FQDN port or IP port. A serverless NEG is a backend that points to a cloud-run app engine, cloud functions or API gateway service that resides in the same region as the NEG. A private service NEG contains a single endpoint. That endpoint resolves to either a Google managed regional API endpoint or a managed service published by using Private Service Connect. A hybrid connectivity NEG points to traffic director services that run outside of Google Cloud, on-premises or other public cloud backends. The focus in this module is on hybrid connectivity NEGs. For more information on using NEGs and a complete list of supported load balancers, please refer to the network endpoint groups overview in the Google Cloud documentation.

#### Hybrid load balancing

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577555

Let's next discuss hybrid load balancing. A hybrid load balancer, lets you extend Cloud load balancing to workloads that run on your existing infrastructure outside of Google Cloud. A hybrid strategy is a pragmatic solution for you to adapt to changing market demands and incrementally modernize the backend services that run your workloads. You can create a hybrid deployment to enable migration to a modern Cloud based solution or a permanent fixture of your organization IT infrastructure. Next, let's look at a few general use cases of hybrid load balancing. Jia, a Cloud network engineer, needs to modernize the symbol corporation IT infrastructure to improve application performance and scalability. Currently, Symbol runs a mix of on-premises applications and applications deployed in Google Kubernetes Engine, or GKE. Traffic is often unpredictable with sudden surges during sales or promotions, overwhelming their on-premises servers. Azumi knows they need a way to handle these spikes without compromising performance or spending a fortune on excess capacity that sits idle most of the time. Additionally, managing two separate load balancing solutions for the premises and GKE environments is becoming increasingly cumbersome. In this example, traffic from clients on the public Internet enters your private on-premises environment, and traffic from another public Cloud provider enters through a Cloud load balancer. The load balancer also gets requests from internal clients. The load balancer sends requests to the services that run your workloads. These services are the load balancer endpoints, and they can be located inside or outside of Google Cloud. You can figure a load balancer backend service to communicate to the external endpoints by using a hybrid NEG. The external environments can use Cloud Interconnect or Cloud VPN to communicate with Google Cloud. The load balancer must be able to reach each service with a valid IP address port combination. The example shows a load balancer backend service with a hybrid and a Zonal NEG. The hybrid NEG connects to endpoints that are on-premises and in other public Clouds. The Zonal NEG points to the Cloud endpoints in the same subnet and zone. A hybrid load balancer requires special configuration only for the backend service. The frontend configuration is the same as any other load balancer. To configure the backend services outside of Google Cloud, first configure one or more hybrid connectivity network endpoint groups or NEGs. Add each non Google Cloud Network endpoint IP port combination to a hybrid connectivity network endpoint group or NEG. Ensure that the IP address and port are reachable from Google Cloud. For hybrid connectivity NEGs, you set the network endpoint type to non GCP Private IP Port. Create the NEG in a Google Cloud zone that is as close as possible to your other environment. For example, if you're hosting a service in a non premises environment in Bengaluru, India, can place the NEG in the Asia South 1A Google Cloud zone, as shown in the example. Next, add a health check to the NEG. Add the hybrid connectivity NEGs to a hybrid load balancer backend. A hybrid connectivity NEG must only include endpoints outside Google Cloud. Traffic might be dropped if a hybrid NEG includes endpoints for resources within a Google Cloud VPC network. You can use hybrid load balancing with the following, global external application load balancer, classic application load balancer, regional external application load balancer, cross region internal application load balancer, regional internal application load balancer, external proxy network load balancer, global and regional, regional internal proxy network load balancer, and cross region internal proxy network load balancer. You choose a load balancer depending on your needs, such as where the clients and workloads are located. To create, delete, or manage a load balancer with mixed zonal and hybrid connectivity NEG backends in a single backend service, you must use the Google Cloud CLI or the Rest API. Regional dynamic routing, and static routes are not supported. The Cloud router used for hybrid connectivity must be enabled with global dynamic routing. The internal application load balancer and hybrid connectivity must be configured in the same region. If they are configured in different regions, you might see backends as healthy, but client requests will not be forwarded to the backend. Ensure that you also review the security settings on your hybrid connectivity configuration. Currently, HA Cloud VPN connections are encrypted by default, using IPSec encryption. Cloud Interconnect connections are not encrypted by default. For more details, go to the encryption in transit in Google Cloud on the Google Cloud website.

#### Traffic management

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577556

Traffic management is key to ensuring optimal network performance and user experience. In this section, we'll delve into a real world use case and its implementation. Bola, a Cloud network engineer at Cymbal Corporation, is responsible for maintaining their video streaming platform. The website accesses the Cymbal store infrastructure using Cloud load balancing. To optimize performance and costs, they want to route user traffic to different server backends based on the requested video quality. For example, 4K versus HD. What should Bola do? Bola can benefit from traffic management. Traffic management provides enhanced features to route load balancer traffic based on criteria that you specify. With traffic management, you can; implement traffic steering based on HTTPS parameters, such as the host, path, headers, and other request parameters. Perform request-based and response-based actions, such as redirects and header transformations, and use traffic policies to fine tune load balancing behavior, such as retry policies, request mirroring, and cross origin resource sharing or cores. The traffic features that are available can vary per load balancer. Check the Google Cloud documentation for details. For example, traffic management overview for a classic application load balancer, traffic management overview for global external application load balancers, and traffic management overview for regional external application load balancers. Recall that in addition to traffic management, Cloud load balancing offers backend services like health checks, session affinity, balancing mode, and capacity scaling. These load balancers support traffic management. Global external Application Load Balancer, Global classic Application Load Balancer, Internal Application Load Balancer, and the Regional external Application Load Balancer. Other load balancers have access to only traffic features available in backend services, such as balancing mode and session affinity. Not all load balancers support all traffic management features. For a complete list of traffic management features supported for each load balancer, refer to the routing and traffic management in the Google Cloud documentation. The URL map contains rules that define the criteria to use to route incoming traffic to a backend service or a backend bucket. Traffic management features are configured in a URL map. In other words, the load balancer uses the URL map to determine where to route incoming traffic. When you configure routing, you can choose between the following modes; simple host and path rule or advanced host path and route rule. Each URL map can contain only one mode or the other mode. A request is first evaluated based on the host rule. If the domain matches a defined host rule, the system uses its associated path matcher to further refine routing. Each host rule consists of a list of one or more hosts and a single path matcher. If no host rules are defined, the request is routed to the default service. The request path is compared against available path rules with the longest, most specific match taking priority. Path rules are evaluated on a longest path matches first basis. You can specify the path rules in any order. Once the best match is found, the request is sent to the corresponding backend service. If no specific host and path rules apply, the request is handled by the default backend service. This setup allows you to create custom routing rules like sending video related requests to a dedicated service while directing general traffic to a different backend. Going back to the scenario, Bola can use the URL map to distribute traffic. The host rule is cymbal.com, which means any host other than cymbal.com, for example, cymbal.org or cymbal.net are directed to the default service. Once the host name cymbal.com matches, the URL is matched for a path rule. The default backend service is video site. Requests with the exact URL path video-hd are directed to the video HD backend service. Requests with the exact URL path video-4k are directed to the video-4k backend service. On this slide, you see a URL map that routes video traffic to the example we covered in the previous slides. This example is shown by using a YAML file. You can also use the Google Cloud Console to configure URL maps. Let's look at how this example works. Default service. The default service defines a service where traffic should be routed when no matching URL is found. You must specify a default service or a backend bucket. The host rules defines a list of host names that are processed by this rule. In this example, there's only one item in the list, which means that only one host rule is defined. This host rule contains an asterisk. The asterisk is a wildcard, which matches all hosts. To see where to find the matching logic to use, look at the value of path matcher. For this host rule, pathMatcher is set to pathmap. Here's a path matchers list, which contains a list of path matching rules. The only element in this list is path map. Each match rule defines logic to process the traffic that is sent to the backend service. In this example, there are two sets of paths. One paths list defines valid URL paths for the video hd. The other paths list defines valid URL paths for the video 4K. If the URL contains a match for one of these paths lists, the load balancer routes the traffic to the corresponding service. If the traffic contains a path that matches none of the paths lists, then it's sent to the default backend video site. In other words, the traffic is sent to the service denoted by pathmatchers/defaultservice. For additional details, refer to the documentation. The advanced routing mode can choose a rule based on a defined priority and includes additional configuration options. Instead of path rules, advanced routing uses route rules. Each URL map can include either simple or advanced rules, but not both. This URL map contains rules that route 95% of the traffic to Service A, and 5% of the traffic is routed to Service B. The example shows a YAML implementation of an advanced routing mode. You can also use the Google Cloud Console to configure URL maps. Let's look at how this example works. When no matching host rule is found, the default service defines a service to use. The field default service is required. The host rules works the same way for simple routing mode. As in the previous example, this host rule uses the asterisk to match all hosts. Because path matcher is set to matcher 1, /pathMatchers/matcher1 defines the matching logic. /pathMatchers/matcher1 contains a list of route rules. The route rules contain a list of one or more match rules and a route action. When URL satisfy the match rules, their traffic is processed by the route action. In this example, there's only one item in match rules. Where prefix match equals an empty string, the prefix match condition matches the URL path prefix, URLs that start with the same string match. In the example, the prefix match is the empty string, which matches all URLs. In other words, all URLs trigger this match rule, and the route action is applied. The route action defines how the traffic is routed. In the example, the route action is set to weightedBackendServices. Weighted backend services is a list of backend services. A weight value is specified for each backend service, representing a percentage of the total traffic. Ninety-five percent of the traffic is sent to Service A, and 5% of the traffic is sent to Service B. The route action can also define traffic policies such as retry policies and cores. For a complete list of route action values, refer to the Google Cloud documentation for the load balancer that you're using. In this example, you might notice that there are two default service key value pairs. One default service is associated with the host rules and the other is associated with the route rules. If there's no matching host rule, the first default service is used. If there's no matching route rule, the second default service is used. Not all load balancers support all traffic management features. For a complete list of traffic management features supported for each load balancer, refer to routing and traffic management in the Google Cloud documentation. Wild cards are supported, but only after a forward slash. For example, /video/* is valid and /video* is invalid. Rule matching does not use regular expressions or substring matching. For example, /videos/hd* does not match /videos/hd-pdq, because -pdq is a substring and also because it comes after the forward slash. /video/* matches /video/hd-pdq. You can also use Gemini to learn more. One trick to get better responses is to assign Gemini a role. Prime the model to assume a specific role, also known as a persona. Adding a role is not always necessary, but can enforce a certain level of expertise when generating a response. Improve performance, and tailor's communication style. This technique is particularly useful for getting the model to perform highly technical tasks or enforcing specific communication styles. The example on the slide shows a sample prompt where Gemini assumes the role of a Google Cloud technical support engineer.

#### Lab Intro: Configuring Traffic Management with a Load Balancer

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577557

In this lab, you will create a regional internal application load balancer with two back ends. Each back end will be an instance group. You will configure the load balancer to create a blue green deployment; the blue deployment refers to the current version of your application. The green deployment refers to a new application version. In this lab exercise, you configure the load balancer to send 70% of the traffic to the blue deployment, and 30% to the green deployment.

#### Configuring Traffic Management with a Load Balancer

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/labs/577558

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/quizzes/577559

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577560

In this module, we began with an overview of load balancing in Google Cloud. We continued with the discussion of hybrid load balancing. You learned that hybrid load balancing can be used to migrate your workloads into Google Cloud or to provide multiple platforms for your workloads. We covered the load balancers that support hybrid load balancing and an overview of the components that you must configure. We then talked about using traffic management with your load balancers. You learned which load balancers support traffic management features. You were introduced to the URL map, where you configured traffic management features. We walked through a simple example of traffic management. In the example, you saw how to configure a URL map to match against incoming traffic and specify where the traffic should be sent. You then applied what you learned in a lab exercise. Finally, you took a brief quiz to test your knowledge.

### Caching and Optimizing Load Balancing

#### Internal Network Load Balancers as next hops

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577561

Let's cover some additional concepts regarding load balancing in this module. In this module, we will discuss using internal network load balancers as next hops, including benefits, caveats, and some use cases. We will also introduce Google Cloud Armor and cloud CDN. We will cover content caching and share some strategies to optimize load balancing. Let's get started. Before we cover how to use an internal network load balancer as a routing next hop, let's discuss why these load balancers are useful. They're fast. Internal network load balancers don't have the overhead associated with other types of cloud load balancers. The reduced overhead makes them fast. An internal network load balancer routes connections directly from clients to the healthy backend without any interruption. There's no immediate device or single point of failure. Client requests to the load balancer IP address go directly to the healthy backend VMs. Unlike other types of load balancers, there's minimal processing of the incoming traffic. Responses from the healthy backend VMs go directly to the clients, not back through the load balancer. TCP responses use direct server return. For more information, see IP addresses for request and return packets in the Google Cloud documentation. Lets consider some use cases for internal network load balancers. You can load balance traffic across multiple VMs that are functioning as gateway or router VMs. You can use gateway virtual appliances as the next hop for a default route. With this configuration, VM instances in your virtual private cloud or VPC network send traffic to the Internet through a set of load balanced virtual gateway VMs. You can send traffic through multiple load balancers in two or more directions by using the same set of multi-NIC gateway or router VMs as backends. To accomplish this result, you create a load balancer and use it as the next hop for a custom static route in each VPC network. Each internal network load balancer operates within a single VPC network, distributing traffic to the network interfaces of backend VMs in that network. In these use cases, the backend services are the gateway VMs, gateway virtual appliances, multi-NIC gateways, and router VMs. Because these resources are all internal, it makes sense to access them through an internal network load balancer. As we discussed a moment ago, these load balancers have lower overhead than other load balancers that Google Cloud offers. Next, let's consider how to access these backends even faster. To specify the next hop, you have three choices as shown in the table. The main difference concerns the location of the next hop load balancer. If the next hop load balancer is in the same VPC network, you can specify the forwarding rule name and the load balancer region. To use a next hop load balancer in a peered VPC network, specify the internal IP address of the forwarding rule. You can also specify a next hop forwarding rule by its resource link. The forwarding rules network must match the routes VPC network. The forwarding rule can be located in either the project that contains the forwarding rules network, a standalone project, or a shared VPC host project, or a shared VPC service project. This use case load balances traffic from internal vms to multiple network address translation gateway instances that route traffic to the Internet. In this example, an internal network load balancer has next hops configured to three compute engine VMs. Each compute engine VM has a NAT gateway that runs on it and has can IP forward set to be true. These VMs then forward traffic to the Internet. Optionally, you can set up the gateways to apply custom logic to fine tune access to the Internet. In addition to exchanging subnet routes, you can configure VPC network peering to export and import custom static and dynamic routes. Custom static routes that have a next hop of the default Internet gateway are excluded. Custom static routes that use next hop internal network load balancers are included. You can configure a hub and spoke topology with your next hop Firewall virtual appliances located in the hub VPC network by doing the following. In the hub VPC network, create an internal network load balancer with Firewall virtual appliances as the backends. In the hub VPC network, create a custom static route with the destination subnet 10.0.2.2.0/24 and set the next hop to be the internal network load balancer, 10.0.1.100. Use VPC network peering to connect the hub VPC network to each of the spoke VPC networks. For each peering, configure the hub network to export its custom routes and configure the corresponding spoke networks to import custom routes. Custom static route created in step two with destination subnet 10.0.2.0/24, the route with the load balancer next hop is one of the routes that the hub network exports. Subject to the routing order, the next hop Firewall appliance load balancer in the hub VPC network is available in the spoke networks. If global access is enabled, the Firewall appliance is available according to the routing order. If global access is disabled, then resources are only available to requesters in the same region. Internal network load balancer 1 shown on the left distributes traffic from the clients to nic0, the primary interface on the backend services. The internal network load balancer 2 shown on the right distributes traffic from clients to nic1, the secondary interface on the backend services. The result is that clients can connect to the backend services through nic0 or nic1. When the load balancer is a next hop for a static route, no special configuration is needed within the client VMs. Client VMs send packets to the load balancer backends through VPC network routing in a bump in the wire fashion. Using an internal pass through network load balancer as a next hop for a static route provides the same benefits as a standalone internal pass through network load balancer. The health check ensures that new connections are routed to healthy backend VMs. By using a managed instance group as a backend, you can configure autoscaling to grow or shrink the set of VMs based on service demand. You must enable global access on the VPC network so that the next hop is usable from all regions. Whether the next hop is usable depends on the global access setting of the load balancer. With global access enabled, the load balancer next hop is accessible in all regions of the VPC network. With global access disabled, the load balancer next hop is only accessible in the same region as the load balancer. With global access disabled, packets sent from another region to a route that uses an internal network load balancer next hop are dropped. Even if all health checks fail, the load balancer next hop is still in effect. Packets processed by the route are sent to one of the next hop load balancer backends. If needed, configure a failover policy. A next hop internal network load balancer must use an IP address that is unique to a load balancer forwarding rule. Only one backend service is unambiguously referenced. Two or more custom static route next hops with the same destination that use different load balancers are never distributed by using ECMP. If the routes have unique priorities, Google Cloud uses the next hop internal network load balancer from the route with the highest priority. If the routes have equal priorities, Google Cloud still selects just one next hop internal network load balancer. For packets with identical source IP addresses routed to the same backend, use the client IP, NO_DESTINATION, CLIENT_IP_NO_DESTINATION session affinity option. There are some additional caveats for using an internal network load balancer as a next hop. For example, pertain to the use of network tags. For additional information on this and other caveats, refer to additional considerations on the internal network load balancers as next hop's page in the Google Cloud documentation.

#### Cloud CDN

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577562

Next, let's discuss content delivery network or CDN, which caches content nearer to users. We'll also talk about CDN Interconnect, which lets select third party content delivery network or CDN providers establish direct interconnect links at edge locations in the Google network. Cloud CDN caches content at the edges of the Google network. This caching provides faster content delivery to users while reducing transmission costs. Content can be cached at CDN nodes as shown on the map. There are over 90 of these cache sites spread across metropolitan areas in Asia Pacific, the Americas, and AMIA. For an updated list, see the cache locations in the Google Cloud documentation. For Cloud CDN performance measured by Stexis, please see the reports on the Citrix website. When setting up the back end service of an application load balancer, you can enable Cloud CDN with a checkbox. Using cache modes, you can control the factors that determine whether Cloud CDN caches your content. Cloud CDN offers three cache modes. The cache modes define how responses are cached, whether Cloud CDN respects cache directives sent by the origin, and how cache TTLs are applied. The available cache modes are USE_ORIGIN_HEADERS, CACHE_ALL_STATIC, and FORCE_CACHE_ALL. USE_ORIGIN_HEADERS mode requires origin responses to set valid cache directives and valid caching headers. CACHE_ALL_STATIC mode automatically caches static content that doesn't have the no store private or no cache directive. Origin responses that set valid caching directives are also cached. FORCE_CACHE_ALL mode unconditionally caches responses, overriding any cache directive set by the origin. If you use a shared back-end with this mode configured, ensure that you don't cache private per user content, such as dynamic HTML or API responses. Let's walk through the Cloud CDN response flow with this diagram. In this example, the application load balancer has two types of back-ends. There are managed VM instance groups in the US central one and Asia East one regions, and there's a Cloud storage bucket in US East one. A URL map decides which back-end to send the content to. The Cloud storage bucket could be used to serve static content, and the instance groups could handle PHP traffic. When a user in San Francisco is the first to access content, the cache site in San Francisco sees that it can't fulfill the request. This situation is called a cache miss. If content is in a nearby cache, Cloud CDN might attempt to get the content from it. For example, if a user in Los Angeles has already accessed the content. Otherwise, the request is forwarded to the application load balancer, which in turn forwards the request to one of your back-ends. Depending on the content that is being served, the request will be forwarded to the US Central One Instance Group or the US East one storage bucket. If the content from the back end is cacheable, the cache site in San Francisco can store it for future requests. In other words, if another user requests the same content in San Francisco, the cache site might now be able to serve that content. This approach shortens the round trip time and saves the origin server from having to process the request. This is called a cache hit. For more information on what content can be cached, please refer to caching overview in the Google Cloud documentation. Each Cloud CDN request is automatically logged within Google Cloud. These logs will indicate a cache hit or cache miss status for each HTTP request of the load balancer. You will explore such logs in the next lab. Cache modes, let you control how content is cached. CDN Interconnect lets select third party content delivery network providers establish direct interconnect links at edge locations in the Google Edge network. These connections let you direct your traffic from your VPC networks to a CDN provider network. For a complete list of CDN providers, refer to the CDN Interconnect overview in the Google Cloud documentation. CDN Interconnect lets you connect directly to select CDN providers from Google Cloud. Your network traffic that egresses from Google Cloud through one of these links benefits from the direct connectivity to supported CDN providers. CDN Interconnect reduces your Cloud CDN cache population costs. If you have a high volume of egress traffic, consider using CDN Interconnect. You can use the CDN interconnect links between Google Cloud and selected providers to automatically optimize the egress traffic and save money. If you're populating the Cloud CDN cache locations with large data files from Google Cloud, this optimization can be especially helpful. Frequent content updates are another typical CDN Interconnect use case. Cloud workloads that frequently update data stored in Cloud CDN cache locations benefit from using CDN Interconnect. The direct link to the Cloud CDN provider reduces latency. Ingress traffic is free for all regions. Egress traffic rates apply only to data that leaves compute engine or Cloud storage. Egress charges for CDN interconnect appear on the invoice as compute engine network egress via carrier peering network. The special pricing for your traffic that egress from Google Cloud to a CDN provider is automatic. Google works with approved CDN partners and supported locations to accept provider IP addresses. Any data that you send to your allow listed CDN provider from Google Cloud is charged at the reduced price. This reduced price applies only to IPv4 traffic. It does not apply to IPv6 traffic. Intra-region pricing for CDN Interconnect applies only to intra-region egress traffic that is sent to Google Approved CDN providers at specific locations. CDN Interconnect does not require any configuration or integration with Cloud load balancing. If your CDN provider is already part of the program, you don't need to do anything. Traffic from supported Google Cloud locations to your CDN provider automatically benefits from the direct connection and reduced pricing. Work with your supported CDN provider to learn what locations are supported. Your supported CDN service provider can also help you correctly configure your deployment to use intra-region egress routes, which costs less than intra-region egress traffic.

#### Google Cloud Armor

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577563

Next, let's discuss the benefit of using Google Cloud Armor with Cloud CDN. Google Cloud Armor and Cloud CDN work in tandem to provide a secure and optimized web experience. Imagine Google Cloud Armor as your application's security guard. It inspects incoming traffic, filters out malicious requests like DDoS attacks and web exploits, and safeguards your backend servers from harm. This protection ensures your application stays up and running, even under attack. Meanwhile, Cloud CDN stores copies of your website's content, images, videos, etc, around the globe. When a user accesses your website, Cloud CDN delivers the content from the location closest to them. This significantly speeds up page load times, providing a seamless experience for your users while also reducing the strain on your primary servers. To protect CDN origin servers, you can use Google Cloud Armor with Cloud CDN. Google Cloud Armor protects your CDN origin server from application attacks, mitigates OWASP top ten risks, and enforces layer 7 filtering policies. There are two types of security policy that affect how Google Cloud Armor works with Cloud CDN, Edge security policies and backend security policies. Use Edge security policies to filter requests before content is served from cache. Use Google Cloud Armor backend security policies to protect requests routed to the backend service. Next, you will explore a lab covering how to use Google Cloud Armor to defend Edge cache.

#### Defending Edge Cache with Cloud Armor

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/labs/577564

#### Load balancer optimization strategies

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577565

Next, let’s discuss some load balancer optimization strategies. Optimizing your cloud load balancer configuration can significantly reduce costs without impacting performance or availability. Here are some key strategies: Autoscaling Dynamically adjust resources: Enable autoscaling to automatically scale backend instances based on real-time traffic demands. This ensures you only pay for the resources you need, eliminating overprovisioning and unnecessary costs. Define scaling thresholds: Set clear thresholds for scaling up and down to optimize resource utilization and prevent unnecessary scaling events. Utilize custom metrics: Leverage custom metrics like CPU usage or response time to trigger autoscaling, ensuring resources are allocated based on specific performance indicators. Rightsizing resources: Choose the right load balancer type: Select the most appropriate load balancer type (for example, Application, Proxy Network, or Passthrough Network) based on your traffic type and requirements. Avoid using a more expensive type than necessary. Match resources to workload: Select the appropriate machine type and size for your backend instances based on their anticipated workload. Overprovisioning resources will lead to higher costs, while underprovisioning can impact performance. Regularly review resource utilization: Monitor the CPU, memory, and network utilization of your backend instances and load balancers. If utilization is consistently low, consider downsizing resources for cost savings. Use Cloud Monitoring and cost management tools: Use Cloud Monitoring: Use Cloud Monitoring to gain insights into your load balancer and backend instance performance. Analyze metrics like CPU usage, memory consumption, and network bandwidth to identify areas for optimization. Leverage Cloud cost management tools: Use tools like Cloud Billing and Cloud cost management to track your load balancing costs and identify potential savings opportunities. These tools can provide detailed cost breakdowns by project, service, and resource, helping you identify underutilized resources or areas for potential consolidation. Cost allocation tags: Implement cost allocation tags to categorize your load balancing costs by department, project, or any other relevant criteria. This allows for more granular cost tracking and facilitates cost optimization efforts. Additional strategies: Scheduled downtimes: Consider scheduling downtimes for non-critical workloads during periods of low traffic to reduce costs. Reserved instances: Use reserved instances for predictable workloads to obtain significant discounts on load balancer resources. Spot instances: Explore using spot instances for non-critical workloads to take advantage of discounted compute resources. Cloud CDN integration: Integrate Cloud CDN with your load balancer to reduce the load on your backend instances and potentially reduce load balancer costs. Remember, cost optimization is an ongoing process. By implementing these strategies and continuously monitoring your cloud resources, you can achieve significant cost savings while ensuring your cloud infrastructure remains efficient and scalable.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/quizzes/577566

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/video/577567

In this module, we began with an overview of load balancing in Google Cloud. We continued with a discussion of hybrid load balancing. You learned that hybrid load balancing can be used to migrate your workloads into Google Cloud or to provide multiple platforms for your workloads. We covered the load balancers that support hybrid load balancing and an overview of the components that you must configure. We then talked about using traffic management with your load balancers. You learned which load balancers support traffic management features. You were introduced to the URL map, where you configure traffic management features. We walked through a simple example of traffic management. In the example, you saw how to configure a URL map to match incoming traffic and specify where the traffic should be sent. You then applied what you learned in a lab exercise. Next, we covered using internal Network Load Balancers as next hops. You learned about some of the major use cases, and some simple topologies were shown. We continued by discussing using Cloud CDN to get content to your clients faster. You also learned about CDN Interconnect to direct traffic from your VPC networks to a supported CDN provider network. You also learned how CDN Interconnect optimizes your Cloud CDN cache population costs. Finally, you took a brief quiz to test your knowledge.

### Course Resources

#### Networking in Google Cloud: Load Balancing Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/1143/documents/577568

### Your Next Steps

## 06: Networking in Google Cloud: Network Security

- https://www.cloudskillsboost.google/paths/14/course_templates/1142

### Welcome to Networking in Google Cloud

#### Course Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525498

Welcome to the fourth course of the "Networking in Google Cloud" series: Network Security! In this course, you'll dive into the services for safeguarding your Google Cloud network infrastructure. The first module, Distributed Denial of Service (DDoS) Protection, covers how to fortify your network against Distributed Denial of Service (DDoS) attacks, ensuring uninterrupted availability of your services. In the second module, Controlling Access to VPC Networks, you'll learn the network access control, enabling you to define permissions for who can access your resources and how. Finally, in the third module, Advanced Security Monitoring and Analysis, we'll explore how to proactively detect and respond to potential threats, keeping your Google Cloud environment secure and resilient. By the end of this course, you'll have a comprehensive understanding of Google Cloud network security.

### Distributed Denial of Service (DDoS) Protection

#### Module Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525499

Welcome to the Distributed Denial of Service Attacks (DDoS) Protection module. Distributed denial of service attacks are a major concern today. They can have a huge—and potentially fatal—impact on businesses if the business is not adequately prepared. We will start this module with a quick discussion on how DDoS attacks work. And then, we will review some DDoS mitigation techniques that are provided by Google Cloud. Then, we will finish up with a review of complementary partner products and a lab where you will get a chance to see some DDoS mitigations in action. OK, let's get started!

#### How DDoS attacks work

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525500

A distributed denial-of-service (or DDoS) attack is a malicious attempt to disrupt normal traffic of a targeted server, service, or network by overwhelming the target or its surrounding infrastructure with a flood of internet traffic from multiple sources. Essentially, it is an attempt to make an online service unavailable by overwhelming it with traffic from multiple sources. DDoS attacks can come from individuals, cybercriminal groups, or can even be state-sponsored. In the diagram, attackers build networks of infected computers, known as 'botnets', by spreading malicious software through emails, websites, and social media. Once infected, these machines can be controlled remotely without their owners' knowledge. They are then used like an army to launch an attack against any target. Some botnets are millions of machines strong. For context, a large attack in 2017 had a strength of around four terabit per second. For reference, the whole internet has a bisection bandwidth of 200 terabits per second. Now, when you compare this to a single Google data center, which has a bisection bandwidth of 1,300 terabits per second, you can see we have internal capacity many times that of any traffic load we can anticipate. This means that, when there is an attack, we have time to isolate it and address it. Over the past few years, Google has observed that distributed denial-of-service (DDoS) attacks are increasing in frequency and growing in size exponentially. On August, 2022, a Google Cloud Armor customer was targeted with a series of HTTPS DDoS attacks which peaked at 46 million requests per second. This is the largest layer 7 DDoS reported to date—at least 76% larger than the previously reported record. In August 2023, we stopped an even larger DDoS attack—7½ times larger—398M rps. Cloud Armor blocked the attack ensuring the customer's service stayed online and continued serving their end-users.

#### Google Cloud mitigations

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525501

Now let's review some DDoS mitigation techniques that are provided by Google Cloud. Kai is a network engineer at Cymbal Corporation. Their current DDoS solution is outdated and resource-intensive, struggling to handle the increasing traffic volume and sophistication of modern attacks. Kai needs a scalable, cloud-based solution that is cost effective and does not require contracts or long-term agreements. A solution that can effectively defend their website without impacting performance. Let us see the ways you can address this challenge. Creating secure applications requires a multi-faceted approach which has been customized to fit your business’ needs, vulnerabilities, and resources. Properly understanding the different options available when facing a DDoS attack can help your organization create a plan to minimize the impact. Let’s look at these generalized strategies in more detail and discuss how Google Cloud helps you implement them. In this lesson, you will learn more about: Leveraging Cloud Load Balancing. Reducing the network attack surface. Isolating internal traffic. Using Cloud CDN. Using API management and monitoring. Leveraging the Google Cloud Armor defense service. Cloud Load Balancing provides built-in defense against infrastructure DDoS attacks— —and no additional configuration is needed. Placing a load balancer in front of your services will filter known-bad traffic streams before they reach your resources. Google Cloud offers load balancing at layer 4 (the transport layer, such as TCP or UDP) and layer 7 (the application layer, generally HTTP or HTTPS). The layer 4 load balancers automatically protect against things like UDP floods and TCP SYN floods. The layer 7 load balancers provide layer 4 protection plus protection from connection-based attacks like Slowloris. Google Cloud load balancers leverage Google’s global DoS mitigation service. If the system detects an attack, it will automatically configure the load balancers to drop or throttle traffic. An attack surface of a software environment is the sum of the different points where an unauthorized user can try to enter data to or extract data from this environment. Keeping the attack surface as small as possible is a basic security measure. Reducing the attack surface means reducing how much exposure your VMs have to the internet. You should host Compute Engine resources that require network communication on the same VPC network. If the resources aren’t related and don’t require network communication among themselves, consider hosting them on different VPC networks. For most applications implemented in Google Cloud, Google also recommends creating separate subnets within a network for each tier of an application (for example, web front end, services layer, and database backend). That is because subnetting is a convenient way to implement inter-network firewall restrictions. You can control individual ingress and egress traffic for compute resources using firewall rules. Be sure you are blocking both unused ports as well as unwanted sources. Remember, you can use firewall tags and service accounts to help control which targets to use for firewall rules. It is also important to ensure you restrict external traffic within your VPCs. Virtual machines should not be given public IP addresses unnecessarily. Even if you need to connect to the VM from the internet, leveraging solutions like Identity-Aware Proxy or bastion hosts can help restrict the internal traffic. You can also connect your on-premise network with your VPC network using VPN IPsec Tunnels or Dedicated Interconnect. Google’s Cloud Content Delivery Network (or CDN) is used to cache web content at over 90 edge locations, or points of presence (POPs), around the globe. Cloud CDN provides very similar protection as Google’s load balancers. In addition, requests for your content are routed to Google’s POPs (points of presence) rather than directly to your resources. Thus, Google Cloud’s resilient network infrastructure absorbs the brunt of attacks. This also naturally reduces the load on your resources even when there are no attacks. For IT, network, and DevOps teams, allowing access to backend services is often required to facilitate interactions between applications, services, customers, and business partners. This access can also introduce vulnerabilities and challenges. Putting an API gateway, or API management, in front of your backend services can help prevent denial of service attacks by: Throttling requests to limit the number of requests per client. Controlling access to API from a single centralized location. Adding the ability to monitor and track all API usage. In Google Cloud, use Apigee for implementing API management. Throttling requests to limit the number of requests per client. Controlling access to API from a single centralized location. Adding the ability to monitor and track all API usage. In Google Cloud, use Apigee for implementing API management. Google Cloud Armor is a DDoS and application defense service. It delivers defense at scale against infrastructure and web application Distributed Denial of Service (DDoS) attacks using Google’s global infrastructure and security systems. Similar to CDNs, Google Cloud Armor protection is delivered at the edge of Google’s network and can block attacks close to their source before they have a chance of affecting your applications. Google Cloud Armor works with the global external Application Load Balancer to provide built-in defenses against infrastructure DDoS attacks. It defends against both network-layer (L3/L4) and application-layer (L7) DDoS attacks, safeguarding your services from being overwhelmed by malicious traffic. Google Cloud Armor comes with pre-defined rulesets specifically designed to protect against the OWASP Top 10 web application vulnerabilities. These include common threats like SQL injection (SQLi), cross-site scripting (XSS), and insecure deserialization. You can easily configure Google Cloud Armor to block traffic originating from specific countries or regions. This is helpful for preventing attacks from known malicious sources or complying with regional regulations. Google Cloud Armor also provides detailed logs for analysis and monitoring of traffic patterns and potential security threats. Google Cloud Armor defense is customized using a security policy which can contain one or more rules. Rules tell your security policy what to do (the action), when to do it (the condition), and where to apply the rule (the target). Google Cloud Armor also provides several predefined rules to defend against cross-site scripting (XSS) and SQL injection (SQLi) application-aware attacks. Google Cloud Armor also provides the following features: Variety of load balancer support: Google Cloud Armor now supports a variety of load balancers: Global external Application Load Balancer Regional external Application Load Balancer Classic Application Load Balancer External proxy Network Load Balancer External passthrough Network Load Balancer Rate limiting: rate-based rules help you protect your applications from a large volume of requests that flood your instances and block access for legitimate users. Adaptive protection: helps you protect your Google Cloud applications, websites, and services against L7 distributed denial-of-service (DDoS) attacks such as HTTP floods and other high-frequency layer 7 (application-level) malicious activity. Cloud Armor bot management with reCAPTCHA Enterprise: helps you evaluate and act on incoming requests that might be from automated clients. Custom rules language: enables you to define prioritized rules with configurable match conditions and actions in a security policy. For the latest Google Cloud Armor updates, check out the Google Cloud Armor release notes. To explore Google Cloud Armor features further, check out the Securing your Network with Cloud Armor quest. Cloud Armor Enterprise expands upon Google Cloud Armor Standard, providing enhanced security capabilities for your applications and infrastructure. It offers flexible pricing models—annual for predictable budgeting or pay-as-you-go for scalability. Cloud Armor Enterprise includes unlimited access to the Web Application Firewall (WAF), covering rules, policies, and requests for comprehensive and simplified protection. Proactive security is bolstered by curated third-party IP lists and Google Threat Intelligence insights, keeping you ahead of emerging threats. The service employs advanced protection mechanisms, including machine learning-powered Adaptive Protection for Layer 7 and robust defenses against DDoS attacks for pass-through endpoints. If you opt for the annual plan, you'll gain access to DDoS bill protection and expert support from the DDoS response team, as well as in-depth visibility into DDoS attack patterns to help you strengthen your security posture. Cloud Armor Enterprise equips you with a robust and adaptable security toolkit to defend against the ever-changing threat landscape and keep your digital assets secure.

#### Types of complementary partner products

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525502

As you have seen, here at Google, we offer some of the best in class platform security, but we did not stop there. Google also partners with a number of security-centric firms. In this section, we will review some of the complementary partner products. There are several different categories of security in our security ecosystem: Data protection, which includes things like: Governance Data loss prevention Data-centric audit and protection Encryption Hardware security modules Infrastructure protection, which includes: DDoS protection Network and application firewalls Intrusion detection and prevention Container security Scanning, logging, and monitoring, which includes a vulnerability scanner and security and information management tools. Identity and user protection, which includes: Single sign on Identity and Access Management Anti-malware Mobile device and application management Cloud access security brokers Configuration, vulnerability, risk, and compliance protection across all areas of your infrastructure. Infrastructure protection helps protect your cloud infrastructure and applications from cyberattacks. There are many industry leaders that provide services that can be leveraged from Google Cloud covering a wide range of solutions, including: Next generation firewalls Web application firewalls Web proxies and cloud gateways Server endpoint protection Distributed denial of service And container security Data protection partners can help protect your data from unauthorized access, as well as internal and external threats through encryption, key management, and policy-driven data loss prevention controls. Logging and monitoring partners help enable visibility and auditability of user and system activities in your infrastructure, while providing policy-driven alerting and reporting. Configuration, vulnerability, risk, and compliance partners can facilitate the visualization and inspection of your network and application deployments for vulnerabilities, security, and compliance risks, and assist with remediation.

#### Lab Intro Configuring Traffic Blocklisting with Google Cloud Armor

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525503

Next, you will see Google Cloud Armor in action. In this lab, you will perform the following tasks: Configure an Application Load Balancer for a simple web application, And use Google Cloud Armor to blocklist an IP address and restrict access to an Application Load Balancer

#### Configuring Traffic Blocklisting with Google Cloud Armor

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/labs/525504

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525505

This concludes the module on DDoS protection. Before we wrap up, let's explore some useful Gemini prompts that can help you with related questions. The slide displays a few sample prompts to get you started. This module provided a comprehensive overview of Distributed Denial of Service (DDoS) attacks, their mechanisms, and how Google Cloud Armor effectively mitigates them. You learned about the various types of DDoS attacks and Google Cloud's multi-layered protection approach. Additionally, you explored complementary partner products that can enhance your defense strategy. In the hands-on lab, you configured Traffic Blocklisting with Google Cloud Armor, and a quiz reinforced your understanding of the key concepts covered in this module.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/quizzes/525506

### Controlling Access to VPC Networks

#### Module Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525507

Welcome to the Controlling access to VPC Networks module. In this module, we’ll cover some ways to restrict access to your VPC networks. We’ll begin with an overview of the IAM (Identity Access Management) resource hierarchy and policy constraints. We will then cover firewall rules and how they further control access to your VPC networks. You will then use what you learned in a lab exercise, Controlling Access to VPC Networks, followed by a brief quiz. We will begin with IAM roles.

#### IAM roles

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525508

Karen, a cloud network administrator, is tasked with ensuring that principles of least privilege are maintained when granting access to cloud network engineers. Sarah must be able to view VPC network firewall rules. Jamal is tasked with tracking user access to cloud networking resources. Kalani must be able to view VPC network configurations. How can Karen grant everyone the access that they need? To answer this question, let’s explore IAM first. In the diagram, you can see a sample IAM resource hierarchy. Let’s use the diagram to review how IAM works. Google Cloud resources are organized hierarchically as shown in this tree structure. The Organization node is the root node in this hierarchy. Folders are the children of the organization. Projects are the children of the folders. And the individual resources are the children of projects. Each resource has exactly one parent. IAM allows you to set policies at all of these levels, where a policy contains a set of roles and members. Let’s go through each of the levels from top to bottom, as resources inherit policies from their parent. The organization resource represents your company. IAM roles granted at this level are inherited by all resources under the organization. The folder resource could represent your department. IAM roles granted at this level are inherited by all resources that the folder contains. Projects represent a trust boundary within your company. Services within the same project have a default level of trust. The IAM policy hierarchy always follows the same path as the Google Cloud resource hierarchy. This means that, if you change the resource hierarchy, the policy hierarchy also changes. For example, moving a project into a different organization will update the project's IAM policy to inherit from the new organization's IAM policy. Another thing to point out is that child policies cannot restrict access granted at the parent level. For example, if someone grants you the editor role for Department X and someone grants you the viewer role at the bookshelf project level, then you still have the editor role for that project. Therefore, it is a best practice to follow the principle of least privilege. The principle applies to identities, roles, and resources. Always select the smallest scope that’s necessary to reduce your exposure to risk. NOTE: Deny policies take precedence over access policies. They provide more granular control. Deny policies were recently introduced so you can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. Each project, folder, and organization can have up to 5 deny policies attached to it. In addition to the basic roles, IAM provides predefined roles that give granular access to specific Google Cloud resources and prevent unwanted access to other resources. These roles are collections of permissions. Most of the time, to do any meaningful operations, you need more than one permission. For example, in this slide, a group of users is granted the network viewer role on project_a. This provides the users of that group a lot of permissions. Some are illustrated on the right side. The permissions are classes and methods in the APIs. For example, compute.networks.list can be broken into the service, resource, and verb, meaning that this permission is used to list all of the VPC networks that project_a contains. Grouping these permissions into roles and having those roles represent abstract functions makes them easier to manage. Also, users can have multiple roles, providing flexibility. In addition to the predefined roles, IAM also provides the ability to create customized IAM roles. You can create a custom IAM role with one or more permissions, and then grant that custom role to users who are part of your organization. In essence, custom roles enable you to enforce the principle of least privilege, ensuring that the user and service accounts in your organization have only the permissions essential to performing their intended functions. For example, you might want a user to create, modify, and delete firewall rules but have read-only permissions to SSL certificates. In this case, the security administrator role provides too many permissions and the network administrator role does not provide enough. So, you can select the corresponding permissions for firewall rules and SSL certificates as shown on the left side along with any other permissions to create a new custom network administrator role. IAM provides a UI and an API for creating and managing custom roles. For more information on custom roles, refer to Custom roles in the Google Cloud documentation. Let’s focus on predefined roles that provide granular access to VPC networking resources. There is the network viewer role, that provides read-only access to all networking resources. For example, if you have software that inspects your network configuration, you could grant that software’s service account the network viewer role. Next, the network administrator role contains permissions to create, modify, and delete networking resources, except for firewall rules and SSL certificates. In other words, the network administrator role allows read-only access to firewall rules, SSL certificates, and instances to view their ephemeral IP addresses. The security administrator role contains permissions to create, modify, and delete firewall rules and SSL certificates. Now, there are other predefined roles for networking resources that relate to Shared VPC, which allow an organization to connect resources from multiple projects to a common VPC network. We will cover Shared VPC, along with those other predefined roles, in a later module of this course. For more information on these roles, see Compute Engine IAM roles and permissions in the Google Cloud documentation. Going back to the use case. Karen uses IAM to ensure the network engineers have just enough access to do their jobs. If a predefined role exists that provides just the access that a cloud network engineer needs, she adds the engineer to it. If no predefined role exists with sufficient permissions, Karen creates a custom role with just enough permissions for the network engineer job responsibilities and then adds the engineer to it.

#### Firewall rules

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525509

Applying rules to all instances in the network means the rule will apply to every instance running in that VPC network without having to tag or mark the instances in any other way. Applying rules to instances tagged with a specified target tag requires any instance needing the firewall rule to be “tagged” with the firewall rule target tag. Lastly, applying firewall rules to specific service accounts will apply those rules to both new instances created and associated with the service account and existing instances if you change their service accounts. Note that changing the service account associated with an instance requires that you stop and restart it for the change to take effect. Google Cloud firewalls are stateful, which means that, for each initiated connection tracked by allow rules in one direction, the return traffic is automatically allowed regardless of any other rules in place. In other words, firewall rules allow bi-directional communication once a session is established. The connection is considered active if at least one packet is sent every 10 minutes. To apply firewall rules to multiple VPC networks in an organization, use firewall policies. Network firewall policies use tags. Tags are key-value pairs defined at the organization level that provide a flexible way to identify and group resources for firewall rules with granular control through IAM permissions. A firewall rule is composed of many settings that are specified by the following five parameters: Direction: rules can be applied depending on the connection direction, values can be ingress or egress. Source or destination: the source parameter is only applicable to ingress rules and the destination parameter is only applicable to egress rules. Firewall targets can be applied to all instances in a network, source tags, and service accounts, and can be further filtered by IP addresses or ranges. Protocol and port: the protocol, such as TCP, UDP, or ICMP and port number. You can specify a protocol, a protocol and one or more ports, a combination of protocols and ports, or nothing. If the protocol is not set, the firewall rule applies to all protocols. Action: an action can be set to either allow or deny, and will determine if the rule permits or blocks traffic. Priority: a numerical value from zero to 65,535, which is used to determine the order the rules are evaluated. Rules are evaluated starting from zero, so a lower number indicates a higher priority. If you do not specify a priority when creating a rule, it is assigned a priority of 1000. When evaluating rules, the first rule that matches is the one that will be applied. If two rules have the same priority, the rule with a deny action overrides a rule with an allow action. Implied IPv4 firewall rules are present in all VPC networks, regardless of how the networks are created, and whether they are auto mode or custom mode VPC networks. The default network has the same implied rules. Implied IPv4 allow egress rule. An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65,535) lets any instance send traffic to any destination, except for traffic blocked by Google Cloud. Implied IPv4 deny ingress rule. An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65,535) protects all instances by blocking incoming connections to them. A higher priority rule might allow incoming access. If IPv6 is enabled, the VPC network also has these two implied rules: The implied IPv6 allow egress rule. An egress rule whose action is allow, destination is any IPv6 address, and priority is the lowest possible (65,535) lets any instance send traffic to any destination, except for traffic blocked by Google Cloud. A higher priority firewall rule may restrict outbound access. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address. Implied IPv6 deny ingress rule. An ingress rule whose action is deny, source is any IPv6 address, and priority is the lowest possible (65,535) protects all instances by blocking incoming connections to them. A higher priority rule might allow incoming access. The implied rules cannot be removed, but they have the lowest possible priorities. For more information, check out the Google Cloud documentation on default firewall rules. In Google Cloud, all projects get a default VPC created automatically. In addition to the implied rules, the default VPC network is pre-populated with firewall rules that allow incoming, or ingress, traffic to instances. The first rule is default-allow-internal, which allows ingress connections for all protocols and ports among instances within the VPC network. It effectively permits incoming connections to VM instances from others in the same network. The other three rules in the default network are default-allow-ssh, default-allow-rdp, and default-allow-icmp. These rules allow port 22, secure shell (ssh), port 3389, remote desktop protocol (RDP), and ICMP traffic respectively, from any source IP address to any instance in the VPC network. All of these rules have the second-to-lowest priority of 65,534. As you may have noticed some of these rules can be a little dangerous. These rules can (and should) be deleted or modified as necessary. Some network traffic is always allowed. For VM instances, VPC firewall rules, and hierarchical firewall policies do not apply to: Packets sent to and received from the Google Cloud metadata server, And packets sent to an IP address assigned to one of the instance's own network interfaces (NICs) where packets stay within the VM itself. IP addresses assigned to an instance's NIC include: The primary internal IPv4 address of the NIC. Any internal IPv4 address from an alias IP range of the NIC. If IPv6 is configured on the subnet, any of the IPv6 addresses assigned to the NIC. An internal or external IPv4 address associated with a forwarding rule for load balancing or protocol forwarding if the instance is a backend for the load balancer or is a target instance for protocol forwarding. Loopback addresses. And, addresses configured as part of networking overlay software you can run within the instance itself. Check out the link in the Course Resources section for more information. There is some network traffic that is always blocked on VPC networks. Google Cloud blocks incoming DHCP offers and acknowledgments from all sources except for DHCP packets coming from the metadata server. External IPv4 and IPv6 addresses only accept TCP, UDP, ICMP, ICMPv6, IPIP, AH, ESP, SCTP, and GRE packets. There are a few firewall rule best practices to help secure instances running in Compute Engine. Keep your firewall rules in line with the model of least privilege. Create rules to explicitly allow only traffic necessary for your applications to communicate. It is always best to minimize direct exposure to the internet. To do this, avoid having “allow” firewall rules defined with the source or destination range set to 0.0.0.0/0. To prevent ports and protocols from being exposed accidentally, create a firewall rule with the lowest priority that blocks all outbound traffic for all protocols and ports. This rule will override the implied egress rule that allows all outbound traffic and instead lock down your Compute Engine instances from making connections. You should then create higher-priority firewall rules for specific Compute Engine instances to open required ports and protocols. This helps prevent ports and protocols from being exposed unnecessarily. Another best practice is to adopt a standard naming convention for firewall rules. The exact format is not critically important, just create a standard and be consistent. An example of a naming convention would be to include the following information in your firewall rules: The direction, which is ingress or egress allow or deny indicating the rule’s action. The service or protocol name. The word “from” or “to” and then a short description of the source or destination. Examples using this formation would be ingress-allow-ssh-from-onprem and, egress-allow-all-to-gcevms. When applying firewall rules, you should consider using service account firewall rules instead of tag-based rules. The reason for this is that tag-based firewall rules can be applied by any user who has the Compute Engine instance administrator role, but users require explicit IAM rights to use a service account. Cloud Next Generation Firewall packages firewall rules into firewall policies. Cloud NGFW consists of more than just Allow or Deny rules at the VPC network level. Cloud NGFW provides the ability to apply: Policies to VPC networks globally or regionally. Policies hierarchically to organizations, folder, and projects— and the ability to delegate an action to a lower level in the hierarchy. Layer 7 filtering, allowing you to control traffic based on applications. And enhanced filtering, based on URL, fully qualified domain names, and geolocations. Cloud NGFW provides an intrusion prevention service to detect and block known attack patterns, and also integrates with Google Threat Intelligence, to stay updated on the latest threats. Hierarchical firewall policies let you create and enforce a consistent firewall policy across your organization. You can assign hierarchical firewall policies to the organization as a whole or to individual folders. These policies contain rules that can explicitly deny or allow connections, as do Virtual Private Cloud (VPC) firewall rules. Global and regional network firewall policies improve upon the previous VPC firewall rules structure. Similar to hierarchical firewall policies, these network firewall policy structures act as a container for firewall rules. Rules defined in a network firewall policy are enforced once the policy is associated with a VPC network, enabling simultaneous batch updates to multiple rules in the same policy. The same network firewall policy can be associated with more than one VPC network, and each VPC network can only have one global network firewall policy and one regional firewall policy per region associated with it. Both global network firewall policies and regional network firewall policies support IAM-governed tags, and all Cloud firewall enhancements moving forward will be delivered on the new network firewall policy constructs. A global network firewall policy provides a global firewall configuration structure to match the global nature of Google Cloud VPC networks. It applies to workloads deployed in all Google Cloud regions in the VPC network. A regional network firewall policy provides a regional firewall configuration structure for Google Cloud firewalls that can only be used in a single target region. When using regional network firewall policies, users can designate a target region for a firewall policy. The firewall configuration data will be applied to workloads only in that specific region and will not be propagated to any other Google Cloud regions. Firewall Insights, a component product of Network Intelligence Center, produces metrics and insights that let you make better decisions about your firewall rules. It provides data about how your firewall rules are being used, exposes misconfigurations, and identifies rules that could be made more strict. Firewall Insights uses Cloud Monitoring metrics and Recommender insights. Cloud Monitoring collects measurements to help you understand how your applications and system services are performing. A collection of these measurements is generically called a metric. The applications and system services being monitored are called monitored resources. Measurements might include the latency of requests to a service, the amount of disk space available on a machine, the number of tables in your SQL database, the number of widgets sold, and so forth. Resources might include virtual machines, database instances, disks, and so forth. Recommender is a service that provides recommendations and insights for using resources on Google Cloud. These recommendations and insights are per-product or per-service, and are generated based on heuristic methods, machine learning, and current resource usage. You can use insights independently from recommendations. Each insight has a specific insight type. Insight types are specific to a single Google Cloud product and resource type. A single product can have multiple insight types, where each provides a different type of insight for a different resource. Cymbal has recently migrated its on-premises networks to Google Cloud. Kwan, a network engineer, is tasked with securing the Cymbal VPC networks. The firewall solution from the on-premises network works, but Kwan wants a flexible solution that can be applied at multiple levels of the Cymbal cloud infrastructure hierarchy. What should Kwan use? Solution: Kwan should use Cloud Firewall. Cloud Firewall includes hierarchical firewall policies that can be applied at the organization, folder, project or VPC level. It also includes global and regional network firewall policies.

#### Lab Intro Configuring VPC Firewalls

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525510

Next, in a lab exercise, you’ll control access to VPC networks. In this lab, you investigate Virtual Private Cloud (VPC) networks and create firewall rules to allow and deny access to a network and instances.

#### Configuring VPC Firewalls

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/labs/525511

#### Cloud IDS

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525512

Let’s talk briefly about another Google Cloud security offering—Cloud IDS. Quinn, a network engineer at Cymbal Corporation, is looking for a way to improve the security of the company's cloud infrastructure. Cymbal has observed a concerning rise in cyberattacks targeting sensitive customer and financial data. These attacks range from unauthorized access attempts to the delivery and execution of malicious software. Additionally, covert monitoring tools and command-and-control attacks attempting to establish communication channels between compromised systems and external servers have been detected. Such threats pose a substantial risk to Cymbal's reputation, customer trust, and regulatory compliance, underscoring the urgent need for advanced security measures. Solution To mitigate these escalating cyber threats, Cymbal has implemented Google Cloud IDS. Cloud IDS is a network security service offered by Google Cloud that provides real-time detection of intrusions, malware, spyware, and command-and-control attacks. With comprehensive monitoring of both internal and external traffic, Cloud IDS offers Cymbal improved visibility into their network and system vulnerabilities. The scalability of the service allows Cymbal to adapt their threat detection capabilities as their infrastructure expands. As a fully managed service, Cloud IDS simplifies security management, enabling Cymbal's IT team to focus on other priorities while ensuring the safeguarding of their critical assets. Cloud IDS is an intrusion detection service that provides threat detection for intrusions, malware, spyware, and command-and-control attacks on your network. Cloud IDS works by creating a Google-managed peered network with mirrored VMs. Traffic in the peered network is mirrored, and then inspected by Palo Alto Networks threat protection technologies to provide advanced threat detection. Cloud IDS provides full visibility into network traffic, including both north-south and east-west traffic, letting you monitor VM-to-VM communication to detect lateral movement. Cloud IDS provides full visibility into network traffic, including both north-south and east-west traffic, letting you monitor VM-to-VM communication to detect lateral movement. Cloud IDS gives you immediate indications when attackers are attempting to breach your network, and the service can also be used for compliance validation, like PCI 11. In addition, Cloud IDS automatically updates all signatures without any user intervention, enabling users to focus on analyzing and resolving threats without managing or updating signatures. To better understand Cloud IDS, it’s important to understand how the service uses endpoints and packet mirroring. Cloud IDS uses a resource known as an IDS endpoint, a zonal resource that can inspect traffic from any zone in its region. Each IDS endpoint receives mirrored traffic and performs threat detection analysis. Cloud IDS uses Google Cloud packet mirroring, which creates a copy of your network traffic. After creating an IDS endpoint, you must attach one or more packet mirroring policies to it. These policies send mirrored traffic to a single IDS endpoint for inspection. The packet mirroring logic sends all traffic from individual VMs to Google-managed IDS VMs. For example, all traffic mirrored from VM1 and VM2 will always be sent to IDS-VM1.

#### Lab Intro Getting Started with Cloud IDS

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525513

Next, we will explore a lab exercise on Cloud IDS. In this lab, you deploy Cloud Intrusion Detection System (Cloud IDS), a next-generation advanced intrusion detection service that provides threat detection for intrusions, malware, spyware, and command-and-control attacks. You simulate multiple attacks and view the threat details in the Google Cloud console.

#### Getting Started with Cloud IDS

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/labs/525514

#### Secure Web Proxy

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525515

Let us next explore Secure Web Proxy and its use case. Secure Web Proxy is a Google Cloud service designed to enhance the security of outbound web traffic (HTTP/S) from various sources, including virtual machines, containers, serverless environments, and workloads outside of Google Cloud. It acts as a gateway, filtering traffic based on configurable policies that leverage cloud identities and web applications. This enables organizations to enforce granular control over web access, improving overall security posture while maintaining flexibility and ease of use. Secure Web Proxy provides the following benefits: Streamlined Cloud migration: Secure Web Proxy simplifies your transition to Google Cloud by maintaining your current security policies for outbound web traffic. This eliminates the need for third-party tools or manual configuration adjustments. Controlled access to external services: By allowing you to define granular access policies, Secure Web Proxy enhances the security of your network. You can establish specific identities for workloads or applications and then apply policies to various web locations. Monitored access to untrusted websites: Secure Web Proxy identifies and logs any traffic that deviates from your established policies, providing you with valuable insights. This allows you to monitor internet usage, uncover potential threats, and respond proactively to safeguard your network. Let’s explore the last one on the list. Kwan, a network engineer at Cymbal Corporation, is staring down a growing network infrastructure headache. Kwan needs a unified, automated solution that simplifies network management across both on-premises and cloud environments, while providing granular security and cost control. Secure Web Proxy lets you apply granular access policies to your egress web traffic so that you can secure your network. This allows you to programmatically restrict cloud workload access to only trusted external web services. Secure Web Proxy enables you to create very specific rules for outgoing web traffic from your cloud environment. This means you can define exactly which external websites and services your cloud workloads are allowed to access. By doing this, you can significantly increase the security of your network. You're essentially creating an allowlist of approved web destinations, preventing your systems from communicating with potentially harmful or unauthorized websites. This programmatic restriction ensures that your cloud workloads interact only with the specific external web services you trust, minimizing the risk of data breaches, malware infections, and other security threats. It's a proactive approach to cybersecurity that helps you maintain control over your network traffic and safeguard your valuable assets.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525516

This concludes the module. Before we wrap up, let's explore some useful Gemini prompts that can help you with related questions. The slide displays a few sample prompts to get you started. In this module, you learned about controlling access to VPC networks using IAM. You saw a sample IAM resource hierarchy and were shown how IAM policies controlled access to the Google Cloud resources. You then saw how policy constraints and firewall rules can fine-tune resource access. We also covered SWP and Cloud IDS. You applied what you learned in a lab exercise and a quiz.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/quizzes/525517

### Advanced Security Monitoring and Analysis

#### Module Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525518

Welcome to the Advanced Security Monitoring and Analysis module. This module provides an overview of packet mirroring for enhanced network security. Learn how to leverage this tool for traffic inspection, threat detection, and troubleshooting. We'll also cover essential network security best practices and test your knowledge with a quiz.

#### Packet Mirroring for network traffic inspection

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525519

Izumi, a network engineer at Cymbal Corporation, needs to monitor the network traffic from selected virtual machines (VMs) to determine if malicious activity is occuring. Organizations face challenges in monitoring and securing specific virtual machines (VMs) within their network. Traditional security measures might not be sufficient to detect sophisticated attacks that span multiple network packets and target specific VMs. This can lead to vulnerabilities and security breaches, compromising sensitive data and critical systems. Packet mirroring offers a solution by creating a copy of network traffic specifically from the selected VMs. This mirrored traffic is then directed to a security analysis platform where it undergoes deep packet inspection. By capturing and analyzing all packets within each flow, security teams can identify anomalies, detect complex attack patterns, and promptly respond to threats targeting those specific VMs. This proactive approach enhances security posture, safeguards sensitive data, and ensures the integrity of critical systems. Packet Mirroring clones the traffic of specific instances in your Virtual Private Cloud (VPC) network and forwards it for examination. Packet Mirroring captures all ingress and egress traffic and packet data, such as payloads and headers. The mirroring happens on the virtual machine (VM) instances, not on the network. Therefore, Packet Mirroring consumes additional bandwidth on the hosts. Packet Mirroring is useful when you need to monitor and analyze your security status. It exports all traffic, not only the traffic between sampling periods. For example, you can use security software that analyzes mirrored traffic to detect all threats or anomalies. Also, you can inspect the full traffic flow to detect application performance issues and to provide network forensics for Payment Card Industry Data Security Standards (PCI DSS) compliance and other regulatory use cases. We will elaborate on this further in the next few slides. Obviously, Packet Mirroring can generate significant data, so collector destination is generally an instance group behind a internal load balance or equivalent technology. One of the major limitations of Packet Mirroring is bandwidth consumption. Packet Mirroring consumes the egress bandwidth of the mirrored instances. However, there is a work-around. Use filters to reduce the traffic collected for mirrored instances. This filter can be used for IP address ranges, protocols, traffic directions, and a lot more. The current maximum number of filters that can be used for Packet Mirroring is 30. For more information, refer to the link in the speaker notes.

#### Network security best practices

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525520

Some of the network security best practices are listed on the slide. This list varies on a case-by-case basis based on the environment. Adopt a zero trust network model. This approach ensures that no user or device is implicitly trusted, regardless of their location inside or outside the organization's network. By verifying both user identity and context during access requests, you shift security controls from the network perimeter to individual users and devices, providing a more robust and granular approach to security. Secure connections between on-prem and Google Cloud: For organizations operating in hybrid or multi-cloud environments, prioritize secure connectivity between all environments to ensure data protection and minimize risks. Leverage Google Cloud's private access options like Cross-Cloud Interconnect, Dedicated/Partner Interconnect, and IPsec VPNs to establish secure, high-speed connections between your on-premises infrastructure and various cloud environments. Additionally, explore Private Service Connect for accessing Google APIs and published services with enhanced security, ensuring seamless communication while maintaining robust security measures. Disable default networks: to enhance network security and avoid IP address conflicts, disable the creation of default networks in Google Cloud projects. Plan your network and IP address allocation strategically across connected deployments and projects to ensure efficient and secure communication. As a best practice, limit the number of VPC networks per project to one for more effective access control. Secure your cloud perimeter with Google Cloud's tools like firewalls and VPC Service Controls. Employ Shared VPC to centralize network management and isolate workloads into separate projects, enhancing security and control. Create firewall policies and rules at multiple levels (organization, folder, VPC network) to allow or deny traffic based on various criteria, including IP addresses, protocols, ports, service accounts, and secure tags. Analyze your network. Google Cloud provides two tools, Cloud IDS and Packet Mirroring, to help you monitor and secure your network traffic in Compute Engine and Google Kubernetes Engine. Cloud IDS provides visibility into your VPC network traffic, while Packet Mirroring allows you to clone and forward specific VM traffic for further analysis and inspection with security tools. Use a web application firewall: strengthen the security of your external web applications and services by implementing Google Cloud Armor, a web application firewall (WAF) that also provides protection against DDoS attacks. For optimal protection of critical workloads, leverage the advanced features offered by Google Cloud Armor's Managed Protection Plus tier. Adopt automated infrastructure provisioning using tools like Terraform, Jenkins, or Cloud Build to create immutable environments for enhanced security and streamlined operations. Leverage Google Cloud's security blueprints as a foundation, or build upon them with your own automation to align with security best practices and guidelines. Monitor your network. Implement VPC Flow Logs and Firewall Rules Logging to gain near real-time visibility into your Google Cloud network traffic and firewall activity. Utilize tools like Cloud Logging, Cloud Monitoring, Firewall Insights, and Network Intelligence Center to track, analyze, and optimize your network security and performance.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/video/525521

This module provided an overview of packet mirroring for enhanced network security. You learned how to leverage this tool for traffic inspection, threat detection, and troubleshooting. The module also covered essential network security best practices and tested your knowledge with a quiz.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/quizzes/525522

### Course Resources

#### Networking in Google Cloud: Network Security Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/1142/documents/525523

### Your Next Steps

## 07: Networking in Google Cloud: Network Architecture

- https://www.cloudskillsboost.google/paths/14/course_templates/1144

### Welcome to Networking in Google Cloud

#### Networking in Google Cloud Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521663

Welcome to the third course of the "Networking in Google Cloud" series: Network Architecture! In this course, you will explore the fundamentals of designing efficient and scalable network architectures within Google Cloud. In the first module, Introduction to Network Architecture, we'll start by introducing you to the core components and concepts of network architecture, including subnets, routes, firewalls, and load balancing. Then in the second module, network topologies, we'll dive into various network topologies commonly used in Google Cloud, discussing their strengths, and weaknesses.

### Introduction to Network Architecture

#### Module Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521664

Welcome to the Introduction to Network Architecture module. In this module, you are introduced to cloud network architecture. You will learn why good cloud network architecture is important and key points to consider when designing a network architecture for your environment. Let’s begin with a cloud network architecture overview.

#### Cloud network architecture overview

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521665

Network architecture refers to the design of a virtual network in Google Cloud. It defines how virtual machines, containers, and other resources connect and communicate with each other within your cloud environment. While Google manages the underlying physical infrastructure, you are responsible for configuring the software, protocols, and Google Cloud managed services that make up your virtual network. This includes components like VPC networks, subnets, firewalls, Cloud Routers, Cloud VPNs, and access points for on-premises connections. The network design impacts the scalability, security, performance, and cost of your network. A well-designed network can support your business growth by providing the scalability, security, performance, and cost-efficiency you need. On the other hand, a poorly designed network can lead to a number of problems, such as outages, security breaches, and slow performance. These problems can disrupt your business operations and damage your reputation. Not all network designs are suitable for all situations. You select network components and protocols based on your organization’s needs and priorities. Tailoring your network architecture to your organization's specific needs is paramount. Start by thoroughly assessing your current and future requirements, considering factors like the number of users, applications, traffic volume, and security considerations. Once you have a clear understanding of your needs, define quantifiable metrics for scalability, security, performance, and cost efficiency. These metrics can serve as a benchmark for evaluating your design choices. Google Cloud offers a comprehensive suite of networking services and tools. Google Cloud offers a comprehensive suite of networking services and tools. Identify the ones that best suit your requirements and leverage them to craft a secure, scalable, and cost-effective network architecture. Google Cloud provides a powerful arsenal of networking services that enable you to build robust and optimized VPC networks. Each service plays a crucial role in achieving essential network characteristics like scalability, security, performance, and cost-efficiency. A VPC network enables secure resource grouping by segmenting your network into distinct subnets based on functionality, security considerations, or project boundaries. You have granular control over access lists and security policies within each VPC network. Cloud VPN establishes secure tunnels for encrypted data communication between your on-premises network and Google Cloud. Cloud Interconnect directly provides high-bandwidth, low-latency connections between your on-premises network and Google Cloud. If needed, you can deploy Cloud VPN over Cloud Interconnect to provide encrypted data communication. Cloud Router dynamically exchanges routes between Virtual Private Cloud Networks and on-premises networks. A Cloud Router also serves as the control plane for Cloud NAT. You have multiple configuration options, including policy-based routing and BGP peering. Cloud Router provides BGP services for the following Google Cloud products: Cloud Interconnect, Cloud VPN, and Router appliance. Load balancers play a vital role in cloud networks by intelligently distributing incoming traffic across multiple backend servers or instances. This optimization prevents overload on individual servers, enhancing performance, scalability, and ensuring high availability for your cloud-based applications. Firewall rules provide granular access control, which is critical in protecting your VPC network. Grouping firewall rules into firewall policies helps you protect your Google Cloud infrastructure more easily. The firewall provides more specific control over traffic entering or leaving individual virtual machine instances. Cloud CDN caches content using Google's global edge network, accelerating web applications. By selecting the most appropriate Network Service Tier, you can optimize network performance and cost based on your specific traffic requirements. Choose Standard Tier for general workloads when speed is not a primary concern. Choose Premium Tier for demanding applications that require higher throughput and lower latency. Let’s take a look at an example: Cymbal Bank acquired a small subsidiary with a banking application on three VMs in one subnet. One VM will host the web server, another VM will host the business logic, and another VM will host the database. This approach was fine but now Cymbal Bank wants to roll out this a banking application to a global audience, with clients in Asia, North America, and Europe. Cymbal Bank wants to avoid bottlenecks. As shown, this network design is better for the following reasons: There are more servers to spread out the processing. Here, we see two sets of VMs that run the web server, the business logic, and the database. Probably, you would use more than two sets of VMs, but only two sets are shown here to make the slide simpler to read to simply the example. A load balancer handles routing client requests to the least busy set of VMs. This way, you reduce the likelihood of a bottleneck. Each VM is in a separate subnet. This approach allows you to apply different security—such as firewall rules and IAM permissions—to each subnet. Therefore, to each VM. For example, using this approach, you can lock down the database to only accept messages from the business logic VM. Depending on the application—and on the clients that use it—there may be other design changes that would be appropriate. For now, it’s important to note that a good network design can help make an application run faster and more securely. In this course, you will learn some common network design approaches.

#### Getting started

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521666

Next, let’s briefly discuss some key considerations to design network architecture. When designing a network architecture, there are a number of key considerations. These include scalability, security, performance, and cost efficiency. Scalability refers to the ability of your network to accommodate increasing demands, whether it's adding more resources or handling growing traffic volume. Security refers to protecting your network from unauthorized access and attacks. Implementing robust security measures like firewalls, access controls, and encryption is are crucial to safeguarding your network from unauthorized access, data breaches, and cyberattacks. Compliance refers to observing guidelines, rules, and restrictions of your organization, industry, and pertinent government bodies. Performance refers to the speed and responsiveness of your network. Optimizing network performance translates to faster data transfer, improved application responsiveness, and a smooth user experience for your applications and services. Cost efficiency refers to the cost of designing, implementing, and operating your network. Striking a balance between achieving your network goals and staying within budget is essential. Google Cloud offers various cost-optimization strategies and tools to help you build a cost-effective network architecture. As a first step in your VPC network design, identify the decision makers and stakeholders that you must satisfy. Stakeholders might include application owners, security architects, solution architects, and operations managers. The stakeholders themselves might change depending on whether you are planning your VPC network for a project, a line of business, or the entire organization. Gather requirements. Engage with stakeholders to understand their specific needs regarding security, performance, scalability, compliance, and budget. This also includes the operational expenditure. This work might involve conducting interviews, workshops, or reviewing existing documentation. You also must identify the people within your organization that can make decisions to approve your network design. For example, engineering managers and accounting personnel. With a clear understanding of the requirements, we can now begin to translate them into a tangible network design. The high-level design outlines the overall structure of the network, including the major components and their relationships. The low-level design provides a more granular view, specifying device configurations, IP addressing schemes, and security protocols. This phase also involves making critical decisions about cloud infrastructure, geographic locations, and connectivity options. Finally, build a BoM (Bill of Materials) and calculate the cost. This comprehensive list includes all hardware, software, licenses, and services required to implement the network. It serves as the basis for calculating the total cost of the project, both in terms of upfront capital expenditures (CapEx) and ongoing operational expenditures (OpEx). This detailed cost analysis allows stakeholders to make informed decisions about the network design and implementation. Let’s now talk through best practices for understanding the overall project and environment. Clearly define the project's objectives and timeline to guide network design decisions. This helps determine resource allocation, architecture complexity, and implementation phases. Assess any existing on-premises or cloud infrastructure that the VPC network needs to integrate with. This includes identifying existing VPCs, subnets, , security configurations, and network policies. 3:47 Lastly, let’s talk through technical considerations and constraints you should consider. Research and understand any relevant security or data privacy regulations your organization needs to follow. This might influence aspects like data segregation, access control, and logging. Consider cost-saving strategies like right-sizing resources, utilizing committed use discounts, and exploring managed services where feasible. Plan for ongoing network management, monitoring, and incident response procedures. This might involve defining roles and responsibilities, automation practices, and logging configurations.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521667

Let’s take a look at an example of how you can use Gemini to help you learn more about network architecture. . You can explore your options by asking Gemini: “What does serverless architecture mean in Google Cloud”? In this module, you were introduced to the concept of network architecture and why it is important for optimal functioning of your Google Cloud environments. You learned about some important design concerns, core design components, and how to get started planning a network architecture.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/quizzes/521668

### Network Topologies

#### Module Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521669

Welcome to the Network Topologies module of the Networking in Google Cloud: Network Architecture course. This module introduces you to network topologies. We'll start with the hub and spoke model, exploring its structure and applications. Additionally, we'll cover alternative topologies like mesh, mirrored, and gated. Through a hands-on lab, you will explore how to implement a hub and spoke using Network Connectivity Center.

#### Hub-and-spoke topology

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521670

Let’s start with a use case scenario. Nur, a network engineer at Cymbal Corporation, is tasked with managing the company's expanding network infrastructure, which encompasses a growing number of remote offices and cloud-based applications. To ensure centralized control and efficient data management, Nur wants a simple network topology that can be centrally managed with simple network administration. Nur decides to use a hub-and-spoke topology. The hub-and-spoke topology is shaped like a star, with the central hub at the center and all the other devices connected to it like spokes on a wheel. Instead of managing each device individually, Nur can configure and monitor everything from the hub, saving Nur time and effort. This topology is a common approach for managing network traffic in Google Cloud environments. One Virtual Private Cloud (VPC) network (represented by the hub) acts as a transit point for the other VPC networks (represented by the spokes). Additionally, spoke devices can be diverse, including remote offices, cloud instances, and on-premises data centers. A hub-and-spoke topology features: A centralized point of control: Nur can easily configure, monitor, and troubleshoot the entire network from the central hub. This eliminates the need to log in to individual devices for management tasks. Simplified network administration. There is no need for complex routing configurations between individual devices. The hub automatically routes traffic between spokes based on predefined rules. Scalability: The hub-and-spoke topology can easily accommodate future network growth by adding more spokes. New devices can be quickly connected to the hub without impacting existing network configurations. Improved security: Centralized security policies can be enforced at the hub, enhancing overall network security. This simplifies security management and ensures consistent protection across all connected devices. A hub-and-spoke network topology is a versatile approach for managing network traffic, but there are different ways to implement it. Here are three common methods: VPC Network Peering allows direct connections between VPCs, enabling efficient communication with low latency. Cloud VPN creates a secure tunnel over the public internet to connect VPCs across regions or even to on-premises networks. While offering flexibility, Cloud VPN may introduce higher latency and require additional configuration compared to VPC peering. Network Connectivity Center (NCC) simplifies building and managing complex network topologies, including hub-and-spoke. It provides centralized configuration, automated provisioning, and integrated security features. However, Network Connectivity Center may come with additional costs compared to basic VPC peering or Cloud VPN setups. In this course, we will cover the NCC implementation. Network Connectivity Center simplifies network management by supporting two types of spokes. VPC spokes connect Virtual Private Clouds, while hybrid spokes connect on-premises networks, using HA VPN tunnels or Cloud Interconnect VLAN attachments with router appliance VMs. You can configure a router appliance instance by installing an image on a Compute Engine VM. You can use an image provided by a supported Network Connectivity Center partner. You can also use a custom image, such as an image that you created. You can then take advantage of logic specific to the router appliance instance to control connectivity between the spoke and the hub. Before you create the hub-and-spoke topology using Network Connectivity Center, here are additional considerations to keep in mind: ensure that IP address spaces between the hub and spoke VPC networks don’t overlap. IPv6 addressing isn’t supported. Privately-used public IP addresses are not supported either. If you need to use either of these two features, consider using VPC Network Peering to create a hub-and-spoke topology. You can’t attach hybrid spokes and VPC spokes to the same hub. For more information, refer to the Considerations section of the Network Connectivity Center Overview in the Google Cloud documentation.

#### Lab Intro: Implement a Hub-and-Spoke Network Using Network Connectivity Center

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521671

Next, let’s try out what you’ve learned. In this lab, you design and implement a classic hub-and-spoke network topology. Your pre-configured environment includes three VPC networks—a central hub and two branches (spoke1 and spoke2). Create virtual machines (VMs) on each network to test connectivity. Begin by verifying connectivity between the VMs within and across VPCs. Then, use NCC to implement a hub and spoke. Retest connectivity to confirm that your hub-and-spoke architecture is fully functional.

#### Implementing a Hub and Spoke using NCC

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/labs/521672

#### Other topologies

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521673

We covered hub and spoke in the previous section. Now, let's explore other topologies. In a mesh topology, devices or network nodes have multiple interconnected links. This contrasts with a hub-and-spoke topology, where devices connect through a central point, that is, the hub. There are two main types of mesh topology. In a full mesh, every node is connected to every other node. This topology can be useful for GKE Enterprise applications, where the microservices are all internally connected. In a partial mesh, only select, strategic nodes are connected. For example, for some failover scenarios, a partial mesh is helpful. You create a partial mesh between VPCs in different regions or zones. You can configure routing rules that direct traffic to the active region or zone normally but also configure automatic failover. If a primary region experiences issues, automatic failover to a nearby region or zone using the mesh links keeps your workloads working normally from a client perspective. Let’s consider some of the benefits of a mesh topology. Mesh topologies provide high availability and resilience. The multiple paths between nodes in a mesh network ensure that if one connection fails, traffic can be rerouted. This makes mesh networks ideal for mission-critical applications that require high uptime. The Google Cloud infrastructure is designed for scalability. Mesh topologies can easily expand with your cloud environment as you add new nodes or Google Cloud regions. A mesh topology can offer the potential for improved performance. The multiple pathways in a mesh can facilitate better load balancing and reduce network congestion, potentially leading to improved application performance. The lack of a single central point of failure also aids in security. By distributing traffic across multiple paths, mesh topology can make it harder for attackers to compromise the entire network. In Google Cloud, mirrored topology means creating a replica of your network infrastructure in another environment or region. Let’s consider some use cases For disaster recovery, an identical standby environment is ready to take over in case of failure. For testing and development, an isolated but otherwise identical configuration as a production environment is available for testing and experimentation. Here are two examples shown. Any actions taken in the testing and development environment will have no effect on production. Finally, by distributing traffic across regions, your workloads can take advantage of having an identical environment available nearby, resulting in improved workload performance. Shown here is an example of a type of gating topology. Gating topologies are essential for managing and securing network traffic flows in cloud environments, particularly in hybrid and multi-cloud scenarios. By implementing these gating mechanisms, you gain fine-grained control over data movement between your cloud and external resources, ensuring security and compliance. There are three types of topologies that restrict access: There are three types of topologies that restrict access: Gated egress: Controls outbound traffic from the cloud. Gated ingress: Controls inbound traffic to the cloud. This is the gating topology type shown here. Gated ingress and egress: Controls inbound and outbound traffic between hybrid and multi-cloud environments. More information about each of these topologies can be found in Gated patterns, in the Google Cloud documentation.

#### Getting topology data

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521674

So far you have learned about various different topologies. If you are wondering if there is a way to visualize the topology of the network, the answer is yes. Network Topology does exactly that. Let’s learn about getting topology data in this section. Network Topology is a visualization tool that shows the topology of the network infrastructure. It serves as a map, illustrating the complex relationships between various components and how they interact. Network Topology offers an "infrastructure view" showcasing elements like VPC networks, hybrid connectivity, and connections to Google-managed services. It goes beyond static configurations by incorporating real-time operational data, revealing traffic paths, throughput, and the current state of connections between various workloads. By presenting this information in a graph format, where nodes and lines represent entities and connections, Network Topology simplifies the understanding of complex networking relationships. This visual representation aids in identifying bottlenecks, optimizing traffic flows, and troubleshooting network issues more effectively. Network Topology actively gathers real-time telemetry and configuration data from Google's infrastructure to create a visual representation of your network resources. It captures various elements like configuration details, performance metrics, and logs to deduce the relationships between resources within a single project or across multiple projects. By consolidating this information, Network Topology generates a comprehensive graph that accurately depicts your entire network deployment. Network Topology maintains a historical record spanning six weeks, offering valuable insights into past network configurations and interactions. This history is organized into hourly snapshots, commencing at the beginning of each hour. Each snapshot captures the fundamental entities and their communication patterns within that specific hour. Even if resources, like instances, are created and deleted within an hour, they are still documented in the snapshot for that duration. This ensures a comprehensive historical representation of your network's evolution. At the lowest level of each hierarchy are base entities, which represent individual resources capable of direct network communication, such as VM instances or GKE pods. When dealing with complex networks containing numerous base entities, a flat view can be overwhelming. To address this, Network Topology aggregates base entities into higher-level hierarchical entities that can be expanded or collapsed. Initially, the graph displays all base entities aggregated into their top-level hierarchy. For example, VM instances are aggregated into instance groups, then into zones. Meanwhile, GKE pods are aggregated into workloads, then namespaces, and finally into clusters. Each entity, whether base or hierarchical, is represented as a circular node in the graph, and each base entity has its own unique hierarchy. For example, load balancers are organized differently than VM instances within the graph. To simplify the view, Network Topology aggregates related resources into hierarchical entities, with each resource type having its own distinct hierarchy. Network Topology visually represents traffic between entities as lines, connecting entities if at least one side is sending traffic. It displays connections across various hierarchy levels, as long as the base entities communicate. For instance, a connection between two regions is shown if at least one VM instance in each region is communicating. The tool supports various traffic protocols (TCP, UDP, ICMP, ICMPV6, ESP, GRE) for specific paths, visualizing traffic within and across VPC networks, between Google Cloud and the internet, and to/from VPN gateways, Interconnect connections, and router appliances. In the GKE view, Network Topology shows traffic within a cluster (between pods on different nodes), and between pods on the same node if intranode visibility is enabled, and between clusters and external IPs. Let’s explore a use case. Charlie, a network administrator, is responsible for a network that includes several load-balanced applications. Charlie has been alerted to a latency issue causing the organization's mobile application to intermittently slow down and time out. Multiple users are experiencing this problem, and Charlie has confirmed that no recent application deployments have occurred. The issue is likely due to a change in the network environment rather than a problem with the application itself. Charlie needs to use the Network Topology tool to get a comprehensive visibility into the cloud network infrastructure. They do this because the graphical representation of the network topologies enables the engineers to quickly understand the connections between resources, identify potential bottlenecks, and to troubleshoot network issues efficiently. Beginning with the external clients in the Americas, Charlie selects the traffic metrics between that region and the shopping-site-lb load balancer. Network Topology then displays charts in the details pane, providing information such as ingress and egress traffic between the selected entities, along with key metrics like queries per second (QPS) and HTTP request latency. Upon reviewing the request latency chart, Charlie notices that the 50th, 95th, and 99th percentile values are all higher than expected. To gain a broader perspective on this latency issue, Charlie extends the time series charts to cover the past 6 weeks for further analysis. After expanding the timeframe, Charlie observes a significant jump in latency that occurred approximately two hours ago, coinciding with the initial reports of the issue. This confirms their suspicion that the increased latency is related to the load balancer. To investigate further, Charlie navigates to the Load balancing page in the Google Cloud console. Their analysis reveals that one of the instances in the load balancer's backend service was taking longer than usual to respond. By removing this problematic instance from service, Charlie successfully resolved the latency issue and restored normal application performance.

#### Best practices

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521675

Now that you’ve learned how to gather topology data, let's next explore some best practices to consider when exploring the right connectivity options. Selecting the right network connectivity solution for hybrid or multicloud environments requires careful consideration of several key factors. These include ensuring the solution meets the required service-level agreements (SLAs) for performance and uptime. For organizations aiming to expand a hub-and-spoke network architecture across multiple VPC networks, establishing centralized hybrid connectivity within a dedicated VPC network, then peering with other projects using custom advertised routes, can be an effective strategy. This approach enables the seamless sharing of both static and dynamically learned routes with peered VPC networks, fostering a centralized configuration that simplifies management and facilitates scalability in your VPC network design. To enhance application accessibility and management, it's recommended to expose applications through APIs using an API gateway or load balancer. A comprehensive API platform, such as Apigee, can be utilized to simplify this process. This type of platform acts as an intermediary for your backend service APIs, providing features like enhanced security, rate limiting, quotas, and analytics for better control and monitoring of your API usage. When leveraging Cloud Load Balancing, utilize the application capacity optimization features whenever possible. These capabilities can be valuable in mitigating capacity challenges that often arise in globally distributed applications, ensuring smoother operation and better user experiences. When deciding where DNS resolution should be performed in a hybrid setup, it is recommended to use two authoritative DNS systems for your private Google Cloud environment and for your on-premises resources that are hosted by existing DNS servers in your on-premises environment. Before we wrap up, here is a quick preview of some prompts you can use with Gemini related to network topology. You can use a prompt such as “Describe a scenario where a  would be the best choice.” Or “What are the advantages and disadvantages of a mesh topology.”

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/quizzes/521676

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/video/521677

In this module, you learned all about network topologies. We started with the popular hub and spoke model, exploring its structure and applications. Additionally, we covered alternative topologies like mesh, mirrored, and gated. Through a hands-on lab, you explored how to implement a hub and spoke using Network Connectivity Center.

### Course Resources

#### Networking in Google Cloud: Network Architecture Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/1144/documents/521678

### Your Next Steps

## 08: Networking in Google Cloud: Hybrid and Multicloud

- https://www.cloudskillsboost.google/paths/14/course_templates/1145

### Welcome to Networking in Google Cloud

#### Course introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575447

our Networking and Google Cloud Series. Hybrid and Multi-Cloud. The first module will walk you through various Cloud connectivity options, with a deep dive into Cloud Interconnect, exploring its different types and functionalities. In the second module we&#39;ll cover Cloud VPN, discussing its implementation, high availability VPN typologies, and the Network Connectivity Center, or NCC, for streamlined management. By the end of this course, you will be able to explain the different connectivity options available to extend your on-premises and other Cloud networks to Google Cloud, and analyze the suitability of different Google Cloud hybrid and multi-Cloud connectivity services for specific use cases. Let&#39;s get started.

### Connectivity Options

#### Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575448

In this module, we begin by providing a brief overview of cloud interconnect and cloud VPN. We then proceed with the discussion of the three types of cloud interconnect, Dedicated Interconnect, Partner Interconnect, and Cross-Cloud Interconnect. You will learn about how they can be useful and also how to set them up. At the end of the module, there&#39;s a short quiz to check your understanding of what you just learned.

#### Connectivity options

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575449

Cloud Interconnect provides a fast connection to Google&#39;s network, Google offers two different cloud interconnects products, choose a product based on your situation and your needs. When you can physically connect to the Google network at a colocation facility, use Dedicated Interconnect. When you cannot connect to the Google network at a colocation facility, but can connect through a service provider, use Partner Interconnected. When you want to connect to other cloud providers directly, use Cross-Cloud Interconnect. Google Cloud also offers Cloud VPN, Cloud VPN can be useful when you have lower bandwidth needs, or when you must encrypt data in transit. In this module, you will learn more about the three types of cloud interconnect. Let&#39;s compare these connection options, all these options provide internal IP address access, between resources in your on premises network, and in your VPC network. The main differences are the connection capacity and the requirements, for using a service. The IPsec VPN tunnels, that cloud VPN offers, have a capacity of 1.5 gigabits per second, to 3 gigabits per second, per tunnel. The tunnels connect to a VPN device in your on premises network. The 1.5 gigabits per second capacity, applies to traffic that traverses the public internet, and the three gigabits per second capacity, applies to traffic that is traversing a direct peering link. If you want to scale this capacity, you can configure multiple tunnels. Dedicated interconnect has a capacity of 10 gigabits per second, or 100 gigabits per second, per link, and requires you to have a connection, in a Google supported colocation facility. You can have up to eight links to achieve multiples of 10 gigabits per second, or up to two links to achieve multiples of 200 gigabits per second, but 10 gigabits per second is the minimum capacity. Partner interconnect has a capacity of 50 megabits per second, to 50 gigabits per second, per connection, and requirements depend on the service provider. Dedicated Interconnect, and Partner Interconnect, do not encrypt data in transit. Secure data in transit at the application layer, using TL&#39;s for example. Cross-Cloud Interconnect enables you to establish high bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Cross-Cloud Interconnect connections are available in two sizes,10 gigabits per second, or 100 gigabits per second.

#### Dedicated Interconnect

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575450

Next, let&#39;s discuss dedicated interconnect. Sam, a network engineer at Simbel Corporation, is tasked with migrating mission critical workloads to Google Cloud. Sam won&#39;t migrate the entire on prem to Google, and it makes sense to migrate a portion of the workload. These workloads require the highest levels of performance and reliability with minimal latency and jitter. Currently, Sam is considering using the public Internet for the connection, but concerns about security and reliability remain. A VPN connection over the Internet is not the top choice, as the public Internet is not most performant and overhead of VPN on packets can impact performance. Sam needs a solution that guarantees consistent high performance connectivity with fiber ports between s one&#39;s on premises network and Google Cloud. The solution must offer dedicated bandwidth, ensuring that mission critical workloads have the resources they need to function flawlessly. Additionally, Sam requires the solution to be highly secure, with the option to enable robust data encryption and isolation from public Internet traffic. Thus, a dedicated interconnect option checks all the requirements. Dedicated interconnect provides direct physical connections between your on premises network and the Google network. Dedicated interconnect enables you to transfer large amounts of data between networks, which can be more cost effective than purchasing additional bandwidth over the public Internet. Upon establishing a VLAN attachment, it is linked with a cloud router. This cloud router initiates a BGP session for both the VLAN attachment and its corresponding on premises peer router. Through this BGP session, the cloud router receives routes advertised by the on premises router. These routes are then integrated into your VPC network as custom dynamic routes. Simultaneously, the cloud router advertises routes for Google Cloud resources to the on premises peer router, ensuring bi directional route exchange. In order to provide redundancy, create at least four interconnect connections, two in each metropolitan areas. Placing a dedicated interconnect connection in more than one domain in a metropolitan area provides redundancy. The domains provide isolation during scheduled maintenance, which means that two domains in the same metropolitan area are not down for maintenance at the same time. If you have a dedicated interconnect connection defined in each of the two domains, scheduled maintenance can only affect a single connection at any given time. A minimum of two Cloud routers should be deployed across at least two separate Google Cloud regions. This is essential even if all your virtual machine or VM instances are located within a single region. In the event of a region wide disruption, Google Cloud can redirect traffic through the unaffected region to ensure continued access to your VMs. Each Cloud router must be linked to a pair of dedicated interconnect connections situated within the same metropolitan area. In the example shown, the network configuration features four dedicated interconnect connections distributed across two separate metropolitan areas and distinct edge availability domains. Each of these regions houses a cloud router within the VPC1 network, with each router maintaining its own independent border gateway protocol, or BGP session. For a complete list of co location facilities and edge availability zones, see all colocation facilities in the Google Cloud documentation. Note that the documentation also refers to an edge availability zone as an interconnect location name. Now, to create a dedicated interconnect connection, follow these order your connection. You can do this within Google Cloud. Next, Google sends you a LOA.CFA, also known as a letter of authorization and connecting facility assignment. The LOA.CFA identifies the connection ports that Google has assigned for your dedicated interconnect connection. The LOA.CFA also grants permission for a vendor in a colocation facility to connect to them. Send LOA.CFAs to the colocation facility so they can complete your connection setup. Your vendor will let you know when the setup is complete. Test the connection, Google sends you automated emails with configuration information for two different tests. First, Google sends an IP address configuration to test light levels on every circuit in a dedicated interconnect connection. After those tests pass, Google sends a final IP address configuration to test the IP connectivity of each connection. Apply these configurations to your Cloud routers so that Google can confirm connectivity. After all tests have passed, your dedicated interconnect connection is ready to use. Create VLAN attachments and establish BGP sessions. You can do this using the Google Cloud console. Configure the on premises routers to establish a BGP session with your cloud router. To configure your on premises router, use the VLAN ID interface, IP address and peering IP address provided by the VLAN attachment. Next, let&#39;s talk about your connection bandwidth. You can purchase bandwidth as one or more circuits. Each circuit can be either 10 gigabits per second or 100 gigabits per second, but not both, as it is not possible to have different types of circuits in the same connection. A connection can have a maximum of 810 gigabits per second circuits or 2100 gigabits per second circuits. Therefore, the maximum connection capacity is either 80 gigabits per second or 200 gigabits per second, depending on which type of circuit you choose.

#### Partner Interconnect

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575451

Next, let&#39;s discuss Partner Interconnect starting with the use case. Ken, a network engineer at Symbol Corporation, is tasked with connecting symbol&#39;s geographically remote data center to Google Cloud. However, the data center&#39;s location doesn&#39;t have access to a Dedicated Interconnect colocation facility, making a direct connection impossible. Ken requires a solution that offers high bandwidth, low latency, and dedicated connectivity to Google Cloud, even without a nearby colocation facility. Ken also needs a setup that uses the partners underlying infrastructure to connect to Google Cloud. Additionally, the solution needs to be secure and reliable, providing robust data protection and ensuring seamless operations. Thus, Partner Interconnect seems to be an excellent fit. Partner Interconnect is similar to Dedicated Interconnect. Dedicated Interconnect and Partner Interconnect have technical feature parity. However, the physical connection for Dedicated Interconnect is made through a supported service provider. Let&#39;s look at a few more differences. Partner Interconnect provides connectivity between your on premises network and your VPC network through a supported service provider. If your data center is in a physical location that can&#39;t reach a Dedicated Interconnect colocation facility, Partner Interconnect is a good option. If your data needs don&#39;t warrant using Dedicated Interconnect, consider using Partner Interconnect. Work with a supported service provider to connect your VPC and on premises networks. Consider placing the Partner Interconnect connection in multiple edge availability domains for redundancy. For a full list of service providers, see supported service providers in the Google Cloud documentation. To create a partner interconnect connection, follow these order your connection from a supported service provider. The service provider will then provide the connectivity needed to create a VLAN attachment. Create a VLAN attachment, which creates a pairing key. The pairing key is unique and lets a service provider identify and connect to the associated cloud router. The service provider uses this key to finish configuring your VLAN attachment. Request a connection from your service provider. Submit the pairing key and other connection details, such as the connection capacity and location. Your service provider configures your connection. They must confirm that they can serve your requested capacity. When the configuration is complete, you&#39;ll receive an email. In the VLAN attachment activate your connection. After the connection is activated, it can start passing traffic. Configure the on-premises routers to establish a BGP session with your cloud router. To configure your on premises routers, use the VLAN id interface IP address and peering IP address provided by the VLAN attachment for layer two connections, traffic passes through the service provider network to reach the VPC network or on premises network. BGP is configured between the on-premises router and a cloud router in the VPC network as shown in the graphic. For layer two connections, you must configure and establish a BGP session between your cloud routers and on premises routers. When you configure cloud router, you configure VLAN attachments. Each VLAN attachment is a logical connection between your on premises network and a single region in your VPC network. When creating a VLAN attachment, specify a cloud router in the region that contains the subnets that you want to reach. The VLAN attachment automatically allocates a VLAN ID and BGP peering IP address. Use that information to configure your on-premises router and establish a BGP session with your cloud router. For partner interconnect, the VLAN attachment uses a connection that your service provider sets up and manages. The service provider completes the circuit. For layer three connections, traffic is passed to the service provider network. Their network then routes a traffic to the correct destination either to the on-premises network or to the VPC network. Connectivity between the on premises network and the service provider network depends on the service provider. For example, the service provider might request that you establish a BGP session with them or configure a static default route to their network. For Layer 3 connections, your service provider establishes a BGP session between your cloud routers and their on-premises routers for each VLAN attachment. You don&#39;t need to configure BGP on your local router, Google and your service provider automatically set the correct BGP configurations. Dedicated Interconnect and Partner Interconnect have technical feature parity. The biggest difference is where you interconnect from a Google colocation facility or from a partner facility. However, there are some other points to consider. Use Partner Interconnect when you cannot physically connect from a colocation facility where Google has presence, but can use a partner colocation facility. Partner Interconnect can be procured quickly. The physical configuration already exists at the service provider, so there&#39;s less infrastructure to set up. Partner Interconnect also is sufficient to support bandwidth needs less than 50 gigabits per second. A partner Interconnect connection can be scaled based on the number and capacity of your VLAN attachments. Thus, the smallest connection is 50 megabits per second. If you need less than 50 megabits per second, consider using cloud VPN.

#### Cross-Cloud Interconnect

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575452

In this module, you will receive a brief overview of Cloud Interconnect and Cloud VPN. You then learn about the three types of cloud inerconnect, Dedicated Interconnect, Partner Interconnect and Cross-Cloud Interconnect. You also learned about how they can be useful and also how to set them up. At the end of the module, there&#39;s a short quiz to check your understanding of what you learned. Rob, a network engineer at Simbel Corporation, is tasked with managing the hybrid cloud environment for Simble, which spans Google Cloud and Microsoft Azure. As Simble&#39;s workload grows, Rob needs a solution to seamlessly connect the two cloud platforms and ensure consistent performance and security across both environments. Cross-Cloud Interconnect is an excellent fit as it offers dedicated high bandwidth connectivity between Google Cloud and Microsoft Azure. This would enable them to efficiently transfer data between the two platforms and eliminate the latency and bandwidth limitations associated with public Internet connections. Additionally, Rob requires a solution to be secure and reliable, providing strong data encryption and redundancy to guarantee business continuity. With Cross-Cloud Interconnect Google provisions a dedicated physical connection between the Google network and that of another supported cloud service provider. Google currently supports Amazon Web Services, Microsoft Azure, Oracle Cloud infrastructure, and Alibaba Cloud for use with Cross-Cloud interconnect. You can use this connection to peer your Google Virtual private cloud or VPC network with your network that&#39;s hosted by the cloud service provider. You identify supported locations where you want Google to place your connections. Then you purchase primary and redundant Cross-Cloud Interconnect ports. You also buy primary and redundant ports from your cloud service provider. After provisioning the connection, Google supports the connection up to the point where it reaches the network of your other cloud service provider. Note that Google does not guarantee uptime from the other cloud service provider and cannot create a support ticket on your behalf. Cross-Cloud Interconnect supports the adoption of an integrated multicloud strategy. Adopting a multicloud architecture lets you avoid being locked in with a single vendor, store data in one cloud while hosting business logic in another. Avoid downtime if one cloud has an outage. Use a second cloud for disaster recovery and maximize business insights by analyzing data in multiple clouds. Without Cross-Cloud Interconnect, the options for setting up connectivity are limited and all are relatively complex. When you use Cross-Cloud Interconnec, you don&#39;t have to deploy your own hardware and you eliminate the need to work with third parties. You can use Cross-Cloud Interconnect as part of a site -o-site data transfer strategy. Site-to-site data transfer is a feature of network connectivity center that lets you use the Google network as a wide area network. Network Connectivity center is discussed later. The cloud interconnect SLA requires you to have two connections at minimum, each in a different edge availability domain of a metropolitan area. This approach gives you 99.9% availability. In order to achieve high availability for critical applications, you should configure two pairs of connections. Each pair must be in a different metropolitan area. Within each metropolitan area, you must use two different edge availability domains. This approach gives you 99.99% availability. Cross-Cloud Interconnect connections are available in 210 gigabits per second or 100 gigabits per second. For more information, see Cross-Cloud Interconnect overview in the Google Cloud documentation. MACsec for Cloud Interconnect helps you secure traffic on Cloud Interconnect connections, specifically between your on premises router and Google&#39;s edge routers. MACsec encrypts traffic on dedicated interconnect between Google&#39;s peering edge router and an on premises router. Partner Interconnect between Google&#39;s peering edge router and the service provider&#39;s peering edge router. Cross Cloud Interconnect between Google&#39;s peering edge router and a router run by remote cloud. MACsec for cloud Interconnect doesn&#39;t provide encryption in transit within Google. For stronger security, we recommend that you use MACsec with other network security protocols such as IPsec and TLS. Let&#39;s take a look at an example of how you can use Gemini to help you connect the network you created to your on premises network. You can explore your options by asking Gemini, which should I use, Cloud VPN or Cloud Interconnect? Please provide details supporting the use of each according to their respective strengths. Another useful prompt is list the steps to generate a report on the past week&#39;s performance of my cloud interconnect link, including bandwidth utilization, latency, and packet loss. Here&#39;s the response to the first prompt. Gemini provides a comparison table that covers the various feature differences between Cloud VPN and Cloud Interconnect.

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575453

the available options for hybrid connectivity with your Google Cloud network, Direct Interconnect, Partner Interconnect, Cross-Cloud Interconnect, and Cloud VPN. You learned how to set up Direct Interconnect, Partner Interconnect, and Cross-Cloud Interconnect, as well as the conditions to consider during the setup.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/quizzes/575454

### Cloud VPN

#### Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575455

Welcome to the Cloud VPN module. Let’s explore Cloud VPN, a service that enables secure encrypted connections between your on-premises networks and Google Cloud. We’ll explore what Cloud VPN is, how it works, and the various use cases it supports.

#### Cloud VPN Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575456

Use Cloud VPN when you don&#39;t need the connection speed of Cloud Interconnect. Cloud VPN is cheaper and easier to set up than Cloud Interconnect. Thus, Cloud VPN is useful for low-volume or low-bandwidth data connections. When you prefer the economics of an IPSec tunnel over the Internet, and when your network infrastructure cannot support dedicated fiber connections to Google Cloud, choose Cloud VPN. You can also use Cloud VPN to encrypt data in transit. With Cloud VPN, you can selectively advertise routes between VPC networks. In contrast, when you set a peering between two VPC networks, all the subnet routes are advertised. Cloud VPN securely connects your peer network to your virtual private cloud or VPC network through IPSec tunnels. The data is encrypted as it passes through the tunnels. The traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by the other VPN gateway. This action protects your data as it travels over the Internet. There are two types of Cloud VPN: high availability VPN or HA, and classic VPN. Google Cloud VPN primarily offers HA VPN; the recommended solution for dynamic routing and enhanced reliability. Classic VPN is available for specific use cases requiring static routing. Next, let&#39;s discuss both Cloud VPN products. Jack is responsible for optimizing the company&#39;s cloud infrastructure, and he&#39;s currently focused on updating their networking strategy within Google Cloud. Symbol Corporation has been relying on classic VPNs to connect their on-premises data center to Google Cloud, but they&#39;ve experienced occasional connectivity issues. Now they are looking to enhance our network reliability and performance. Considering the critical nature of Symbols operations, the switch to high-availability VPNs and Google Cloud will be suitable. The company&#39;s applications demand a highly available, secure connection to Cloud services and a simplified setup. HA VPNs could provide the necessary redundancy and failover capabilities to ensure uninterrupted connectivity. The advantages of HA VPNs over classic VPNs include improved availability, automatic failover, and better performance. The switch to HA VPN will meet the networking requirements and contribute to a more resilient and efficient infrastructure.

#### HA VPN Topologies

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575457

Let us next cover some of the HA VPN topologies. HA VPN supports site-to-site VPN for different configuration topologies. These topologies an HA VPN gateway to peer VPN devices, an HA VPN gateway to an Amazon Web Services, or AWS, virtual private gateway, two HA VPN gateways connected to each other, and HA VPN to Compute Engine VM instances. Let’s look at each of these topologies. HA VPN has three typical peer gateway configurations, an HA VPN gateway to two separate peer VPN devices, each with its own IP address, one peer VPN device that uses two separate IP addresses, and one peer VPN device that uses one IP address. Let’s walk through an example. In this topology, one HA VPN gateway connects to two peer devices. Here we see the HA VPN gateway and two VPN tunnels that connect to the peer devices. Next, we’ll look at the peer devices in the on-premises network. Here you can see the on-premises network and the tunnels that come from the HA VPN gateway. Each peer device has one interface and one external IP address. The HA VPN gateway uses two tunnels, one tunnel to each peer device. If your peer side gateway is hardware based, having a second peer side gateway provides redundancy and failover on that side of the connection. In Google Cloud, the redundancy type for this configuration takes the value to IPS redundancy. The example shown here provides 99.99% availability. When configuring an HA VPN external VPN gateway to AWS, you can use either a transit gateway or virtual private gateway. Only the transit gateway supports equal cost multipath or ECMP routing. When enabled, ECMP distributes traffic equally across active tunnels. Let’s walk through an example. In this topology, you configure three major gateway components. An HA VPN gateway in Google Cloud with two interfaces, two AWS virtual private gateways that connect to your HA VPN gateway, and an external VPN gateway resource in Google Cloud that represents your AWS virtual private gateway. This resource provides information to Google Cloud about your AWS gateway. Here you can see the Google Cloud components and their connections through the Internet to the AWS components. The AWS components in this topology are shown here, along with their connections to the HA VPN gateway and cloud router in Google Cloud. The supported topology requires two AWS Site-to-Site VPN connections, Connection 0 and Connection 1, each with two external IP addresses. This topology yields four external IP addresses in AWS: Connection 0, IP0 and IP1, and Connection 1, IP0 and IP1. The HA VPN has two connections, Interface 0 and Interface 1, each with two tunnels. For more information regarding using HA VPN to connect to a Microsoft Azure gateway, see Using Cloud VPN with Microsoft Azure VPN Gateway. You can connect two Google Cloud VPC networks together by using an HA VPN gateway in each network. Let’s walk through a sample topology. Like the other two samples that you’ve seen, it’s divided into two parts. Here you see a Google Cloud project with a VPC network called network-a. There’s an HA VPN gateway and a cloud router instance that connects to VPC network-b, which is not visible here. Each ha VPN gateway has two interfaces. You connect interface 0 on the HA VPN gateway in network-a to interface 0 on the HA VPN in network-b. You do the same for the interface 1 connecting from the HA VPN gateway in network-a to the HA VPN gateway in network-b. Next, let’s look at network-b. Here you see the Google Cloud project that contains network-b. You see all the connections from the HA VPN gateway that link to network-a. When you deploy HA VPN over Cloud Interconnect, you create two operational tiers. The cloud interconnect tier, which includes the VLAN attachments and the cloud router for Cloud Interconnect. The HA VPN tier, which includes the HA VPN gateways and the tunnels, and the cloud router for HA VPN. Each tier requires its own Cloud Router. The Cloud Router for Cloud Interconnect is used exclusively to exchange VPN gateway prefixes between the VLAN attachments. This Cloud Router is used only by the VLAN attachments of the Cloud Interconnect tier. It cannot be used in the HA VPN tier. The Cloud Router for HA VPN exchanges prefixes between your VPC network and your on-premises network. You configure the cloud router for HA VPN and its BGP sessions in the same way you would for regular HA VPN deployment. You build the HA VPN tier on top of the Cloud Interconnect tier. Therefore, the HA VPN tier requires that the Cloud Interconnect tier, based on either dedicated interconnect or partner interconnect, is properly configured and operational. The diagram shown here depicts an HA VPN over Cloud Interconnect deployment. To learn more about configuration, see configure HA VPN over Cloud Interconnect. HA VPN allows you to establish a secure connection between an HA VPN gateway and compute engine VMs. Functioning as network virtual appliances running IPSEC VPN. The VM instances can be deployed within a single zone or distributed across multiple zones. This configuration supports connecting an HA VPN gateway to two compute engine VMs, each residing in separate VPCs and zones with unique external IP addresses. The VMs act as VPN peer devices. Shown here, the HA VPN gateway is situated in a virtual private cloud network named network-a, while the two VMs are located in network-b. Both VPC networks are within the us-central1 region. The HA VPN gateway in network a is configured to use the external IP addresses of each VM in network-b. It is also possible to have the HA VPN gateway n-VMs in different regions. This topology is recommended for enhanced availability. HA VPN also provides 99.99% service availability. Use HA VPN for BGP routing and to support multiple tunnels. When you create an HA VPN gateway, Google Cloud automatically chooses two external IPV 4 addresses. One for each of its fixed number of two interfaces. Each IPV 4 address is automatically chosen from a unique address pool to support high availability. VPN tunnels connected to HA VPN gateways must use dynamic BGP routing. Use an active passive configuration for consistent bandwidth experience. Active/active configurations may offer a less consistent experience. Unless combined traffic for both tunnels is within single tunnel capacity, failure can cause the available bandwidth to be cut in half. You can also use some useful prompts, focusing specifically on how Gemini, like AI could assist with cloud VPN tasks and analysis. One such example is how can I connect a VPC network in Google Cloud to an on-premises private network? Gemini will provide the two options available on Google Cloud, Cloud VPN and Cloud Interconnect.

#### Influence best path selection

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575458

influence the best path selection. When a Cloud router advertises prefixes to a BGP pier, it includes a priority for each prefix in the advertisement. The advertisement is the BGP message, and it includes a root priority. The root priority is stored in the BGP multi-exit discriminator or MED attribute, which influences root selection. You can change the advertised root priority value on each Cloud VPN tunnel or VLAN attachment. This value is then assigned to the BGP MED attribute. The MED value affects the BGP best path selection. By changing the advertised root priority, you influence the best path selection. For more information about other factors that influence the best path selection, see the BGP protocol standard on the ietf.org website or a networking tutorial. The advertised root priority value applies to all prefixes advertised by the BGP session associated with the Cloud VPN tunnel or VLAN attachment. Base priorities are whole numbers from 0-65535. The highest possible base priority is zero. In other words, a lower number indicates a higher priority. The default base priority is 100. If you don&#39;t specify a base priority, the default priority is used. Next, let&#39;s talk about the region to region costs that can be added to the base priority to set the MED attribute on the BGP session. This cost only applies when the VPC is in global dynamic routing mode. Region to region costs are from 201 through 9,999. The value depends on the distance, latency, and other factors between two regions. Google generates the region to region cost values and you can&#39;t modify them. Next, let&#39;s look at a sample topology to see how this works. In the graphic, you see an example of region to region costs that Google calculates. Regions that are closer have a lower region cost for traffic that flows between them. For example, the regional cost between US Central 1 and Europe West 1 is lower than the cost between Europe West 1 and US West 1. For more information, see advertise prefixes and priorities in the Google Cloud documentation.

#### Network Connectivity Center

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575459

let&#39;s explore Network Connectivity Center or NCC; a centralized hub for managing your Google Cloud network connections. Zeek, a network engineer at Symbol Corporation, is tasked with connecting VPC networks across multiple regions to support their global expansion. Their current network architecture relies on a complex web of manual configurations and lacks centralized visibility, making it difficult to manage and troubleshoot. Zeek needs a solution that simplifies connectivity, provides comprehensive monitoring, and scales seamlessly to accommodate their growing network. Network connectivity center emerges as the perfect solution for Zeek. NCC establishes a centralized hub for managing all VPC connections, streamlining configuration, and eliminating the need for complex manual processes. Its comprehensive visibility into network traffic and performance metrics allows Zeek to quickly identify and resolve issues. Additionally, the inherent scalability of NCC ensures that Zeek can easily add new VPC connections as Symbol expands, ensuring their network remains robust and reliable. NCC adds some useful features to Google Cloud Networking. You can integrate third-party network appliances between external sites to your VPC networks or even between VPC networks. For supported regions, you can use the Google Cloud network to configure site-to-site data transfer. After configuration, external sites use Google Cloud network as a WAN, which reduces latency. Network Connectivity Center also lets you centrally manage multi-site configurations between your external sites and Google Cloud.

#### Lab intro: Configuring Google Cloud HA VPN

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575460

Let's next explore how to configure an HA VPN. In this lab, you create a global VPC called vpc- demo, with two custom subnets in REGION 2 and REGION 1. In this VPC, you add a Compute Engine instance in each region. You then create a second VPC called on -prem to simulate a customer's on-premises data center. In this second VPC, you add a subnet in region REGION 1 and a Compute Engine instance running in this region. Finally, you add an HA VPN and a cloud router in each VPC and run two tunnels from each HA VPN gateway before testing the configuration to verify the 99.99% SLA.

#### Configuring Google Cloud HA VPN

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/labs/575461

#### Debrief

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/video/575462

In this module, you learned about Cloud VPN and how it can be useful for lower bandwidth needs and to implement encrypted connections. In the lab, Configuring Google Cloud HA VPN, you created a simulated on-premises network and a global VPC. You then connected them using a high-availability VPN gateway with two tunnels to test and verify the 99.99% service availability. At the end of the module, you learned how Network Connectivity Center centrally manages hybrid, multi-site connectivity using a hub and spoke architecture.

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/quizzes/575463

### Course Resources

#### Networking in Google Cloud: Hybrid and Multicloud Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/1145/documents/575464

### Your Next Steps

## 09: Logging and Monitoring in Google Cloud

- https://www.cloudskillsboost.google/paths/14/course_templates/99

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533931

Welcome to the two-part course on Logging, Monitoring, and Observability in Google Cloud. The core operations tools in Google Cloud break down into two major categories. The operations-focused components and the application performance management tools. The first course is Logging and Monitoring in Google Cloud, which is this course, and covers the operations-focused components, including Logging, Monitoring, and Service Monitoring. This course tends to be more for personnel who are primarily interested in infrastructure, and keeping that infrastructure up, running, and error-free. The second course is Observability in Google Cloud, which covers the application performance management tools, including Error Reporting, Trace, and Profiler. This course in contrast, tends to be more for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute products. But it isn’t fair to think of these tools as belonging purely to either of these two groups. A developer would, of course, sometimes need access to logs or monitoring metrics, just like an operation team member might need to trace latency. This course is designed to equip Cloud Architects, Administrators, SysOps personnel, Cloud Developers, and DevOps personnel with the essential skills and knowledge needed to excel in logging and monitoring your applications and workloads on Google Cloud. The prerequisites for this course are: Basic scripting or coding ability, Google Cloud Fundamentals: Core Infrastructure or equivalent experience, and proficiency with command-line tools and Linux operating system environments. In this course, we will delve into the critical aspects of Cloud Logging and Cloud Monitoring on Google Cloud. Logging and monitoring play a pivotal role in maintaining the health and security of your cloud resources. You will learn: The purpose and capabilities of Google Cloud Observability, How to Implement monitoring for multiple cloud projects, Create alerting policies, uptime checks and alerts to identify and resolve problems quickly, And collect, analyse and export logging data. By the end of this course, you will have a solid grasp of how to implement and manage cloud logging and monitoring solutions within Google Cloud, enhancing your skills to maintain the reliability and security of your infrastructure. Let's get started!

### Introduction to Google Cloud Observability

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533932

Hey everyone, welcome to the first module of this course, Introduction to Google Cloud Observabilty. In this module I will provide you with a quick introduction to Google Cloud Observabilty. We will cover what it is and why it is important. Let us look at the objectives of the module. Google Cloud Observability consists of three broad categories, Logging, Monitoring, and Application Performance Management. In this module, we will start with an overview of why we need these tools, and then we get to know both the observabilty and the Application Performance Management products. We will explore what the Google Cloud Observability architecture consists of to understand how the three pieces Cloud Logging, Cloud Monitoring and APM are connected. So, let's get started!

#### Need for Google Cloud observability

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533933

We start this section with an overview of why we need these tools, and then we’ll spend a little time understanding the role of monitoring in product reliability. We will explore the significance of the four golden signals in measuring the system’s performance and reliability. We then move on to explore the products in Google Cloud Observability. If you've ever worked with on-premises environments, you know that you can physically touch the servers. If an application becomes unresponsive, someone can physically determine why that happened. In the cloud though, the servers aren't yours—they're Google’s—and you can’t physically inspect them. So the question becomes, how do you know what's happening with your server, or database, or application? The answer is by using Google’s integrated observability tools. In this slide, we will dive a little deeper to understand the four distinct recurring user needs for observability. Visibility into system health: Users want to understand what is happening with their application and system. They rely on a service that provides a clear mental model for how their application is working on Google Cloud. They need a report on the overall health of systems. The services should help answer questions such as “are my systems functioning?” or “”do my systems have sufficient resources available? ” Error reporting and alerting: Users want to monitor their service at a glance through healthy/unhealthy status icons or red/green indicators. Customers appreciate any proactive alerting, anomaly detection, or guidance on issues. Ideally, they want to avoid connecting the dots themselves. Efficient troubleshooting: Users don’t want multiple tabs open. They need a system that can proactively correlate relevant signals and make it easy to search across different data sources, like logs and metrics. If possible, the service needs to be opinionated about the potential cause of the issue and recommend a meaningful direction for the customer to start their investigation. It should allow users to immediately act on what they discover. For instance, a metric indicating insufficient quota should be accompanied by a button to increase quota. Performance improvement: Users need a service that can perform retrospective analysis. Generally, help them plan intelligently by analyzing trends and understand how changes in the system affect its performance. Let’s begin with monitoring. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and lessen their pain. In Google's Site Reliability Engineering book, which is available to read at landing.google.com/sre/books, monitoring is defined as: "Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes." An application client normally only sees the public side of a product, and as a result, developers and business stakeholders both tend to think that the most crucial way to make the client happy is by spending the most time and effort on developing that part of the product. However, to be truly reliable, even the very best products still must be deployed into environments with enough capacity to handle the anticipated client load. Great products also need thorough testing, preferably automated testing, and a refined continuous integration/continuous development (CI/CD) release pipeline. Postmortems and root cause analyses are the DevOps team's way of letting the client know why an incident happened and why it is unlikely to happen again. In this context we are discussing a system or software failure, but the term “incident” can also be used to describe a breach of security. Transparency here is key to building trust. We need our products to improve continually, and we need monitoring data to ensure that happens. We need dashboards to provide business intelligence so our DevOps personnel have the data they need to do their jobs. We need automated alerts because humans tend to look at things only when there's something important to look at. An even better option is to construct automated systems to handle as many alerts as possible so humans only have to look at the most critical issues. Typically, there's some triggering event: a system outage, data loss, a monitoring failure, or some form of manual intervention. The trigger leads to a response by both automated systems and DevOps personnel. Many times the response starts by examining signal data that comes in through monitoring. The impact of the issue is evaluated and escalated when needed, and an initial response is formulated. Throughout, good SREs will strive to keep the customer informed and respond when appropriate. Finally, we need monitoring tools that help provide data crucial to debugging application functional and performance issues. We’ll look more closely at Google’s integrated monitoring tools a bit later in this module. There are “four golden signals” that measure a system’s performance and reliability. They are latency, traffic, saturation, and errors. Latency measures how long it takes a particular part of a system to return a result. Latency is important because it directly affects the user experience. Changes in latency could indicate emerging issues. Its values may be tied to capacity demands. It can be used to measure system improvements. But how is it measured? Sample latency metrics include page load latency, number of requests waiting for a thread, query duration, service response time, transaction duration, time to first response and time to complete data return. The next signal is traffic, which measures how many requests are reaching your system. Traffic is important because it’s an indicator of current system demand. Its historical trends are used for capacity planning. It’s a core measure when calculating infrastructure spend. Sample traffic metrics include number of HTTP requests per second, number of requests for static vs. dynamic content, number of concurrent sessions, and many more. The third signal is saturation, which measures how close to capacity a service is. It’s important to note, though, that capacity is often a subjective measure, that depends on the underlying service or application. Saturation is important because it's an indicator of how full the service is. It focuses on the most constrained resources. It’s frequently tied to degrading performance as capacity is reached. Sample capacity metrics include percentage memory utilization, percentage of thread pool utilization, percentage of cache utilization and many more. The fourth signal is errors, which are events that measure system failures or other issues. Errors are often raised when a flaw, failure, or fault in a computer program or system causes it to produce incorrect or unexpected results, or behave in unintended ways. Errors might indicate configuration or capacity issues or service level objective violations. Sample error metrics include wrong answers or incorrect content, number of 400/500 HTTP codes, number of failed requests, number of exceptions and many more. Now, let's return to the observability concept. Observability starts with signals, which are metric, logging, and trace data captured and integrated into Google products from the hardware layer up. From those products the signal data flows into Google Cloud Observability tools where it can be visualized in dashboards and through the Metrics Explorer. Automated and custom logs can be dissected and analyzed in the Logs Explorer. Services can be monitored for compliance with service level objectives (SLOs), and error budgets can be tracked. Health checks can be used to check uptime and latency for external-facing sites and services. When incidents occur signal data can generate automated alerts to code or, through various information channels, to key personnel. Error Reporting can help operations and developer teams spot, count, and analyze crashes in cloud-based services. The visualization and analysis tools can then help troubleshoot what's happening in Google Cloud. Ultimately, you won't miss that easy server access, because Google provides more precise insights into your Cloud install than you ever had on-premises. Let’s explore the products most applicable for those in operations roles that work with Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace, and Cloud Profiler.

#### Cloud Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533934

We defined general monitoring and its benefits in the previous section. Let us take a look at Cloud Monitoring features and benefits. Cloud Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. It collects metrics, events, and metadata from projects, logs, services, systems, agents, custom code, and various common application components, including Cassandra, Nginx, Apache Web Server, Elasticsearch, and many others. Monitoring ingests that data and generates insights via dashboards, Metrics Explorer charts, and automated alerts. Cloud Monitoring provides many advanced capabilities that helps address the monitoring challenges and these include: Many free metrics: On 100+ monitored resources, over 1,500 metrics are immediately available with no cost. You can find out more about this at Google Cloud Observability pricing. Open source standards: Leverage Prometheus and Open Telemetry to collect metrics across compute workloads. Customization for key workloads: Cloud Monitoring offers custom visualization capabilities for GKE through Google Cloud Managed Service for Prometheus and for Compute Engine through Ops Agent. In-context visualizations and alerts: View relevant telemetry data alongside your workloads across Google Cloud.

#### Cloud Logging

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533935

The next service we will explore is Cloud Logging. Google's Cloud Logging allows users to collect, store, search, analyze, monitor, and alert on log entries and events. It provides automatic ingestion with simple controls for routing, storing, and displaying your log data. It leverages tools like Log Analytics to view trends, or Error Reporting and Log Explorer to quickly examine problems. Like I mentioned earlier, logging has multiple aspects such as collection, analysis, export and retention. Cloud Logging enables you to automatically collect cloud events and configuration changes. You can aggregate and centralize logs at a organizational level, project level and folder level based on your needs. Most log analysis start with Google Cloud’s integrated Logs Explorer. You can run queries and analyze log data with Log Analytics. Logging entries can also be exported to several destinations for alternative or further analysis. Export log data as files to Google Cloud Storage, or as messages through Pub/Sub, or into BigQuery tables. Pub/Sub messages can be analyzed in near-real time using custom code or stream processing technologies like Dataflow. BigQuery allows analysts to examine logging data through SQL queries. And archived log files in Cloud Storage can be analyzed with several tools and techniques. Logs-based metrics may be created and integrated into Cloud Monitoring dashboards, alerts, and service SLOs. Default log retention in Cloud Logging depends on the log type. Data access logs are retained by default for 30 days, but this is configurable up to a max of 3650 days. Admin logs are stored by default for 400 days. Alternatively you can also export logs to Google Cloud Storage or BigQuery to extend retention. Let us next look at a few use cases. A developer would love to get started quickly, thus we have out of the box collection of system metrics and logs and integration into popular logging SDKs and library. Cloud Logging also allows developers to do real time analysis, debugging and troubleshooting of your code. For convenient access and visibility, stack traces are automatically mapped to error types. Operators also take massive advantage of our offering. These include collecting telemetry that is not limited to Google Cloud, centralization of all the logs for users, teams and organizations. You are in control of retention periods and location of the logs. You can also understand log volume, cost and set alerts on important application metrics. You can export logs for storage, analysis and also integrate with third-party services. Lastly, security operations, or SecOps are in charge of ensuring that all access is authorized and that bad actors are not navigating your network. With audit logs, network telemetry and log analysis it can be achieved in a streamlined way.

#### Error Reporting

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533936

The next service we will explore is Error Reporting. Error Reporting counts, analyzes, and aggregates the crashes in your running cloud services. Error reporting enables you to perform a lot of advanced functionalities that ensures your application runs smoothly. These include: Real time processing: Application errors are processed and displayed in the interface within seconds. You can quickly view and understand errors as a dedicated page displays the details of the error: bar chart over time, list of affected versions, request URL and link to the request log. It provides instant notification. Do not wait for your users to report problems. Error Reporting is always watching your service and instantly alerts you when a new application error cannot be grouped with existing ones. Directly jump from a notification to the details of the new error. Crashes in most modern languages are exceptions which are not caught and are handled by the code itself. Its management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details: time chart, occurrences, affected user count, first- and last-seen dates, and a cleaned exception stack trace. You can also create alerts to receive notifications on new errors.

#### Application Performance Management Tools

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533937

The last section is about Application Performance Management. This include Cloud Profiler and Cloud Trace. Cloud Trace is a tracing system that collects latency data from your distributed applications and displays it in the Google Cloud console. Trace can capture traces from applications deployed on App Engine, Compute Engine VMs, and Google Kubernetes Engine containers. Now you can analyze changes to applications’ latency profiles through Google Cloud Console and on Android devices. Using the latency reports feature, you can view performance insights in near-real time. Automatically analyze all of your application's traces to generate in-depth latency reports to surface performance degradations. Continuously gather and analyze trace data to automatically identify recent changes to application performance. Poorly performing code increases the latency and cost of applications and web services every day, without anyone knowing or doing anything about it. Cloud Profiler changes this by using statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete CPU and heap picture of an application without slowing it down. With broad platform support that includes Compute Engine VMs, App Engine, and Kubernetes, it allows developers to analyze applications running anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Python, and Node.js. Cloud Profiler presents the call hierarchy and resource consumption of the relevant function in an interactive flame graph that helps developers understand which paths consume the most resources and the different ways in which their code is actually called. Overall, Google Cloud Observability helps you explore both the known and unknown issues underlying your workloads. The products are user focussed designed to understand a customer’s journey with SLO monitoring, uptime checks, tracing and more. They are open, flexible and leverage several popular open source projects like Prometheus, OpenTelemetry, and Fluentbit. Integrated for ease through automatic ingestion, connect data sets, in-context telemetry across Google Cloud service views. They also provide meaningful analysis and alerting through powerful analysis tools, leverage alerting for both automated and human-led resolutions.

#### Module Summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533938

In this module, we’ve explored the core observability tools in Google Cloud, including Cloud Logging, Cloud Monitoring, and Error Reporting, and the application performance management tools, including Cloud Trace, and Cloud Profiler. Now that we have a foundation, let’s move on to cover the various tools in greater detail.

#### Quiz - Introduction to Google Cloud Observability

- https://www.cloudskillsboost.google/paths/14/course_templates/99/quizzes/533939

### Monitoring Critical Systems

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533940

Let's spend a little time talking about how Google Cloud helps you monitor critical systems. Monitoring is all about keeping track of exactly what's happening with the resources that we launched inside of Google Cloud. In this module, let’s take a look at options and best practices as they relate to monitoring project architectures. It’s important to make some early architectural decisions before starting monitoring. We will examine some of the default dashboards created by Google, and see how to use them appropriately. We will create charts and use them to build custom dashboards to show resource consumption and application load. And, we will define uptime checks to track liveliness and latency. We will also cover the purpose of using MQL for monitoring.

#### Monitoring Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533941

We first start with an overview of monitoring. Then, we explore this concept in detail as we advance with the rest of the module. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and reduce their problems.

#### Cloud Monitoring achitecture patterns

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533942

Let's now learn more about some common monitoring architecture patterns. A typical Cloud Monitoring architecture includes 3 layers, a data collection layer, a data storage layer, and a data analysis and visualization layer. A data collection layer collects metrics, logs, and traces from cloud-based systems. In Cloud Monitoring, the data collection layer includes Google Cloud services such as Google Kubernetes Engine, Compute Engine, App Engine etc,. A data storage layer stores the collected data and routes to the configured visualization and analysis layer. In Cloud Monitoring, this layer includes the Cloud Monitoring API that helps triage the metrics collected to be stored for further analysis. A data analysis and visualization layer: This layer analyzes the collected data to identify problems and trends and presents the analyzed data in a way that is easy to understand. In Cloud Monitoring, this layer comprise of various features within Cloud Monitoring such as Dashboards to visualize data, Uptime checks to monitor applications, Alerting policies to configure alerts and notifications to notify of events that need attention. One of the most common uses of Cloud Monitoring is platform monitoring Blackbox monitoring of the platform enables users to get visibility into the performance of their Google Cloud services. With Google Cloud, this is enabled by default and system metrics are automatically collected without any user effort. Google Cloud Monitoring is the recommended solution for Platform monitoring. System metrics from Google Cloud are available at no cost to customers. These metrics provide information about how the service is operating. Over 1500 metrics across more than 100 Google Cloud services automatically. For example, Compute Engine reports over 25 unique metrics for each virtual machine (VM) instance. However, if customers, in traditional enterprise cohorts, are using third party products for monitoring and want to aggregate their Google Cloud metrics into those partner products, they can use Cloud Monitoring APIs to ingest these metrics. For applications or workloads deployed in GKE, many customers prefer a Prometheus-based solution for monitoring. We fully embrace that monitoring approach and provide customers a new way to leverage Prometheus based monitoring using Google Managed Prometheus (GMP). GMP is a part of Cloud Monitoring and it makes GKE cluster and workload metrics available as Prometheus data. It can ingest monitoring data exposed in Prometheus format, it supports PromQL compatible query language and has natively integrated the Prometheus expression browser, and Prometheus compatible rule evaluation. For application workloads in GKE, we recommend that customers use Google Managed Prometheus. For applications or workloads deployed in Compute Engine, customers should use the Ops Agent to collect in-process metrics and to collect metrics from third party applications that run in your VMs. Ops agent today supports more than 30 plugins for different open source and ISV software along with a collection of richer and more fine grained metrics at the OS level for Windows and Linux (many flavors). The Ops agent is based on OpenTelemetry standards so custom applications developed by customers can leverage OTEL client libraries for instrumenting their code and generate the needed telemetry. The Ops agent can collect these custom metrics and make them available in Cloud Monitoring as well. While this ecosystem of third party plugins will continue to expand, if users need support for other software products or services, consider using a partner product like Datadog or NewRelic. If you choose to use partner products, they can collect system metrics from the Google Cloud platform by using the native API-based integrations With Google's partner BindPlane by Blue Medora, you can import monitoring and logging data from both on-premises VMs and other cloud providers, such as Amazon Web Services (AWS), Microsoft Azure, Alibaba Cloud, and IBM Cloud into Google Cloud. The following diagram shows how Cloud Monitoring and BindPlane can provide a single pane of glass for a hybrid cloud. This architecture has the following advantages: In addition to monitoring resources like VMs, Blue Medora has built-in deep integration for over 150 popular data sources. There are no additional licensing costs for using BindPlane. BindPlane metrics are imported into Monitoring as custom metrics, which are chargeable. Likewise, BindPlane logs are charged at the same rate as other Cloud Logging logs.

#### Monitoring multiple projects

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533943

We have explored a few architecture patterns, let us next explore how you can monitor multiple projects from a single project by using metrics scope. When you go to monitoring settings for a project, you can see that the current metrics scope only has a single project in it, the one it is currently viewing. When you create a Google Cloud project, that project hosts a metrics scope and becomes the scoping project for that scope. It stores the alerts, uptime checks, dashboards, and monitoring groups that you configure for the scope. For example, if you create a staging project and then access monitoring, you can see the metrics for the resources in the staging project. This happens for every project you create. Each project creates a metrics scope for itself and hosts monitoring configuration for itself. But what if you want to centralize how that data is stored and how it's accessed? Since it's possible for one metrics scope to monitor multiple projects, and also a project can be monitored from only a single metrics scope, you will have to decide which relationship will work best for your organizational culture, and this particular project. Let us explore option one where every project is monitored locally, in that project. The advantages are clear and obvious separation for each project. If the project contains development-related resources, it's easy to provide access to the dev personnel. Project resources and monitoring resources all in the same place. Easy to automate, since monitoring becomes a standard part of the initial project setup. Let us discuss the disadvantage. If the application is larger than a single project, you will have limited visibility into application performance. Strategy B: Single metrics scope is used for monitoring large units of projects. You can add multiple projects to an existing scope. Now, monitoring data for all projects in that scope will be visible. This will let you create dashboards, showing resources from all the projects in the scope, or alerting policies that apply to resources in multiple projects as long as they're in the metrics scope. Note that the recommended approach for production deployments is to create a dedicated project to host monitoring configuration data and use its metrics scope to set up monitoring for the projects that have actual resources in them. This way, should a project not be necessary anymore and get deleted, the monitoring configuration for all the other projects won't be impacted. The advantage is a single pane of glass that provides visibility into the entire group of related projects. Compare non-prod and prod environments easily. The disadvantage is that anyone with IAM permissions to access Cloud Monitoring will be able to see metrics for all environments. Monitoring in prod is typically divided among different teams. This approach would not preserve that separation. Although the metric data and log entries remain in the individual projects, any user who has been granted the role Monitoring Viewer will have access to the dashboards and have access to all data by default. This means that a role assigned to one person on one project applies equally to all projects monitored by that metrics scope. Remember, metrics scope only affects and controls Google Cloud resources related to Cloud Monitoring. Other tools covered in this course, such as Cloud Logging, Error Reporting, and the Application Performance Management (APM) tools, are strictly project-based and do not rely upon the configuration of the metrics scope or the monitoring IAM roles.

#### Data model and dashboards

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533944

In this section we will explore more on the Monitoring Data model and see how we can visualize these metrics as charts in a dashboard. Let us start by understanding the Cloud Monitoring data model. In general terms, monitoring data is recorded in time series. Each individual time series includes four pieces of information relevant to this discussion: The metric field describes the metric itself and records two aspects: The metric-label that represents one combination of label values. The metric type specifies the available labels and describes what is represented by the data points. The resource field records: The resource-label represents one combination of label values. The specific monitored resource from which the data was collected. The metricKind and valueType fields tell you how to interpret the values. The value type is the data type for the measurements. Each time series records the value type (type ValueType) for its data points. Each time series includes the metric kind (type MetricKind) for its data points. The kind of metric data tells you how to interpret the values relative to each other. Cloud Monitoring metrics are one of three kinds: A gauge metric, a delta metric, or a cumulative metric. The points field is an array of timestamped values. The metric type tells you what the values represent. The sample time series has an array with a single data point; in most time series, the array has many more values. Navigate to the Time Series List API Explorer, which can be found in the reference documentation for Google Cloud Monitoring. Locate the API Explorer widget and edit the filter to include the metric type "logging.googleapis.com/log_entry_count" and resource type "gce_instance". Specify the desired start and end time for your data. Upon successful execution, you'll receive the time series data shown on the next slide. As mentioned earlier monitoring data is recorded in time series. The example shown here is a complete instance of a single time series. Most time series include a lot more data points; this one covers a one-minute interval. All time series have the same structure, with the following fields: The metric field indicates that the metric collected is a set of activity logs of the type log_entry_count with a severity level INFO. The resource field which indicates that the resource is a Compute Engine instance with the details on the specific instance and project ID. The metric kind is of the type DELTA and value type integer. Points are actual array of time stamp and value of the metric. First, identify the Google Cloud Monitoring resources you want to monitor. Next, check the Monitoring Dashboards for auto-created dashboard. When you can't find what you need, use the Metrics Explorer. You can monitor any of the more than 1500 metrics, custom metrics and can even build custom dashboards. Dashboards are a way for you to view and analyze metric data that is important to you. They give you graphical representations on the main signal data in such a way as to help you make key decisions about your Google Cloud-based resources. One of the changing aspects of monitoring is Google's commitment to providing more opinionated default information. Google Cloud sees that your project contains Compute Engine VMs, or a GKE Cluster, so Monitoring auto-creates dashboards for you that radiate the information that Google thinks is important for those two resource types. As you add more resources, Google will continue to add more default dashboards. These dashboards form a great monitoring foundation on which you can build. You can also use the Dashboard Builder to visualize application metrics that you are interested in. You can select the chart type and filter the metrics based on your requirements. Frequently, the easiest way to start chart creation is to build an ad-hoc chart with Google's Metrics Explorer. Metrics Explorer lets you build charts for any metric collected by your project. With it, you can Save charts you create to a custom dashboard. Share charts by their URL. View the configuration for charts as JSON. Most importantly, you can use Metrics Explorer as a tool to explore data that you don't need to display long term on a custom dashboard. As seen on this slide, the Metrics Explorer interface consists of three primary regions: A configuration region, where you pick the metric and its options, The chart that displays the selected metric The display panel, where you can configure the axis, set a threshold lines, and more. You define a chart by specifying both what data should display and how the chart should display it. Metric: To populate the chart, you must specify at least one pair of values, the monitored resource type (or monitored resource, or just resource), and the metric type (also called the metric descriptor, or just metric). Filter: You can reduce the amount of data returned for a metric by specifying a filter. Filtering removes data from the chart by excluding time series that don't meet your criteria. The result is fewer lines on the chart and, hopefully, a better signal to noise ratio. When you click in the Filter field, a panel that contains lists of criteria by which you can filter appears. In broad strokes, you can filter by resource group, by name, by resource label, and by the metric label. In this example, we filter by machine type. The zone can then be compared to a direct value, like "e2-medium," or by using the “=” operator, to any valid regular expression. You can check the documentation If you want to see the fully supported filter syntax. Grouping: You can reduce the amount of data returned for a metric by combining different time series. To combine multiple time series, you typically specify a grouping and a function. Grouping is done by label values. The function defines how all time-series data within a group are combined into a new time series. Alignment: Alignment creates a new time series in which the raw data has been regularized in time so it can be combined with other aligned time series. Alignment produces time series with regularly spaced data. A time series is a set of data points in temporal order. To align a time series is to break the data points into regular buckets of time, the alignment period. Multiple time series must be aligned before they can be combined. Alignment is a prerequisite to aggregation across time series, and monitoring does it automatically, by using default values. You can override these defaults by using the alignment options, which are the Alignment function and the Min alignment period. The alignment period determines the length of time for subdividing the time series. For example, you can break a time series into one-minute chunks or one-hour chunks in the Min interval field. The data in each period is summarized so that a single value represents that period. The default alignment period, which is also the minimum, is one minute. Although you can set the alignment interval for your data, time series might be realigned when you change the time interval displayed on a chart or change the zoom level. The alignment function determines how to summarize the data in each alignment period. The functions include the sum, the mean, and so forth. Valid alignment choices depend on the kind and type of metric data a time series stores. Click any of the legend column headers to sort by that field. The legend columns included in the chart's display are configurable. A chart's widget type and its analysis mode setting determine how the chart displays data. For example, when you create a line chart, each time series is shown by a line with a unique color. However, you can configure a line chart to display statistical measures such as the mean and moving average. There are three analysis modes: Standard mode displays each time series with a unique color. Stats mode displays common statistical measures for the data in a chart. X-Ray mode displays each time series with a translucent gray color. Each line is faint, and where lines overlap or cross, the points appear brighter. Therefore, this mode is most useful on charts with many lines. Overlapping lines create bands of brightness, which indicate the normal behavior within a metrics group. Compare to past: When you use Compare to Past mode on a chart, the legend is modified to include a second “values” column. The current Value column becomes Today, and the past values column is named appropriately—for example, Last Week. Threshold line: The Threshold option creates a horizontal line from a point on the Y-axis. The line provides a visual reference for the chosen threshold value. You can add a threshold that refers to a value on the left Y-axis or the right Y-axis. The Legend Alias field lets you customize a description for the time series on your chart. These descriptions appear on the tooltip for the chart and on the chart legend in the Name column. By default, the descriptions in the legend are created for you from the values of different labels in your time series. Because the system selects the labels, the results might not be helpful to you. To build a template for descriptions, use this field. You can enter plain text and templates in the Legend Alias field. When you add a template, you add an expression that is evaluated when the legend is displayed. To add a legend template to a chart, do the following: In the Display pane, expand Legend Alias. Click + (Plus) and select an entry from the menu. In this example, you see the mix of the variables. You can see the resulting output in the chart legend.

#### Query metrics

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533945

Before we wrap our discussion on dashboards, charts, and the Metrics Explorer, let’s examine a more versatile way of interacting with metrics by leveraging the query languages such as Monitoring Query Language (MQL) and PromQL. MQL: MQL is an advanced query language. It provides an expressive, text-based interface to Cloud Monitoring time-series data. By using MQL, you can retrieve, filter, and manipulate time-series data. PromQL provides an alternative to the Metrics Explorer menu-driven and Monitoring Query Language (MQL) interfaces for creating charts and dashboards. You can use PromQL to query and chart Cloud Monitoring data from the following sources: Google Cloud services, like Google Kubernetes Engine or Compute Engine, that write metrics described in the lists of Cloud Monitoring system metrics. User-defined metrics, like log-based metrics and Cloud Monitoring user-defined metrics. Google Cloud Managed Service for Prometheus, the fully managed multi-cloud solution for Prometheus from Google Cloud. For information about the managed service, including support from PromQL, see Google Cloud Managed Service for Prometheus. You can also use tools like Grafana to chart metric data ingested into Cloud Monitoring. Available metrics include metrics from Managed Service for Prometheus and Cloud Monitoring metrics documented in the lists of metrics. These are a few examples of when you can use MQL: Create ratio-based charts and alerts Perform time-shift analysis (compare metric data week over week, month over month, year over year, etc.) Apply mathematical, logical, table operations, and other functions to metrics Fetch, join, and aggregate over multiple metrics Select by arbitrary, rather than predefined, percentile values Create new labels to aggregate data by, using arbitrary string manipulations including regular expressions Whether you need to perform joins, display arbitrary percentages, or even make advanced calculations, the use cases for MQL are unlimited. MQL is built using operations and functions. Operations are linked together by using the common “pipe” idiom, where the output of one operation becomes the input to the next. Linking operations makes it possible to build complex queries incrementally. In the same way you compose and chain commands and data through pipes on the Linux command line, you can fetch metrics and apply operations by using MQL. For a more advanced example, suppose you built a distributed web service that runs on Compute Engine VM instances and uses Cloud Load Balancing, and you want to analyze error rate, which is one of the SRE “golden signals.” You want to see a chart that displays the ratio of requests that return HTTP 500 responses (internal errors) to the total number of requests; that is, the request-failure ratio. The loadbalancing.googleapis.com/https/request_count metric type has a response_code_class label, which captures the class of response codes. This query uses an aggregation expression built on the ratio of two sums: The first sum uses the if function to count 500-valued HTTP responses and a count of 0 for other HTTP response codes. The sum function computes the count of the requests that returned 500. The second sum adds up the counts for all requests, as represented by val(). The two sums are then divided, which results in the ratio of 500 responses to all responses. You can use queries by navigating to Metrics explorer and simply click the CODE button. You can use the radio button to switch between MQL and PromQL. We will cover PromQL in a later module.

#### Uptime checks

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533946

Another monitoring component before we wrap this module is uptime checks. Uptime checks can be configured to test the availability of your public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource you can check are App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. For each uptime check, you can create an alerting policy and view the latency of each global location. Uptime checks can help us ensure that our externally facing services are running and that we aren’t burning our error budgets unnecessarily. Here is an example of an HTTP uptime check. The resource is checked every minute with a 10-second timeout. Uptime checks that do not get a response within this timeout period are considered failures. So far, there is a 100% uptime with no outages. Uptime checks are easy to create. In Monitoring, navigate to Uptime Checks and click Create Uptime Check. Give the uptime check a name or title that is descriptive. Select the check type protocol, the resource type, and appropriate information for that resource type. A URL, for example, would need a hostname and an optional page path. A number of optional advanced options are available, including logging failures, narrowing the locations in the world from where test connections are made, the addition of custom headers, check timeout, and authentication. The interface also makes it easy to create an alert for failing uptime checks. Cloud Monitoring empowers users with the ability to monitor multiple projects from a single metrics scope. In this exercise, you start with three Google Cloud projects: two have monitorable resources, and you will use the third one to host a metrics scope. You attach the two resource projects to the metrics scope, build uptime checks, and construct a centralized dashboard.

#### Monitoring and Dashboarding Multiple Projects

- https://www.cloudskillsboost.google/paths/14/course_templates/99/labs/533947

#### Module summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533948

In this module, you learned how to: Use Cloud Monitoring to view metrics for multiple cloud projects, Explain the different types of dashboards and charts that can be built, Create an uptime check, Explain the purpose of using MQL for monitoring. Great job and keep up the good work.

#### Quiz - Monitoring critical systems

- https://www.cloudskillsboost.google/paths/14/course_templates/99/quizzes/533949

### Alerting Policies

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533950

In the previous module, we explored how to monitor resources using Cloud Monitoring. Let's us next learn how you can receive timely awareness to problems in your cloud applications so you can resolve the problems quickly. Upon completion of this module, you will be able to explain why SLI, SLO and SLA are important, define alerting policies and discuss alerting strategies. You will also be able to explain error budgets, identify types of alerts and common uses for each and also use Cloud Monitoring to manage services.

#### SLI, SLO, and SLA

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533951

The three terms that are frequently used in this course are SLI, SLO and SLA. Before we cover alerting strategy, it is important to understand what SLI, SLO and SLA are. Service level indicators, or SLIs, are carefully selected monitoring metrics that measure one aspect of a service's reliability. Ideally, SLIs should have a close linear relationship with your users' experience of that reliability, and we recommend expressing them as the ratio of two numbers: the number of good events divided by the count of all valid events. A Service level objective, or SLO, combines a service level indicator with a target reliability. If you express your SLIs as is commonly recommended, your SLOs will generally be somewhere just short of 100%, for example, 99.9%, or "three nines." You can't measure everything, so when possible, you should choose SLOs that are S.M.A.R.T. SLOs should be specific. "Hey everyone, is the site fast enough for you?" is not specific; it's subjective. "The 95th percentile of results are returned in under 100ms." That's specific. They need to be based on indicators that are measurable. A lot of monitoring is numbers, grouped over time, with math applied. An SLI must be a number or a delta, something we can measure and place in a mathematical equation. SLO goals should be achievable. "100% Availability" might sound good, but it's not possible to obtain, let alone maintain, over an extended window of time. SLOs should be relevant. Does it matter to the user? Will it help achieve application-related goals? If not, then it’s a poor metric. And SLOs should be time-bound. You want a service to be 99% available? That’s fine. Is that per year? Per month? Per day? Does the calculation look at specific windows of set time, from Sunday to Sunday for example, or is it a rolling period of the last seven days? If we don't know the answers to those types of questions, it can’t be measured accurately. And then there are Service Level Agreements, or SLAs, which are commitments made to your customers that your systems and applications will have only a certain amount of “down time.” An SLA describes the minimum levels of service that you promise to provide to your customers and what happens when you break that promise. If your service has paying customers, an SLA may include some way of compensating them with refunds or credits when that service has an outage that is longer than this agreement allows. To give you the opportunity to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA. For SLOs, SLIs and SLAs to help improve service reliability, all parts of the business must agree that they are an accurate measure of user experience and must also agree to use them as a primary driver for decision making. Being out of SLO must have concrete, well-documented consequences, just as there are consequences for breaching SLAs. For example, slowing down the rate of change and directing more engineering effort towards eliminating risks and improving reliability are actions that could be taken to get your product back to meeting its SLOs faster. Operations teams need strong executive support to enforce these consequences and effect change in your development practice. Here is an example of a SLA, which is to maintain an error rate of less than 0.3% for the billing system. Here error rate is a quantifiable measure which is the SLI and 0.3 is the specific target set which is the SLO in this case. If your service has paying customers, you probably have some way of compensating them with refunds or credits when that service has an outage. Your criteria for compensation are usually written into a service level agreement, which describes the minimum levels of service that you promise to provide and what happens when you break that promise. The problem with SLAs is that you're only incentivized to promise the minimum level of service and compensation that will stop your customers from replacing you with a competitor. When reliability falls far short of the levels of service that keep your customers happy, this contributes to a perception that your service is unreliable, customers often feel the impact of reliability problems before these promises are breached, Compensating your customers all the time can get expensive, so what targets do you hold yourself to internally? When does your monitoring system trigger an operational response? To give you the breathing room to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA.

#### Developing an alerting strategy

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533952

Now that we have a solid understanding of SLA, SLI and SLO, let us look at how to develop an alerting strategy next. An alert is an automated notification sent by Google Cloud through some notification channel to an external application, ticketing system, or person. If you are wondering why is the alert being sent? The answer perhaps is that a service is down, or an SLO isn't being met. Regardless, an alert is generated when something needs to change. The events are processed through a time series: a series of event data points broken into successive, equally spaced windows of time. Based on need, the duration of each window and the math applied to the member data points inside each window are both configurable. Because of the time series, events can be summarized, error rates can be calculated, and alerts can be triggered where appropriate. A great time to generate alerts is when a system is heading to spend all of its error budget before the allocated time window. An error budget is perfection minus SLO. SLIs are things that are measured, and SLOs represent achievable targets. If the SLO target is "90% of requests must return in 200 ms," then the error budget is 100% - 90% = 10%. Several attributes should be considered when attempting to measure the accuracy or effectiveness of a particular alerting strategy. Precision is the proportion of alerts detected that were relevant to the sum of relevant and irrelevant alerts. It’s decreased by false alerts. Recall is the proportion of alerts detected that were relevant to the sum of relevant alerts and missed alerts. It’s decreased by missing alerts. Precision can be seen as a measure of exactness, whereas recall is a measure of completeness. Detection time can be defined as how long it takes the system to notice an alert condition. Long detection times can negatively affect the error budget, but alerting too fast can generate false positives. Reset time measures how long alerts fire after an issue has been resolved. Continued alerts on repaired systems can lead to confusion. Error budgeting 101 would state that when the error count, or whatever is being measured, is trending to be greater than the allowed error budget, an alert should be generated. Both the SLO itself, and the idea of “trending toward” require windows of time over which they are calculated. In this subject space, the window term is used in two main ways: The SLO itself will be measured over a window of time, say an availability SLO of 99.9% over every 30-day period. Alert triggering will have a window over which it watches for trends. For example, an alert might fire if the percentage of errors over any 60-minute period exceeds 0.1%. One of the alerting decisions you and your team have to make is window length. The window is a regular-length subdivision of the SLO total time. Imagine you set a Google Cloud spend budget of $1,000 a month. When do you want to receive an alert? When the $1,000 is spent? Or when the predicted spend is trending past the $1,000? Of course, the latter. Now, the same concept, but this time imagine a 99.9% availability SLO over 30 days. You don't want to get an alert when your error budget is already gone. By then it's too late to do anything about the problem. One option is small windows. Smaller windows tend to yield faster alert detections and shorter reset times, but they also tend to decrease precision because of their tendency toward false positives. In our 99.9% availability SLO for over 30 days, a 10-minute window would alert in 0.6 seconds if a full outage occurs and would consume only 0.02% of the error budget. Longer windows tend to yield better precision, because they have longer to confirm that an error is really occurring. But reset and detection times are also longer. That means you spend more error budget before the alert triggers. In our same 99.9% availability for SLO over 30 days, a 36-hour window would alert in 2 minutes and 10 seconds if a full outage occurs, but would consume a full 5% of the error budget. One trick might be to use short windows, but add a successive failure count. One window failing won’t trigger the alert, but when three fail in a row the error is triggered. This way, the error is spotted quickly but treated as an anomaly until the duration or error count is reached. This is what you do when your car starts making a sound. You don't immediately freak out, but you pay attention and try to determine whether it's a real issue or a fluke. The downside is that precision typically has an inverse relationship to recall. As the precision goes up, as you avoid false positives, you let the problem continue to happen. If the "pay attention but don't alert yet" duration is 10 minutes, a 100% outage for 5 minutes is not detected. As a result, if errors spike up and down, they might never be detected. So how do we get good precision and recall? This is achieved with multiple conditions. Many variables affect a good alerting strategy, including the amount of traffic, the error budget, peak and slow periods, to name a few. The fallacy is believing that you have to choose a single option. Define multiple conditions in an alerting policy to get better precision, recall, detection time, and rest time. You can also define multiple alerts through multiple channels. Perhaps a short window condition generates an alert, but it takes the form of a Pub/Sub message to a Cloud Run container, which then uses complex logic to check multiple other conditions before deciding whether a human gets a notification. See the SRE Workbook for more information. And alerts should always be prioritized based on customer impact and SLA. Don't involve humans unless the alert meets some threshold for criticality. You can use severity levels as an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. You can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services. The notification channels were enhanced to accept this data—including Email, Webhooks, PubSub, and PagerDuty. This enables further automation and customization based on importance wherever the notifications are consumed. High-priority alerts might go to Slack, SMS, and/or maybe even a third-party solution like PagerDuty. You can even use multiple channels together for redundancy. Low-priority alerts might be logged, sent through email, or inserted into a support ticket management system.

#### Creating alerts

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533953

We've discussed some of the alerting concepts and strategies. Let's look at how to create alerts in Google Cloud. Google Cloud defines alerts by using alerting policies. An alerting policy has: A name One or more alert conditions Notifications Documentation For the name, use something descriptive so you can recognize alerts after the fact. Organizational naming conventions can be a great help. Alert policies can also be created from the gcloud CLI, the API and Terraform. It starts with an alert policy definition in either a JSON or YAML format. One neat trick when learning the correct file format is to create an alert using the Google Cloud console. Then use the gcloud monitoring policies list and the describe commands to see the corresponding definition file. The alerting API and gcloud CLI can create, retrieve, and delete alerting policies. [Reference: https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/channels/create] The alerting policies are of two types. Metric based alerting: Policies used to track metric data collected by Cloud Monitoring are called metric-based alerting policies. You can add a metric-based alerting policy to your Google Cloud project by using the Google Cloud console. A classic example of a metric-based alerting policy is to notify when the application is running on a VM that has high latency for a significant time period. Log based alerting: Log-based alerting is used to notify anytime a specific message occurs in a log. You can add a log-based alerting policy to your Google Cloud project by using the Logs Explorer in Cloud Logging or by using the Cloud Monitoring API. An example of log-based alerting policy is to notify when a human user accesses the security key of a service account. The alert condition is where you spend the most alerting policy time and make the most decisions. The alert condition is where you decide what's being monitored and under what condition an alert should be generated. Notice how the web interface combines the heart of the Metrics Explorer with a configuration condition. You start with a target resource and metric you want the alert to monitor. You can filter, group by, and aggregate to the exact measure you require. Then the yes-no decision logic for triggering the alert notification is configured. It includes the trigger condition, threshold, and duration. There are three types of conditions for metric-based alerts: Metric-threshold conditions trigger when the values of a metric are more than, or less than, a threshold for a specific duration window. Metric-absence conditions trigger when there is an absence of measurements for a duration window. Forecast conditions predict the future behavior of the measurements by using previous data. These conditions trigger when there is a prediction that a time series will violate the threshold within a forecast window. An alert might have zero to many notification options selected, and they each can be of a different type. There are direct-to-human notification channels (Email, SMS, Slack, Mobile Push), and for third-party integration use Webhook and Pub/Sub. Manage your notifications and incidents by adding user-defined labels to an alerting policy. Because user-defined labels are included in notifications, if you add labels that indicate the severity of an incident, then the notification contains information that can help you prioritize your alerts for investigation. If you send notifications to a third-party service like PagerDuty, Webhooks, or Pub/Sub then you can parse the JSON payload and route the notification according to its severity so that your team doesn't miss critical information. A notification channel decides how the alert is sent to the recipient. Alerts can be routed to any third-party service. Email alerts are easy and informative, but they can become notification spam if you aren't careful. SMS is a great option for fast notifications, but choose the recipient carefully. Slack is very popular in support circles. The Google Cloud app for mobile devices is a valid option. PagerDuty is a third-party on-call management and incident response service. Webhooks and Pub/Sub are excellent options when you want to alert users to external systems or code. The documentation option is designed to give the alert recipient additional information they might find helpful. Use the documentation section to guide your troubleshooting. Include internal playbooks, landing links and dynamic labels The default alert contains information about which alert is failing and why, so think of this more like an easy button. If there's a standard solution to this particular alert, adding a reference to it here might be a good example of proper documentation inclusion. Then again, if it was that easy, automate it! Here, you see an alert notification sent out through an email. Notice how many details about exactly what went wrong are automatically included in the email body. The bottom documentation section can also be used to augment the provided information. When one or more alert policies are created, the alerting web interface provides a summary of incidents and alerting events. An event occurs when the conditions for an alerting policy are met. When an event occurs, Cloud Monitoring opens an incident. In the Alerting window, the Summary pane lists the number of incidents, and the Incidents pane displays the ten most recent incidents. Each incident is in one of three states: Incidents firing: If an incident is open, the alerting policy's set of conditions is being met. Or there’s no data to indicate that the condition is no longer met. Open incidents usually indicate a new or unhandled alert. Acknowledged incidents: A technician spots a new open alert. Before one starts to investigate, they mark it as acknowledged as a signal to others that someone is dealing with the issue. Alert policies displays the number of alerting policies created. The Incidents pane displays the most recent open incidents. To list the most recent incidents in the table, including those that are closed, click Show closed incidents. Snooze displays the recently configured snoozes. When you want to temporarily prevent alerts from being created and notifications from being sent, or to prevent repeated notifications from being sent for an open incident, you create a snooze. For example, you might create a snooze when you have an escalating outage and you want to reduce the number of new notifications. Groups provide a mechanism for alerting on the behavior of a set of resources instead of individual resources. For example, you can create an alerting policy that is triggered if some resources in the group violate a condition (for example, CPU load), instead of having each resource inform you of violations individually. Groups can contain subgroups and can be up to six levels deep. One application for groups and subgroups is the management of physical or logical topologies. For example, with groups, you can separate your monitoring of production resources from your monitoring of test or development resources. You can also create subgroups to monitor your production resources by zone. Resources can belong to multiple groups. You define the one-to-many membership criteria for your groups. A resource belongs to a group if the resource meets the membership criteria of the group. Membership criteria can be based on resource name or type, Cloud Projects, network tag, resource label, security group, region, or App Engine app or service. Logs-based metrics are extracted from Cloud Monitoring and are based on the content of log entries. For example, the metrics can record the number of log entries that contain particular messages, or they can extract latency information reported in log entries. You can use logs-based metrics in Cloud Monitoring charts and alerting policies. As we covered earlier in this module, an alerting policy describes a set of conditions that you want to monitor. When you create an alerting policy, you must also specify its conditions: what is monitored and when to trigger an alert. The logs-based metrics serve as the basis for an alerting condition. To create alerting policies by using Terraform, start by creating a description of the conditions under which some aspect of your system is considered to be "unhealthy" and the ways to notify people or services about this state. Shown on slides is a basic monitoring alert policy with the name alert_policy. The code includes three required arguments: Display_name: This argument helps identify the policy with a name that can be seen on the dashboard. Combiner: This argument defines how the results of multiple conditions have to combined. Conditions: This argument defines a list of conditions that are combined based on the combiner. A policy can have up to six conditions. For some more policy examples, visit the links listed on this slide. These can be accessed from this module’s student PDF under Course Resources.

#### Alerting in Google Cloud

- https://www.cloudskillsboost.google/paths/14/course_templates/99/labs/533954

#### Service Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533955

Now that we’ve examined alerts, their use, and their creation, let’s see how the Service Monitoring console and API can help. Modern applications are composed of multiple services connected together, and when something fails, it often seems like many things fail at the same time. To help manage this complexity, SLO monitoring helps with SLO and alert creation. With Service Monitoring, you get the answers to the following questions: What are your services? What functionality do those services expose to internal and external customers? What are your promises and commitments regarding the availability and performance of those services, and are your services meeting them? For microservices-based apps, what are the inter-service dependencies? How can you use that knowledge to double check new code rollouts and triage problems if a service degradation occurs? Can you look at all the monitoring signals for a service holistically to reduce mean time to repair (MTTR)? Cloud Monitoring can identify potential or candidate services for the following types: GKE namespaces GKE services GKE workloads Cloud Run services The Service Monitoring consolidated services overview page is your point of entry. Services pane provides a summary of the health of your various services. Here, you can see the service name, type, SLO status, and whether any SLO-related alerts are firing. To monitor or view details for a specific service, click the service name. You can also filter by entering a value in the Filter text box to apply additional conditions. Service Monitoring can approach SLO compliance calculations in two fundamental ways. Request-based SLOs use a ratio of good requests to total requests. For example, we want a request-based SLO with a latency below 100 ms for at least 95% of requests. So It is convenient for us if 98% of requests were faster than 100 ms. Window-based SLOs use a ratio of the number of good versus bad measurement intervals, or windows. So each window represents a data point, instead of all the data points that comprise the window. For example, take a 95th percentile latency SLO that needs to be less than 100 ms for at least 99% of 10-minute windows. Here, a compliant window is a 10-minute period over which 95% of the requests were less than 100 ms. So It is convenient for us if 99% of 10-minute windows were compliant. Let's look at another pair of window-based versus request-based SLO examples. Imagine you get 1,000,000 requests a month, and your compliance period is rolling for 30 days. If you are looking for a 99.9% request-based SLO, that translates to 1,000 total bad requests every 30 days. However, a 99.9% windows-based SLO which is averaged across 1-minute windows, allows a total of 43 bad windows, or 43,200 total windows * 99.9% = 43,157 good windows. Windows-based SLOs can be tricky because they can hide burst-related failures. If the system returns nothing but errors, but only every Friday morning from 9:00-9:05, then you will never violate your SLO. However, not many people prefer to use the system first thing Friday morning. Service Monitoring makes SLO creation easy. On the Services overview page, select one of the listed services. If a service is built on a Google Cloud compute technology that supports Service Monitoring, it will be automatically listed. Next, click Create SLO. Select an option from the SLI metric. The options include: Availability is a ratio of the number of successful responses to the number of all responses. Latency is the ratio of the number of calls that are below the specified Latency Threshold to the number of all calls. The option Other gives you the Metrics Explorer window and lets you create your own SLI from the beginning. As previously discussed, you can also select request-based or windows-based SLOs here. In the Compliance Period section, select the Period Type and the Period Length. The two compliance period types are calendar-based and rolling. In the Performance Goal section, enter a percentage in the Goal field to set the performance target for the SLI. Service Monitoring uses this value to calculate the error budget you have for this SLO. You can create an alert by just clicking Create alerting policy. Click an individual service on the Services Overview page to view its details. There, you can see existing SLOs and, by expanding them, their details. The SLI status, the error budget remaining, and the current level of SLO compliance are all displayed. If alerts have been set, their status is also displayed. Service Monitoring can trigger an alert when a service is heading to violate an SLO. The alerting policy uses a lookback window to examine a period for trends. The burn rate threshold is then used to determine whether an alert should be raised. In this example, we are using a 60-minute window. A burn rate threshold of 1 would use 100% of the error budget by the end of the 7-day period. In this case, if we see a trend that would burn through our total error budget in 1/10th of those seven days or faster, then we should raise an alert. After the SLO is created, it’s easy to monitor the SLI status, error budget, compliance, and alert status. In this case, I’ve selected to view the Service level indicator, so it’s displaying the current SLI values. You could also examine the Error budget or Alerts firing tabs for more detail on those.

#### Service Monitoring

- https://www.cloudskillsboost.google/paths/14/course_templates/99/labs/533956

#### Module summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533957

In this module, you learned to explain why SLI, SLO and SLA are important, to define alerting policies and discuss alerting strategies. You also learnt to explain error budget, identify types of alerts and common uses for each and also use Cloud Monitoring to manage services.

#### Quiz - Alerting Policies

- https://www.cloudskillsboost.google/paths/14/course_templates/99/quizzes/533958

### Advanced Logging and Analysis

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533959

In this module, we examine some of Google Cloud's advanced logging and analysis capabilities. Specifically, in this module you learn to: Use Log Explorer features, Explain the features and benefits of log-based metrics, Define log sinks (inclusion filters) and exclusion filters, Explain how Big query can be used to analyze logs, Use Log Analytics on Google Cloud.

#### Cloud Logging overview and architecture

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533960

As we learned earlier, Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. In this section we will go through logging architecture and understand its use cases. Logs is one of the top most visited sections in Google Cloud console and one of most transitional, which indicated that it is an important component of many scenarios. End users need logs for troubleshooting and information gathering but don’t want to be overwhelmed with the data. Logs are the pulse of your workloads and application. Cloud Logging helps to: Gather data from various workloads:. This data is required to troubleshoot and understand the workload and application needs. Analyze large volumes of data: Tools like Error Reporting, Log Explorer, and Log Analytics let you focus from large sets of data. Route and store logs: Route your logs to the region or service of your choice for additional compliance or business benefits. Get Compliance Insights: Leverage audit and app logs for compliance patterns and issues. We will cover this in the Audit logs module in detail. Cloud Logging architecture consists of the following components: Log Collections: These are the places where log data originates. Log sources can be Google Cloud services, such as Compute Engine, App Engine, and Kubernetes Engine, or your own applications. Log Routing: The Log Router is responsible for routing log data to its destination. The Log Router uses a combination of inclusion filters and exclusion filters to determine which log data is routed to each destination. Log sinks: Log sinks are destinations where log data is stored. Cloud Logging supports a variety of log sinks, including: Cloud Logging log buckets: These are storage buckets that are specifically designed for storing log data. Pub/Sub topics: These topics can be used to route log data to other services, such as third-party logging solutions. BigQuery: This is a fully-managed, petabyte-scale analytics data warehouse that can be used to store and analyze log data. Cloud Storage buckets: Provides storage of log data in Cloud Storage. Log entries are stored as JSON files. Log Analysis: Cloud Logging provides several tools to analyze logs. Logs Explorer is optimized for troubleshooting use cases with features like log streaming, a log resource explorer and a histogram for visualization. Error Reporting help users react to critical application errors through automated error grouping and notifications. Logs-based metrics, dashboards and alerting provide other ways to understand and make logs actionable. Log Analytics feature expands the toolset to include ad hoc log analysis capabilities.

#### Log types and collection

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533961

Let us start with Log Collection and move our way forward to routing and storage and finally visualization. The Google Cloud platform logs visible to you in Cloud Logging vary, depending on which Google Cloud resources you're using in your Google Cloud project or organization. Let's explore the key log categories. Platform logs are logs written by Google Cloud services. These logs can help you debug and troubleshoot issues, and help you better understand the Google Cloud services you're using. For example, VPC Flow Logs record a sample of network flows sent from and received by VM instances. Component logs are similar to platform logs, but they are generated by Google-provided software components that run on your systems. For example, GKE provides software components that users can run on their own VM or in their own data center. Logs are generated from the user's GKE instances and sent to a user's Cloud project. GKE uses the logs or their metadata to provide user support. Security logs help you answer "who did what, where, and when." Cloud Audit Logs provide information about administrative activities and accesses within your Google Cloud resources. Access Transparency provides you with logs of actions taken by Google staff when accessing your Google Cloud content. User-written logs are logs written by custom applications and services. Typically, these logs are written to Cloud Logging by using one of the following methods: Ops Agent, Cloud Logging API, Cloud Logging client libraries. Multi-cloud logs and Hybrid-cloud logs refer to logs from other cloud providers like Microsoft Azure and logs from on-premises infrastructure. You can programmatically send application logs to Cloud Logging by using client libraries or by using one of the Logging agents. When you can't use them, or when you only want to experiment, you can write logs by using the gcloud logging write command or by sending HTTP commands to the Cloud Logging API endpoint entries.write. If you're using one of the agents, then your applications can use any established logging framework to emit logs. For example, in container environments like Google Kubernetes Engine or Container-Optimized OS, the agents automatically collect logs from stdout and stderr. On virtual machines (VMs), the agents collect logs from known file locations or logging services like the Windows Event Log, journald, or syslogd. Serverless compute services like Cloud Run and Cloud Run functions, include simple runtime logging by default. Logs written to stdout or stderr will appear automatically in the Google Cloud console.

#### Storing, routing and exporting the logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533962

Now that we understand the log collection, let's look at how logs can be routed and exported for long-term storage and analysis. What we call Cloud Logging is actually a collection of components exposed through a centralized logging API. Log Router: Entries are passed through the API and fed to Log Router. Log Router is optimized for processing streaming data, reliably buffering it, and sending it to any combination of log storage and sink (export) locations. By default, log entries are fed into one of the default logs storage buckets. Exclusion filters might be created to partially or totally prevent this behavior. Log sinks run in parallel with the default log flow and might be used to direct entries to external locations. Log storage: Locations might include additional Cloud Logging buckets, Cloud Storage, BigQuery, Pub/Sub, or external projects. Inclusion and exclusion filters can control exactly which logging entries end up at a particular destination, and which are ignored completely. For each Google Cloud project, Logging automatically creates two logs buckets: _Required and _Default, and corresponding log sinks with the same names. All logs generated in the project are stored in one of these two locations: _Required bucket holds Admin Activity audit logs, System Event audit logs, and Access Transparency logs, and retains them for 400 days. You aren't charged for the logs stored in _Required, and the retention period of the logs stored here cannot be modified. You cannot delete or modify this bucket. _Default bucket holds all other ingested logs in a Google Cloud project, except for the logs held in the _Required bucket. Standard Cloud Logging pricing applies to these logs. Log entries held in the _Default bucket are retained for 30 days, unless you apply custom retention rules. You can't delete this bucket, but you can disable the _Default log sink that routes logs to this bucket. The Logs Storage page displays a summary of statistics for the logs that your project is receiving, including: Current total volume: The amount of logs your project has received since the first date of the current month. Previous month volume: The amount of logs your project received in the last calendar month. Projected volume by EOM: The estimated amount of logs your project will receive by the end of the current month, based on current usage. You can view the total usage by resource type for the current total volume. The link opens Metrics Explorer, which lets you build charts for any metric collected by your project. For more information on using Metrics Explorer, refer to the documentation. Log Router sinks can be used to forward copies of some or all of your log entries to non-default locations. Use cases include storing logs for extended periods, querying logs with SQL, and access control. Here, you see we‚Äôve started creating a sink by generating a log query for a particular subset of entries. We will pass that subset to one of the available sink locations. There are several sink locations, depending on need: Cloud Logging bucket works well to help pre-separate log entries into a distinct log storage bucket. BigQuery dataset allows the SQL query power of BigQuery to be brought to bear on large and complex log entries. Cloud Storage bucket is a simple external Cloud Storage location, perhaps for long-term storage or processing with other systems. Pub/Sub topic can export log entries to message handling third-party applications or systems created with code and running somewhere like Dataflow or Cloud Run functions. Splunk is used to integrate logs into existing Splunk-based system. The Other project option is useful to help control access to a subset of log entries. The process for creating log sinks mimics that of creating log exclusions. It involves writing a query that selects the log entries you want to export in Logs Explorer, and choosing a destination of Cloud Storage, BigQuery, or Pub/Sub. The query and destination are held in an object called a sink. Sinks can be created in Google Cloud projects, organizations, folders, and billing accounts. Use Logs Explorer to build a query that selects the logs you want to exclude. Save the query to use when building the exclusion. Use the Log Explorer query to create an exclusion filter that filters the unwanted entries out of the sink. Give the exclusion a name and add the filter for log entries to exclude. It might be helpful to leave some representative events, depending on the exclusion. Create the exclusion and it will go into effect immediately. Use the Navigation menu to initiate editing of that entity. Take care here, because excluded log events will be lost forever. Over the next several slides, we will investigate some possible log export processing options. Here, for example, we are exporting through Pub/Sub, to Dataflow, to BigQuery. Dataflow is an excellent option if you're looking for real-time log processing at scale. In this example, the Dataflow job could react to real-time issues, while streaming the logs into BigQuery for longer-term analysis. Sink pipelines targeting Cloud Storage tend to work best when your needs align with Cloud Storage strengths. For example, long-term retention, reduced storage costs, and configurable object lifecycles. Cloud Storage features include automated storage class changes, auto-delete, and guaranteed retention. Here, we have an example organization that wants to integrate the logging data from Google Cloud, back into an on-premises Splunk instance. You can ingest logs into Splunk you can either stream logs using Pub/Sub to Splunk Dataflow or using the Splunk Add-on for Google Cloud. Pub/Sub is one of the options available for exporting to Splunk, or to other third-party System Information and Event Management (SIEM) software packages. A common logging need is centralized log aggregation for auditing, retention, or non-repudiation purposes. Aggregated sinks allow for easy exporting of logging entries without a one-to-one setup. The sink destination can be any of the destinations discussed up to now. There are three available Google Cloud Logging aggregation levels. We've discussed a project-level log sink. It exports all the logs for a specific project and a log filter can be specified in the sink definition to include or exclude certain log types. A folder-level log sink aggregates logs on the folder level and can include logs from children resources (subfolders, projects). And for a global view, an organization-level log sink can aggregate logs on the organization level and can also include logs from children resources (subfolders, projects). Security practitioners onboard Google Cloud logs for security analytics. By performing security analytics, you help your organization prevent, detect, and respond to threats like malware, phishing, ransomware, and poorly configured assets. One of the steps in security log analytics workflow is to create aggregate sinks and route those logs to a single destination depending on the choice of security analytics tool, such as Log Analytics, BigQuery, Chronicle, or a third-party security information and event management (SIEM) technology. Logs are aggregated from your organization, including any contained folders, projects, and billing accounts. There are a few naming conventions that apply to log entry fields: For log entry fields that are part of the LogEntry type, the corresponding BigQuery field names are precisely the same as the log entry fields. For any user-supplied fields, the letter case is normalized to lowercase, but the naming is otherwise preserved. For fields in structured payloads, as long as the @type specifier is not present, the letter case is normalized to lowercase, but naming is otherwise preserved. For information on structured payloads where the @type specifier is present, see the Payload fields with @type documentation. You can see some examples on the current slide. Here's a sample query over the Compute Engine logs. It retrieves log entries for multiple log types over multiple days. The query searches the last three days (today -2) of the syslog and apache-access logs. The query retrieves results for the single Compute Engine instance ID seen in the where clause. In this BigQuery example, we are looking for unsuccessful App Engine requests from the last month. Notice how the from clause is constructing the table data range. The status not equal to 200 is examining the HTTP status for anything that isn't 200. That is to say, anything that isn't a successful response.

#### Query and view logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533963

Once you have collected logs and routed to the right destination, now is the time to query and view logs. The Logs Explorer interface lets you retrieve logs, parse and analyze log data, and refine your query parameters. The Logs Explorer contains the following panes: Action toolbar to refine logs to projects or storage views, share a link and learn about logs explorer. Query pane is where you can build queries, view recently viewed and saved queries and a lot more. Results Toolbar can be used to quickly show or hide logs and histogram pane and create a log based metric or alert. Jump to now option helps query and view the current time results. Query results is the details of results with a summary and timestamp that helps troubleshoot further. Log fields pane is used to filter your options based on various factors such as a resource type, log name, project ID, etc,. Histogram is where the query result is visualized as histogram bars, where each bar is a time range and is color coded based on severity. Ultimately, it’s the query that selects the entries displayed by Logs Explorer. Queries may be created directly with the Logging Query Language (LQL), using the drop-down menus, the logs field explorer, or by clicking fields in the results themselves. Start with what you know about the entry you’re trying to find. If it belongs to a resource, a particular log file, or has a known severity, use the query builder drop-down menus. The query builder drop-down menu makes it easy to start narrowing your log choices. Resource: Lets you specify resource.type. You can select a single resource at a time to add to the Query builder. Entries use the logical operator AND. Log name: Lets you specify logName. You can select multiple log names at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. Severity: Lets you specify severity. You can select multiple severity levels at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. The next several slides are included for reference. Advanced queries support multiple comparison operators as seen here. The equal and not equal operators help filter values that match or not match a value assigned to a field name. These are useful when you search for a specific resource type or id that you want to evaluate. The numeric ordering operators are handy when searching for logs filtering a timestamp or duration. The colon operation helps check if a value exists. This is useful when you want to match a substring within a log entry field. To test if a missing or defaulted field exists without testing for a particular value in the field, use the :* comparison. If you’re looking for a specific set of log entries and have a rough idea when they would have been generated, start by narrowing to a specific time range. You can select one of the pre-created choices, set a custom range, or jump to a particular time. The Log fields panel offers a high-level summary of logs data and provides a more efficient way to refine a query. It shows the count of log entries, sorted by decreasing count, for the given log field. The log field counts correspond to the time range used by the Histogram panel. You can add fields from the Log fields panel to the Query builder to narrow down and refine a query by clicking a field. When a query is run, the log field counts are incrementally loaded as the log entries are progressively scanned. Once the query is complete, which is indicated by the completion of the blue progress bar, you see the total counts for all log fields. The histogram panel lets you visualize the distribution of logs over time. Visualization makes it easier to see trends in your logs data and troubleshoot problems. For example, the severity colors make it easy to spot an increasing number of errors even when the volume of requests is relatively constant. To analyze your log data, point to a bar in the Histogram panel and select Jump to time to drill into a narrower time range. A new query runs with that time-range restriction. Advanced queries support the AND, OR, and NOT Boolean expressions for joining queries. A couple of things to keep in mind include: Ensure to use the all caps for the operator name. The NOT operator has the highest precedence, followed by OR and AND in that order. The Boolean operators AND and OR are short-circuit operators. Here is a simple recipe for finding entries. When you’re trying to find log entries, as mentioned earlier, start with what you know: the log filename, resource name, even a bit of the contents of the logged message might work. Full text searches are slow, but they may be effective. For example, you might search for “/score called”. If possible, restrict text searches to an entry region, like jsonPayload:”/score called”, or even better, jsonPayload.message=”/score called”. You can use the built-in SEARCH function to find strings in your log data as shown on slide. SEARCH([query]) SEARCH([field], [query]) Both forms of the SEARCH function contain a query argument, which must be formatted as a string literal. In the first form, the entire log entry is searched. In the second form, you specify the field in the log entry to search. The Logging query language supports different ways that you can search your log data. When searching for a string, it is more efficient to use the SEARCH function than to perform a global search or a substring search. However, you can't use the SEARCH function to match non-text fields. Some tips on finding log entries quickly: Search for specific values of indexed fields, like the name of the log entry, resource type, and resource labels. Apply constraints on resource.type and resource.labels, resource.type = "gke_cluster" and resource.labels.namespace = "my-cool-namespace" These fields are preferentially indexed in our storage and can make a huge difference for query performance. As seen in the example, be specific on which logs you’re searching by referring to it or them by name. Limit the time range that you’re searching to reduce the log data that is being queried.

#### Using log-based metrics

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533964

Let's next focus about generating monitoring metrics from logging data. Logs-based metrics derive metric data from the content of log entries. For example, metrics can track the number of entries that contain specific messages or extract latency information that is reported in the logs. These metrics transform into time series data and use it in Cloud Monitoring Charts and Alerting Policies. There are two types of log-based metrics: System-defined log-based metrics, provided by Cloud Logging can be used by all Google Cloud projects. System-defined log-based metrics are calculated only from logs that have been ingested by Logging. If a log has been explicitly excluded from ingestion by Cloud Logging, it isn't included in these metrics. User-defined log-based metrics, created by you to track things in your Google Cloud project that are of particular interest to you. For example, you might create a log-based metric to count the number of log entries that match a given filter. Log-based metrics are suitable when you want to do any of the following: Count the occurrences of a message, like a warning or error, in your logs and receive a notification when the number of occurrences crosses a threshold. Observe trends in your data, like latency values in your logs, and receive a notification if the values change in an unacceptable way. Create charts to display the numeric data extracted from your logs. A refresher of the key IAM roles that relate to logging and monitoring. First, on the logging side: Logs Configuration Writers can list, create, get, update, and delete log-based metrics. Logs Viewers can view existing metrics. On the monitoring side, Monitoring Viewers can read the time series in log-based metrics. And finally, Logging Admins, Editors, and Owners are all broad-level roles that can create log-based metrics. There are three types of log-based metrics: counter or distribution. All predefined system log-based metrics are the counter type, but user-defined metrics can be either counter, distribution or boolean types. Counter metrics count the number of log entries matching an advanced logs query. So, if we simply wanted to know how many of our "/score called" entries were generated, we could create a counter. Distribution metrics record the statistical distribution of the extracted log values in histogram buckets. The extracted values are not recorded individually. Their distribution across the configured buckets is recorded, along with the count, mean, and sum of squared deviations of the values. Boolean metrics record where a log entry matches a specified filter System-defined log-based metrics apply at the Google Cloud project level. These metrics are calculated by the Log Router and apply to logs only in the Google Cloud project in which they're received. User-defined log-based metrics can apply at either the Google Cloud project level or at the level of a specific log bucket: Project-level metrics are calculated like system-defined log-based metrics; these user-defined log-based metrics apply to logs only in the Google Cloud project in which they're received. Bucket-scoped metrics apply to logs in the log bucket in which they're received, regardless of the Google Cloud project in which the log entries originated. With bucket-scoped log-based metrics, you can create log-based metrics that can evaluate logs in the following cases: Logs that are routed from one project to a bucket in another project. Logs that are routed into a bucket through an aggregated sink. Before we create a log-based metric, let's generate some logging entries. Here we see a basic NodeJS app built with the simple and lightweight Express web server. The app is run as a managed container on the Cloud Run service. The code watches for a request to come into the server on the '/score' path. When a /score request arrives, the code generates a random score between 1 to 100, and it then creates a log entry. Earlier code, not shown on this slide, created a unique identifier for the container serving this request in containerID and a random value called funFactor. The log entry contains the text "/score called‚Äù, the random score, the container ID, and the fun factor. Lastly, a basic message, also containing the score, is sent back to the browser. Use the Query builder to access project logs. In the list of entries, we've located one of the "/score called" entries. Now we can filter to select those entries by clicking "/score called", and selecting Show matching entries. Imagine we've generated some load on our Cloud Run sample application, and we'd like to use the log events to generate a log-based metric. As defined earlier there are two fundamental log-based metric types, System log-based metrics and User-defined log-based metrics. The latter is what we are creating now. Note the Create Metric button at the top of the interface. This is an example of a basic flow for creating log-based metrics: You start by finding the log with the requisite data. Then you filter it to the required entries. Create a metric. Pick your metric type (Counter or Distribution). If Distribution, then set configurations. And finally, add labels as needed. Like many cloud resources, labels can be applied to log-based metrics. Their prime use is to help with group-by and filtering tasks in Cloud Monitoring. Labels allow log-based metrics to contain multiple time series‚Äîone for each label value. All log-based metrics come with some default labels and you can create additional user-defined labels in both counter-type and distribution-type metrics by specifying extractor expressions. An extractor expression tells Cloud Logging how to extract the value of the label from log entries. You can specify the label's value as either of the following: The entire contents of a named field in the LogEntry object. A part of a named field that matches a regular expression (regexp). You can extract labels from the LogEntry built-in fields, such as httpRequest.status, or from one of the payload fields, textPayload, jsonPayload, or protoPayload. Label with care. A metric can support up to ten user-defined labels, and once created, a metric cannot be removed. Also, each log-based metric is limited to about 30,000 active time series. Each label can grow the time series count significantly. For example, if your log entries come from 100 resources, such as VM instances, and you define a label with 20 possible values, then you can have up to 2,000 time series for your metric. User-defined labels can be created when creating a log-based metric. The label form requires: Name is an identifier which will be used to label in Monitoring. Description describes the label. Try to be as specific as possible. Choose String, Boolean, or Integer for label type. For Field name, enter the name of the log entry field that contains the value of the label. This field supports autocomplete. Extraction regular expression: If the value of your label consists of the field's entire contents, then you can leave this field empty. Otherwise, specify a regular expression (regexp) that extracts the label value from the field value.

#### Log analytics

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533965

Next let us take a look at one of the new feature in Cloud Logging, Log Analytics. Log Analytics gives you the analytical power of BigQuery within the Cloud Logging console and provides you with a new user interface that's optimized for analyzing your logs. When you create a bucket and activate analytics on it, Cloud Logging makes the logs data available in both the new Log Analytics interface and BigQuery; you don't have to route and manage a separate copy of the data in BigQuery. You can still query and examine the data as usual in Cloud Logging with the Logging query language. Logs are written to the Logging API via client libraries, stdout/fluentbit agent or directly via API,. Logs Explorer helps with troubleshooting and getting to the root cause with search, filter, histogram and suggested search. Log Router routes logs to the Logging Sink for the Logs Bucket. Log Analytics, analyze application performance, data access and network access patterns. Log Analytics pipeline maps logs to BigQuery tables (JSON, STRING, INT64, RECORD, etc..) and writes to BigQuery. Use the same logs data in Log Analytics directly from BigQuery to report on aggregated application and business data found in logs. The logs data in your analytics-enabled buckets is different than logs routed to BigQuery via traditional export in the following ways: Log data in BigQuery is managed by Cloud Logging. BigQuery ingestion and storage costs are included in your Logging costs. Data residency and lifecycle are managed by Cloud Logging. You can query your logs on Log Analytics-enabled buckets directly in Cloud Logging via the new Log Analytics UI. The Log Analytics UI is optimized for viewing unstructured log data. You can also access your logs data in BigQuery using a read-only view if you want to combine your logs data with other data in BigQuery. You can turn on or off access to your analytics-enabled buckets in BigQuery by turning on or off the option that connects the logs to BigQuery. When the option is enabled, you can query the logs directly from BigQuery, including joining your logs data with other BigQuery datasets. To create an analytics-enabled bucket by using the console: Navigate to Logs Storage. Click Create log bucket. Select Upgrade to use Log Analytics. Note: Upgrading a bucket to use Log Analytics is permanent. You can't downgrade the log bucket to remove the use of Log Analytics. Log Analytics is useful in multiple aspects. Let's look at different fields' perspectives, starting with DevOps. DevOps: For a DevOps specialist it is important to quickly troubleshoot an issue that requires to reduce Mean Time to Repair (MTTR). Log Analytics includes capabilities to count the top requests grouped by response type and severity, which allows engineers to diagnose the issues. Security A security personal is interested in finding all the audit logs associated with a specific use over the past month. Log Analytics help better investigate the security -related attacks with queries over large volumes of security data. For more information, refer to the documentation. IT or Network Operations IT or Network Operations is interested in identifying network issues for GKE instances that are using VPC and firewall rules. Log Analytics in this case provides better network insights and management through advanced log aggregation capabilities. For more information, refer to the documentation.

#### Log Analytics on Google Cloud

- https://www.cloudskillsboost.google/paths/14/course_templates/99/labs/533966

#### Module Summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533967

Specifically, in this module you learn to: Use Log Explorer features, Explain the features and benefits of log-based metrics, Define log sinks, inclusion filters and exclusion filters, Explain how BigQuery can be used to analyze logs, Use Log Analytics on Google Cloud.

#### Quiz - Advanced Logging and Analysis

- https://www.cloudskillsboost.google/paths/14/course_templates/99/quizzes/533968

### Working with Audit Logs

#### Module Overview

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533969

In this module, we investigate the core Cloud Audit Logs that Google Cloud collects. In this module, you’ll learn to: Explain Cloud Audit Logs. List and explain different audit logs. Explain the features and functionalities of the different audit logs. List the best practices to implement audit logs.

#### Cloud Audit Logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533970

We start with an overview of what audit logs do for us. Then we move to using the optional Data Access audit logs, explore the format of audit log entries, and wrap up with some logging best practices. In terms of sheer volume of useful information, probably the most important group of logs in Google Cloud are the Cloud Audit Logs. Cloud Audit Logs helps answer the question, "Who did what, where, and when? ” It maintains four audit logs for each Google Cloud project, folder, and organization: Admin Activity audit logs System Event audit logs Data Access audit logs Policy Denied audit logs All Google Cloud services will eventually provide audit logs. For now, see the Google services with audit logs documentation for coverage details. Admin Activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions. They are always on, are retained for 400 days, and are available at no charge. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. System Event audit logs contain log entries for Google Cloud administrative actions that modify the configuration of resources. System Event audit logs are generated by Google systems; they are not driven by direct user action. They are always enabled, free, and retained for 400 days. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. Data Access audit logs contain API calls that read the configuration or metadata of resources. Also, user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs don’t record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users). Data Access audit logs also don’t record the data-access operations on resources that can be accessed without logging into Google Cloud. They are disabled by default (except for BigQuery), and when enabled, the default retention is 30 days. To view these logs, you must have the IAM roles Logging/Private Logs Viewer or Project/Owner. When a security policy is violated, policy denied audit logs records when access to a user or service account is denied by Google Cloud service. Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs. However, you can use exclusion filters to prevent Policy Denied audit logs from being ingested and stored in Cloud Logging. To view and filter audit logs: Navigate to Logs Explorer Filter by using the Log name drop-down menu. Note: typing cloudaudit into the filter box is frequently quicker than scrolling. If one of the four audit logs is missing, that simply means it doesn’t currently have any entries. The example here filters the logs by a project and you can select the log entries you would like to audit. You can even use the query builder to filter audit logs. This query is auto populated in the query section when using the UI. For details check out the documentation page. Whether it's a hardware support engineer, or a rep working on a ticket, having dedicated experts manage parts of the infrastructure is a key benefit of operating in Google Cloud. Very similar to Cloud Audit logs, Access Transparency logs help by providing logs of accesses to your data by human Googlers (as opposed to automated systems). Enterprises with appropriate support packages can enable the logs, and receive the log events in near-real time. The log events are surfaced through the APIs, Cloud Logging, and Security Command Center. Access Transparency logs give you different information than Cloud Audit Logs. Cloud Audit Logs record the actions that members of your Google Cloud organization have taken in your Google Cloud resources, whereas Access Transparency logs record the actions taken by Google personnel.

#### Data Access audit logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533971

Let’s continue by looking at Data Access audit logs. Data Access audit logs can be enabled at various levels in the resource hierarchy. These levels include: Organization Folder Project, Resources, and Billing accounts You can also exempt principals from recording data access logs. The final configuration of Data Access audit logs is the union of the configurations. For example, at a project level, you can enable logs for a Google Cloud service. But you can't disable logs for a Google Cloud service that is enabled in a parent organization or folder. The added logging does add to the cost, currently: $0.50 per gigabyte for ingestion. Data Access audit logs are disabled by default, for everything but BigQuery. They may be enabled and configured at the organization, folder, project, or service level. You can control what type of information is kept in the audit logs. There are three types of Data Access audit logs information: Admin-read records operations that read metadata or configuration information. For example, you looked at the configurations for your bucket. Data-read records operations that read user-provided data. For example, you listed files and then downloaded one from Cloud Storage. Data-write records operations that write user-provided data. For example, you created a new Cloud Storage file. You can exempt specific users or groups from having their data accesses recorded. This functionality is useful when you want to reduce the cost and noise associated with the volume of logs that are not of your interest. Data Access audit logs can be of high volume, so cost associated is directly proportional to the volume of data logs. You can also use the Google Cloud CLI or the API to enable Data Access audit logs. If you're using the gcloud CLI frequently, the easiest way is to get the current IAM policies, as seen in step 1, and write them to a file. Then you can edit the /tmp/policy.yaml file to add or edit the auditLogConfigs. You can also add the log details per service, like this example is enabling logging for Cloud Run. You can even enable logging on all services. Then, as seen in step 3, you would set that as the new IAM policy.

#### Audit logs entry format

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533972

Now that you learned to enable the logs you need, let’s examine the logging entries themselves. Every audit log entry in Cloud Logging is an object of type LogEntry. What distinguishes an audit log entry from other log entries is the protoPayload field, which contains an AuditLog object that stores the audit logging data. Note the log name, which tells us that we’re looking at an example from Data Access audit logs. Identify the principal generating log by looking at the principalEmail. The operation field only exists for a large or long-running audit log entries. Google has a standard List of official service names. You can use this list as a handy reference. On this slide, you can tell we’re looking at a query that was run in BigQuery. If you expanded the serviceData field, you could actually see the query itself. So, when someone at your organization runs that unexpected, $40,000 query, you can see who ran it and what the query was.

#### Best practices

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533973

Let’s go over a few best practices before finishing this module. Like anything in the cloud, start by planning first. Spend time and create a solid plan for Data Access audit logs. Think organization, folder, then project. Like most organizations, some of your projects will be very specialized, but usually, they do break down into common organizational types. Then, create a test project and experiment to see if the logging works the way you expect. Then roll out the plan, and don't forget automation (Infrastructure as Code). Remember that Data Access audit logs can be enabled as high as the organization level. The advantage would be detailed information on exactly who accessed, edited, and deleted what, and when. The disadvantage is that Data Access logs can grow to be large, and are billed per gigabyte. This also results in higher queries per second based on the number of data access requests. Infrastructure as Code (IaC) is essentially the process of automating the creation and modifications to your infrastructure using a platform. The platform supports configuration files, which can be put through a CI/CD (Continuous integration and continuous deployment) pipeline, like with code. Terraform is an open source package from HashiCorp or paid for enterprise version. It isn't hosted directly in Google Cloud, though it’s installed by default in Cloud Shell. State management is a decision point for your organization. It can be remote or local. Remote options include using Cloud Storage or Terraform Cloud. Local storage involves setting up something local to your organization, or using the pay-to-use HashiCorp Enterprise service. Audit logs also inform you about the resources provisioned using an IaC tool. It is really useful if you want to control log sink filters at the project level. With terraform you can set default include/exclude filters and have them applied to every project. To manage your Google Cloud organization's logs, you can aggregate them from across your organization into a single Cloud Logging bucket. It is recommended to create user-defined buckets to centralize or subdivide your log storage. Based on compliance and usage requirements, customize your logs storage by choosing where your logs are stored and defining the data retention period. Some organization might have latency, compliance, and availability requirements in specific regions. Configure a default storage location to automatically apply a region in which buckets are created for log data. By default, Cloud Logging encrypts customer content stored at rest. Your organization might have advanced encryption requirements that the default encryption at rest doesn't provide. To meet your organization's requirements, instead of Google managing the key encryption keys that protect your data, configure customer-managed encryption keys (CMEK) to control and manage your own encryption. We've discussed the options and benefits of exporting logs. Again, make this part of your plan. Start by deciding what, if anything, you will export from Aggregated Exports at the organization level. Next, decide what options you will use, project by project, folder by folder, and so on. Then, carefully consider your filters—both what they leave in, and what they leave out. Filters apply to all logs, not just to exports. Lastly, carefully consider what, if anything, you will fully exclude from logging. Remember that excluded entries will be gone forever. Side-channel leakage of data through logs is a common issue. You need to be careful about who gets which kind of access and to which logs. Remember some of the discussions earlier in this course on monitoring metrics scope? And how to monitor a current project? That's where your security starts. Are you monitoring project by project, or are you selectively grouping work projects into higher-level monitored projects? Use appropriate IAM controls on both Google Cloud-based and exported logs, and only allow minimal access required to get the job done. Especially scrutinize the Data Access audit log permissions, because they often contain Personally Identifiable Information (PII). Log buckets store logs, including audit logs. Log views control access to logs in a log bucket. Custom log views can be created to control access to logs from specific projects or users. This helps protect sensitive data and ensures only authorized users have access. Lastly, a few access scenarios, starting with operational monitoring. Let’s explore your high-level teams and assignments. By job, a CTO will have the organization admin role, so they can assign permissions to the security team and service accounts. The CTO can then give the security team logging.viewer so they can view the Admin Activity audit logs. Also, logging.privateLogViewer, so they can view the Data Access audit logs. The view permissions are assigned at the organization level, so they are global. Access control to data exported to Cloud Storage or BigQuery will be secured selectively with IAM. You might also want to explore Sensitive Data Protection to redact the PII. Data in the Data Access audit logs is deemed as personally identifiable information (PII) for this organization. Integrating the application with Sensitive Data Protection gives the ability to redact sensitive PII data when viewing Data Access logs whether they are in the Data Access audit logs or from the historical archive in Cloud Storage. Moving on to development teams. The security team is unchanged from the last slide. They already have logging.viewer, and logging.privateLogViewer from the global assignment. The development team might get logging.viewer at the folder level so they can see the Admin Activity audit logs for the projects within their development control. They probably also need logging.privateLogViewer at the dev folder so they can see the Data Access audit logs. Limit data they test with though, so they aren't viewing actual customer information. Again, use Cloud Storage or BigQuery IAM to control access to exported logs. Prebuilding dashboards might also be a good option. For external auditors, provide pre-created dashboards where possible. If they need broad access, you can make them with Logs Viewer role at the organization level. For BigQuery, they could be BigQuery Data Viewer on the exported dataset. For Cloud Storage, again, you could use IAM, but also remember the temporary access URLs that Cloud Storage supports.

#### Cloud Audit Logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/labs/533974

#### Module Summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533975

In this module, you learned to: Explain Cloud Audit Logs. List and explain different audit logs. Explain the features and functionalities of the different audit logs. List the Best Practices to implement audit logs.

#### Quiz - Working with Audit Logs

- https://www.cloudskillsboost.google/paths/14/course_templates/99/quizzes/533976

### Course Summary

#### Course 1 Summary

- https://www.cloudskillsboost.google/paths/14/course_templates/99/video/533977

This brings us to the end of Logging and Monitoring in Google Cloud course. In this course we learned the purpose and capabilities of Google Cloud Observability, how to implement monitoring for multiple cloud projects, create alerting policies, uptime checks and alerts to identify and resolve problems quickly, use Cloud Logging to collect logs and export for further analysis. If you’re interested in learning more about application performance management, please go to the next course ”Observability in Google Cloud”. See you next time!

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/99/documents/533978

### Your Next Steps

## 10: Implementing Cloud Load Balancing for Compute Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/648

### Implementing Cloud Load Balancing for Compute Engine

#### Set Up Network Load Balancers

- https://www.cloudskillsboost.google/paths/14/course_templates/648/labs/567898

#### Set Up Application Load Balancers

- https://www.cloudskillsboost.google/paths/14/course_templates/648/labs/567899

#### Using an Internal Application Load Balancer

- https://www.cloudskillsboost.google/paths/14/course_templates/648/labs/567900

#### Implement Load Balancing on Compute Engine: Challenge Lab

- https://www.cloudskillsboost.google/paths/14/course_templates/648/labs/567901

### Your Next Steps

## 11: Configure Google Kubernetes Engine Networking

- https://www.cloudskillsboost.google/paths/14/course_templates/660

### GKE Networking Overview

#### Introduction

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551858

#### Clusters and services

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551859

#### Creating Services and Ingress Resources

- https://www.cloudskillsboost.google/paths/14/course_templates/660/labs/551860

#### Pod DNS

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551861

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/660/quizzes/551862

### Anthos Service Mesh

#### Introduction to Anthos Service Mesh

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551863

#### Installing Cloud Service Mesh on Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/660/labs/551864

#### Lab Review

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551865

#### Architecture

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551866

#### Installation

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551867

#### Life of a request in the mesh

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551868

#### Mesh telemetry and instrumentation

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551869

#### Anthos Service Mesh dashboards

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551870

#### Anthos Service Mesh pricing and support

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551871

#### AHYBRID041 Observing Anthos Services

- https://www.cloudskillsboost.google/paths/14/course_templates/660/labs/551872

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/660/quizzes/551873

### Pod Networking

#### Controlling pod access

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551874

#### How to Use a Network Policy on Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/14/course_templates/660/labs/551875

#### Masquerading IP addresses

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551876

#### Discontiguous multi-pod CIDR

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551877

#### Configuring Privately Used Public IPs (PUPIs) with GKE

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551878

#### Cloud NAT and GKE

- https://www.cloudskillsboost.google/paths/14/course_templates/660/documents/551879

#### Quiz

- https://www.cloudskillsboost.google/paths/14/course_templates/660/quizzes/551880

### Your Next Steps

## 12: Build a Secure Google Cloud Network

- https://www.cloudskillsboost.google/paths/14/course_templates/654

### Build a Secure Google Cloud Network

#### Securing Virtual Machines using Chrome Enterprise Premium

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551821

#### Multiple VPC Networks

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551822

#### VPC Networks - Controlling Access

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551823

#### Application Load Balancer with Cloud Armor

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551824

#### Create an Internal Load Balancer

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551825

#### Build a Secure Google Cloud Network: Challenge Lab

- https://www.cloudskillsboost.google/paths/14/course_templates/654/labs/551826

### Your Next Steps

## 13: Connecting Cloud Networks with NCC

- https://www.cloudskillsboost.google/paths/14/course_templates/1364

### Connecting Cloud Networks with NCC

#### Establish Site to Site Connectivity with HA-VPN using NCC

- https://www.cloudskillsboost.google/paths/14/course_templates/1364/labs/555695

#### Establish VPC to VPC Connectivity using NCC

- https://www.cloudskillsboost.google/paths/14/course_templates/1364/labs/555696

#### Establish Hybrid Network Connectivity with NCC

- https://www.cloudskillsboost.google/paths/14/course_templates/1364/labs/555697

#### Connecting Cloud Networks with NCC: Challenge Lab

- https://www.cloudskillsboost.google/paths/14/course_templates/1364/labs/555698

### Your Next Steps


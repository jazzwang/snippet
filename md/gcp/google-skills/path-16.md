# Data Engineer Learning Path

- https://www.cloudskillsboost.google/paths/16

[TOC]

## 02: Preparing for your Professional Data Engineer Journey

- https://www.cloudskillsboost.google/paths/16/course_templates/72

### Introduction to the Professional Data Engineer (PDE) Certification

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517249

Are you ready to start preparing for the Professional Data Engineer certification exam? In this course, you will assess your exam readiness, and form a personalized workbook that will guide you through your certification journey. Hi, I’m Ajay, a Technical Curriculum Developer at Google Cloud. I’ll introduce you to the Data Engineer certification, and how to prepare for it. This content is designed for experienced Data Engineers that are ready to become Google Cloud certified. Before taking this course, you should have experience designing and managing data solutions using Google Cloud. In this course, you’ll be exposed to and engage with exam topics through a series of lectures and quizzes. Upon completion of this course you’ll understand the domains covered on the exam, And you’ll be equipped to identify gaps in your knowledge and skills for each domain. Enroll in this course to prepare to become a certified Google Cloud Data Engineer.

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517250

Welcome to Preparing for Your Professional Data Engineer Journey. In this course, you learn about the skills covered on the Professional Data Engineer certification exam. Modules 1 through 5 each point to one section of the exam guide. In Module 6, you prepare for the next steps in your journey and create a study plan. However, it’s important to clarify that this course by itself will not prepare you to take the certification exam. This course is not a “cram session.” The exam is purposefully calibrated to test your ability to apply the knowledge required of a Professional Data Engineer, not merely repeat it. Cram sessions have minimal impact on your ability to pass the exam. Instead, the goal of this course is to help you better structure your preparation time for the exam. You learn about the scope of each exam section and assess your current knowledge and skills using diagnostic questions. You then review where to find additional tools and resources to include in your study plan. In this introductory module, you learn about the role of a Professional Data Engineer. We discuss the types of resources available to support your study, and how to use the workbook to create your study plan.

#### Introduction to the Professional Data Engineer role

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517251

So… what exactly is the role of a Professional Data Engineer? Let’s review the job role description: A Professional Data Engineer makes data usable and valuable for others by collecting, transforming, and publishing data. This individual evaluates and selects products and services to meet business and regulatory requirements. A Professional Data Engineer creates and manages robust data processing systems. This includes the ability to design, build, deploy, monitor, maintain, and secure data processing workloads. For more information, visit the Professional Data Engineer Certification page. In this course, you put yourself in the role of a Professional Data Engineer working for Cymbal Retail, a fictional company. Cymbal Retail is amongst the largest B2C retailers in the world serving customers in 52 countries and territories. The founders started with brick and mortar stores and had the foresight to continuously invest in technologies over many decades. Cymbal Retail has seen significant changes in purchasing behavior, with accelerated growth in online purchases in recent years even as they continue to grow their brick and mortar presence. Let’s explore some of what your role will involve at Cymbal Retail. Cymbal Retail has multiple data centers of their own across the world. Cymbal Retail is in the process of transitioning these data centers to being entirely on Google Cloud, with a significant portion of their data and applications already moved. When migrating and operating, as a Professional Data Engineer you will need to ensure that Cymbal Retail complies with all local regulations. You will also need knowledge of security around data and the tools available for compliance. Customer delight has been the guiding purpose for Cymbal Retail through generations. The company has made multiple acquisitions across the world to serve customers with agility. As a Professional Data Engineer, you must be able to integrate products and solutions available within Google Cloud with technologies employed by the acquired company. You have four key technology areas of focus that drive your short term technology strategy: rapidity in the entire supply chain, accuracy of analytics and predictions, centralized control and security, and controlled costs. Throughout every decision, the data engineering team has to find the right balance to meet the business needs. Cymbal Retail’s management wants to be on top of all activities across the organization. The management team requires up to date information at their fingertips in easily understandable visuals. They also want the rest of the organization to employ data in their own decision making. Analysts should have the tools to ask questions of the data. It is one of your responsibilities as a Professional Data Engineer to make data available to all parts of the organization that need it. Data should be available quickly and in easily consumable ways. As you continue through this course, you will continue to explore the role of a Professional Data Engineer at Cymbal Retail. We use this scenario to illustrate the types of considerations and tasks that correspond to each section of the exam guide. Cymbal Retail’s business needs also provide context for many of the diagnostic questions you encounter along the way, and highlight areas that you should spend more time learning about.

#### Certification value and benefits

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517252

Why become a Google Cloud Certified Professional Data Engineer? Certification value has skyrocketed. Becoming Google certified gives you industry recognition. It validates your technical expertise and can be the starting point to for the next phase of your career. You may be curious about what differentiates a “professional” cloud certification from an “associate” level one. The professional level certification expects the exam taker to know how to evaluate case studies and design solutions to meet business requirements—in addition to knowing about technical requirements—for customer implementation.

#### Certification process

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517253

As you explore the role of the Professional Data Engineer at Cymbal Healthcare in this course, you also explore different sections of the exam guide which forms the basis for the certification exam. In the following modules, you take diagnostic questions to assess your knowledge of each section of the exam guide. The exam guide for the Professional Data Engineer is divided into five sections, each containing one or more objectives. We focus on where you can find resources at the section objective level. You can find the exam guide on the certification page at cloud.google.com/learn/certification/guides/data-engineer. The certification page will always have the latest available version of the exam guide. It's important to note that there are separate teams developing the Professional Data Engineer exam questions and developing the courses and exam preparation materials. The course developers and instructors don’t know what questions will be on your certification exam. The goal of the course is to determine what you know and what you don’t know to help you prepare a study plan and get ready for the Professional Data Engineer job role and exam. Later, when you take the exam, you will demonstrate whether you have the skills and knowledge required to earn the certification. Throughout this course, you are pointed to specific resources and documentation that can help you fill the gaps you identify through the diagnostic questions. Let’s go over the types of resources you may want to include in your study plan. Google provides resources to help you develop your skills and experience with Google Cloud products and services. The learning path for this certification includes online courses, online practice labs, and practice questions. The courses recommended for the Professional Data Engineer certification can be taken on demand or as instructor-led courses. The instructor-led course Data Engineering on Google Cloud is equivalent to the following series of on demand courses: Modernizing Data Lakes and Data Warehouses with Google Cloud; Building Batch Data Pipelines on Google Cloud; Building Resilient Streaming Analytics Systems on Google Cloud; and Smart Analytics, Machine Learning, and AI on Google Cloud. The instructor-led course Serverless Data Processing with Dataflow is available as an on demand 3-course series: Serverless Data Processing with Dataflow: Foundations; Serverless Data Processing with Dataflow: Develop Pipelines; and Serverless Data Processing with Dataflow: Operations. You learn more about how these courses relate to the sections of the exam guide as you complete the modules in this course. The skill badges provide hands-on experience working in Google Cloud. Skill badges are learning paths made up of labs that give you practice with Google Cloud services or solutions. Pass the challenge lab and you will receive a shareable credential that recognizes your ability to solve real-world problems with your cloud knowledge. As we review the diagnostic questions in this course, you also get recommendations for skill badges to include in your study plan. You can access the Professional Data Engineer Learning Path at www.cloudskillsboost.google/paths/16. Sample questions are another resource that you can use to prepare. The diagnostic questions in this course are designed to help you identify your knowledge gaps. On the certification page, Google provides a set of sample questions that can help you familiarize yourself with the format of the exam questions. Once you complete the question set, you will receive feedback describing the rationale for the correct answers. The sample questions provide a good opportunity to practice taking the type of scenario-based, application-level questions on the exam. The exam questions present you with a scenario, explains the goal or what you are trying to achieve, and asks you what you would do in that situation. Google also supplies official public documentation for its products and services. In each of the following modules, you learn about specific documentation resources to help you study that section in preparation for the exam.

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517254

One of the primary goals for this course is to help you devise a study strategy that focuses on the areas you need to work on. Let’s quickly explore how the course is set up. The course - and your course workbook - focuses on each section of the exam guide in turn. To help you craft a study strategy, you take diagnostic questions as part of each module. Many of these questions relate to our Cymbal Retail scenario and ask you to apply concepts you will need to be familiar with as an Professional Data Engineer. You can take these questions using an on-demand quiz or using your workbook. Keep in mind that these diagnostic questions are meant to help you identify gaps in your knowledge, but they don’t represent all possible topics on the exam. Remember, we don’t expect that you answer all these questions correctly right now. This is meant to be a course that you take toward the beginning of your preparation journey, and many of you may not be experts yet. If there are any terms or concepts that are confusing, please ask your instructor. After you individually answer the diagnostic questions in each module, you review the questions and the correct answers to identify what you need to study and where you can find more information. As you review the questions, there are hints with bolded text as to what is important in the question and how you might evaluate the responses. Remember these tips for multiple choice questions: Read the question stem carefully. Make sure you understand exactly what the question is asking. Try to anticipate the correct answer before looking at the options. You should be able to come up with the correct answer just from reading the question stem. You may find that more than one answer may be possible on multiple choice tests. Take questions at face value. If certain details are omitted, then they are unlikely to contribute to the selection of the best answer. Pay attention to qualifiers ("usually", "all", "never", "none") and key words ("the best", "the least", "except"). We review select questions related to each section objective. As we cover each objective, you learn more about where the key concepts appear in Google Cloud documentation, specific courses and modules, or specific skill badges. At the end of each section objective, there is a list of related resources. Mark or highlight the specific resources you need in your study plan. In the final part of your workbook there is a template to help you identify weekly goals and study activities. We discuss more about putting together weekly goals at the end of this course. Now that you know about the overall setup of this course and how to use the workbook, let’s get started by exploring section 1 of the exam guide.

#### Course Workbook

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517255

### Designing Data Processing Systems

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517256

Welcome to Module 1: Designing Data Processing Systems In this module, you’ll explore considerations for designing data processing systems, which correspond to section 1 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Migrating data from private data centers to Google Cloud for Cymbal Retail

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517257

Let’s begin by exploring the role of a Professional Data Engineer in migrating Cymbal Retail’s data from its private data centers to Google Cloud. Let’s look at the various challenges a Professional Data Engineer will face in accomplishing this task. Cymbal Retail’s existing private data centers are going to be decommissioned in the next two years. You need to help migrate existing data and data processing systems to Google Cloud. Similar migrations have to also be conducted in acquired companies with their own private data centers. As a Professional Data Engineer, your role involves ensuring that there is a uniform approach to managing and securing the data. Controlling access at a granular level based on employee role is critical to complying with regional laws and industry regulations. You need to understand and deploy options to control the security of data via encryption. Whether during migration or in the course of regular operations, certain data has to be captured and stored, but never revealed to other parts of the organization, for example, analysts. You need to employ tools to redact information, and you need be able to extend these tools as required to meet future requirements. You will be required to move data reliably from external sources into Google Cloud. Sometimes, the data movement might be done over many days. In other cases, data needs to be quickly moved as and when it arrives in other data sources for up to date analytics. As data is moved into Google Cloud, it will also need to be assessed for quality and cataloged for easy discoverability by consumers of the data. Cymbal Retail’s data is used simultaneously by different divisions to manage their business—finance, marketing, purchasing, sales, compliance, legal, store operations, and more. However, all of these groups do not need access to all the data. You should ensure fine-grained control on the data which is strictly on a need-to-know basis.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517258

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

#### Diagnostic Questions Practice Demo

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517259

The diagnostic questions in this course can be challenging, and some of them have several layers or decisions to make. Let’s examine some approaches you can use when analyzing complicated questions through a detailed discussion of a few diagnostic questions from this first module. Let’s start with question 3. First, consider the question stem. We can break the stem up into multiple parts. The first part of the stem is the scenario, which gives you context for the question: “You are migrating on-premises data to a data warehouse on Google Cloud. This data will be made available to business analysts. Local regulations require that customer information including credit card numbers, phone numbers, and email IDs be captured, but not used in analysis.” In this scenario you learn some key information: Your data includes personally identifiable information (PII), but it cannot be used in analysis. Next, you can identify the goal statement, or what it is you need to accomplish: “You need to use a reliable, recommended solution to redact the sensitive data.” From this goal, you know to look for options that are reliable (accurate and consistent in performance) and follow Google-recommended practices. So, what should you do? Start by noticing patterns in the responses. Two options, A and D, have you use the Cloud Data Loss Prevention (DLP) API. Option B has you manually delete columns, and Option C has you create a regular expression to delete information. You can eliminate option B right away - it is not reliable. Columns might contain sensitive data, even if it is not indicated by the title. For example, there could be a comments column with sensitive private information. You can also eliminate option C. Determining the regular expressions that match different data types can be more tedious and less accurate than using Cloud DLP. This leaves you with the two options using Cloud DLP, and you need to determine the correct usage. Look at the differences between them. In option A, you identify and redact data that matches certain infoTypes. In option B, you perform date shifting of entries. You can eliminate option D, because date shifting the entries does not redact the personal identifiable information (PII) like credit card numbers, phone numbers, and email IDs. This leaves you with option A. Cloud Data Loss Prevention helps you discover, classify, and protect your most sensitive data. There are predefined infoTypes that you can employ to identify and redact specific data types. Now, let’s examine question 4. First, let’s break the stem up into multiple parts. It begins with the scenario: “Your data and applications reside in multiple geographies on Google Cloud. Some regional laws require you to hold your own keys outside of the cloud provider environment, whereas other laws are less restrictive and allow storing keys with the same provider who stores the data.” There is a key point here - to comply with all the laws of some regions your organization operates in, you need to hold your own keys outside of the cloud provider environment. Next, you can identify the goal: you need a solution that can centrally manage all your keys. So, what should you do? In this question, each of the responses has you choose a different option. You can eliminate one of the responses right away… … because confidential computing does not affect the storage of data. Of the remaining responses, all three use services to help manage your keys. You need to determine where the keys will be stored. Options B and C store keys on Google Cloud. Option D stores keys on an external key management partner. You can eliminate the options that store keys on Google Cloud, because they won’t meet the requirement for the data and the keys to be stored in different provider environments. That leaves you with option D as the correct answer. Because you need a single solution that also has to store keys externally, this would be the appropriate choice. Use the approach that helps you as you answer diagnostic questions throughout this course. Remember to read the question stem carefully, and identify the key points and requirements. Some questions have multiple parts. Consider each of the requirements posed by the question. Find the similarities and differences in the responses, and eliminate responses that do not meet the requirements.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517260

#### Your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517261

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517262

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517263

### Ingesting and Processing  Data

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517264

Welcome to Module 2: Ingesting and Processing Data. In this module you’ll explore considerations for ingesting and processing data, which corresponds to section 2 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Ingesting and processing data for Cymbal Retail

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517265

Let’s begin by exploring the role of a Professional Data Engineer in handling data ingestion and data processing for Cymbal Retail, which includes using the various services provided by Google Cloud to support data ingestion and data processing. Cymbal Retail receives data from multiple sources, both internal and external. As the business has grown, the volume of ingested data has also increased exponentially. Processing the data has become increasingly complex and costly. In Cymbal Retail’s current on-premises data centers, Spark and Hadoop jobs are executed on pre-configured, static infrastructure. Part of your role involves determining how to lift-and-shift these jobs to Google Cloud. You have to design the architecture of the data ingestion and processing. Some data can be directly loaded into data warehouses using an extract and load approach, while others might be transformed before being uploaded into the data warehouse. Building, deploying, and operating effective flexible data pipelines for all the stages of data processing is a primary expectation from you as a Professional Data Engineer. You need to identify and deploy the right approach between EL, ETL, or ELT and choose the right Google Cloud tools for the job. Cymbal Retail’s customers want features that require the increased use of real-time data. In this regard, tools like open source Apache Beam and the hosted Dataflow are important skills for a data professional. Your knowledge of ways to apply different types of windowing for various use cases will provide the right approach to analyze streaming data. Your role also requires you to optimize all data ingestions and data processings tasks. Your optimizations should bring considerable savings on effort and cost, while improving availability and responsiveness. As the volume of data and scale of processing increases, Cymbal Retail does not want the latency, effort, or cost to increase linearly, or worse, exponentially. Your early design decisions on automation and orchestration could reduce effort later on.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517266

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, you’re not expected to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments, kindly use your Course Workbook.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517267

#### Your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517268

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517269

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517270

### Storing Data

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517271

Welcome to Module 3: Storing the Data. In this module you’ll explore considerations for storing data, which corresponds to section 3 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Storing Cymbal Retail’s data

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517272

Let’s begin by exploring the role of a Professional Data Engineer in storing data in Google Cloud for Cymbal Retail, including data architecture, database selection, and data lifecycle management. As a Professional Data Engineer, you role is deeply intertwined with how Cymbal Retail uses data. Cymbal Retail ingests, stores, processes, and analyzes many types of data, including documents, images, text, and video. There are close to a thousand databases used across the organization containing structured and unstructured data. Google Cloud has a variety of databases, both SQL and NoSQL, and also products suitable for data lakes, data warehouses, and data meshes. You need to know the difference between each of them and choose the appropriate storage solution, which is essential to ensuring efficiency of access and cost. All of this data, running into petabytes, has to meet the multi-modal purpose of cost-efficient storage, quick retrieval for analytics, and secure access. To comply with industry and regional data privacy laws, Cymbal Retail needs to have clear and demonstrable governance controls in place. Your data engineering team must ensure that data follows a well-defined lifecycle. Each region's laws define a strict timeline for the retention and deletion of customer data. You need to set controls in place so that individuals do not accidentally breach rules. It is also your responsibility as a Professional Data Engineer to continue to maintain Cymbal Retail’s data for effective use. As business requirements change, you need to evolve the data architecture to meet demand. Appreciation of the goals of the business and the ability to map them to data decisions make an excellent data engineer.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517273

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517274

#### Your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517275

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder, this course isn’t designed to teach you everything you need to know for the exam, and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517276

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517277

### Preparing and Using Data for Analysis

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517278

Welcome to Module 4: Preparing and Using Data for Analysis. In this module, you’ll explore considerations when preparing and using data for analysis, which corresponds to section 4 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Preparing and using Cymbal Retail’s data for analysis

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517279

Let’s begin by exploring the role of a Professional Data Engineer in preparing and analysing data for Cymbal Retail. The management at Cymbal Retail understands that data becomes most useful when it can be used to make decisions. They have employed advances in data based decision-making for several decades. The company's leaders need analytics and decision support systems to be flexible, agile, and customizable to their needs. One of your key responsibilities as a Professional Data Engineer is to help support the business in effectively using data to make decisions. As much as up-to-date analytics is useful for Cymbal Retail, the management realizes that their partners would benefit from it too. You need to ensure data is shared in a secure, limited way. Partner-specific reports are available. These partner accounts are controlled for access and granularity by the data engineering team. Industry and government bodies frequently request data from Cymbal Retail, which the company is obliged to provide in a short turnaround duration. Any data access is limited strictly on a need to know basis. As a Professional Data Engineer, your role also involves supporting Cymbal Retail’s goals of advancing its machine learning technologies. Fine-tuned predictions in estimating demand and individual customer behavior has helped reduce inventory while increasing sales. The adoption of the latest machine learning and artificial intelligence technologies has been key to this development. As Cymbal Retail is investing more in the adoption of relevant machine learning technologies, data engineering must process the data to make it usable to build machine learning models and build such models.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517280

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517281

#### Your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517282

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder, this course isn’t designed to teach you everything you need to know for the exam—and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and Skill Badges you’ll want to emphasize in your study plan.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517283

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517284

### Maintaining and Automating Data Workloads

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517285

Welcome to Module 5: Maintaining and Automating Data Workloads. In this module you’ll explore considerations for maintaining and automating data workloads, which corresponds to section 5 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Maintaining and automating data workloads at Cymbal Retail

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517286

Let’s begin by exploring the role of a Professional Data Engineer in helping Cymbal Retail maintain and automate its data workloads. As part of the Data Engineering team at Cymbal Retail you are responsible for ensuring cost effectiveness and correctness of data. The team has understood that automation and observability are cornerstones to providing a reliable and scalable service to end customers, internal teams, and to vendors. You need to be familiar with monitoring and managing active workloads to balance compute capacity, error handling, and costs. Cost control, though extremely important, cannot be detrimental to the end user experience. Web pages and purchase workflows have to be fast and frictionless. Availability has to be high, as a customer who has even a one-time bad experience is a customer lost. In order to balance the competing demands of user experience and cost-efficiency, you need to research and obtain the best pricing available from Google Cloud. To help achieve both reliability and cost control, Cymbal Retail wants to automate workloads and scale repeatable data processing. As a Professional Data Engineer, you need to regularly look for opportunities to optimize through automation and to boost efficiency. Taking a hands off approach to maintenance with appropriately set notifications and thresholds can save considerable time and effort while achieving scale. Marketing and sales teams need the flexibility to run campaigns quickly. These deals could bring in big spikes in demand for a short period, which results in valuable sales. Previously the marketing and sales teams would inform the data engineering team of such events weeks or months in advance, which would help them arrange the required computing and data resources. This has been found to be prohibitive. You need to find solutions that give considerable autonomy and independence to the marketing and sales teams to run their campaigns without being restricted by the technology demands.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517287

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517288

#### Your study plan

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517289

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

#### Study plan resources

- https://www.cloudskillsboost.google/paths/16/course_templates/72/documents/517290

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/16/course_templates/72/quizzes/517291

### Summary

#### Module Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517292

Welcome to Module 6: Your Next Steps. In this module, you focus on creating your individualized study plan. In this module, you use the notes you’ve been taking throughout this course to put together a study plan for each week in your Professional Data Engineer journey.

#### Weekly Study Goals

- https://www.cloudskillsboost.google/paths/16/course_templates/72/video/517293

Now that you’ve explored all five sections of the exam guide, consider what you’ve learned about your knowledge and skills through the diagnostic questions in this course. You should have a better understanding of what areas you need to focus on and what resources are available. Think about the answers to these questions: When will you take the exam? How many weeks does that give you to prepare? How many hours can you realistically spend preparing for the exam each week? How many total hours will you prepare? Be sure to leave enough time at the end of your plan to retake the diagnostic questions and the sample questions, and fill in any gaps in your knowledge that may remain. Take a few minutes to think about how much time you will allocate to preparing for the exam and note your answers in the workbook. The number of weeks in your preparation journey will depend on various factors, such as your prior experience collecting, transforming, and publishing data in Google Cloud, your comfort level with different products and services, and how much time you have available to dedicate to studying each week. You might choose to focus on specific courses or Skill Badges each week, such as in this sample study plan, or instead focus your study on a specific topic, such as data processing with Dataflow. Once you have a high-level idea of how many weeks that you have to study and how you want to determine your weekly focus, you want to build out a plan with weekly goals and study activities. Use the template in your workbook to plan your study goals for each week. Consider: What exam guide sections or topic areas will you focus on? What courses (or specific modules) will help you learn more? What Skill Badges or labs will you use to practice your skills? What documentation links will you review? What additional resources will you use, such as sample questions? You may do some or all of these study activities each week. Let’s review an example. For example, if you’ve identified using BigQuery for data warehousing as a particular area you need to study, you might choose to structure your study for a week to include targeted modules from the on-demand training, a related skill badge for hands-on practice, and documentation. Alternatively, you might choose one week to complete an entire course, and another week to focus on a skill badge. You can determine the approach that fits your existing skill set. Find the weekly study template at the end of your workbook. Duplicate the weekly template for the number of weeks in your individual preparation journey. Remember, you may need to adjust your plans based on the areas where you need to learn more. For more information about the resources we’ve discussed in this course, refer to your notes and the student copy of the slides. To register for the exam, follow the link on the Professional Data Engineer certification information page, at https://cloud.google.com/learn/certification/data-engineer. And with that, good luck with your preparation journey, and all the very best for your exam!

### Your Next Steps

## 03: Introduction to Data Engineering on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/1157

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521317

Hello, and welcome to Introduction to Data Engineering on Google Cloud. I'm Damon, and I am a curriculum developer at Google. Whether you already work in data engineering and want to learn how to be successful on Google Cloud, or you are looking to progress in your career, this course will help you get started. Through a series of lectures, quizzes, and hands-on labs, you will learn the fundamentals of data engineering on Google Cloud. This course was designed for data engineers or anyone interested in preparing and sorting datasets for further usage in their organization. This involves using tools such as, but not limited to, Dataflow, Dataproc, Cloud Composer, BigQuery, Bigtable, and Datastream. In this course, you learn about the duties and responsibilities of a data engineer, identify data engineering tasks and core components used on Google Cloud to accomplish those tasks, understand how to create and deploy data pipelines of varying patterns on Google Cloud, and identify and utilize various automation techniques on Google Cloud to complete data engineering tasks. The course is divided into six modules designed to address the learning objectives. First, you look at data engineering tasks and components on Google Cloud. Next, you explore data replication and migration. Then you explore each of the three main data pipeline patterns; extract and load, extract, load, and transform, and extract, transform, and load. You conclude the course by examining automation techniques important to a data engineer.

### Data Engineering Tasks and Components

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521318

In this module, first you'll learn about the role of a data engineer. Second, we will cover the differences between a data source and a data sync. Then, we will review different types of data formats that a data engineer will encounter. The next topic addresses the options for storing data on Google Cloud. Then we cover the choices available for metadata management. Finally, we will look at the features of Analytics Hub, that allow you to easily share datasets both within and outside your organization.

#### The Role of a Data Engineer

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521319

What does a data engineer do? At a basic level, a data engineer builds data pipelines. Why does the data engineer build data pipelines? Because they want to get their data into a place such as a dashboard, or report, or machine learning model, from where the business can make data-driven decisions. The data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is by itself not very useful. Once data becomes useful, the data engineer will often apply updates or transformations to add new value to the data. Of course, new data environments require data management practices to ensure currency and accuracy. Finally, data engineers create processes and operations to move data usage into production settings. In the most basic sense, a data engineer moves data from data sources to data syncs in four stages: replicate and migrate, ingest, transform, and store. The replicate and migrate stage of a data pipeline focuses on the tools and options to bring data from external or internal systems into Google cloud for further refinement. There are a wide variety of tools and options at your disposal. They will be covered in more detail throughout this course.

#### Data Sources Versus Data Sinks

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521320

The ingest stage of a data pipeline is the point where data becomes a data source and is available for usage downstream. Think of a data source as the starting point of your data journey. It is raw, unprocessed data waiting to be transformed into valuable insights. Any system, application, or platform that creates, stores, or shares data can be considered a data source. Two examples of Google Cloud products used in the ingest phase are Cloud storage, a data lake holding various types of data sources, and Pub/Sub, an asynchronous messaging system delivering data from external systems. The transform stage of a data pipeline represents action taken on a data source to adjust, modify, join, or customize a data source so that it matches a specific downstream data or reporting requirement. There are three main transformation patterns: extract and load, extract, load, and transform, and extract, transform, and load. You explore each of these patterns in their own modules later in the course. The store stage of a data pipeline represents the last step when we deposit data in its final form. A data sync is the final stop in the data journey. It's where processed and transformed data is stored for future use, analysis, and decision-making. Think of it as the reservoir at the end of the river, where valuable information is collected and readily available. Two examples of Google Cloud products used in the store phase are BigQuery, a serverless data warehouse, and Bigtable, a highly scalable no SQL database.

#### Data Formats

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521321

Data exists in two primary formats, unstructured and structured. Unstructured data is information stored in a non-tabular form, such as documents, images, and audio files. Unstructured data is usually suited for Cloud Storage, but BigQuery also offers the capability to store unstructured data via object tables. There is also structured data, which represents information stored in tables, rows, and columns.

#### Storage Solution Options on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521322

There are several key products on Google Cloud that are used by data engineers. One main product is Cloud storage. Unstructured data is usually well suited to be stored in Cloud Storage. Within Cloud Storage, objects are accessed by using HTTP requests, including ranged GETS to retrieve portions of the data. The only key is the object name. There is object metadata, but the object itself is treated as unstructured bytes. The scale of the system allows for serving large static content and accepting user-uploaded content including videos, photos, and files. Objects can be up to five terabytes each. Cloud Storage is built for availability, durability, scalability, and consistency. It's an ideal solution for hosting static websites and storing images, videos, objects, and blobs, and any unstructured data. Cloud Storage has four primary storage classes; standard storage, nearline storage, coldline storage, and archive storage. The classes are differentiated by the expected period of object access. You have a full range of cost effective storage services for structured data to choose from when developing with Google Cloud. No one size fits all, and your choice of storage and database solutions will depend on your application and workload. Cloud SQL is Google Cloud's managed relational database service. AlloyDB is a fully managed, high- performance PostgreSQL database service from Google Cloud. Spanner is Google Cloud's fully managed relational database service that offers both strong consistency and horizontal scalability. Firestore is a fast, fully managed, serverless, NoSQL document database built for automatic scaling, high performance, and ease of application development. BigQuery is a fully managed, serverless enterprise data warehouse for analytics. Bigtable is a high-performance NoSQL database service. Bigtable is built for fast key-value lookup and supports consistent sub-10 millisecond latency. The two key concepts in data engineering are that of the data lake and the data warehouse. A data lake is a vast repository for storing raw unprocessed data in various formats, including unstructured, semi-structured, and structured. It serves as a centralized storage solution for diverse data types, enabling flexible use cases like data science, applications, and business decision making. A data warehouse is a structured repository designed for storing pre-processed and aggregated data from multiple sources. Primarily used for long term business analysis, it enables efficient querying and reporting for informed decision making. Data warehouses often operate as standalone systems, independent of other data storage solutions. BigQuery is a fully managed, serverless enterprise data warehouse for analytics. BigQuery has built-in features like machine learning, geospatial analysis, and business intelligence. BigQuery can scan terabytes in seconds and petabytes in minutes. BigQuery is a great solution for online analytical processing, or OLAP, workloads for big data exploration and processing. BigQuery is also well-suited for reporting with business intelligence tools. BigQuery has several easy to use options for accessing data. The first is via the Google Cloud console's SQL editor. The second is via the bq command line tool which is part of the Cloud SDK. The last is via a robust REST API which supports calls in seven programming languages. BigQuery organizes data tables into units called datasets. These datasets are scoped to your Google Cloud project. When you reference a table from the command line in SQL queries or code, you refer to it by using the construct, project.dataset.table. Access control is through IAM and is at the dataset, table, view, or column level. In order to query data in a table or view, you need at least read permissions on the table or view.

#### Metadata Management Options on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521323

Metadata is a key element to making data more manageable and useful across an organization. Dataplex is a comprehensive data management solution that allows you to centrally discover, manage, monitor, and govern distributed data across your organization. With Dataplex, you can break down data silos, centralize security and governance, while enabling distributed ownership, and easily search and discover data based on business contexts. Dataplex also offers built-in data intelligence, support for open-source tools, and a robust partner ecosystem, helping you to trust your data and accelerate time to insights. Dataplex lets you standardize and unify metadata, security policies, governance, classification, and data life cycle management across this distributed data. Another common use case is when your data is accessible only to data engineers, and is later refined and made available to data scientists and analysts. In this case, you can set up a lake to have the following: A raw zone for the data, which is accessed by data engineers and data scientists. A curated zone for the data, which is accessed by all users.

#### Sharing Datasets using Analytics Hub

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521324

Sharing data is challenging especially outside of your organization. You need to consider security and permissions, destination options for data pipelines, data freshness and accuracy, and finally, usage monitoring. Analytics Hub was created to meet these data sharing challenges. Analytics Hub helps organizations unlock the value of data sharing, leading to new insights and business value. With Analytics Hub, you create a rich data ecosystem by publishing and subscribing to analytics-ready datasets. Because data is shared in place, data providers are able to control and monitor how their data is being used. Analytics Hub provides a self-service way to access valuable and trusted data assets, including data provided by Google. Finally, Analytics Hub provides an opportunity to monetize data assets. Analytics Hub removes the tasks of building the infrastructure required for monetization.

#### Lab Intro: Loading Data into BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521325

In this lab, you practice loading data into BigQuery. The primary objective of this lab is to load data into BigQuery using both the command-line interface and the Google Cloud console. You also experience loading several datasets into BigQuery and using the Data Description Language, or DDL.

#### Loading data into BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521326

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521327

### Data Replication and Migration

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521328

In this module, first, you review the baseline Google Cloud data replication and migration architecture. Second, you will cover the options and use cases for the gcloud command line tool. Then you will review the functionality and use cases for the Storage Transfer Service. The next topic addresses the functionality and use cases for the Transfer Appliance. Finally, you will look at the features and deployment of Datastream.

#### Replication and Migration Architecture

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521329

The replicate and migrate stage of a data pipeline focuses on the tools and options to bring data from external or internal systems into Google Cloud for further refinement. Google Cloud provides a comprehensive suite of tools to migrate and replicate your data. Start replicating and migrating data by using tools like the 'gcloud storage' command, Transfer Appliance, Storage Transfer Service, or Datastream. You can then transform the data as needed before finally storing it within Google Cloud. Data can originate from on-premises or multi-cloud environments, including file systems, object stores, HDFS, and relational databases. Google Cloud offers options for one-off transfers, scheduled replications, and change data capture, ultimately landing data in Cloud Storage or BigQuery. Google Cloud provides additional workload migration with options for various database types. Leverage Database Migration Service for seamless transitions from Oracle, MySQL, PostgreSQL, and SQL Server. For other data formats or complex migrations, use ETL tools like DataFlow with a wide range of templates that handle NoSQL or non-relational databases. Your target destination can be Cloud SQL, AlloyDB, or BigQuery, depending on your needs. The ease of migrating data depends heavily on data size and network bandwidth. With one terabyte of data, a 100 gigabits per second network takes about two minutes to transfer, while the same size on a 100 megabits per second network takes 30 hours. The 'gcloud storage' command or Storage Transfer Service are suitable for smaller datasets. For larger datasets, consider Transfer Appliance for faster offline transfer.

#### The gcloud Command Line Tool

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521330

You can use the 'gcloud storage' command to transfer small to medium-sized datasets to Cloud Storage. The data can originate from various on-premise sources like file systems, object stores, or HDFS. The 'cp' command, shown in the code snippet, facilitates these ad-hoc transfers directly to Cloud Storage.

#### Moving Datasets

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521331

To move larger datasets, consider using Storage Transfer Service. Storage Transfer Service efficiently moves large datasets from on-premises, multicloud file systems, object stores (including Amazon S3 and Azure Blob Storage), and HDFS into Cloud Storage. It boasts high transfer speeds (up to tens of gigabits per second) and supports scheduled transfers for convenient data migration. Transfer appliance is Google's solution for moving massive datasets offline. Google provides the hardware, you transfer your data onto it, then ship it back. It is ideal for scenarios with limited bandwidth or very large transfers, and it comes in multiple sizes to suit your needs.

#### Datastream

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521332

Datastream enables continuous replication of your on-premises or multi-cloud relational databases such as Oracle, MySQL, PostgresSQ,L or SQL Server into Google Cloud. Datastream offers change data capture options for historical backfill or allows you to just propagate new changes with data landing in Cloud Storage or BigqQuery for analytics. You have flexibility in connectivity options and can selectively replicate data at the schema, table, or column level. Datastream enables real-time data replication from source systems for various use cases. It supports direct replication into BigQuery for analytics, allows custom data processing in Dataflow before loading into BigQuery, and facilitates event-driven architectures. Additionally, Datastream can be used with Dataflow templates for seamless database replication and migration tasks, making it a versatile tool for integrating data into Google Cloud. Datastream taps into the source database's write-ahead log (WAL) to capture and process changes for propagation downstream. Datastream supports reading the logging mechanisms for the specific source database such as LogMiner for Oracle, binary log for MySQL, PostgreSQL's logical decoding, and transaction logs from SQL Server. These change events such as inserts, updates, and deletes are then processed by Datastream and transformed into structured formats like Avro or JSON, ready for storage in Google Cloud, typically in BigQuery tables, enabling near real-time data replication for analytics and other use cases. Datastream event messages contain two main sections: generic metadata and payload. Metadata provides context about the data, like source table, timestamps, and related information. Payload contains the actual data changes in a key-value format, reflecting column names and their corresponding values. This structure allows for efficient and organized data replication and tracking of changes. Datastream event messages also include source-specific metadata in addition to generic metadata and payload. This metadata provides context about the data's origin within the source system, including details like the database name, schema, table, change type (such as INSERT), and other system-specific identifiers. This additional information helps track data lineage and understand the context of changes replicated from the source database. Datastream simplifies data replication by using unified data types to map between different source and destination databases. This means that regardless of whether your source data is in Oracle as number, MySQL as decimal, PostgreSQL, as numeric, or SQL Server as decimal, Datastream will consistently represent it as decimal during replication. When this data lands in Google Cloud, it can be further transformed into format-specific data types in different file types or destinations, such as Avro as decimal, JSON as number, or stored natively in BigQuery tables as numeric. This ensures data type consistency and compatibility across different database systems, streamlining the data replication process. In summary, Google Cloud offers several data migration and replication options. The 'gcloud storage' command is suitable for smaller online transfers. Storage Transfer Service handles larger online transfers efficiently. Transfer Appliance is ideal for massive offline data migrations, and Datastream provides continuous online replication of structured data, supporting both batch and streaming velocities. Choose the option that best fits your data size, transfer type, and data availability requirements.

#### Lab Intro: Datastream: PostgreSQL Replication to BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521333

In this lab, you use Datastream to replicate data from PostgreSQL to BigQuery. You prepare and load a Cloud SQL for PostgreSQL instance. You create Datastream connection profiles for the source and target. You then create a Datastream processing stream and start replication. Finally, you validate the replication in BigQuery.

#### Datastream: PostgreSQL Replication to BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521334

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521335

### The Extract and Load Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521336

In this module, first, you review the baseline extract and load architecture diagram. Second, you explore the options of the bq command line tool. Then, you review the functionality and use cases for the BigQuery Data Transfer Service. Finally, you look at the functionality and use cases for BigLake as a non extract-load pattern.

#### Extract and Load Architecture

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521337

The extract and load data pipeline pattern focuses on the tools and options to bring data into BigQuery by eliminating the need for upfront transformation. Extract and load greatly simplifies data ingestion into BigQuery. Extract and load leverages tools like 'bq load' and Data Transfer Service to directly load data from various sources or uses external tables and BigLake tables to make data accessible via BigQuery. This pattern also offers scheduling capabilities and eliminates the need for data copying, promoting efficiency in data pipelines. BigQuery provides extensive flexibility in data handling. It supports loading data from various formats like Avro, Parquet, ORC, CSV, JSON, as well as Google Cloud Firestore exports. Similarly, you can export BigQuery artifacts, including query results and table data, into formats like CSV, JSON, Avro, and Parquet, facilitating easy integration with other tools and systems. BigQuery offers two ways to load data: through its friendly user interface for file uploads, or via the LOAD DATA SQL statement. The UI simplifies the process, allowing you to select files, specify formats, and even auto-detect schema. LOAD DATA provides more control, ideal for automation and appending or overriding existing tables.

#### The bq Command Line Tool

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521338

The Cloud SDK's bq command offers a programmatic way to interact with BigQuery. You can create BigQuery objects like datasets and tables, put the familiar Linux command, bq mk. The bq load command efficiently loads data into BigQuery tables. Key parameters for bq load include specifying the source format such as CSV, skipping header rows, and defining the target dataset and table. You can load data from multiple files in Cloud Storage using wildcards and optionally provide a schema file for the table structure. These options provide flexibility and control for loading data into BigQuery from various sources.

#### BigQuery Data Transfer Service

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521339

The BigQuery Data Transfer Service enables seamless loading of structured data from diverse sources, like SaaS applications, object stores, and other data warehouses into BigQuery. The service provides scheduling options for recurring or on-demand transfers, along with configuration options for data source details and destination settings. It is a managed and serverless solution, eliminating infrastructure management overhead. In addition, its no code approach simplifies data transfer setup and management.

#### BigLake

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521340

BigQuery's data access capabilities extend beyond its own storage. BigQuery allows you to query data residing in sources like Cloud Storage, Google Sheets, and Bigtable using external tables. Additionally, BigLake tables provide a way to query data across Cloud Storage, and even other cloud object stores, expanding BigQuery's reach and flexibility for data analysis. BigQuery offers flexibility in analyzing structured data. You can load data into permanent BigQuery tables for high-performance analytics, but with data movement involved. External tables allow you to query data directly in Cloud Storage without loading it into BigQuery, which is suitable for less frequent access. BigLake tables provide the best of both worlds: high-performance analytics on data in Cloud Storage without the need to load it into BigQuery, and without data movement. BigQuery external tables bridge the gap between Google Sheets and BigQuery, enabling direct querying of sheets data within BigQuery. By specifying the Google Sheets URL and format, users can treat the sheet as a table in BigQuery, simplifying data analysis across platforms. However, be aware that querying external tables may have limitations, like slower performance and the unavailability of cost estimation, table preview, and query caching. BigLake extends BigQuery's capabilities, providing a unified interface to query data directly from your data lake and other sources without moving or copying it. BigLake leverages Apache Arrow for efficient data handling and offers fine-grained security and metadata caching. With BigLake, you can seamlessly access data across data lakes and data warehouses using familiar BigQuery tools. BigLake tables provide a seamless querying experience, allowing you to interact with data stored in external sources, like Cloud Storage, just like you would with data in native BigQuery tables. You can use standard SQL queries to access and analyze the data within BigLake tables, including SELECT statements and joins. Behind the scenes, BigLake leverages metadata caching to enhance query performance, even though the data physically resides outside BigQuery. However, some features like query cost estimation and table preview are not available for BigLake tables due to the external nature of the data. BigLake maintains a metadata cache. The cache stores details about external data. For example, it can contain details about Parquet files stored in Cloud Storage, such as file size, row count, and column statistics like minimum/maximum values. This cache allows querying via BigQuery to skip listing all objects, prune files, and partitions faster, and enable dynamic predicate pushdown, resulting in improved query performance. The cache allows querying by Spark to access metadata statistics that the Spark-BigQuery connector can leverage to speed up queries. The metadata cache has configurable staleness from 30 minutes to seven days, and it can be refreshed automatically or manually. External tables in BigQuery require users to have separate permissions for both the table itself and the underlying data source. This can lead to more complex access management. BigLake tables offer a streamlined approach. Access is delegated through a service account decoupling table access from the data source. This simplifies permission management and enhances security. In summary, both external and BigLake tables enable querying data residing outside of BigQuery, but BigLake offers broader capabilities. BigLake supports a wide range of data formats and storage locations, including object stores across multiple cloud providers, and provides advanced security features like column-level and row-level security. External tables are simpler to set up, but lack fine-grained security controls. BigLake tables offer enhanced performance, security, and flexibility for querying external data, making them suitable for enterprise data lake use cases.

#### Lab Intro: BigLake: Qwik Start

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521341

In this lab, you use BigLake to connect to various external data sources. You configure a connection resource and set up access to a Cloud Storage data lake. You create and query a BigLake table and set up access control policies. Finally, you upgrade an existing external table to be a BigLake table.

#### BigLake: Qwik Start

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521342

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521343

### The Extract, Load, and Transform Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521344

In this module, first, you review the baseline extract, load, and transform architecture diagram. Second, you look at a common ELT pipeline on Google Cloud. Then you review BigQuery's SQL scripting and scheduling capabilities. Finally, you look at the functionality and use cases for Dataform.

#### Extract, Load, and Transform (ELT) Architecture

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521345

Extract, load, and transform centers around data being loaded into BigQuery first. Once data is loaded, there are multiple ways to transform it. Procedural languages like SQL can be used to transform data. Scheduled queries can be used to transform data on a regular basis. Scripting and programming languages like Python can be used to transform data. And a tool like Dataform simplifies transformation beyond basic programming options. In an extract, load, and transform pattern pipeline, structured data is first loaded into BigQuery staging tables. Transformations are then applied within BigQuery itself using SQL scripts or tools like Dataform with SQL workflows. The transformed data is finally moved to production tables in BigQuery ready for use. This approach leverages BigQuery's processing power for efficient data transformation.

#### SQL Scripting and Scheduling with BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521346

With support for procedural language, BigQuery allows the execution of multiple SQL statements in sequence with shared state. This enables the automation of tasks like table creation, implementation of complex logic using constructs like IF and WHILE, and the use of transactions for data integrity. You can also declare variables and reference system variables within your procedural code. BigQuery supports user-defined functions, or UDFs, for custom data transformations using SQL or JavaScript. These UDFs can be persistent or temporary, and it is recommended to use SQL UDFs when possible. JavaScript UDFs offer the flexibility to use external libraries, and community-contributed UDFs are available for reuse. Stored procedures are pre-compiled SQL statement collections, streamlining database operations by encapsulating complex logic for enhanced performance and maintainability. Benefits include reusability, parameterization for flexible input, and transaction handling. Stored procedures are called from applications or within SQL scripts, promoting modular design. BigQuery supports running stored procedures for Apache Spark. Apache Spark stored procedures on BigQuery can be defined in the BigQuery PySpark editor or using the CREATE PROCEDURE statement with Python, Java, or Scala code. The code can be stored in a Cloud Storage file or defined inline within the BigQuery SQL editor. Remote functions extend BigQuery's capabilities by integrating with Cloud Run functions. This enables complex data transformations using Python code. You define the remote function in BigQuery, specifying the connection and endpoint to your Cloud Run function. This function can then be called directly within your SQL queries, similar to a UDF, allowing for seamless integration of custom logic. The example here shows a Python function that gets a list of signed URLs pointing to Cloud Storage objects and returns the length for these objects. In BigQuery, we just need to register the function. We call it object_length(). Then, we can use it as is in SQL. Jupyter Notebooks coupled with BigQuery DataFrames facilitate efficient data exploration and transformation. This integration emphasizes the ability to handle large datasets that exceed runtime memory, perform complex data manipulations using SQL or Python, and schedule notebook executions. The seamless integration of BigQuery DataFrames and popular visualization libraries further streamlines the entire process. BigQuery offers the option to save and schedule queries for repeated use. You can save queries, manage versions, and share them with others. Scheduling allows you to automate query execution by setting frequency, start and end times, and result destinations. Dataform is recommended for more complex SQL workflows. Often, there are needs to perform additional tasks after a scheduled query is executed in BigQuery. These include tasks such as triggering subsequent SQL scripts, running data quality tests on the output, or configuring security measures. In an ideal situation, these actions can be automated, ensuring data pipelines remain efficient and reliable.

#### Dataform

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521347

Dataform is a serverless framework that simplifies the development and management of ELT pipelines using SQL, Dataform enables data transformation within BigQuery, ensuring data quality and providing documentation, This approach streamlines the process of moving data from source systems to production tables in BigQuery, making operations more efficient and manageable, Dataform streamlines data operations in BigQuery by unifying transformation, assertion, and automation, Without Dataform, tasks like defining tables, managing code, testing data quality, and scheduling pipelines would be time-consuming and prone to errors, They could also involve multiple tools and manual processes, Dataform simplifies these tasks within BigQuery, improving efficiency and data reliability, Dataform and BigQuery work together to manage SQL workflows, With Dataform, developers create and compile SQL workflows using SQL and JavaScript, Dataform then performs real-time compilation, including dependency checks and error handling, Finally, the compiled SQL workflows are executed within BigQuery, enabling SQL transformations and materialization either on-demand or through scheduled runs, Development using Dataform utilizes workspaces containing default files and folders, Key folders include 'definitions' for sqlx files and 'includes' for JavaScript files, The ,gitignore file is used for managing Git commits, Developers may also use package,json and package-lock,json for handling JavaScript dependencies, The 'workflow settings,yaml' file stores, project compilation settings, and custom files like README,md can also be added, The sqlx file structure provides a clear framework for organizing SQL code and associated tasks, It begins with a config block for metadata and data quality tests, followed by a js block to define reusable JavaScript functions. The pre_operations block handles SQL statements executed before the main SQL body, which defines the core SQL logic. Finally, the post_operations block contains SQL statements to be run after the main execution, ensuring a structured and efficient workflow. sqlx development streamlines SQL code by replacing repetitive patterns with concise definitions. The code example demonstrates how a complex CASE statement for categorizing countries can be replaced with a simple function call $(mapping.region("country")). This approach improves code readability and maintainability by reducing boilerplate code and promoting reusability. With Dataform, table and view definitions should be created in a specific manner so that they can be compiled into SQL statements. Key configuration types are: declaration for referencing existing BigQuery tables. table for creating or replacing tables with a SELECT statement, incremental for creating tables and updating them with new data, and view for creating or replacing views, which can optionally be materialized. Dataform offers assertions to define data quality tests, ensuring data consistency and accuracy. Assertions can be written in SQL or JavaScript, providing flexibility for complex checks. Operations allow you to run custom SQL statements before, after, or during pipeline execution. These two options enable custom data transformations, data quality checks, and other tasks within your workflows. By combining assertions and operations, Dataform empowers you to create robust and reliable data pipelines in BigQuery. Dataform provides two methods to manage dependencies, implicit declaration and explicit declaration. Implicit declaration is when you reference tables or views directly within your SQL using the ref() function. Explicit declaration is when you list dependencies within a config block using the dependencies array. It is also possible to use the resolve() function to reference without creating a dependency. Dataform allows you to compile user-defined table definitions into executable SQL scripts. The sample code shows a customer_details table being created or replaced based on a customer_source table using a SELECT statement. Dataform manages the dependencies between these tables and orchestrates their execution within a workflow. This process streamlines data transformation and ensures efficient data pipeline management. Dataform SQL workflows are best visualized in graph format. The sample workflow starts with a declaration of customer_source followed by a customer_intermediate table, likely derived from a source system as a pre-processed data source. Next, customer_rowConsistency applies assertions for data quality checks. The graph then splits into two paths. In one path, an operation named customer_ml_training is invoked. It performs operations on the validated data. In the other path, a view named customer_prod_view is created. There are several scheduling and execution mechanisms for Dataform SQL workflows. One path is through internal triggers. These include manual execution in the Dataform UI or scheduled configurations within Dataform itself. The other path is through external triggers. These include tools like Cloud Scheduler and Cloud Composer. Ultimately, all workflows are executed within BigQuery, showcasing its central role in this process.

#### Lab Intro: Create and Execute a SQL Workflow in Dataform

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521348

In this lab, you use Dataform to create and execute a SQL workflow. First, you create a Dataform repository. Second, you create and initialize a Dataform development workspace, then you create and execute a SQL workflow. Finally, you view execution logs in Dataform to confirm completion.

#### Create and execute a SQL workflow in Dataform

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521349

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521350

### The Extract, Transform, and Load Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521351

In this module, first, you review the baseline extract, transform, and load architecture diagram. Second, you look at the GUI tools on Google Cloud used for ETL data pipelines. Third, you review batch data processing using Dataproc. Then, you examine using Dataproc Serverless for Spark for ETL. Next, you review streaming data processing options on Google Cloud. Finally, you examine the role Bigtable plays in data pipelines.

#### Extract, Transform, and Load (ETL) Architecture

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521352

The extract, transform, and load data pipeline pattern focuses on data being adjusted or transformed prior to being loaded into BigQuery. Google Cloud offers a variety of services to handle distributed data processing. For developers who prefer visual interfaces, there are user-friendly tools like Dataprep and Data Fusion. If you favor open-source frameworks, Dataproc and Dataflow are an option. And to streamline your workflows, template support is provided across various stages of data processing, from extraction and loading to full-fledged transformation.

#### Google Cloud GUI Tools for ETL Data Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521353

Dataprep by Trifacta simplifies data transformation by offering a serverless, no-code solution. It connects to various data sources, provides pre-built transformation functions, and allows users to chain these functions together into recipes. The resulting transformation flows can be executed seamlessly with Dataflow, while also providing capabilities for scheduling and monitoring. Dataprep provides a visual interface where you can see the impact of your data transformations before applying them. It also offers intelligent suggestions to help you refine your data cleaning and preparation process, like extracting specific values or replacing patterns within your data. Data Fusion is a user friendly, GUI-based tool designed for enterprise data integration. Data Fusion seamlessly connects to various data sources, both on-premises and in the cloud. You can build data pipelines without coding using its drag-and-drop interface and pre-built transformations. The platform is extensible, allowing for custom plugins, and it executes on powerful Hadoop/Spark clusters for efficient processing. Data Fusion Studio easily allows the creation of data pipelines with visual tools. The example pipeline shows two SAP tables being used as data sources. The two tables are joined together and then one outbound leg is written to a Cloud Storage bucket. The other leg undergoes an Add-Datetime transformation and is outputted to BigQuery. The pipeline also highlights the ability to preview data at different stages.

#### Batch Data Processing Using Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521354

Dataproc allows you to seamlessly run your Apache Hadoop and Spark workloads on Google Cloud. You can leverage HDFS data stored on Cloud Storage and use Dataproc to perform transformations with Spark jobs. The results can then be easily stored in various destinations like Cloud Storage, BigQuery, or NoSQL databases like Bigtable, all within the Google Cloud ecosystem. Dataproc is Google Cloud's managed service for data processing using Hadoop and Spark. It offers flexibility with runtimes on GCE, GKE, and Serverless Spark, and provides a rich, open-sourced ecosystem. Dataproc simplifies cluster management with workflow templates, autoscaling, and the option for both permanent and ephemeral clusters. It also integrates seamlessly with other Google Cloud storage services, eliminating the need for disk-based HDFS. Dataproc clusters on Google Compute Engine offer flexible storage options. Clusters can utilize HDFS on persistent disks for cluster storage, or leverage other Google Cloud storage services like Cloud Storage for persistent data. Additionally, Dataproc integrates with BigQuery and Bigtable using connectors, enabling seamless interaction with these data stores. This setup allows users to choose the most suitable storage solution for their specific needs while taking advantage of Dataproc's processing capabilities. Dataproc Workflow Templates allow you to define and manage complex data processing workflows with dependencies between jobs. You can specify these workflows in a YAML file, providing details about the jobs like Hadoop or Spark, their order of execution, and any required parameters. These templates can then be submitted to Google Cloud using the gcloud command line tool, where they will be executed on either a managed ephemeral cluster or an existing predefined cluster. Apache Spark is a versatile framework for data processing, offering various capabilities through its components like Spark SQL for structured data, Spark Streaming for real-time data, MLlib for machine learning, and GraphX for graph processing. Spark supports multiple languages including R, SQL, Python, Scala and Java, making it accessible to a wide range of users. With these features, Spark excels in tasks like data engineering, machine learning, analytics, and many more. Dataproc Serverless for Spark simplifies Spark workload execution by eliminating cluster management. It offers automatic scaling, cost efficiency with pay-per-execution pricing, faster deployment, and no resource contention. Users can focus solely on writing and executing their code, making it ideal for various Spark use cases like batch processing, interactive notebooks, and Vertex AI pipelines. Dataproc Serverless for Spark offers two main execution modes: Serverless for batches and Serverless for interactive notebook sessions. Batches are submitted using the gcloud command-line tool and are ideal for automated or scheduled jobs. Interactive sessions leverage JupyterLab, either locally or within the Google Cloud environment, for interactive development and exploration. The platform also supports features like BigQuery external procedures, templates, custom containers, and a pay-as-you-go pricing model. Dataproc Serverless for Spark seamlessly integrates with various Google Cloud services, enhancing its functionality and usability. It leverages Dataproc History Server and Dataproc Metastore for persistent storage and metadata management. It interacts with BigQuery for data warehousing and analytics, and with Vertex AI workbench for machine learning tasks. Additionally, it utilizes Cloud Storage and other storage services for data storage and retrieval. Behind the scenes, it creates and manages ephemeral clusters for efficient job execution. The lifecycle of an interactive notebook session begins with its creation, where various configurations like runtime version and network settings are defined. Once active, the session allows for code development and execution, with the kernel transitioning between idle and busy states. The session eventually reaches a shutdown phase, either manually triggered or due to inactivity, leading to the kernel being shut down and its state becoming unknown.

#### Lab Intro: Use Dataproc Serverless for Spark to Load BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521355

In this lab, you use Dataproc Serverless for Spark to load BigQuery. First, you configure the environment. Next, you download lab assets. You then configure and execute the Spark code. Finally, you view the data in BigQuery.

#### Use Dataproc Serverless for Spark to Load BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521356

#### Streaming Data Processing Options

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521357

Batch processing involves analyzing a fixed set of store data, suitable for tasks like payroll or billing systems. On the other hand, streaming data processing handles a continuous flow of data from various sources, making it ideal for real-time applications like fraud or intrusion detection. Streaming ETL workflows on Google Cloud involve the continuous ingestion of event data, often through Pub/Sub. This data is often processed in real-time using Dataflow, allowing for transformations and enrichment. Finally, the processed data is loaded into various destinations like BigQuery for analytics, enabling near real-time insights, or a Bigtable for NoSQL storage. Pub/Sub can efficiently manage high volumes of event data. Pub/Sub acts as a central hub, receiving events like 'New employee' or 'New contractor' from various sources. Pub/Sub then distributes these events to relevant systems like badge activation, facilities, and account provisioning, ensuring reliable delivery and enabling decoupled, asynchronous communication between systems. Dataflow leverages the Apache Beam programming framework to efficiently process both batch and streaming data. This unified approach simplifies development, allowing you to use languages like Java, Python, or Go. Dataflow seamlessly integrates with other Google Cloud services and offers features like a pipeline runner, serverless execution, templates, and notebooks for a streamlined experience. This code example demonstrates how to use Apache Beam to stream messages from Pub/Sub, transform them using a parsing function, and then write the results into BigQuery. The ReadFromPubSub function retrieves messages, Beam. Map() applies the parsing transformation, and WriteToBigQuery loads the transformed data into a specified BigQuery table, creating the table if necessary and appending new data to it. Dataflow templates allow you to create reusable pipelines for recurring tasks. You can separate the pipeline design from its deployment, making it easier to manage and update. By using parameters, you can customize the pipeline for different inputs, increasing its versatility. These templated pipelines can be easily deployed through various methods, and Google provides pre-built templates for common scenarios.

#### Bigtable and Data Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521358

Bigtable is an excellent choice for handling streaming data pipelines that require millisecond-level latency analytics. Bigtable utilizes a wide-column data model with column families, allowing for flexible schema design. Row keys serve as efficient indexes for quick data access. Bigtable's high-throughput and low latency capabilities make it suitable for applications like time series data, IoT, financial data, and machine learning, especially when dealing with large datasets. In summary, Google Cloud provides various services for ETL processing. Dataprep is ideal for data wrangling tasks and offers a serverless option. Data Fusion excels at data integration, particularly in hybrid and multicloud environments, utilizing the open-source CDAP framework. Dataproc handles ETL workloads with support for Hadoop, Spark, and other open source tools, with Serverless Spark as a serverless option. Lastly, Dataflow, built on Apache Beam, is recommended for both batch and streaming ETL workloads, and provides a serverless architecture.

#### Lab Intro: Creating a Streaming Data Pipeline for a Real-Time Dashboard with Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521359

In this lab, you create a streaming data pipeline for a real-time dashboard with Dataflow. You create a Dataflow job from a template. You then monitor a pipeline loading data into BigQuery. After that, you examine the data loaded using SQL. Finally, you visualize key metrics using Looker Studio.

#### Creating a Streaming Data Pipeline for a Real-Time Dashboard with Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521360

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521361

### Automation Techniques

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521362

In this module, first, you review the automation patterns and options available for pipelines. Second, you explore Cloud Scheduler and workflows. Then, you review the functionality and use cases for Cloud Composer. Next, you review the capabilities of Cloud Run functions. Finally, you look at the functionality and automation use cases for Eventarc.

#### Automation Patterns and Options for Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521363

On Google Cloud, ELT and ETL workloads can be automated for recurring execution. For example, in a scheduled ELT, a defined schedule triggers data extraction from BigQuery, transformation via Dataform, and loading back into BigQuery. Meanwhile, in the example shown for an event-driven ETL, a file upload to Cloud Storage initiates a batch process using Dataproc, culminating in data landing in Cloud Storage. Google Cloud offers a suite of services to automate and orchestrate your workloads. For scheduled tasks or one-off jobs, you can leverage Cloud Scheduler and Cloud Composer. If your workflows require orchestration, Cloud Composer is the ideal choice. To trigger actions based on events, consider using Cloud Run functions or Eventarc.

#### Cloud Scheduler and Workflows

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521364

Cloud Scheduler empowers you to automate tasks by invoking your workloads at specified recurring intervals. It grants you the flexibility to define both the frequency and precise time of day for job execution. Triggers can be based on HTTP/S calls, App Engine HTTP calls, Pub/Sub messages, or Workflows. Cloud Scheduler can be used to trigger a Dataform SQL workflow. In the example code, a scheduled job in Cloud Scheduler initiates the process defined in a YAML config file. The workflow involves two main steps: creating a compilation result from your Dataform code, and then triggering a workflow invocation using that result, ensuring only specific parts of your Dataform project execute based on included tags.

#### Cloud Composer

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521365

Cloud Composer acts as a central orchestrator, seamlessly integrating your pipelines across diverse systems, whether on Google Cloud, on-premises, or even multicloud environments. Cloud Composer leverages Apache Airflow, incorporating essential elements like operators, tasks, and dependencies to define and manage your workflows. Additionally, Cloud Composer offers robust features for triggering, monitoring, and logging, ensuring comprehensive control over your pipeline executions. Developing and executing workflows using Apache Airflow and Cloud Composer is easily done using Python. First, you leverage Apache Airflow operators to craft your directed acyclic graph, or DAG, defining the tasks and their dependencies. Next, the DAG is deployed to Cloud Composer, which handles the parsing and scheduling of your workflow. Cloud Composer further manages the execution of your tasks, incorporating features like error handling, retries, and monitoring to ensure smooth operation. With minimal effort, Cloud composer can be used to run a data analytics DAG. In the example code, the workflow retrieves a file from Cloud storage, loads it into BigQuery, and then performs a JOIN operation with an existing BigQuery table. The joined results are then inserted into a new BigQuery table. Finally, Dataproc is used for further data transformation.

#### Cloud Run Functions

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521366

Cloud Run functions allow you to execute code in response to various Google Cloud events. These events can originate from sources like HTTP requests, Pub/Sub messages, Cloud Storage changes, Firestore updates, or custom events through Eventarc. When triggered, a Cloud Run function provides a serverless execution environment where your code runs, supporting multiple programming languages for flexibility. Cloud Run functions easily automate routine tasks on Google Cloud. In the example code, a Dataproc workflow template is triggered after a file is uploaded to Cloud Storage. A Cloud Run function is used to capture the Cloud Storage new file event and call the Dataproc API. The Dataproc API then executes the specified workflow template, using the uploaded file as an input parameter. The final result of the workflow execution is stored in Cloud Storage.

#### Eventarc

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521367

Eventarc enables the creation of a unified event-driven architecture for loosely coupled services. Eventarc connects various event sources, including Google Cloud services, third-party systems, and custom events via Pub/Sub to a range of event targets like Cloud Run functions, and more. By using a standardized CloudEvent message format, Eventarc simplifies the integration of diverse systems and facilitates the development of responsive scalable applications. Eventarc enables deep monitoring of logging and other events which occur less frequently on Google Cloud. In the example code, Eventarc is used to trigger actions in response to data insertion events in BigQuery. When an insert operation occurs in a BigQuery table, it generates a Cloud Audit Log event. Eventarc can capture this event and initiate various actions such as rebuilding a dashboard, retraining an ML model, or executing any other custom action based on the specific requirements. In summary, there are various Google Cloud data-related automation options. Cloud Scheduler and Cloud Composer are suitable for scheduled or manual triggers, while Cloud Run functions and Eventarc are event-driven. Cloud Scheduler offers low coding effort with YAML, and Cloud Composer requires medium effort with Python. Cloud Run functions support multiple languages, while Eventarc is language agnostic. As a final note, all options except Cloud Composer are serverless.

#### Lab Intro: Use Cloud Run Functions to Load BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521368

In this lab, you create a Cloud Run function to load BigQuery. You create a Cloud Run function using the Cloud SDK. You then deploy and test the Cloud Run function. Finally, you view data in BigQuery and review Cloud Run function logs.

#### Use Cloud Run Functions to Load BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/labs/521369

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/quizzes/521370

### Course Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/video/521371

This concludes our course, Introduction to Data Engineering on Google Cloud. In this course, you learned about the duties and responsibilities of a data engineer, how to accomplish data engineering tasks, and core components used on Google Cloud to accomplish those tasks. How to create and deploy data pipelines of varying patterns on Google Cloud, and how to identify and utilize various automation techniques on Google Cloud to complete data engineering tasks. We hope you had a great learning experience.

#### Course Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/1157/documents/521372

### Your Next Steps

## 04: Modernizing Data Lakes and Data Warehouses with Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/54

### Introduction

#### Course series introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521373

Damon: Hello, and welcome to the Data Engineering on Google Cloud course series. I'm Damon, and I'm a technical curriculum developer at Google. Together with my fellow instructors, we look forward to showing you how to design data processing systems, build end to end data pipelines, analyze data, and implement machine learning. In addition to video lectures, you will also complete a series of hands-on labs. As part of the data engineering learning path, we will first discuss the differences between data lakes and data warehouses, the two key components of any data pipeline. This course highlights use cases for each type of storage, and dives into the available data lake and data warehouse solutions on Google Cloud in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a Cloud environment. Data pipelines typically fall under one of the extract load, extract load transform, or extract transform load paradigms. So the next course, building batch data pipelines, describes which paradigm should be used and when for batch data. Furthermore, it covers several technologies on Google Cloud for data transformation, including BigQuery, executing spark on data proc, pipeline graphs and data fusion, and serverless data processing with Dataflow. Processing streaming data is becoming increasingly popular as streaming enables organizations to get real time metrics on operations. So the third course covers how to build streaming data pipelines on Google Cloud. Pub Sub is the primary product for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Dataflow, and how to store or process records in BigQuery, or Big Table for analysis. Incorporating machine learning into data pipelines increases the ability of organizations to extract insights from their data. The final course covers several ways for machine learning to be included in data pipelines on Google Cloud depending on the level of customization required. For little to no customization, the course covers Auto ML. For more tailored machine learning capabilities, the course introduces notebooks and BigQuery machine learning. Also, the final course covers how to productionize machine learning solutions using Kubeflow.

#### Course introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521374

Damon: Welcome to modernizing data lakes and data warehouses with Google Cloud, the first course of the data engineering learning path. We'll start off by describing the role of a data engineer. We'll talk about a data engineer's clients and what the benefits of a successful data pipeline are for your organization. Also, we will explain why data engineering should be done in a Cloud environment. We'll concentrate on data lakes and data warehouses in this course, these are the two key components of any data pipeline. We'll describe the differences between data lakes and data warehouses, and highlight use cases for each type of storage. Also, we'll go into the available data lake and data warehouse solutions on Google Cloud in some technical detail. Finally, you'll get hands-on experience with data lakes and data warehouses by using quick labs.

### Introduction to Data Engineering

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521375

>> In this introduction to data engineering module, we'll describe the role of a data engineer and motivate the claim why data engineering should be done in the Cloud. A data engineer is someone who builds data pipelines. And so we'll start by looking at what this means, what kinds of pipelines a data engineer builds and their purpose. We'll look at the challenges associated with the practice of data engineering, and how many of those challenges are easier to address when you build your data pipelines in the Cloud. Next, we'll introduce you to BigQuery, Google Cloud's petabyte scale serverless data warehouse. Having defined what data lakes and data warehouses are, we'll then discuss these in more detail. Data engineers may be responsible for both the backend transactional database systems that support a company's applications, and the data warehouses that support their analytic workloads. In this lesson, we'll explore the differences between databases and data warehouses, and the Google Cloud's solutions for each of these workloads. Since a data warehouse also serves other teams, it's crucial to learn how to partner effectively with them. As part of being an effective partner, your engineering team will be asked to set up data access policies and overall governance of how data is to be used and not used by your users. We'll discuss how to provide access to the data warehouse while keeping to data governance best practices. We'll also discuss productionizing the whole operation and automating and monitoring as much of it as possible. Finally, we'll look at a case study of how a Google Cloud customer solved a specific business problem before you complete a hands-on lab where you will use BigQuery to analyze data.

#### The role of a data engineer

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521376

person: Let's start by exploring the role of a data engineer in a little more detail. What does a data engineer do? A data engineer builds data pipelines. Why does the data engineer build data pipelines? Because they want to get their data into a place, such as a dashboard or report or machine learning model, from where the business can make data-driven decisions. The data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is, by itself, not very useful. One term you will hear a lot when you do data engineering is the concept of a data lake. A data lake brings together data from across the enterprise into a single location. So you might get the data from a relational database or from a spreadsheet and store the raw data in a data lake. One option for this single location to store the raw data is to store it in a Cloud Storage bucket. What are the key considerations when deciding between data lake options? What do you think? There are some considerations that you need to keep in mind as you build a data lake. Does your data lake handle all the types of data you have? Can it all fit into a Cloud Storage bucket? If you have an RDBMS, you might need to put the data in Cloud SQL, a managed database, rather than Cloud Storage. Can it elastically scale to meet the demand? As your data collected increases, will you run out of disk space? This is more a problem with on-premises systems than with Cloud. Does it support high-throughput ingestion? What is the network bandwidth? Do you have edge points of presence? Is there fine-grained access control to objects? Do users need to seek within a file, or is it enough to get a file as a whole? Cloud Storage is blob storage, so you might need to think about the granularity of what you store. Can other tools connect easily? How do they access the store? Don't lose sight of the fact that the purpose of a data lake is to make data accessible for analytics. We mentioned our first Google Cloud product, the Cloud Storage bucket, which is a good option for staging all of your raw data in one place before building transformation pipelines into your data warehouse. Why choose Google Cloud Storage? Commonly, businesses use Cloud Storage as a backup and archival utility for their businesses. Because of Google's many data center locations and high network availability, storing data in a Cloud Storage bucket is durable and performant. For a data engineer, you will often use a Cloud Storage bucket as part of your data lake to store many different raw data files, such as CSV, JSON, or Avro. You could then load or query them directly from BigQuery as a data warehouse. Later in the course, you'll create Cloud Shell buckets using the Cloud console and command line, like you see here. Other Google Cloud products and services can easily query and integrate with your bucket once you've got it set up and loaded with data. Speaking of loading data, what if your raw data needs additional processing? You may need to extract the data from its original location, transform it, and then load it in. One option is to carry out data processing. This is often done using Dataproc or Dataflow. We'll discuss using these products to carry out batch pipelines later in this course. But what if batch pipelines are not enough? What if you need real-time analytics on data that arrives continuously and endlessly? In that case, you might receive the data in Pub/Sub, transform it using Dataflow, and stream it into BigQuery. We'll discuss streaming pipelines later in this course.

#### Data engineering challenges

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521377

>> Let's look at some of the challenges that a data engineer faces. As a data engineer, you'll usually encounter a few problems when building data pipelines. You might find it difficult to access the data that you need, you might find that the data, even after you access it, doesn't have the quality that's required by the analytics or machine learning model. You plan to build a model, and even if the data quality exists, you might find that the transformations require computational resources that might not be available to you. And finally, you might run into challenges around query performance, and being able to run all of the queries and all of the transformations that you need with the computational resources that you have. Let's take the first challenge of consolidating disparate datasets, data formats and managing access at scale. For example, you want to compute the customer acquisition cost, how much does it cost in terms of marketing and promotions and discounts to acquire a customer? That data might be scattered across a variety of marketing products and customer relationship management software. And finding a tool that can analyze all of this data might be difficult, because it might come from different organizations, different tools, and different schemas, and maybe some of that data is not even structured. So in order to find something as essential to your business as how much getting a new customer costs so that you can figure out what kind of discounts to offer to keep them from turning, you can't have your data exist in silos. So what makes data access so difficult? Primarily, this is because data in many businesses is siloed by departments, and each department creates its own transactional systems to support its own business processes. So for example, you might have operational systems that correspond to store systems, have a different operational system maintained by your product warehouses that manages your inventory, and have a marketing department that manages all the promotions given that you need to do an analytic query on, such as, give me all the in store promotions for recent orders and their inventory levels. You need to know how to combine data from the stores, from the promotions, and from the inventory levels. And because these are all stored in separate systems, some of which have restricted access, building an analytic system that uses all three of these datasets to answer an ad hoc query like this can be very difficult. The second challenge is that cleaning, formatting, and getting the data ready for insights requires you to build ETL pipelines. ETL pipelines are usually necessary to ensure data accuracy and quality. The cleaned and transformed data are typically stored not in a data lake but in a data warehouse. A data warehouse is a consolidated place to store the data and all the data are easily joinable and queryable. Unlike a data lake where the data is in the raw format, in the data warehouse, the data is stored in a way that makes it efficient to query. Because data becomes useful only after you clean it up, you should assume that any raw data that you collect from source systems need to be cleaned and transformed. And if you are transforming it, you might as well transform it into a format that makes it efficient to query. In other words, ETL the data and store it in a data warehouse. Let's say you're a retailer, and you have to consolidate data from multiple source systems. Think about what the use case is. Suppose the use case is to get the best performing in store promotions in France, you need to get the data from the stores and you have to get the data from the promotions. But perhaps the stored data is missing information. Maybe some of the transactions are in cash. And for those, perhaps there is no information on who the customer is. Or some transactions might be spread over multiple receipts, and you might need to combine those transactions because they come from the same customer. Or perhaps the timestamps of the products are stored in local time, whereas you have to spread across the globe. And so before you do anything, you need to convert everything into UTC. Similarly, the promotions may not be stored in the transaction database at all. They might be just a text file that somebody loads on their web page and has a list of codes that are used by the web application to apply discounts. It can be extremely difficult to do a query like finding the best performing in store promotions because the data has so many problems. Whenever you have data like this, you need to get the raw data and transform it into a form with which you can actually carry out the necessary analysis. It is obviously best if you can do this sort of cleanup and consolidation just once and store the resulting data to make further analysis easy. That's the point of a data warehouse. If you need to do so much consolidation and cleanup, a common problem that arises is where to carry out this compute. The availability of computation resources can be a challenge. If you're on an on-premises system, data engineers will need to manage server and cluster capacity and make sure that enough capacity exists to carry out the ETL jobs. The problem is that the compute needed by these ETL jobs is not constant over time. Very often it varies week to week, and depending on factors like holidays and promotional sales. This means that when traffic is low, you're wasting money, and when traffic is high, your jobs are taking way too long. Once your data is in your data warehouse, you need to optimize the queries your users are running to make the most efficient use of your compute resources. If you're managing an on-premise data analytics cluster, you will be responsible for choosing a query engine and installing the query engine software and keeping it up to date as well as provisioning any more servers for additional capacity. Isn't there a better way to manage server overhead so we can focus on insights?

#### Introduction to BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521378

There is a much better way to manage server overhead so we can focus on insights. It is to use a serverless data warehouse. BigQuery is Google Cloud’s petabyte-scale serverless data warehouse. You don’t have to manage clusters. Just focus on insights. The BigQuery service replaces the typical hardware setup for a traditional data warehouse. That is, it serves as a collective home for all analytical data in an organization. Datasets are collections of tables that can be divided along business lines or a given analytical domain. Each dataset is tied to a Google Cloud project. A data lake might contain files in Cloud Storage or Google Drive or even transactional data from Bigtable. BigQuery can define a schema and issue queries directly on external data as federated data sources. Database tables and views function the same way in BigQuery as they do in a traditional data warehouse, allowing BigQuery to support queries written in a standard SQL dialect which is ANSI: 2011 compliant. Identity and Access Management is used to grant permission to perform specific actions in BigQuery. This replaces the SQL GRANT and REVOKE statements that are used to manage access permissions in traditional SQL databases. A key consideration behind agility is being able to do more with less, and it’s important to make sure that you're not doing things that don’t add value. If you do work that is common across multiple industries, it's probably not something that your business wants to pay for. The cloud lets you, the data engineer, spend less time managing hardware and more time doing things that are much more customized and specific to the business. You don’t have to be concerned about provisioning and reliability and utilization improvements in performance or tuning on the cloud, so you can spend all your time thinking about how to get better insights from your data. You don't need to provision resources before using BigQuery, unlike many RDBMS systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. Storage resources are allocated as you consume them and deallocated as you remove data or drop tables. Query resources are allocated according to query type and complexity. Each query uses some number of slots, which are units of computation that comprise a certain amount of CPU and RAM.

#### Data lakes and data warehouses

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521379

We've defined what a data lake is and what a data warehouse is. Let's look at these in a bit more detail. Recall that we emphasized that the data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is by itself not very useful. We said that raw data gets replicated and stored in a data lake. In order to make that data usable, you will use Extract Transform Load or ETL pipelines to make the data usable and store this more usable data in a data warehouse. Let's consider what are the key considerations when deciding between data warehouse options? We need to ask ourselves these questions. The data warehouse is going to definitely serve as a sink, you're going to store data in it. But will it be fed by a batch pipeline or by a streaming pipeline? Does the warehouse need to be accurate up to the minute? Or is it enough to load data into it once a day or once a week? Will the data warehouse scale to meet my needs? Many cluster based data warehouses will set per cluster concurrent query limits. Will those query limits cause a problem? Will the cluster size be large enough to store and traverse your data? How is the data organized, cataloged and access controlled? Will you be able to share access to the data to all your stakeholders? What happens if they want to query the data? Who will pay for the querying? Is the warehouse designed for performance? Again, carefully consider concurrent query performance and whether that performance comes out of the box, or whether you need to go around creating indexes and tuning the data warehouse. Finally, what level of maintenance is required by your engineering team? Traditional data warehouses are hard to manage and operate. They were designed for a batch paradigm of data analytics and for operational reporting needs. The data in the data warehouse was meant to be used by only a few management folks for reporting purposes. BigQuery is a modern data warehouse that changes the conventional mode of data warehousing. Here, we can see some of the key comparisons between a traditional data warehouse and BigQuery. BigQuery provides mechanisms for automated data transfer, empowers business applications using technology that teams already know and use, so everyone has access to data insights. You can create read-only shared data sources that both internal and external users can query, and make query results accessible for anyone through user-friendly tools such as Looker, Google Sheets, Tableau, or Google Data Studio. BigQuery lays the foundation for AI, it's possible to train TensorFlow and Google Cloud Machine Learning models directly with datasets stored in BigQuery, and BigQuery ML can be used to build and train machine learning models with simple SQL. Another extended capability is BigQuery GIS, which allows organizations to analyze geographic data in BigQuery essential to many critical business decisions that revolve around location data. BigQuery also allows organizations to analyze business events real time as they unfold by automatically ingesting data and making it immediately available to query in their data warehouse. This is supported by the ability of BigQuery to ingest up to 100,000 rows of data per second and for petabytes of data to be queried at lightning fast speeds. Due to Google's fully managed serverless infrastructure and globally available network, BigQuery eliminates the work associated with provisioning and maintaining a traditional data warehousing infrastructure. BigQuery also simplifies data operations through the use of identity and access management to control user access to resources, creating roles and groups and assigning permissions for running jobs and queries in a project and also providing automatic data backup and replications. Even though we talked about getting data into BigQuery by running ETL pipelines, there is another option. That is to treat BigQuery as a query engine, and allow it to query the data in place. For example, you can use BigQuery to directly query database data in Cloud SQL, that is, managed relational databases like Postgres SQL and MySQL. You can also use and MySQL. You can also use BigQuery to directly query files on Cloud Storage as long as these files are in formats like CSV or Parquet. The real power comes when you can leave your data in place and still join it against other data in the data warehouse. Let's take a look.

#### Transactional databases versus data warehouses

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521380

Data engineers may be responsible for both the backend transactional database systems that support your company's applications AND the data warehouses that support your analytic workloads. In this lesson, you'll explore the differences between databases and data warehouses and the Google Cloud solutions for each workload. If you have SQL Server MySQL or PostgresSQL as your relational database, you can migrate it to Cloud SQL, which is Google Cloud's fully managed relational database solution. Cloud SQL delivers high performance and scalability with up to 64 terabytes of storage capacity, 60,000 IOPS and 624 gigabytes of RAM per instance. You can take advantage of storage auto-scale to handle growing database needs with zero downtime. One question you might get asked is: "Why not simply use Cloud SQL for reporting workflows? You can run SQL directly on the database, right?" This is a great question and will be answered in greater detail in the "Building a Data Warehouse" module. Google Cloud has several options for RDBMS's, including Spanner and Cloud SQL. When considering Cloud SQL, be aware that Cloud SQL is optimized to be a database for transactions or writes, and BigQuery is a data warehouse optimized for reporting workloads (mostly reads). The fundamental architecture of these data storage options is quite different. Cloud SQL databases are RECORD-based storage, meaning the entire record must be opened on disk. Even if you just selected a single column in your query. BigQuery is COLUMN based storage, which as you might guess, allows for really wide reporting schemas, since you can simply read individual columns out from disk. This isn't to say RDBMS's aren't as performant as Data Warehouses. They serve two different purposes. RDBMS helps your business manage new transactions. Take this point of sale terminal at a storefront. Each order and product is likely written out as new records in a relational database somewhere. This database may store all of the orders received from their website. All of the products listed in the catalog or the number of items in their inventory. This is so that when an existing order is changed, it can be quickly updated in the database. Transactional systems allow for a single row in a database table to be modified in a consistent way. They also are built on certain relational database principles like referential integrity to guard against cases like a customer ordering a product that doesn't exist in the product table. So where does all this raw data end up In our data lake and data warehouse discussion? What's the complete picture? Here it is. Our operational systems, like our relational databases that store online orders, inventory and promotions, are our raw data sources on the left. Note that this isn't exhaustive, you could have other source systems that are manual like CSV files or spreadsheets too. These upstream data sources get gathered together in a single consolidated location in our Data Lake, which is designed for durability and high availability. Once in the data lake, the data often needs to be processed via transformations that then output the data into our data warehouse, where it is ready for use by downstream teams. Here are three quick examples of other teams that often build pipelines on our data warehouse. An ML team may build a pipeline to get features for their models. An engineering team may be using our data as part of their data warehouse. And a BI team may want to build dashboards using some of our data. So who works on these teams and how do they partner with our data engineering team?

#### Partner effectively with other data teams

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521381

Since a data warehouse also serves other teams, it is crucial to learn how to partner effectively with them. Remember that once you’ve got data where it can be useful and it’s in a usable condition we need to add new value to the data through analytics and machine learning. What teams might rely on our data? There are many data teams that rely on your data warehouse and partnerships with data engineering to build and maintain new data pipelines. The three most common clients are: The machine learning engineer, The data or BI analyst, and Other data engineers. Let’s examine how each of these roles interacts with your new data warehouse and how data engineers can best partner with them. As you’ll see in our course on machine learning, an ML team’s models rely on having lots of high quality input data to create, train, test, evaluate, and serve their models. They will often partner with data engineering teams to build pipelines and datasets for use in their models. Two common questions you may get asked are … “How long does it take for a transaction to make it from raw data all the way into the data warehouse”? They’re asking this because any data that they train their models on must also be available at prediction-time as well. If there is a long delay in collecting and aggregating the raw data it will impact the ML team’s ability to create useful models. A second question that you will definitely get asked is how difficult it would be to add more columns or rows of data into certain datasets. Again, the ML team relies on teasing out relationships between the columns of data and having a rich history to train models on. You will earn the trust of your partner ML teams by making your datasets easily discoverable, documented, and available to ML teams to experiment on quickly. A unique feature of BigQuery is that you can create high-performing machine learning models directly in BigQuery using just SQL by using Bigquery ML. Here is the actual model code to CREATE a model, EVALUATE it, and then MAKE predictions. You’ll see this again in our lectures on machine learning later on. Other critical stakeholders are your business intelligence and data analyst teams that rely on good clean data to query for insights and build dashboards. These teams need datasets that have clearly defined schema definitions, the ability to quickly preview rows, and the performance to scale to many concurrent dashboard users. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine. BI Engine is a fast, in-memory analysis service that is built directly into BigQuery and available to speed up your business intelligence applications. Historically, BI teams would have to build, manage, and optimize their own BI servers and OLAP cubes to support reporting applications. Now, with BI Engine, you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and servers as a fast in-memory intelligent caching service that maintains state. One last group of stakeholders are other data engineers that rely on the uptime and performance of your data warehouse and pipelines for their downstream data lakes and data warehouses. They will often ask “How can you ensure that the data pipeline we depend on will always be available when we need it?” Or, “We are noticing high demand for certain really popular datasets. How can you monitor and scale the health of your entire data ecosystem?” One popular way is to use the built-in Cloud Monitoring of all resources on Google Cloud. Since Google Cloud Storage and BigQuery are resources, you can set up alerts and notifications for metrics like “Statement Scanned Bytes” or “Query Count” so you can better track usage and performance. Here are two other reasons why Cloud Monitoring is used. One Is for tracking spending of all the different resources used and two is for what the billing trends are for your team or organization. And lastly, you can use the Cloud Audit Logs to view actual query job information to see granular level details about which queries were executed and by whom. This is useful if you have sensitive datasets that you need to monitor closely. A topic we will discuss more next.

#### Manage data access and governance

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521382

>> As part of being an effective partner, your engineering team will be asked to set up data access policies and overall governance of how data is to be used and not used by your users. This is what we mean when we say a data engineer must manage the data. This includes critical topics such as privacy and security. What are some key considerations when managing certain datasets? Clearly communicating a data governance model for who should and should not have access. How is personally identifiable information like phone numbers or email addresses handled? And even more basic tasks like how will our end users discover the different datasets we have for analysis? One solution for data governance is the Cloud data catalog and the data loss prevention API. The data catalog makes all the metadata about your datasets available to search for your users. You group datasets together with tags, flag certain columns as sensitive et cetera. Why is this useful? If you have many different datasets with many different tables, which different users have different access levels to? The data catalog provides a single unified user experience for discovering those datasets quickly. No more hunting for specific table names and SQL first. Often used in conjunction with data catalog is the Cloud Data Loss Prevention API, or DLP API, which helps you better understand and manage sensitive data. It provides fast, scalable classification and reduction for sensitive data elements like credit card numbers, names, social security numbers, US and selected international identifier numbers, phone numbers and Google Cloud credentials.

#### Demo: Finding PII in your dataset with the DLP API

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521383

>> In this demo, we're going to use the Cloud Data Loss Prevention API to redact personally identifiable information like email addresses, phone numbers, that sort of thing, PII data, at scale. Again, find all these demos under the data engineering demos course folder in our public repository. This one is pretty short, we're just going to be using the web tool, and you can invoke the API and experiment with that yourself. So navigating to the web demo, I'm just going to copy that link. So imagine in BigQuery, you've got like email addresses or something like that inside of your data. How do we actually proactively identify it? So this demo will actually take you to a page that has just a tax code file inside of here. Inside of the file, it actually explains a little bit about the DLP API itself. You can have this, it identifies this, it just looks through here, this unstructured data, and it structures it and basically says, "Hey, I'm reading through here, yeah, it's all good. Hey, somebody put a phone number inside of their comments, I don't want to have this go to anybody." I'm going to actually flag that as very high likelihood that it's this type, it is a phone number, here is the string, here is where it is. And you can inform the model whether or not that's a good result or a poor result. So what we can do is we can hide that welcome text, and we can paste in our own examples where we know this is a personally identifiable information, and we can see if it catches it. Boom, immediately. Credit card number, very high. US driver's license, very high. Email address, part of the email address, you also have the domain and where the data is as well. So you imagine this is a trivial demo. This is just four lines here, but imagine your inherited dataset that is terabytes. How can you automatically and at scale, run through, identify it, and then also use the different components of the API to run hash functions or obfuscation functions to redact that data so that PII doesn't leak out there? All right, that's DLP, the API.

#### Build production-ready pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521384

Person: Once your data lakes and data warehouses are set up and your governance policy is in place, it's time to productionalize the whole operation and automate and monitor as much of it as we can. That's what we mean when we say productionalize the data process. It has to be an end-to-end and scalable data processing system. Your data engineering team is responsible for the health of the plumbing-- that is, the pipelines-- and ensuring that the data is available and up to date for analytic and ML workloads. Common questions that you should ask at this phase are: "How can we ensure pipeline health and data cleanliness?" "How do we productionalize these pipelines to minimize maintenance and maximize uptime?" "How do we respond and adapt to changing schemas and business needs?" And, "Are we using the latest data engineering tools and best practices?" One common workflow orchestration tool used by enterprises is Apache Airflow. Google Cloud has a fully-managed version of Airflow called "Cloud Composer." Cloud Composer helps your data engineering team orchestrate all the pieces to the data engineering puzzle that we have discussed so far and even more that you haven't come across yet. For example, when a new CSV file gets dropped into Cloud Storage, you can automatically have that trigger an event that kicks off a data processing workflow and puts that data directly into your data warehouse. The power of this tool comes from the fact that Google Cloud big data products and services have API endpoints that you can call. A Cloud Composer job can then run every night or every hour and kick off your entire pipeline from raw data to the data lake and into the data warehouse for you. We'll discuss workflow orchestration in greater detail in later modules, and you'll do a lab on Cloud Composer as well.

#### Google Cloud customer case study

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521385

Person: We have looked at a lot of different aspects of what a data engineer has to do. Let's look at a case study of how a Google Cloud customer solves a specific business problem. That will help tie all these different aspects together. Twitter has large amounts of data and they also have high-powered sales teams and marketing teams which, for a long time, did not have access to the data and couldn't use that data for carrying out the analysis that they wanted to be able to do. Much of the data was stored on Hadoop clusters that were completely overtaxed. So Twitter replicated some of that data from HDFS onto Cloud Storage, loaded it into BigQuery, and provided BigQuery to the rest of the organization. These were some of the most frequently requested data sets within Twitter, and they discovered that with ready access to the data, many people who were not data analysts are now analyzing data and making better decisions as a result. For more information, a link to the blog post is available in the PDF version of this content under "course resources."

#### Recap

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521386

Let’s summarize the major topics we covered so far in this introduction. Recall that your data sources are your upstream systems like RDBMS’ and other raw data that comes from your business in different formats. Data lakes -- your consolidated location for raw data that is durable and highly available. In this example our data lake is Cloud Storage. And data warehouses which are the end result of preprocessing the raw data in your data lake and getting it ready for analytic and ML workloads. You’ll notice a lot of other Google Cloud product icons here like batch and streaming data into your lake and running ML on your data. We’ll cover those topics in detail later in this course. A useful cheatsheet to bookmark as a reference is the “Google Cloud Products in 4 words or less” which is actively maintained on GitHub by our Google Developer Relations team. It’s also a great way to stay on top of new products and services that come out just by following the GitHub commits!

#### Lab Intro: Using BigQuery to do Analysis

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521387

>> Now it's time to practice analyzing data with BigQuery in your lab. In this lab, you will execute interactive queries in the BigQuery console and then combine and run analytics on multiple datasets.

#### Using BigQuery to do Analysis

- https://www.cloudskillsboost.google/paths/16/course_templates/54/labs/521388

#### Quiz: Introduction to Data Engineering

- https://www.cloudskillsboost.google/paths/16/course_templates/54/quizzes/521389

### Building a Data Lake

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521390

>> Welcome to the module on building a data lake. We'll start by revisiting what data lakes are, and discuss your data storage and options for extracting, transforming, and loading your data into Google Cloud. Then we'll do a deep dive into why Google Cloud Storage is a popular choice to serve as a data lake. Securing your data lake running on Cloud Storage is of paramount importance. We'll discuss the key security features that you need to know as a data engineer to control access to your objects. Cloud Storage isn't your only choice when it comes to storing data in a data lake on Google Cloud. So we'll look at the storing of different data types. Finally, we'll look at Cloud SQL, the default choice for OLTP or Online Transaction Processing Workloads on Google Cloud. You'll also do a hands-on lab where you'll practice creating a data lake for your relational data with Cloud SQL.

#### Introduction to data lakes

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521391

Let’s start with a discussion about what data lakes are and where they fit in as a critical component of your overall data engineering ecosystem. What is a data lake after all? It’s a fairly broad term, but it generally describes a place where you can securely store various types of data of all scales, for processing and analytics. Data lakes are typically used to drive data analytics, data science and ML workloads, or batch and streaming data pipelines. Data lakes will accept all types of data. Finally, data lakes are portable, on-premise or in the cloud. Here’s where Data Lakes fit into the overall data engineering ecosystem for your team. You have to start with some originating system or systems that are the source of all your data -- those are your data sources. Then, as a data engineer, you need to build reliable ways of retrieving and storing that data -- those are your data sinks. The first line of defense in an enterprise data environment is your data lake. Again it’s the central “give me whatever data at whatever volume, variety of formats, and velocity you got” I can take it. We’ll cover the key considerations and options for building a data lake in this module. Once your data is off the source systems and inside your environment -- generally considerable cleanup and processing is required to transform that data into a useful format for the business. It will then end up in your Data Warehouse. That’s our focus for the next module. What actually performs the cleanup and processing of data? Those are your data pipelines! They are responsible for doing the transformations and processing on your data at scale and bring your entire system to life with fresh newly processed data for analysis. An additional abstraction layer above your pipelines is what I will call your entire workflow. You will often need to coordinate efforts between many different components at a regular or event-driven cadence. While your data pipeline may process data from your lake to your warehouse, your orchestration workflow may be the one responsible for kicking off that data pipeline in the first place when it noticed that there was new raw data available from a source. Before we move into what cloud products can fit what roles, I want to leave you with an analogy that helps disambiguate these components. Picture yourself in the world of civil engineering for a moment. You’re tasked with building an amazing skyscraper in a downtown city. Before you break ground, you need to ensure you have all the raw materials that you’re going to need to achieve your end objective. Sure -- some materials could be sourced later in the project but let’s keep this example simple. The act of bringing the steel, the concrete, the water, the wood, the sand, the glass from wherever source elsewhere in the city onto your construction site is analogous to data coming from source systems and into your lake. Great! Now you have all these raw materials but you can’t use them as-is to build your building. You need to cut the wood and metal, measure and format the glass before it is suited for the purpose of building the building. The end-result, the cut glass, shaped metal, that is the formatted data that is stored in your data warehouse. It’s ready to be used to directly add value to your business -- which in our analogy is building the building. How did you transform these raw materials into useful pieces? On a construction site that’s the job of the worker. As you’ll see later when we talk about Data Pipelines, the individual unit behind the scenes is literally called a worker (which is just a Virtual Machine) and it takes some small piece of data and transforms it for you. What about the building itself? That’s whatever end-goal or goals you have for this engineering project. In the data engineering world, the shiny new building could be a brand new analytical insight that wasn’t possible before, or a machine learning model, or whatever else you want to achieve now that you have the cleaned data available. The last piece of the analogy is the orchestration layer. On a construction site you have a manager or a supervisor that directs when work is to be done and any dependencies. They could say “once the new metal gets here, send it to this area of the site for cutting and shaping, and then alert this other team that it’s available for building”. In the data engineering world that’s your orchestration layer or overall workflow. So you might say “Everytime a new piece of CSV data drops into this Cloud Storage bucket I want you to automatically pass it to our data pipeline for processing. AND, once it’s done processing, I want you -- the pipeline -- to stream it into the data warehouse. AND, once it’s in the data warehouse, I will notify the machine learning model that new cleaned training data is available for training and direct it to start training a new model version.” Can you see the graph of actions building? What if one step fails? What if you want to run that every day? You’re beginning to see the need for an orchestrator which, in our solutioning, will be Apache Airflow running on Cloud Composer later. Let’s bring back one example solution architecture diagram that you saw earlier in the course. The data lake here is Cloud Storage buckets right in the center of the diagram. It’s your consolidated location for raw data that is durable and highly available. In this example our Data Lake is a Cloud Storage but that doesn’t mean Cloud Storage is your only option for Data Lakes. Cloud Storage is one of a few good options to serve as a Data Lake. In other examples we will look at, BigQuery may be your Data Lake AND your Data Warehouse and you don’t use Cloud Storage buckets at all. This is why it’s so important to understand what you want to do first and then find which solutions best meet your needs. Regardless of which cloud tools and technologies you use -- your data lake generally serves as that single consolidated place for all your raw data. Think of it as a durable staging area. The data may end up in many other places like a transformation pipeline that cleans it up, moves it to the warehouse, and then it’s read by a machine learning model BUT it all starts with getting that data into your Lake first. Let’s do a quick overview on some of the core Google Cloud Big Data products that you need to know as a data engineer and will practice later in your labs. Here is a list of big data and ML products organized by where you would likely find them in a typical data processing workload from storing the data on the left, to ingesting it into your cloud-native tools for analysis, training machine learning models, and serving up insights. In this Data Lake module, we will focus on two of the foundational Storage products which will make up your Data Lake: Cloud Storage and Cloud SQL for your relational data. Later in the course, you will practice with Bigtable as well when you do high-throughput streaming pipelines. You may be surprised to not see BigQuery in the storage column. Generally BigQuery is used as a Data Warehouse -- what’s the core difference between a Data Lake and Data Warehouse then? A data lake is essentially the place where you capture every aspect of your business operations. Because you want to capture every aspect you tend to store the data in its natural raw format - - the format in which it is produced by your application so you may have a log file and the log files stored as is.. . in a data lake. You can basically store anything that you want and because you want to store it all…. .you tend to store these things as object blobs or files. The advantage of the data lake’s flexibility as a central collection point is also the problem. With a data lake, the data format is very much driven by the application that writes the data and it is in whatever format it is. The advantage of a data lake is that whenever the application gets upgraded it can start writing the new data immediately because it's just a capture of whatever raw data exists. So how do you take this flexible and large amount of raw data and do something useful with it? Enter the Data Warehouse. On the other hand a data warehouse is much more thoughtful. You might load the data into a data warehouse only after you have a schema defined and the use case identified. You might take the raw data that exists in a data lake, and transform it, organize it, process it, clean it up and then store it in a data warehouse. Why are you getting the data warehouse? Maybe because the data in the data warehouse is used to generate charts, reports, dashboards, and so on. The idea is that because the schema is consistent and shared across all of the applications, someone can go ahead and analyze the data and derive insights from it much faster. So a data warehouse tends to be structured and semi-structured data that is organized and placed in a format that makes it conducive for querying and analysis.

#### Data storage and ETL options on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521392

Next, let’s discuss your data storage and Extract, Transform, and Load options on Google Cloud. Your options on Google Cloud for building a Data Lake are your storage solutions you saw earlier. You’ve got Cloud Storage as a great catch-all, Cloud SQL and Spanner for Relational Data, and Firestore and Bigtable for NoSQL data. Choosing which option or options to use depends heavily on your use case and what you’re trying to build. In this lesson, we will focus on Cloud Storage and Cloud SQL, but you will see the NoSQL options like Bigtable later on in the course when we talk about very high-throughput streaming. So how do you decide on which path to take for your lake? The final destination for where your data lands on the cloud and the paths that you take to get your data to the cloud depends on where your data is now. How big your data is- this is the Volume component of the 3 Vs of Big Data. And ultimately where does it have to go? In architecture diagrams the ending point for the data is called a data sink. A common data sink after a data lake is your data warehouse. Don’t forget a critical thing to consider is how much processing and transformation your data needs before it is useful to your business. Now you may ask, do I complete the processing before I load it into my Data Lake or afterward before it gets shipped off somewhere else? Let’s talk about these patterns. The method that you use to load the data into the cloud depends on how much transformation is needed from the raw data that you have, to the final format you want it in. In this lesson, we will look at some of the considerations for the final format that you want it in. The simplest case might be that you have data in a format that is readily ingested by the cloud product that you want to store it in. Let's say for example you have your data in Avro format and you want to store the data in BigQuery because your use case fits what BigQuery is good at. Then what you do is simply E-L or Extract and Load. BigQuery will directly load Avro files. E-L refers to when data can be imported "as is" into a system. Examples include importing data from a database, where the source and the target have the same schema. One of the features that makes BigQuery unique is that -- as you saw before with the federated query example -- you may end up not even loading the data into BigQuery and still can query off of it. Avro, ORC, and Parquet files are all now supported for federated querying. The T in E-L-T is TRANSFORM. That’s when the data loaded into the cloud product isn’t in the final format you want it in. You may want to clean it up. Or maybe you want to transform the data in some way, for example if data needs to be corrected. In other words, you would extract from your on-premise system, load into the cloud product, and then do the transformation. That's an extract load and transform, or E-L-T. You tend to do this when the amount of transformation that's needed is not very high and the transformation will not greatly reduce the amount of data that you have. E-L-T allows raw data to be loaded directly into the target and transformed there. For example, in BigQuery, you could use SQL to transform the data and write a new table. The third option is extract, transform, and load - or E-T-L. This is the case when you want to extract the data, apply a bunch of processing to it, and then load it into the cloud product. This is usually what you pick when this transformation is essential or if this transformation greatly reduces the data size, so by transforming the data before loading it into the cloud you might be able to greatly reduce the network bandwidth that you need. If you have your data in some binary proprietary format, and you need to convert it before loading, then you need E-T-L as well. E-T-L is a data integration process in which transformation takes place in an intermediate service before it is loaded into the target. For example, the data might be transformed in a data pipeline like Dataflow before being loaded into BigQuery.

#### Build a data lake using Cloud Storage

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521393

Person: Cloud Storage is the essential storage service for working with data, especially unstructured data in the cloud. Let's do a deep dive into why Google Cloud Storage is a popular choice to serve as a data lake. Data in Cloud Storage persists beyond the lifetime of VMs or clusters, i.e. it is persistent. It is also relatively inexpensive compared to the cost of compute. So, for example, you might find it more advantageous to cache the results of previous computations in Cloud Storage. Or if you don't need an application running all the time, you might find it helpful to save the state of your application into Cloud Storage and shut down the machine it is running on when you don't need it. Cloud Storage is an object store, so it just stores and retrieves binary objects without regard to what data is contained in the objects. However, to some extent, it also provides file system compatibility and can make objects look like and work as if they were files so you can copy files in and out of it. Data stored in Cloud Storage will basically stay there forever. In other words, it is durable, but is available instantly. It is strongly consistent. You can share data globally, but it is encrypted and completely controlled and private if you want it to be. It is a global service, and you can reach the data from anywhere. In other words, it offers global availability. But the data can also be kept in a single geographic location if you need that. Data is served up with moderate latency and high throughput. As a data engineer, you need to understand how Cloud Storage accomplishes these apparently contradictory qualities, and when and how to employ them in your solutions. A lot of Cloud Storage's amazing properties have to do with the fact that it is an object store, and other features are built on top of that base. The two main entities in Cloud Storage are buckets and objects. Buckets are containers for objects, and objects exist inside of buckets and not apart from them. So buckets are containers for data. Buckets are identified in a single global unique namespace. So that means once a name is given to a bucket, it cannot be used by anyone else unless and until that bucket is deleted and the name is released. Having a global namespace for buckets simplifies locating any particular bucket. When a bucket is created, it is associated with a particular region or with multiple regions. Choosing a region close to where the data will be processed will reduce latency. And if you are processing the data using cloud services within the region, it will save you on network egress charges. When an object is stored, Cloud Storage replicates the object. It monitors the replicas, and if one of them is lost or corrupted, it replaces it with a fresh copy. This is how Cloud Storage gets many 9s of durability. For a multi-region bucket, the objects are replicated across regions. And for a single-region bucket, the objects are replicated across zones. In any case, when the object is retrieved, it is served up from the closest replica to the requester, and that is how low latency occurs. Multiple requesters could be retrieving the objects at the same time from different replicas, and that is how high throughput is achieved. Finally, the objects are stored with metadata. Metadata is information about the object. Additional Cloud Storage features use the metadata for purposes such as access control, compression, encryption and life cycle management. For example, Cloud Storage knows when an object was stored and it can be set to automatically delete after a period of time. This feature uses the object metadata to determine when to delete the object. You may have a variety of storage requirements for a multitude of use cases. Cloud Storage offers different classes to cater for these requirements, and these are based on how often data is accessed. Standard storage is best for data that is frequently accessed, also referred to as hot data, and/or stored for only brief periods of time. When used in a region, colocating your resources maximizes the performance for data-intensive computations and can reduce network charges. When used in a dual region, you still get optimized performance when accessing Google Cloud products that are located in one of the associated regions, but you also get the improved availability that comes from storing data in geographically separate locations. When used in a multi-region, standard storage is appropriate for storing data that is accessed around the world, such as serving website content, streaming videos, executing interactive workloads or serving data supporting mobile and gaming applications. Nearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline storage is a better choice than standard storage in scenarios where slightly lower availability, a 30-day minimum storage duration and costs for data access are acceptable trade-offs for lowered at-rest storage costs. Nearline storage is ideal for data you plan to read or modify on average once per month or less. Nearline storage is appropriate for data backup, long-tail multimedia content and data archiving. Coldline storage is a very low-cost, highly durable storage service for storing infrequently accessed data. Coldline storage is a better choice than standard storage or nearline storage in scenarios where slightly lower availability, a 90-day minimum storage duration and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs. Coldline storage is ideal for data you plan to read or modify at most once a quarter. Archive storage is the lowest-cost, highly durable storage service for data archiving, online backup and disaster recovery. Archive storage has higher costs for data access and operations as well as a 365-day minimum storage duration. Archive storage is the best choice for data that you plan to access less than once a year. For example, cold data storage such as data stored for legal or regulatory reasons, and disaster recovery. Cloud Storage is unique in a number of ways. It has a single API, millisecond data access latency, and 11 9s durability across all storage classes. Cloud Storage also offers object life cycle management, which uses policies to automatically move data to lower-cost storage classes as it is accessed less frequently throughout its life. Cloud Storage uses the bucket name and object name to simulate a file system. This is how it works. The bucket name is the first term in the URI. A forward slash is appended to it, and then it is concatenated with the object name. The object name allows the forward slash character as a valid character in the name. The very long object name with forward slash characters in it looks like a file system path, even though it is just a single name. In the example shown, the bucket name is declass. The object name is de/modules/02/script.sh. The forward slashes are just characters in the name. If this path were in a file system, it would appear as a set of nested directories beginning with declass. Now for all practical purposes, it works like a file system, but there are some differences. For example, imagine that you wanted to move all the files in the 02 directory to the 03 directory inside the modules directory. In a file system, you would have actual directory structures, and you would simply modify the file system metadata so that the entire move is atomic. But in an object store simulating a file system, you would have to search through all the objects contained in the bucket for names that had 02 in the right position in the name. Then you would have to edit each object name and rename them using 03. This would produce apparently the same result, moving the files between directories. However, instead of working with a dozen files in a directory, the system had to search over possibly thousands of objects in the bucket to locate the ones with the right names and change each of them. So the performance characteristics are different. It might take longer to move a dozen objects from directory 02 to directory 03 depending on how many other objects are stored in the bucket. During the move, there will be list inconsistency, with some files in the old directory and some in the new directory. A best practice is to avoid the use of sensitive information as part of bucket names because bucket names are in a global namespace. The data in the buckets can be kept private if you need it to be. Cloud Storage can be accessed using a file access method. That allows you, for example, to use a copy command from a local file directly to Cloud Storage. Use the tool [Indistinct] to do this. Cloud Storage can also be accessed over the web. The site, storage.cloud.google.com, uses TLS HTTPS to transport your data, which protects credentials as well as data in transit. Cloud Storage has many object management features. For example, you can set a retention policy on all objects in the bucket. For example, the objects should expire after 30 days. You can also use versioning so that multiple versions of an object are tracked and available if necessary. You might even set up life cycle management to automatically move objects that haven't been accessed in 30 days to nearline, and after 90 days to coldline.

#### Secure Cloud Storage

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521394

Person: Securing your data lake running on Cloud Storage is of paramount importance. We'll discuss the key security features you need to know as a data engineer to control access to your objects. Cloud Storage implements two completely separate but overlapping methods of controlling access to objects, IAM policy and access control lists. IAM is standard across the Google Cloud. It is set at the bucket level and applies uniform access rules to all objects within a bucket. Access control lists can be applied at the bucket level or on individual objects, so it provides more fine-grained access control. The IAM controls are as you would expect. IAM provides project roles and bucket roles, including bucket reader, bucket writer and bucket owner. The ability to create or change access control lists is an IAM bucket role. And the ability to create and delete buckets and to set IAM policy is a project level role. Custom roles are available. Project level viewer, editor and owner roles make users members of special internal groups that give them access by being members of bucket roles. See the online documentation for details. When you create a bucket, you are offered the option of disabling access lists and only using IAM. Access lists are currently enabled by default. This choice used to be immutable, but now you can disable access lists even if they were in-force previously. As an example, you might give some bob@example.com reader access to a bucket through IAM, and also give them write access to a specific file in that bucket through access control lists. You can give such permissions to service accounts associated with individual applications as well. All data in Google Cloud is encrypted at rest and in transit. There is no way to turn this encryption off. The encryption is done by Google using encryption keys that we manage, Google-managed encryption keys, or GMEK. We use two levels of encryption. First, the data is encrypted using a data encryption key. Then the data encryption key itself is encrypted using a key encryption key, or KEK. The KEKs are automatically rotated on a schedule, and the current KEK stored in Cloud KMS, Cloud Key Management Service. You don't have to do anything. This is automatic behavior. If you want to manage the KEK yourself, you can. Instead of Google managing the encryption key, you can control the creation and existence of the KEK that is used. This is called customer-managed encryption keys, or CMEK. You can avoid Cloud KMS completely and supply your own encryption and rotation mechanism. This is called CSEK, or customer-supplied encryption keys. Which data encryption option you use depends on business, legal and regulatory requirements. Please talk to your company's legal counsel. The fourth encryption option is client-side encryption. Client-side encryption simply means that you have encrypted the data before it is uploaded and have to decrypt the data yourself before it is used. Cloud Storage still performs GMEK, CMEK or CSEK encryption on the object. It has no knowledge of the extra layer of encryption you may have added. Cloud Storage supports logging of data access, and these logs are immutable. In addition to Cloud audit logs and Cloud Storage access logs, there are various holds and locks that you can place on the data itself. For audit purposes, you can place a hold on an object, and all operations that could change or delete the object are suspended until the hold is released. You can also lock a bucket, and no changes or deletions can occur until the lock is released. Finally, there is the lock retention policy previously discussed, and it continues to remain in effect and prevent deletion, whether a bucket lock or object hold are in-force or not. Data locking is different from encryption. Where encryption prevents someone from understanding the data, locking prevents them from modifying the data. There are a whole host of special use cases supported by Cloud Storage. For example, decompressive coding. By default, the data you upload is the same data you get back from Cloud Storage. This includes gzip archives, which usually are returned as gzip archives. However, if you tag an object properly in metadata, you can cause Cloud Storage to decompress the file as it is being served. Benefits of the smaller compressed file are faster upload and lower storage costs compared with the uncompressed files. You can set up a bucket to be requester pays on access. Normally, if data is accessed from a different region, you will have to pay network egress charges, but you can make the requester pay so that you pay only for data storage. You can create a signed URL to anonymously share an object in Cloud Storage, and even have the URL expire after a period of time. It is possible to upload an object in pieces and create a composite object without having to concatenate the pieces after upload. There are a lot of useful features in Cloud Storage, but we have to move on.

#### Store all sorts of data types

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521395

As highlighted before, Cloud Storage isn’t your only choice when it comes to storing data on Google Cloud. You don’t want to use Cloud Storage for transactional workloads. Even though the latency of Cloud Storage is low, it is not low enough to support high-frequency writes. For transactional workloads, use Cloud SQL or Firestore depending on whether you want to use SQL or No-SQL. You also don’t want to use Cloud Storage for analytics on structured data. If you do that, you will spend a significant amount of compute parsing data -- it is better to use Bigtable or BigQuery for analytics workloads on structured data, depending on the latency required. We keep talking about transactional versus analytics workloads. What exactly do we mean? Transactional workloads are ones where you require fast inserts and updates. You want to maintain a snapshot, a current state of the system. The tradeoff is that queries tend to be relatively simple and tend to affect only a few records. For example, in a banking system, depositing your salary to your account is a transaction. It updates the balance field. The bank is doing online transaction processing or O-L-T-P. An analytics workload, on the other hand, tends to read the entire dataset and is often used for planning or decision support. The data might come from a transaction processing system, but it is often consolidated from many O-L-T-P systems. For example, a bank regulator might require us to provide a report of every customer who transferred more than $10,000 to an overseas account. They might ask the bank to include customers who try to transfer the $10,000 in smaller chunks over the period of a week. A report like this will require scanning through a significantly large dataset and require a complex query that involves aggregating over moving time windows. This is an example of online analytical processing or an O-LAP workload. The reason we treat these use cases differently is that transactional systems are write heavy. These tend to be operational systems. For example, a retailer’s catalog data will require updating every time the retailer adds a new item or changes the price. The inventory data will need to be updated every time the retailer sells an item. This is because the catalog and inventory systems have to maintain an up-to-the-moment snapshot of the business. Analytical systems can be periodically populated from the operational systems. We could use this once a day to generate a report of items in our catalog whose sales are increasing, but whose inventory levels are low. Such a report will have to read a bunch of data, but not have to write much. O-LAP systems are read-focused. Recall what we said. Analytical systems can be periodically populated from the operational systems. Data engineers build the pipelines to populate the O-LAP system from the OLTP system. One simple way might be to export the database as a file and load it into the data warehouse. This is what we called E-L. On Google Cloud, the data warehouse tends to be BigQuery. There is a limit to the size of the data that you can directly load to BigQuery. This is because your network might be a bottleneck. Rather than load the data directly into BigQuery, it can be much more convenient to first load it to Cloud Storage and load from Cloud Storage to BigQuery. Loading from Cloud Storage will also be faster because of the high throughput it offers. Getting back to the discussion on transactional workloads, you have a few options for relational databases. The default choice here is Cloud SQL, but if you require a globally distributed database, then use Spanner. You’d want a globally distributed database if your database will see updates from applications running in different geographic regions. The True Time capability of Spanner is very appealing for this kind of use case. Another reason you might choose Spanner is if your database is too big to fit into a single Cloud SQL instance. If our database is many gigabytes, you need a distributed database. The scalability of Spanner is very appealing for this use case. Other than that, you’d use Cloud SQL because it is more cost-effective. For analytics workloads, the default choice is BigQuery. However, if you require high-throughput inserts, more than millions of rows per second, or if you require low latency, on the order of milliseconds, use Bigtable. Other than that, you’d use BigQuery because it is more cost-effective.

#### Cloud SQL as a relational data lake

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521396

Cloud SQL, we said, is the default choice for OLTP (or online transaction processing) workloads on Google Cloud. Let’s take a quick look. Cloud SQL is an easy-to-use service that delivers fully managed relational databases. Cloud SQL lets you hand off to Google the mundane, but necessary and often time-consuming tasks—like applying patches and updates, managing backups, and configuring replications—so you can put your focus on building great applications. Cloud SQL is our managed service for third party RDBMSs. It supports MySQL, PostgreSQL, and Microsoft SQL Server and additional RDBMSs will be added over time. What this means is that we provide a compute engine instance that has MySQL already installed. We’ll manage the instance on your behalf. We’ll do backups, we'll do security updates, and update the minor versions of the software so that you don't have to worry about that. In other words, Google Cloud manages the MySQL database to the point where you can treat it as a service. We even do DBA-like things. You can tell us to add a failover replica for your database. We’ll manage it for you, and you’ll have a 99.95% availability SLA. Another benefit of Cloud SQL instances is that they are accessible by other Google Cloud services and even external services. You can use Cloud SQL with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. You can authorize Compute Engine instances to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might be used to, like SQL Workbench, Toad and other external applications using standard MySQL drivers. One of the advantages of Google managing your database is that you get the benefits of Google security. Cloud SQL customer data is encrypted when on Google's internal networks and when stored in database tables, temporary files, and backups. Every Cloud SQL instance includes a network firewall, allowing you to control network access to your database instance by granting access. Cloud SQL is easy to use: it doesn't require any software installation or maintenance. And Google manages the backups. Cloud SQL takes care of securely storing your backed-up data and makes it easy for you to restore from a backup and perform a point-in-time recovery to a specific state of an instance. Cloud SQL retains up to 7 backups for each instance, which is included in the cost of your instance. You can vertically scale Cloud SQL -- just increase your machine size. Scale up to 64 processor cores and more than 100 GB of RAM. Horizontally, you can quickly scale out with read replicas. Google Cloud SQL supports three read replica scenarios: Cloud SQL instances replicating from a Cloud SQL primary instance Cloud SQL instances replicating from an external primary instance External MySQL instances replicating from a Cloud SQL primary instance If you need horizontal read-write scaling, consider Spanner. For the special case of failover, Cloud SQL supports this. Cloud SQL instances can be configured with a failover replica in a different zone in the same region. Then, Cloud SQL data is replicated across zones within a region for durability. In the unlikely event of a data center outage, a Cloud SQL instance will automatically become available in another zone. All changes made to data on the primary are replicated to failover. If the primary instance’s zone has an outage, Cloud SQL automatically fails over to the replica. If the primary has issues not caused by a zone outage, failover doesn’t occur. You can, however, initiate failover manually. There are a few caveats: Note that the failover replica is charged as a separate instance. When a zonal outage occurs and your primary fails over to your failover replica, any existing connections to the instance are closed. However, your application can reconnect using the same connection string or IP address; you do not need to update your application after a failover. After the failover, the replica becomes the primary, and Cloud SQL automatically creates a new failover replica in another zone. If you located your Cloud SQL instance to be near other resources, such as a Compute Engine instance, you can relocate your Cloud SQL instance back to its original zone when the zone becomes available. Otherwise, there is no need to relocate your instance after a failover. You can use the failover replica as a read replica to offload read operations from the primary. We keep saying that Cloud SQL is fully managed. We have also used the word serverless to describe BigQuery, for example. What’s the difference? As a fully managed service, Cloud SQL provides you access much like you have with an on-premises installation. For instance, you can directly connect to the Cloud SQL instance through the gcloud client and perform tasks directly through SQL. However, Google helps you manage the instance by automating backups, setting up failover instances, and so on. Serverless is the next step up. You can treat a serverless product as just an API that you are calling. You pay for using the product, but don’t have to manage any servers. BigQuery is serverless. So are Pub/Sub for asynchronous messaging and Dataflow for parallel data processing. You can think of Cloud Storage as being serverless as well. Sure, Cloud Storage uses disks, but you never actually interact with the hardware. One of the unique things about Google Cloud is that you can build a data processing pipeline of well-designed components all of which are fully serverless. Dataproc, on the other hand, is fully managed. It helps you run Spark and Hadoop workloads without having to worry about setup. Given the choice between doing a brand-new project on BigQuery or Dataflow (which are serverless), and Dataproc (which is fully managed), which one should you choose? All other things being equal, choose the serverless product.

#### Lab Intro: Loading Taxi Data into Google Cloud SQL

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521397

Person: In this next lab, you practice creating a data lake and bringing in all of your relational data from outside the cloud into a Google Cloud SQL-hosted environment. Specifically, you'll first create a Google Cloud SQL instance which can hold multiple databases. After the instance is up, you'll create a new Cloud SQL database, and then import some text data into Cloud SQL. Lastly, you'll check that data for integrity. Good luck.

#### Loading Taxi Data into Google Cloud SQL 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/54/labs/521398

#### Quiz: Building a Data Lake

- https://www.cloudskillsboost.google/paths/16/course_templates/54/quizzes/521399

### Building a Data Warehouse

#### Module Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521400

Person: Hello and welcome to the Building a Data Warehouse module. This is the third module in the course, Modernizing Data Lakes and Data Warehouses with Google Cloud. We'll start by describing what makes a modern data warehouse. We'll also talk about what distinguishes a data lake from an enterprise data warehouse. Then we're going to introduce BigQuery, a data warehouse solution on Google Cloud. Once you're familiar with the basics of BigQuery, we'll talk about how BigQuery organizes your data and then how to load new data into BigQuery. You'll also have the opportunity to load data into BigQuery through a hands-on lab. Finally, we'll dive into the world of data warehouse schemas. We'll talk about efficient data warehouse schema design and take a closer look at BigQuery support for nested and repeated fields and why this is such a popular schema design for enterprises. You'll get some experience working with JSON and array data in BigQuery through a hands-on lab. We'll end by discussing how you can optimize the tables in your data warehouse with partitioning and clustering.

#### The modern data warehouse

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521401

>> Let's start by describing what makes a modern data warehouse. An enterprise data warehouse should consolidate data from many sources. If you recall from the previous module, a data lake does something very similar. The key difference between the two is the word consolidate. A data warehouse imposes a schema. A data lake is just raw data, but an enterprise data warehouse brings the data together and makes it available for querying and data processing. To use a data warehouse, an analyst needs to know the schema of the data. However, unlike for a data lake, the analyst doesn't have to write code to read and parse the data. Another reason to consolidate all your data besides standardizing the format and making it available for querying is making sure the query results are meaningful. You want to make sure the data is clean, accurate and consistent. The purpose of a data warehouse is not to store data. That's the purpose of a data lake. If you have raw data that you want to keep around but not necessarily query, don't bother with cleaning and streamlining it, leave it in a data lake. All data in a data warehouse should be available for querying. It's important to ensure that those queries are quick. You don't want people waiting hours or days for results. We described an enterprise data warehouse and how it's different from a data lake. What makes a data warehouse modern? Businesses' data requirements continue to grow, you want to make sure the data warehouse can deal with datasets that don't fit into memory. Typically, this is gigabytes to terabytes of data, but occasionally can be petabytes. You don't want separate warehouses for different datasets. Instead, you want a single data warehouse that can scale from gigabytes to petabytes of data. Second, you want the data warehouse to be serverless and fully no-ops. You don't want to be limited to clusters that you need to maintain, or indexes that you need to fine-tune. Removing these responsibilities will allow data analysts to carry out ad hoc queries faster, which is important because you want the data warehouse to increase the speed at which your business makes decisions. Next, your data warehouse is not productive if it allows you to do queries but doesn't support rich visualization and reporting. Ideally, your data warehouse can seamlessly plug into whichever visualization or reporting tool your business is most familiar with. Similarly, because the data warehouse requires clean and consistent data, you will often have to build data pipelines to bring data into the warehouse. The modern data warehouse should be able to integrate with an ecosystem of processing tools for building ETL pipelines. Your data pipeline should be capable of constantly refreshing data in the warehouse in order to keep it up to date. You need to be able to stream data into the warehouse and not rely on batch updates. Also, predictive analytics is becoming increasingly important for data analysts. As a result, a modern data warehouse has to support machine learning without moving the data out of the warehouse. Last but not least, in a modern data warehouse, it should be possible to impose enterprise grade security like data exfiltration constraints. It should also be possible to share data and queries with collaborators.

#### Introduction to BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521402

In this lesson, we’re going to introduce BigQuery, a data warehouse solution on Google Cloud. BigQuery has many capabilities that make it an ideal data warehouse. When we talked about a modern data warehouse, we talked about having the warehouse be able to scale from gigabytes to petabytes seamlessly. We talked about being able to do ad hoc queries and no-ops. BigQuery cost-effectively handles large, petabyte-scale datasets for storage and querying. In fact it’s similar to the cost of Cloud Storage. This enables you to store your data without having to worry about archiving off older data to save on storage. Unlike traditional data warehouses, BigQuery has features like GIS and machine learning built in. It also provides capabilities to stream data in, so you can analyze your data in near real time. Because it’s part of Google Cloud, you get all of the security benefits the cloud provides while also being able to share datasets and queries. BigQuery supports standard SQL queries and is compatible with ANSI SQL 2011. BigQuery is a serverless fully-managed service, which means that the BigQuery engineering team takes care of updates and maintenance for you. Upgrades don’t require downtime or hinder system performance. For example, data aging and expiration can be a cumbersome operation in traditional data warehouses. In BigQuery, you just supply a table expiration flag at the time of table creation or update a table to add this feature. The table will automatically expire when it reaches that age or duration. Many traditional systems require resource-intensive vacuum processes to run at various intervals to reshuffle and sort data blocks and recover space. BigQuery has no equivalent of the vacuum process, because the storage engine continuously manages and optimizes how data is stored and replicated. Also, because BigQuery doesn't use indexes on tables, you don't need to rebuild these. The bottom line is that you can free up real work hours by not having to worry about common database management tasks. BigQuery is implemented in two parts: a storage engine … … and an analytic engine as illustrated. The separation of compute and storage is a common theme in Google Cloud and works effectively because of Google’s petabit network called Jupiter. Jupiter allows blazing fast communication between compute and storage. BigQuery data is physically stored on Google's distributed file system, called Colossus, which ensures durability by using erasure encoding to store redundant chunks of the data on multiple physical disks. Moreover, the data is replicated to multiple data centers. You don't need to provision resources before using BigQuery, unlike many RDBMS systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. Storage resources are allocated as you consume them and deallocated as you remove data or drop tables. Query resources are allocated according to query type and complexity. Each query uses some number of slots, which are units of computation that comprise a certain amount of CPU and RAM. So what makes BigQuery fast? BigQuery tables are column-oriented, compared to traditional RDBMS tables which are row-oriented. Row-oriented tables are efficient for making updates to data contained in fields. For OLTP systems, row-oriented tables are necessary because OLTP systems have frequent updates. Analytics is slow on row-oriented tables because queries have to read all the fields in a row and, depending on the kind of indexing or key, queries may have to read extra rows and fields to find the information that is requested. BigQuery, however, is an O-LAP system. It’s meant for analytics. BigQuery tables are immutable and are optimized for reading and appending data. BigQuery tables are not optimized for updating. BigQuery leverages the fact that most queries involve few columns, so it only reads the columns required for the query. BigQuery is very efficient in this sense and is the reason tables are column-oriented. Every table has a schema. You can enter the schema manually through the Cloud Console, or by supplying a JSON file. BigQuery is implemented using a microservice architecture, so there are no virtual machines to configure and maintain. Under the hood, analytics throughput is measured in BigQuery slots. A BigQuery slot is a unit of computational capacity required to execute SQL queries. BigQuery automatically calculates how many slots are required at each stage in a query, depending on size and complexity. A BigQuery slot is a combination of CPU, memory, and networking resources. It also includes a number of supporting technologies and sub-services. Note that each slot doesn’t necessarily have the same specification during query execution. Some slots may have more memory than others, or more CPU or more I/O. On the right hand side you can see how multiple slots work together under the hood to execute a query. You can imagine a slot operates here the same way as a worker node in a cluster for distributed processing of big data. When executing a query, BigQuery may split it up into one or multiple stages that contain different tasks to do. Tasks are assigned to workers to perform the work in parallel. In this example, we have 2 stages. In the first one, the workers will pick up a subset of the data they need to work on from BigQuery’s storage, then they apply a specific filter to their data (coming from the WHERE clause) and do a partial count of the data that is left (which is specified as COUNT() in the SELECT clause). Each worker will send its intermediate result to stage 2, where one worker will create the final result set by summing up all the counts it received by the previous workers. This result set is then presented to us in the UI. We will look at this again in our module around BigQuery performance. If a single, simple query is submitted that needs fewer slots than are available, the query will generally execute faster.

#### Demo: Querying TB of data in seconds

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521403

>> Welcome to the Big Data demo using BigQuery on Google Cloud Platform. Here, we're going to show the serverless scaling features of BigQuery, how it scales up automatically behind the scenes without your intervention to query large datasets. We're going to talk about 10 billion rows of Wikipedia data. So the first things first, we're going to follow the demo scripts, all these demos in the code that I'm going to be running, everything I'm going to be running is inside of our demos folder in our public repository. So first up, we're actually going to copy the query on the clipboard, and navigate to- no, I don't want to search for it- navigate to BigQuery. Inside the Google Cloud Platform, I already have BigQuery open, but if you needed to navigate to it, navigation, I have it pinned up here. It's kind of like starring it, but if you scroll all the way down, under big data, you have BigQuery, and to promote things, you don't have to continuously scroll and search, I just pin them. So I'm often using AI platform notebooks for machine learning work, composer for data engineering work, BigQuery for data analysis work. Once you're inside of BigQuery, I'm going to paste in our query in the query editor window. And you notice where you're getting datasets from, where your data actually lives is under resources. And one of the very popular public datasets that's available is Wikipedia. So inside of- it's one of many, so you can actually get airline data for flights, Reddit data, geographic data, and Wikipedia benchmark for very, very large datasets. If you were given a script, one of my favorite hotkeys that you can choose is you can actually hold down, it's- on a Mac, its command key. On a Windows, I think it's Ctrl, and that'll actually highlight- or I think it's the Windows key- that'll highlight all the tables inside of your query, and it turns them into buttons. So if you clicked on this, you automatically get back to the schema. So it's a great way to iterate between what are the columns, and what are the details and the preview of the data versus the query results as well. So again, that's just that cool hotkey, all the shortcuts that I mentioned are available, if you open up that modal, you will get the shortcuts there as well. So 10 billion rows, is this really 10 billion rows? The fastest way we can find that out is in the details. It is just about a gigabyte. Yeah, there we go. 10 billions, 600 and million rows of Wikipedia data. What type of data are we talking about? What are we actually going to be querying here? The schema is not too wide. It is the year, month and day, the Wikimedia project, and the language that it's in and the title of the Wikipedia page and how many views it has. And it's just a lot of rows. So what are we going to do? What type of operation are we going to do? Well, you can see our query, when we're going to run it, it's going to go through 10 billion rows, which is about 415 gigabytes of data. Let's see how fast it does that. It's going to return not only just columns, but it's going to do a calculation. So it's basically to say, give me the language that Wikipedia page was written in, give me the title of that page, give me the total number of views, where somewhere in the title of any of these articles the name Google was featured and it has to be a capital G. Okay, SQL is case sensitive, I'll show you how to ignore that in just a second with a function. And of course, anytime you're doing aggregations, you need to group by, and we want to have the pages that have Google somewhere in the title, the top pages of by view count first, which I'm assuming is just going to be a page called Google. But let's go ahead and run this. How long does it take to process 400 gigabytes? And we're running. And again, you're not a DBA, you're not managing the indexes or anything like that, you just have your SQL query. It's not even our dataset, we're just using somebody else's dataset. And you can see how long it took for- when I recorded this video, we got 10 seconds, 415 gigabytes processing, and here's your inside. So it reached out and it found out 10 billion rows, 10 billion pages of Wikipedia data that's stored here, looked into- a like is a rather expensive operation, it's got to not only look at the columns, it's going to look into that string value, find the word Google will somewhere appears anywhere within there. The wildcard character percentage sign is any characters before, any characters after, and sum up those total views, and it did that pretty quickly. So in total, there are 214,000 pages with Google somewhere in the name, the most popular pages is the English page for Google, the Spanish page for Google shortly after that, and then Google Earth, Google Maps, and then Chrome as well. Now, of course, if you wanted to make this not case sensitive, one of the things that you could do is you could do, say, I wanted to wrap the title and everything is going to be uppercase, and then you would have to do this as well. So you just match like for like. So if you're doing wildcard operators using Like, it's a good idea to use upper or lower, or if you're experienced with regex, you can do that as well. So that is 10 billion. And you can see what the really cool thing behind the scenes is on the execution details, you can see how it actually did this. So it took you, the human, while you're watching it, 10 seconds, you're just watching it. Behind the scenes, it took all of the computers, if you were to do it serially, linearly, stack all the computers, all the work that they did, it would be two hours and 38 minutes for one computer to do it, essentially. But that's the beauty of distributed parallel processing that happened behind the scenes. These, you don't have to care about how many virtual machines were spun up to do this work, but in aggregate, they did about three- almost three hours of work automatically, and they shared a lot of data in between themselves as well. And you can see the process of going from those 10 billion records, all the way down after the aggregations to outputting the result that you see there. All right, that's cool. 10 billion. Let's see if we can do 100 billion. So let's see, if we have a dataset, I think it's literally just adding another zero, why not? Why not go bigger, right? And again, if you want to get back to that dataset, I'm going to hotkey it. We have more information here? Yeah, we do, we got the title. I think it's largely the same schema, details. Okay, cool. We got a real big dataset, we got six terabytes, water records, same principle, expensive operation, we're going to go into every single field. How long do you think it's going to process to take to go through 100 billion records, open up every single title, and then see whether or not somewhere in that title is a string of the letters Google. Once it's got that result, it has to take that and all of its friends of the other 100 billion, or those that match, and then sum them all together. So the virtual machines have to communicate with each other when they're doing aggregations, that's where that shuffling step comes into play. Let's see how much data it's going to process. So less than a minute, just over 30 seconds, it went through 4.1 terabytes of data, and it gave us the result there. And you can see almost a full day of computing, if you're going to be doing that just on a single machine. And you don't even- it doesn't even tell you how many machines were there behind the scenes. So that slot time is a phenomenally interesting metric that just shows you the scale. You waited 31 seconds, behind the scenes, you don't even have to manage them, we're using 24 hours, essentially of compute, boom, just like that. And when you don't need any more, obviously, you're not paying for those machines, you're just paying for the bytes of data that were processed. All right, that's the demo of BigQuery at scale.

#### Get started with BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521404

Now that you’re familiar with the basics of BigQuery, it’s time to talk about how BigQuery organizes your data. BigQuery organizes data tables into units called datasets. These datasets are scoped to your Google Cloud project. When you reference a table from the command line in SQL queries or in code, you refer to it by using the construct: project.dataset.table. What are some reasons to structure your information into projects, datasets, and tables? These multiple scopes —project, dataset, and table— can help you structure your information logically. You can use multiple datasets to separate tables pertaining to different analytical domains, and you can use project-level scoping to isolate datasets from each other according to your business needs. Also, as we will discuss later, you can align projects to billing and use datasets for access control. You store data in separate tables based on logical schema considerations. In the queries we saw earlier, we wrote the query in SQL and selected Run on the UI. What this did was to submit a QueryJob to the BigQuery service. The BigQuery query service is separate from the BigQuery storage service. However, they are designed to collaborate and be used together. In this case, we were querying native tables in the bigquery-public-data project. Querying native tables is the most common case, and is the most performant way to use BigQuery. BigQuery is most efficient when working with data contained in its own storage service. The storage service and the query service work together to internally organize the data to make queries efficient over huge datasets of terabytes and petabytes in size. The query service can also run query jobs on data contained in other locations, such as tables in CSV files hosted in Cloud Storage. So you can query data in external tables or from external sources without loading it into BigQuery. These are called federated queries. In either case, the query service puts the results into a temporary table and the user interface pulls and displays the data in the temporary table. This temporary table is stored for 24 hours, so if you run the exact same query again, and if the results would not be different, then BigQuery will simply return a pointer to the cached results. Queries that can be served from the cache do not incur any charges. It is also possible to request that the query job write to a destination table. In that case, you get to control when the table is deleted. Because the destination table is permanent, and not temporary, you will get charged for the storage of the results. To calculate pricing, you can use BigQuery's query validator in combination with the pricing calculator for estimates. The query validator provides an estimate of the size of data that will be processed during a query. You can plug this into the calculator to find an estimate of how much running the query will cost. You can separate cost of storage and cost of queries. By separating projects A and B, it’s possible to share data without giving access to run jobs. In this diagram, Users 1 and 2 have access to run jobs and access the datasets in their own respective Projects. If they run a query, that job is billed to their own project. What if User 1 needs the ability to access Dataset D in Project B? The person who owns Project B can allow User 1 to query Project B Dataset D and the charges will go to Project A when executed from Project A. The public dataset project owner granted all authenticated users access to use their data. The special setting allAuthenticatedUsers makes a dataset public. Authenticated users must use BigQuery within their own project and have access to run BigQuery jobs so that they can query the Public Dataset. The billing for the query goes to their project, even though the query is using public or shared data. In summary, the cost of a query is always assigned to the active project from where the query is executed. The active project for a user is displayed at the top of the Cloud console or set by an environmental variable in the Cloud Shell or client tools. The project is what the billing is associated with. For example, if you queried a table that belongs to the 'bigquery-public-data' project, the storage costs are billed to that data project. To run a query, you need to be logged in to the Cloud console. You will run a query in your own Google Cloud project and the query charges are billed to your project, not to the public data project. In order to run a query in a project, you need Identity Access Management permission to submit a job. Remember that running a query means that you must be able to submit a query job to the service. Access control is through IAM and is at the dataset, table/view, or column level. In order to query data in a table or view, you need at least read permissions on the table or view. Like Cloud Storage, BigQuery datasets can be regional or multi-regional. Regional datasets are replicated across multiple zones in the region. As with Cloud Storage, BigQuery storage encrypts data at rest and over the wire using Google-managed encryption keys. It’s also possible to use customer-managed encryption keys. Authentication is through IAM, so it’s possible to use Gmail addresses or Google Workspace accounts for this task. Access control is through IAM roles and involves giving permissions. We discussed two of those in read access and the ability to submit query jobs. However, many other permissions are possible. Remember that access control is at the level of datasets, tables, views, or columns. When you provide access to a dataset, either read or write, you provide access to all the tables in that dataset. Logs in BigQuery are immutable and are available to be exported to Cloud Operations. Admin activities and system events are all logged. An example of a system event is table expiration. If, when creating a table, you configure it to expire in 30 days, at the end of 30 days a system event will be generated and logged. You will also get immutable logs of every access that happens to a dataset under your project. BigQuery provides predefined roles for controlling access to resources. You can also create custom IAM roles consisting of your defined set of permissions, and then assign those roles to users or groups. You can assign a role to a Google email address or to a Google Workspace Group. An important aspect of operating a data warehouse is allowing shared but controlled access against the same data to different groups of users. For example, finance, HR, and marketing departments all access the same tables, but their levels of access differ. Traditional data warehousing tools make this possible by enforcing row-level security. You can achieve the same results in BigQuery with access control to datasets, tables, views, or columns, or by defining authorized views and row-level permissions. Sharing access to datasets is easy. Traditionally, onboarding new data analysts involved significant lead time. To enable analysts to run simple queries, you had to show them where data sources resided and set up ODBC connections and tools and access rights. Using Google Cloud, you can greatly accelerate an analyst's time to productivity. To onboard an analyst on Google Cloud, you grant access to relevant project(s), introduce them to the Cloud console and BigQuery web UI, and share some queries to help them get acquainted with the data. The Cloud console provides a centralized view of all assets in your Google Cloud environment. The most relevant asset to data analysts might be Cloud Storage buckets, where they can collaborate on files. The BigQuery web UI presents the list of datasets that the analyst has access to. Analysts can perform tasks in the Cloud console according to the role you grant them, such as viewing metadata, previewing data, executing, and saving and sharing queries. When you provide read access to a dataset to a user, every table in that dataset is readable by that user. What if you want more fine-grained control? In addition to access controls at the table or column level, you can use views. In this example, we are creating a view in Dataset B, and the view is a subset of the table data in Dataset A. Now, by providing users with access to Dataset B, we are creating an authorized view that is only a subset of the original data. Note that you cannot export data from a view, and dataset B has to be in the same region or multi-region as dataset A. A view is a SQL query that looks like and has properties similar to a table. You can query a view just like you query a table. BigQuery supports materialized views as well. These are views that are persisted so that the table does not need to be queried every time the view is used. BigQuery will keep the materialized view refreshed and up to date with the contents of the source table. Giving view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying source data. With column-level security you can achieve a similar use case, so that you can define when to show the content of the column, or when to hide or obfuscate it. You will define a Policy Tag in BigQuery and assign the corresponding users/groups to it. These users will now be able to see the column’s content. You can also define data masking rules and assign them to the Policy Tag and users/groups, so that the content is obfuscated / nullified / or transformed using your own custom logic. If a user/group isn’t included into the Policy Tag’s definition, they won’t be able to query the column at all. In BigQuery, row-level security involves the creation of row-level access policies on a target BigQuery table. This policy then acts as a filter to hide or display certain rows of data, depending on whether a user or group is in an allowed list. An authorized user, with the IAM roles BigQuery Admin or BigQuery DataOwner, can create row-level access policies on a BigQuery table. When you create a row-level access policy, you specify the table by name, and which users or groups (called the grantee-list) should have access to certain row data. The policy includes the data on which you wish to filter, called the filter_expression. The filter_expression functions like a WHERE clause in a typical query. In this example, users in the group:apac can only see partners from the APAC region. In BigQuery, materialized views periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and whenever possible reads only delta changes from the base table to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables. Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base table. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries.

#### Load data into BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521405

Next, we’ll talk about how to load new data into BigQuery. Recall from an earlier module that the method you use to load data depends on how much transformation is needed. E-L, or Extract and Load, is used when data is imported as-is where the source and target have the same schema. E-L-T, or Extract, Load, Transform, is used when raw data will be loaded directly into the target and transformed there. E-T-L, or Extract, Transform, Load, is used when transformation occurs in an intermediate service before it is loaded into the target. You might say that the simplest case is E-L. If the data is usable in its original form, there’s no need for transformation. Just load it. You can batch load data into BigQuery. In addition to CSV, you can also use data files with delimiters other than commas by using the field_delimiter flag. BigQuery supports loading gzip compressed files. However, loading compressed files isn't as fast as loading uncompressed files. For time-sensitive scenarios or scenarios in which transferring uncompressed files to Cloud Storage is bandwidth- or time-constrained, conduct a quick loading test to see which alternative works best. Because load jobs are asynchronous, you don't need to maintain a client connection while the job is being executed. More importantly, load jobs don't affect your other BigQuery resources. A load job creates a destination table if one doesn't already exist. BigQuery determines the data schema as follows: If your data is in Avro format, which is self-describing, BigQuery can determine the schema directly. If the data is in JSON or CSV format, BigQuery can auto-detect the schema, but manual verification is recommended. You can specify a schema explicitly by passing the schema as an argument to the load job. Ongoing load jobs can append to the same table using the same procedure as the initial load, but do not require the schema to be passed with each job. If your CSV files always contain a header row that should be ignored after the initial load and table creation, you can use the skip_leading_rows flag to ignore the row. For details, see the documentation on BigQuery load flags. BigQuery sets daily limits on the number and size of load jobs that you can perform per project and per table. In addition, BigQuery sets limits on the sizes of individual load files and records. You can launch load jobs through the BigQuery web UI. To automate the process, you can set up Cloud Functions to listen to a Cloud Storage event that is associated with new files arriving in a given bucket and launch a BigQuery load job. BigQuery can import data stored in the JSON file format, as long as it is newline delimited. It can also import files in Avro, Parquet, and ORC format. The most common import is with CSV files, which are the bridge between BigQuery and spreadsheets. BigQuery can also directly import Firestore and Datastore export files. Another way that BigQuery can import data is through the API. Basically, any place where you can get code to run, can theoretically insert data into BigQuery tables. You could use the API from a Compute Engine instance, a container on Kubernetes, App Engine, or from Cloud Functions. However, you would have to recreate the data processing foundation in these cases. In practice, the API is mainly used from either Dataproc or Dataflow. The BigQuery Data Transfer Service provides connectors and pre-built BigQuery load jobs that perform the transformations necessary to load report data from various services directly into BigQuery. Cloud Storage can be useful in the E-L process. You can transfer files to Cloud Storage in the schema that is native to the existing on-premises data storage and then load those files into BigQuery. BigQuery is a managed service, so you don't have the overhead of operating, maintaining or securing the system. A typical Data Warehouse system requires a lot of code for coordination and interfacing. You can get BigQuery Data Transfer Service running without coding. The core of BigQuery Data Transfer Service is scheduled and automatic transfers of data from wherever it is located (in your data center, on other clouds, in SaaS services) in to BigQuery. Transfering the data is only the first part of building a data warehouse. If you were assembling your own system, you would need to stage the data so that it can be cleaned (data quality), and transformed (E-L-T, extract, load, transform), and processed (put into its final and stable form). A common issue with Data Warehouse systems is late arriving data. For example, a cash register closes late and does not report its daily receipts during the scheduled transfer period. To complete the data, you would need to detect that not all of the data was received, and then request the missing data to fill in the gap. This is called "data backfill" and it is one of the automatic processes provided by BigQuery Data Transfer Service. "Backfilling data" means adding missing past data to make a dataset complete with no gaps and to keep all analytic processes working as expected. Use the data transfer service for repeated, periodic, scheduled imports of data directly from Software as a Service systems into tables in BigQuery. The BigQuery Data Transfer Service provides connectors, transformation templates, and the scheduling. The connectors establish secure communications with the source service and collect standard data, exports, and reports. This information is transformed within BigQuery. The transformations can be quite complicated, resulting in from 25 to 60 tables. And the transfer can be scheduled to repeat as frequently as once a day. The BigQuery Data Transfer Service can also be used to efficiently move data between regions. Notice that you don't need Cloud Storage buckets. BigQuery Data Transfer Service runs BigQuery jobs that transform reports from SaaS sources into BigQuery Tables and Views. Google offers several connectors, including Campaign Manager, Cloud Storage, Amazon S3, Google Ad Manager, Google Ads, Google Play transfers, YouTube channel, YouTube content owner, Teradata migration, and over 100 other connectors through partners. It is a common practice to automate execution of queries based on a schedule or event and cache the results for later consumption. You can schedule queries to run on a recurring basis. Scheduled queries must be written in standard SQL, which can include Data Definition Language and Data Manipulation Language statements. The query string and destination table can be parameterized, allowing you to organize query results by date and time. By maintaining a complete 7-day history of changes against your tables, BigQuery allows you to query a point-in-time snapshot of your data. You can easily revert changes without having to request a recovery from backups. This slide shows how to do a SELECT query to query the table as of 24 hours ago. Because this is a SELECT query, you can do more than just restore a table. You can join against some other table or correct the value of individual columns. You can also do this using the BigQuery command-line tool as shown in the second snippet. Here, we’re restoring data as of 120 seconds ago. You can recover a deleted table only if another table with the same ID in the dataset has not been created. In particular, this means you cannot recover a deleted table if it is being streamed to. Chances are that the streaming pipeline would have already created an empty table and started pushing rows into it. Also be careful using “CREATE OR REPLACE TABLE” because this makes the table irrecoverable. Keep in mind if your data transformations are simple enough, you may be able to do them with just SQL. If Cloud Storage is part of your workflow, you can load files from Cloud Storage into staging tables in BigQuery first, and then transform the data into the ideal schema for BigQuery by using BigQuery SQL commands. BigQuery supports standard DML statements such as insert, update, delete, and merge. There are no limits on D-M-L statements. However you should not treat BigQuery as an O-L-T-P system. The underlying infrastructure is not structured to perform optimally as an O-L-T-P. There are other more appropriate products on Google Cloud for such workloads. BigQuery also supports DDL statements like CREATE OR REPLACE TABLE. In the example on this slide, the replace statement is used to transform a string of genres into an ARRAY. We’ll cover ARRAYs in greater detail later in the course. Lastly, what if your transformations went beyond what functions were currently available in BigQuery? Well, you can create your own! BigQuery supports user-defined functions, or UDF. A UDF enables you to create a function using another SQL expression or an external programming language. JavaScript is currently the only external language supported. We strongly suggest you use Standard SQL though, because BigQuery can optimize the execution of SQL much better than it can for JavaScript. UDFs allow you to extend the built-in SQL functions. UDFs take a list of values, which can be ARRAYs or STRUCTs, and return a single value, which can also be an ARRAY or STRUCT. UDFs written in JavaScript can include external resources, such as encryption or other libraries. Previously, UDFs were temporary functions only. This meant you could only use them for the current query or command-line session. When you create a UDF, BigQuery persists it and stores it as an object in your database. What this means is you can share your UDFs with other team members or even publically if you wanted to. The BigQuery team has a public GitHub repo for common User Defined Functions at the link you see here.

#### Lab Intro: Loading Data into BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521406

>> Now it's time to get some hands-on experience with a lab. In this lab, you're going to practice loading data into BigQuery. The primary objective of this lab is to load data into BigQuery using both the command line interface and the Cloud Console. You'll also get experience loading several datasets into BigQuery and using the data description, language or DDL.

#### Loading data into BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/labs/521407

#### Explore schemas

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521408

>> Now let's dive into the world of Data Warehouse schemas. Designing efficient schemas that scale is a core job responsibility of any data engineering team. BigQuery hosts many public datasets and schemas for you to explore on popular topics like daily weather readings, taxi cab logs, health data, and more. Let's explore some of these public dataset schemas using SQL.

#### Demo: Exploring Schemas

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521409

>> Welcome back to another BigQuery demo. In this one, it's a little bit of a meta demo, meaning we're going to be using a lot of BigQuery metadata that's stored inside of BigQuery native storage, and some unique functions like information schema, and tables to explore the metadata of a dataset that's given to you. Why is this important? Well, as a data engineer, you're often given a dataset or tables within a dataset or multiple datasets as part of a GCP project, and you'd have a very short amount of time to figure out a lot of key information about those tables. How many tables are there? How many columns are there? Are any of those columns partitioned or clustered columns? What's the data size? When were the tables last updated? And yes, you can go in and click on the UI and individual places inside of BigQuery to find that information, but it wouldn't be amazing to use SQL to query information about the metadata that already exists inside of BigQuery. So we're going to do that with a couple quick queries here, but first, we need a dataset. As usual, we're going to be going to the BigQuery public datasets and getting some information there. So I'm going to copy the first query inside of there. And inside of BigQuery, we're going to paste the query. Now, if you notice, I have my project BigQuery public data, and then I have a table. I'm just using- there's many datasets inside of BigQuery public data. I'm using the baseball dataset, which has multiple tables. If you notice, I'm using this suffix underscore underscore tables, which is interesting. So if you hold down the Command key on your Macs, if you have a Mac, or use a shortcut there for the Windows, you'll see that for tables, if I'm just bringing this up, it has some interesting metadata. So it has your project, your dataset, your table ID and it has some short but useful information about when the table was created, when it was last modified or updated, how many rows it has, you don't need to do this select count star, what's the size of the table in bytes, and whether or not it's a table or a view. I think table is one, view is two. Now, I already have the- the whole point of this query is because this is unreadable to me, it's just milliseconds from the dawn of time or Unix epoch or something like that. And the size in bytes, I can't do the conversion in my head. So that's literally all the rest of my query does here for you, is it just as a little bit of rounding from bytes to gigabytes, from the millisecond timestamp to the actual readable timestamp. So actually running that query will just translate those results into something that's a little bit more readable. So we have three tables inside of our baseball dataset, we have games wide, games post wide, and some baseball schedules. You can see the largest table here is about two gigabytes, and it has 761,000 rows. I can infer without much analysis that there are over almost 1,700,000 baseball games that we can have analysis on, immediately right off the bat, no select star queries or anything like that. All right, so that's just information about it, and again, you can just replace baseball with say, New York. And then boom, you can get all the information about the New York data tables that are in there, a lot more tables. And you can see that there is a Yellow Taxi Cab trips, 311 service requests for New Yorkers, a lot of motor vehicle collisions. So New York has a lot of tables in that dataset for BigQuery public data. And you can see the row count in there as well, large, very large datasets. But literally, you're just plugging in your project, your dataset, and you get a lot of metadata about the tables. So it's comparing information across tables very easily. So now if we're going to drill in to the columns of data, how many columns of data are present in one of those tables? Again, I'm just going to be using baseball, for example, but you can replace that with one that's interesting to you. Here, we're using information schema dot columns. Executing that, let's get all that data. All right, here we go. So we have the table, which is baseball, that's the dataset it belongs to, the table name is games wide. Don't forget, we also have a couple more tables in this dataset, we're not doing a Where clause filter. So we have 306 columns in total. So no kidding, that's a very wide table. But hopefully, you'll be able to see games wide, there should be two other tables, games post wide, and there was another one there as well. So if you wanted to do a filter for just a single table, you could see, let's just do a quick Where clause filter, where the actual table name is just this one singular table, games wide. And then we'll see just simply by row count, this table has 145 columns in it. Wow, that is a wide table. No kidding. And then the ordering position of the columns, whether or not it's allowed to be No, the data type, is it a generative column? Is it hidden? Here's really interesting, as a data engineer, is this a- is there a partitioning on that table itself? And if there is partitioning, is there clustering as well, which is super interesting. So that's literally what this next query is going to do for us, is just a very simple Where clause basically to say, "Hey, for performance reasons, one of the easiest things that you can do is just have a partitioned and clustered columns in very large datasets if your use case warrants it." Let's see if there are any in this particular dataset. So I'm going to run this. And it's going to look again filtering on the partition column. And then no. So it's very easy, look through all of the different tables within that baseball dataset, and then you got nothing. Let's see if the New York one has any in it as well. No, no partitioned, no clustered columns there. So again, some immediate insight that you can give back is, "Hey, let's explore the benefit of partitioned or clustered columns." All right, the last query that I'm going to show you here is, you can basically get metadata across the different datasets just by doing a simple union all. I've chosen about 10 or 15 interesting ones from the BigQuery public datasets. And I want to see, we looked at the first table, three tables inside of the baseball dataset, I want to basically list all the tables from all these different datasets. And I want to order it by the dataset that has the table with the most rows. So let's see which table out of all of these has the most row, just pasting in that query. I think it'll probably be those taxi transactions. So let's see. Getting all that metadata together- Oh, I was wrong. Wow. So we have- and again, you can change this with the timestamp seconds query that I had a little bit earlier. It's actually the GitHub repository, their files inside of GitHub, and the row count- Wow, can I buy a comma? - is 2.3- is that billion? Yeah, 2.3 billion. And you can see the size that are on disk there as well. So it's GitHub, GitHub, GitHub. Wikipedia is up there. It's only number two, though. And then here's my New York taxi cabs there as well. And again, this is just unioning together, unioning again inside of SQLs mashing together rows vertically, whereas joins is joining together columns horizontally, and it just mashes them together here as well. Lastly, I'll show you a bonus query, I'm not going to run it for you, though, is if you wanted to, you want to view all the datasets within your given GCP project, actually, a lot, I will run this for you because it looks pretty cool. I'm going to copy this. And I want to see, let's see if- data to insights, this is a public- at least the tables within here are public. Let's see if we can- So all I'm doing is basically saying, if somebody just gives you the project name, just the project name, you want to query all the datasets they're within. It looks like yeah, we don't have- it's permissions denied. So we can use our own project. I have a couple of tables inside of here, so I'm copying the project name. Hopefully, I'm admin for my own project. And essentially, what you're going to see is I've just a bunch of example datasets from previous demos. And you'll see from, for example, you'll have all of the different datasets returned, and then within there, all the tables and views inside of them. And then let's see which ones have been modified most recently, last modified time descending. So let's go ahead and run that. Again, you would just replace your project name inside of there as well. So the use case is somebody gives you a GCP account, and you're like, "Well, I really want to just look at what came up inside of here." All right, cool. So we have, it looks like I was doing some machine learning stuff on movie recommendations data most recently. Yep. And I actually get the dataset description, and it's been around for 39 days last modified in there as well. So you can get a general sense for all of the- and this is- I think this is only ones that actually have a description for it- all of the different dataset values and table values within a given project, assuming you have access to query it. And that's what information schema dot schemata does, and you can get schemata options. A little bit more advanced but that's also there for you as well. A really interesting use case is all the way at the bottom, let me get this on screen. All the way at the bottom, this link right here will show you, hey, if I wanted to recreate- check this out- if I weren't to recreate all the tables in my dataset, so for example, if I wanted to just have something that says, hey, I have a staging environment or a production environment, I need to recreate all these data tables, or at least the schema for them to populate them with data. And then doing the Create or replace statements, if I have 100 different datasets here, it's awful. Is there a way to programmatically create that? And there is. So you can actually say, the pre-canned query that's given to you actually uses a Create function, and it respects partitioning and it actually does all of this stuff for you. And it can catch together those Create or Replace table names and the list of the columns. And if it has partitioning and clustering and options like a description, it will all do that for you. So for example, the output looks like this, you want to get a single dot SQL file that will recreate all of your projects and all of your datasets. Boom, here you go. There's the table one, my dataset population by zip. Boom, there's the table two, this is the GitHub commits or something like that as well. Why is this useful? Well, it's really a good idea to track your schema changes over time, like if your column definitions change. So generally what I would do on a production project is generate this, check this into version control somewhere, and anytime there's changes to my schema, I would regenerate this and then check it into version control so I can see how my schema has evolved over time. A little bit more advanced use case but that is out there for you as well.

#### Schema design

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521410

>> Next we will talk about efficient data warehouse schema design. Take a look at the original data table here and the normalized data tables which contain the same data. The data in the original table is organized visually as you might have used merge cells or columns in a spreadsheet. But if you had to write an algorithm to process the data, how might you approach it? Access could be by rows, by columns, by rows then columns, and the different approaches would perform differently based on the query. Also, your method might not be parallelizable. The original data can be interpreted and stored in many ways in a database. normalizing the data means turning it into a relational system. This stores the data efficiently and makes query processing a clear and direct task. Normalizing increases the orderliness of the data, it is useful for saving space. Many people with database experience will recognize this procedure. Normalizing data usually happens when a schema is designed for a database. Denormalizing is the strategy of allowing duplicate field values for a column in a table in the data to gain processing performance. Data is repeated rather than being relational. Flattened data takes more storage, but the flattened non-relational organization makes queries more efficient because they can be processed in parallel using columnar processing. Specifically, denormalizing data enables BigQuery to more efficiently distribute processing among slots resulting in more parallel processing and better query performance. You would usually denormalize data before loading it into BigQuery. However, there are cases where denormalizing data is bad for performance. Specifically, if you have to group by a column with a one to many relationship. In the example shown, order ID is such a column. In this example, to group the data, it must be shuffled. That often happens by transferring the data over a network between servers or systems. shuffling is slow. Fortunately, BigQuery supports a method to improve the situation. BigQuery supports columns with nested and repeated data. In this example, a denormalized flattened table is compared with one that has been denormalized and the schema takes advantage of nested and repeated fields. Order ID is a repeated field. Because this is declared in advance, BigQuery can store and process the data respecting some of the original organization in the data. Specifically, all order details for each order are co-located, which makes retrieval of the whole order more efficient. For this reason, nested and repeated fields are useful for working with data that originates in relational databases. Nested columns can be understood as a form of repeated field. It preserves the relational qualities of the original data and schema while enabling columnar and parallel processing of the repeated nested fields. It is the best alternative for data that already has a relational pattern to it. Turning the relation into a nested or repeated field improves BigQuery Performance. Nested and repeated fields help BigQuery work with data source in relational databases. Look for nested and repeated fields whenever BigQuery is used in a hybrid solution in conjunction with traditional databases.

#### Nested and repeated fields

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521411

>> Let's take a closer look at BigQuery's support for nested and repeated fields and why this is such a popular schema design for enterprises. I'll illustrate by using an example from a real business running on Google Cloud. GoJek is a company in Indonesia that is well known for its ride booking service. And they process over 13 petabytes of data on BigQuery per month from queries to support business decisions. What kind of decisions? For GoJek, they track whenever a new customer places an order, like hail a ride with their mobile app, that order is stored in an orders table. Each order has a single pickup location and drop off destination. For a single order, you could have one or many events like ride ordered, ride confirmed, drive on route, drop off complete, et cetera. As a data engineer, how would you efficiently store these different pieces of data in your data warehouse? Keep in mind, you need to support a large user base querying petabytes per month. Well, as you saw earlier, we could store one fact in one place with the normalization route, which is typical for relational systems. Or we could go the fully denormalized route and just store all levels of granularity in a single big table, where you would have one order ID like 123 repeated in a row for each event that happens on that order. Faster for querying, sure, but what are the drawbacks? For relational schemas, normalized schemas, often the most intensive computational workloads are joins across very large tables. Remember, RDBMSs are record based, so they have to open each record entirely, and pull out the join key from each table where a match exists. And that's assuming you know all the tables that need to be joined together. Imagine for each new piece of information about an order like promotion codes, or user information, and you could be talking about 10 plus table join. The alternative has different drawbacks. Pre-joining all your tables into one massive table makes reading data faster, but you now have to be really careful if you have data at different levels of granularity. In our example, each row would be at the level of granularity of a specific event like driver confirmed for a given order. What does that mean for an order ID like 123? It is duplicated for each event in that order. Imagine if you're looking to join higher level information like the revenue per order, and you now have to be exceedingly careful with aggregations to not double or triple count your duplicate order IDs. See the problem? One common solution in enterprise data warehouse schemas is to take advantage of nested and repeated data fields. You can have one row for each order, and repeated values within that one row for data that is at a more granular level. For example, you could simply have an array of timestamps as your events. Let's see an example to illustrate this point. Here you see it clearly, shown here on screen are just four rows for four unique order IDs. Notice all that gray space in between the rows. That's because the event status and event time is at a deeper level of granularity. That means there are multiple repeated values for these events per each order. An array is a perfect data type to handle this repeated value and keep all the benefits of storing that data in a single row. I mentioned the fields event dot status and event dot time. If this is one giant table, what is a dot doing in those column names? There are no other table aliases we've joined on. What's up with those fields. Event, pickup, and destination are what are called struct or structured data type fields in SQL. This isn't BigQuery specific. Structs are standard SQL data types and BigQuery just supports them really well. Structs you can think of as pre-joined tables within a table. So instead of having a separate table for event and pickup and destination, you simply nest them within your main table. So let's recap. You can go deep into a single field and have it be more granular than the rest by using an array data type like you see here for status and time. And you can have really wide schemas by using structs which allow you to have multiple fields of the same or different data types within them, much like a separate table would. The major benefit of structs is that the data is conceptually pre-joined already. So it's much faster to query. People often ask, with really wide schemas like 100 columns, how is it still fast to query? Remember that BigQuery is column based storage, not record based when storing data out on disk. If you did just account order underscore ID here to get your total orders, BigQuery wouldn't even care that you have 99 other columns, some of which are more granular with array data types, it wouldn't even look at them. That gives you the best of both worlds if you're an analyst, lots of data all in one place, and no issues with multiple granularity pitfalls when doing aggregations. Now it's your turn to practice reading one of these schemas that has nested and repeated fields, take a moment and spot those structs. As a hint, you can look at the field name to see any field with a dot in the name or you can look at the data type for any field values of the type record, which means struct. Did you get them all? Here are the four strokes in this dataset you saw earlier: events, pickups, destination, and duration. Duration is a new one, but we can simply keep adding more dimensions to our dataset by adding more structs. Remember, structs let you build really wide and informative schemas. Now it's time to go deep. Find the array data types in this schema. As a hint, look at the mode and find the repeated values. Got them? In this schema, the repeated value is the event struct, which means here we have an array of event structs with each having a status and time possibly. A critical point I like to make here is that struct and array data types in SQL can be absolutely independent of each other. You can have a regular column in SQL be an array column that has nothing to do with any struct. Likewise, you can have a struct that has zero array field types in its columns. The benefit of using them together is that arrays allow a given field to go deep into granularity, and structs allow you to organize all those useful fields into logical containers instead of separate tables. So here's the cheat sheet. Structs are a type of record when looking at a schema, and arrays are of mode repeated. Arrays can be of any single type, like an array of floats, or an array of strings, et cetera. Arrays can be part of a regular field or be part of a nested field nestled inside of a struct, a single table can have zero to many structs. And lastly, the real mind bending point is that a struct can have other structs nested inside of it as you will soon see in your upcoming Lab, which uses the real Google Analytics schema. We've been talking a lot about nested and repeated fields. So you're probably wondering what to do with your existing star schema, snowflake and third normal form data. The great news is that BigQuery also works well with those schema types. Use arrays and structs when your data naturally arrives in that format, and you'll benefit immediately from optimal performance. For the other schema types, bring them directly to BigQuery and you'll likely be pleased with the performance.

#### Demo: Nested and repeated fields

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521412

>> In this demo, we're going to be querying a cool dataset. This is a Bitcoin dataset in BigQuery. And what makes it cool besides the subject matter is it actually has nested and repeated columns. So if this is the first time that you've worked with structs or arrays, This demo will be super useful for you. You get all these demos in our public repository under the data engineering course within demos. So let's follow along. So the first thing that we want to do is access the datasets. This first query is going to give us where the dataset comes from. So inside of GCP, I'm going to paste the query there. One of my favorite things is one of the shortcuts that you can do. I'm on a Mac. So if I just hold the Command key down, I can highlight all the tables that are in the query. So if you're just given a query from a demo, you can see we've got a Bitcoin public dataset as a project, the dataset is Bitcoin blockchain. The table name, we have two, the blocks for inside of the blockchain, you can't have a blockchain without blocks, and you've got the transactions as well. So once these are highlighted, if you click on the table name, it will actually take you to the schema, which is super useful. So again, dataset, and then two different tables. Clicking on blocks, we can take a look at how much data is in here just by looking at the metadata in BigQuery. Schema, those are the different columns that we have available, a lot more on that in just a second. Let's see how big this dataset is. It is half a terabyte, 540,000 rows of blocks that are here. And as you see in a minute, you can have multiple transactions as part of a Bitcoin block that's solved by Bitcoin miners. You can actually see that when you preview the data. If you haven't seen nested and repeated fields before, this may look a little surprising. So let's see, I'll zoom out just for a little bit. And you can see one row ID and it's you got the block ID, it's just an identifier for the individual block, which is a record on this shared distributed ledger, that is blockchain for the specific Bitcoin network. And if you scroll over, you see a lot of gray area here, what's going on with that? Where's row number two? Well, row number two doesn't show up because you have different levels of granularity inside of the same data table. And this is because you have nested data and repeated columns in there as well. So you can see, for a given block, a block can have more than one transaction. So this block ending in A3876 has multiple transactions as it's scrolled to the right. It has as part of this, miners have confirmed that this Bitcoin transaction, either sending or receiving Bitcoins, is in there as well and ending in 89- ending in 1995. A few more as well. So it's kind of like if you've used merged cells inside of Excel or Google Spreadsheets, similar concept. This is a nested field because it has a prefix transactions dot transaction ID, that means it's a struct. And it's a repeated field because you can have more than one value, in repeated, when you hear that word repeated field, immediately think of the data type arrays. So how do you query this stuff? Right? So we've got our query here, we're going to get- give us the block ID. You saw that one, there's multiple sequence numbers that could be a part of the block one, the most- the latest, the maximum. And also, as I mentioned before, you can have multiple transactions, Bitcoin transactions that are part of a single block on the register. So we got blocks, and as you saw here, the first query, we're not going to use this nested column, we're going to pretend like the transaction struct, transactions dot whatever, transactions dot inputs, transaction dot transaction ID doesn't exist, we're actually going to just join on a separate table, a transactions table that also has the same information. And this is more typical if you've used relational schemas before and you've got just another table that you just do a join against using a common key- in this particular case, it's block ID- to get the count of those transactions. I'm going to show you yes, this will absolutely work, and we're going to query it, as the demo is going to tell you a little bit. You want to look at that slot time consumed, that somewhat kind of compute power distributing parallels behind the scenes, and the byte shuffled, which is those VMs behind the scenes talking to each other. So it processed about 48 gigabytes, did it in about 12 seconds. On your execution details right here, you can see, if you are going to process this in linear 101 machine, it could take two hours, if not more. And we had to shuffle around 18 gigabytes of data in order for all those VMs to talk to each other and get that data in there. And largely you can see as you scroll down, most of that work was done in the join. This is the query kind of execution plan, you can drill open and get a little bit of the pseudocode of how it actually runs behind the scenes and processes that. So showing the- now that you know that inside of that table, you don't need to have the transactions as a join, it's already part of the table in a wide schema as part of blocks, you also have this transactions struct, a struct is just a container. For other fields, you can think of it as a table that's already been pre-joined. To further illustrate that point, going back to the schema, you can scroll down, you can see anything of type record is a struct, S-T-R-U-C-T. And that is not unique to BigQuery, that's a generic SQL, BigQuery that supports it. And you can see transactions that struct has a bunch of different nested columns as part of it, and some of those columns happen to be repeated. What that means is structs can have other structs as their children, so transactions dot inputs dot something else. And those children, those column fields can also have the- they can be repeated, meaning that their column type is in array of strings, in array of integers, you're going to see what that means in the later half of this demo. So let's paste this, if you remember, that was shuffling a lot of query bytes. Now, instead of doing the join, you notice we only have one table here, and it processed in zero seconds because I ran this before, it was cached. Any time you're doing a performance demo instead of more query settings, you can disable cache results as well. Wouldn't that be great, right? So let's see, it took about 12 seconds for the first query to run, it's processing I think about like 40 gigabytes or something like that. So we're going to see what the performance improvement is. So it's a little bit faster, it's seven seconds, and about half the data that's being processed. And you can notice that a dramatic shift in the number of bytes shuffled because all the data is in one place. BigQuery supports what we call de-normalization, having super wide schemas in a single table, because BigQuery is column based. And you can see, we're not shuffling gigabytes of data anymore. Like, you know, I can't remember what the last number was, it was 40 gigabytes or something like that. But the amount here that we're shuffling is just megabytes, 88 megabytes, very trivial amount. And as well as the slot time consumed, this isn't two hours, it's just nine minutes. So it's much, much, much faster to do the query instead of doing the join to have this separate nested field via the struct inside of there as well. So that's how you query. For those following along at home, if you're wondering like, from this table as B comma here, the comma actually is the exact same thing as saying a comma is an implicit cross join. Inside of the world of structs, this is a correlated cross join. So the analogy is, you have multiple tables. Conceptually, you can think of these as tables, multiple child tables as part of a parent table. That's how I like to think of it. So your structs, these record values, or kind of like multiple tables within the same table. And you can- in order to access their columns, you have to do what's called a correlated cross join. So you can write it out, cross join, generally, folks just exclude that. And when you need to unpack these struct values, you can just do the comma into them. And that's how you get the values out of the struct. So it's similar to joining, not joining any other tables, there's no- you join command inside of the referring to another table. Okay, part two, that's explaining structs, and you can see that's much faster and processes a lot less resources. But that just covers the nested part, the repeated part is what's super interesting to me, because you have different levels of granularity. One block, as you saw previously had multiple transactions inside of it. Let's find something interesting. So you're going to learn a little bit about Bitcoin here, but don't worry, it'll apply to any of your datasets even if you're not working in cryptocurrency. So what we want to do is, inside of this dataset- I'm going to pull up in this query- inside of this dataset is a total amount of Bitcoins that are trading hands as part of the distributed ledger here. So I'm going to open up the blocks and let's preview the table. And the actual output column, how much is done in this transaction, let me zoom out a little bit. So that's the block ID all the way on the right. And again, this is a public dataset, is what's called output Satoshis. Satoshi in the Bitcoin world is, I think, if you multiply it by this number here, it's the smallest atomic unit, so you can't get below one Satoshi. If you multiply it by this number here, which is like 1E8 I think, you get actually- you can convert this, it's a direct correlation to an actual value in Bitcoin. So you'll- generally, you always see huge dollar, but it's kind of like cents versus dollars if you're in the US. So we want to get the block ID, we want to get convert this timestamp numerical value into a timestamp that we can read. We want to get the transaction ID and we want to get the value, the original Satoshi value and then the actual BTC or Bitcoin value. Same thing as before, we want to get the highest amount of bitcoins in a given transaction, the top 10. So you see it's the same thing as before. We have a struct that has transactions and we also have a struct that has inputs, that's fine. I don't think we actually need that because we're not actually doing anything on the inputs. But when you run this, you're going to get the extremely popular error that says "can't access some field with a type array." Inside of there is a integer, repeated value. So what does that actually mean? So you get that error, and you did your correlated across joins that we talked before with the comma, and you're previewing things. And you're going like, what- I just literally- it's literally called outputs dot outputs. So that's literally the column value here, what's going on. I already did my unpacking of transactions, beta transactions as T. So it's T dot outputs. Outputs, great, absolutely. What's not working? Well, you see that there's multiple for a given transaction. So where's the transaction? Transaction ID is right here. So this one, follow it visually as we scroll to the right. There's multiple possible output values for Satoshis, for a given transaction ID. So you have data that's on two different levels of granularity. So the first thing that we have to do is we have to un-nest or unpack this array of values and get them on individual rows. So row one, row two, row three, because right now they're all nested up in an array as part of one row. How do you do the unpacking of arrays? Well, that's with the un-nest command. So what that actually looks like is we want to take the- let's find the array field. Let me just get to us here. And we want to un-nest the- the actual column name, it's a nested struct within transactions, much like there was transactions dot inputs or transactions dot outputs, we've already crossed joined transactions as T. So I don't need to type in full transactions dot outputs. You can use this, this one's already been aliased. So we can use transactions T dot outputs. So we're going to break apart that repeated struct. How do you break it apart? You use unnest, as give it a creative alias, I put some notes in the demo about making sure that you have got correct aliases. And knowing that this is inside of our dataset, it's blocks dot transactions dot output. Anytime you're talking dots, you're using a lot of structs inside of your dataset, which helps you with the performance, and then we can literally, instead of saying the outputs there, we can just say, all right, after you've broken everything apart, this is the final result in order to unpack those arrays and get them on a single row value. So when you actually query this, you're not going to see these gray spaces anymore, everything's going to be on the same level of granularity. And then hopefully, you'll get the top 10 transactions for Bitcoin value, the timestamp and then the block that they belong to. So let's take a look while this runs. Running, running, running, running, here we go. So this block had a transaction that was the largest. How large are we talking? So that huge number of Satoshis which I just can't convert in my head, that's what we did in the query, is 500,000 Bitcoin, it's a lot of Bitcoin, and on 2011. So you can see this is how you can access those array values automatically using the unnest breaking them apart. So last thing we want to do is say, all right, you mentioned previously that you can have more than one transaction for a given block. So how Bitcoin works is you have blocks that are solved by mathematical hashing by these large compute power to mine and kind of solve these blocks. And as part of those blocks getting solved, there are unconfirmed transactions, people sending and receiving cryptocurrency that get recorded into the shared distributed ledger that is a block. So one block can have many transactions associated with it. And to prove that, the last part of the demo has literally just- this is the previous query that we just ran- is just taking that block that has that really large Bitcoin transaction, and just saying, "All right, well, what other transactions are part of that block? And can we find that?" It's going to be one- if you remember that number, that five with a lot of zeros on it, hopefully, we'll be able to find that within this. All I did was, I found this block ID, and I just want to filter out the entire blocks table and just pull that up. And this should have the block. You can see again, one row has many different field values in it. That's where you get the nested because the structs and repeated, because these are repeated data type values of the arrays, multiple different transactions in here, scrolling all over to the right, let's try to find that. Boom, there it is. That's a super large value, but it wasn't the only transaction that appeared inside of that block. You can see a bunch of other transactions that are in there as well. There's a lot of interesting web resources if you literally just Googled Bitcoin, or whatever cryptocurrency network, and the block ID, there's a lot of websites that literally will build web interfaces that show you a little bit more information about the distributed ledger and all the transactions for that. But for our purposes, it's just a great example to show you the differences between a highly normalized schema- normalized means many different tables that you have to join together- versus denormalized, which is where you can have a lot of what looks like conceptually, tables join within tables, like transactions, transactions dot inputs has other columns, transactions dot outputs has other columns there as well. So don't be afraid if you see really, really long scroll bar schemas inside of BigQuery. That is a performance best practice.

#### Design the optimal schema for BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521413

>> Let's recap some of the ways to design the schema of tables to improve query performance and lower query costs. It's much more efficient to define your schema to use nested repeated fields instead of joins. Suppose you have orders and purchase items for each order. In a traditional relational database system, you'd have two tables, one table for purchase items, and another for orders with a foreign key to connect the two tables. In BigQuery, it's much more efficient if you store each order in a row and have a nested repeated column called purchase_item. Arrays are a native type in BigQuery. Learn to think in terms of arrays. When you have dimension tables that are smaller than 10 gigabytes, keep them normalized. The exception to this is if the table rarely goes through update and delete operations. If you cannot define your schema in terms of nested repeated fields, you have to make a decision on whether to keep the data in two tables or denormalize the tables into one big flattened table. As a datasets tables increase in size, the performance impact of a join increases. At some point, it can be better to denormalize your data. The crossover point is around 10 gigabytes. If your tables are less than 10 gigabytes, keep the tables separate and do a join.

#### Lab Intro: Working with JSON and Array data in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521414

>> In the next lab, you'll get some experience working with JSON and array data in BigQuery. The objectives of this lab are to load semi-structured JSON data into BigQuery and to learn how to create and query arrays and structs. You will also query nested and repeated fields.

#### Working with JSON and Array data in BigQuery 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/54/labs/521415

#### Optimize with partitioning and clustering

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521416

>> Next up is optimizing with partitioning and clustering. In a table partitions by a date or a timestamp column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partition's table maintains these properties across all operations that modify it, query jobs, data manipulation language, DML statements, Data Definition Language, DDL statements, load jobs and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases. One of the ways you can optimize the tables in your data warehouse is to reduce the cost and amount of data read by partitioning your tables. For example, assume we have partitioned this table by the event date column. BigQuery will then change its internal storage so the dates are stored in separate shards. Now, when you run a query with a WHERE clause that looks for dates between 01-03 and 01-04, BigQuery will have to read only two fifths of the full dataset. This can lead to dramatic cost and time savings. You enable partitioning during the table creation process. This slide shows how to migrate an existing table to an ingestion time partitioned table. Using a destination table, it will cost you one table scan. As new records are added to the table, they will be put into the right partition. BigQuery creates new date based partitions automatically with no need for additional maintenance. In addition, you can specify an expiration time for data in the partitions. Partitioning can be set by ingestion time on a timestamp, date or date time column, or based on a range of an integer column. Here, we are partitioning customer_ID in the range zero to 100 in increments of 10. Although more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query. The good practice is to require that queries always include the partition filter, make sure that the partition field is isolated on the left side, because that's the only way BigQuery can quickly discard unnecessary partitions. An example of this in practice can be found in the blog Optimizing BigQuery, Cluster Your Tables. A link to the blog is available in the course resources. Clustering can improve the performance of certain types of queries, such as queries that use Filter clauses, and those that aggregate data. When data is written to a clustered table by a query or a load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data. Similarly, when you submit a query that aggregates data based on the values and the clustering columns, performance is improved, because the sorted blocks co-locate rows with similar values. In this example, the table is partitioned by event date, and clustered by user ID. Now, because the query looks for partitions in a specific range, only two of the five partitions are considered. Because the query looks for user ID in a specific range, BigQuery can jump to the row range and read only those rows for each of the columns needed. You set up clustering at table creation time. Here, we are creating the table partitioning by event date and clustering by user ID. We are also telling BigQuery to expire partitions that are more than three days old. The columns you specify in the cluster are used to co-locate related data. When you cluster a table using multiple columns, the order of columns you specify is important. The order of the specified columns determines the sort order of the data. Over time, as more and more operations modify a table, the degree to which the data is sorted begins to weaken, and the table becomes only partially sorted. In a partially sorted table, queries that use the clustering columns may need to scan more blocks compared to a table that is fully sorted. You can re-cluster the data in the entire table by running a select asterisk query that selects from and overwrites the table. But guess what, you don't need to do that anymore. The great news is that BigQuery now periodically does auto re-clustering for you. So you don't need to worry about your clusters getting out of date as you get new data. Automatic re-clustering is absolutely free and automatically happens in the background. You don't need to do anything additional to enable this. Partitioning provides a way to obtain accurate cost estimates for queries and guarantees improved cost and performance. Clustering provides additional cost and performance benefits in addition to the partitioning benefits. BigQuery supports clustering for both partitioned and non-partitioned tables. When you use clustering and partitioning together, the data can be partitioned by a date, date time or timestamp column, and then clustered on a different set of columns. In this case, data in each partition is clustered based on the values of the clustering columns. Partitioning provides a way to obtain accurate cost estimates for queries. Keep in mind, if you don't have partitioned columns, and you want the benefits of clustering, you can create a fake underscore date column of type date and have all the values be null.

#### Lab Intro: Partitioned Tables in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521417

In this lab, you practice creating date-partitioned tables in BigQuery. Specifically, you query a partitioned dataset, and then you'll create dataset partitions to improve the query performance and reduce the overall cost.

#### Partitioned Tables in Google BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/54/labs/521418

#### Review

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521419

>> I started by describing what makes a modern data warehouse and what distinguishes a data lake from an enterprise data warehouse. You were then introduced to BigQuery, a scalable data warehouse solution on Google Cloud. You don't need to provision resources before using BigQuery unlike with many relational database systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. BigQuery enables you to structure your information into datasets, projects, and tables. You can use multiple datasets to separate tables pertaining to different analytical domains. And you can use project level scoping to isolate datasets from each other according to your business needs. Also, you can align projects to billing and use datasets for access control. BigQuery allows you to batch load source data into a BigQuery table in a single batch operation. For example, the data source could be a CSV file, an external database, or a set of log files. BigQuery Data Transfer Service enables you to run batch transfers on a schedule. Streaming allows you to continually send smaller batches of data in real time, so the data is available for querying as it arrives. You can also use SQL to generate data and store the results in BigQuery. Also, some third party applications and services provide connectors that can ingest data into BigQuery. The table schema provides structure to the data. Remember that every table has a schema which you can enter manually or provide a JSON file with the structure. Those table schemas can also have array data types, which makes them repeated and or struct data types, which makes them nested. This type of denormalization will often give you a performance boost because it avoids intensive joins. You can also setup table partitioning and clustering to reduce the amount of data scanned and speed up your queries.

#### Building a Data Warehouse

- https://www.cloudskillsboost.google/paths/16/course_templates/54/quizzes/521420

### Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/54/video/521421

>> Let's review some key concepts we covered in this course on data lakes and data warehouses. The primary role of a data engineer is to build data pipelines. The ultimate purpose of a data pipeline is to enable stakeholders in an organization to use data to make faster and better decisions. While the role of a data engineer is not new, being able to build data pipelines entirely in the Cloud is relatively new. We argue that doing data engineering in the cloud is advantageous because you can separate compute from storage, and you don't have to worry about managing infrastructure and even software. This allows you to spend more time on what matters, getting insights from data. We introduced data lakes and data warehouses and discussed the key differences between the two. At a high level, a data lake is a place to store unprocessed data, while a data warehouse is a place to store transformed data that you ultimately want to use for analytics, machine learning and dashboards. Next, we discussed Cloud storage as the data lake solution on Google Cloud in some technical depth. We also presented other Google Cloud solutions for low latency requirements, transactional workloads, and structured data. We introduced BigQuery as the data warehouse solution on Google Cloud. We discussed partitioning and clustering in BigQuery as techniques for improving query performance. Also, we talked about EL, ELT, and ETL, and how these relate to data lakes and warehouses. Finally, we presented some reference architectures on Google Cloud for streaming and batch data pipelines. The hope is that these reference architectures serve as a starting point for your data pipeline. Congratulations on completing modernizing data lakes and data warehouses with Google Cloud. Building batch data pipelines on Google Cloud is the second course of the data engineering on Google Cloud core series. We hope to see you there.

### Course Resources

#### Modernizing Data Lakes and Data Warehouses with Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/54/documents/521422

### Your Next Steps

## 05: Building Batch Data Pipelines on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/53

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509028

Damon: Welcome to Building Batch Data Pipelines on Google Cloud. I'm Damon, and I'm a technical curriculum creator at Google. Booting batch data pipeline on Google Cloud is the second course of the Data Engineering on Google Cloud core series. In the previous course, we talked about data lakes and data warehouses as storage options for your data. In this course, we'll concentrate on how to build batch data pipelines to get data into storage using Extract Load, Extract Transform Load, and Extract Load Transform routines. We'll cover several technologies for data transformation on Google Cloud. Including BigQuery, Executing Spark on Dataproc, serverless data processing with Dataflow, and leveraging Google Cloud in pipelines with Cloud Data Fusion and Cloud Composer.

### Introduction to Building Batch Data Pipelines

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509029

person: What are batch pipelines? These are pipelines that process a bounded amount of data and then exit. For example, you might have a batch pipeline that runs once a day. It takes all the credit, debit, and money transfer transactions over that day, balances the books, and writes out the reconciled data to the data warehouse. If you are going to write such a pipeline to balance the books, should you use EL, ELT, or ETL? EL, remember, is extract and load. ELT loads the data as is and then transforms on the fly. ETL extracts the data, transforms it, then loads it into a data warehouse. Deciding which to use depends on the kinds of transformations you need and data quality considerations. We will look at how to build EL and ELT pipelines in BigQuery, the circumstances when EL and ELT are not appropriate, and why you might want to use ETL.

#### EL, ELT, ETL

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509030

person: Let's start with a quick recap of EL, ELT and ETL. EL is Extract and Load. This refers to when data can be imported as is into a system. ELT or Extract, Load and Transform, allows raw data to be loaded directly into the target and transformed whenever it is needed. For example, you might provide access to the raw data through a view that determines whether the user wants all transactions or only reconciled ones. ETL or Extract, Transform and Load, is a data integration process in which transformation takes place in an intermediate service before it is loaded into the target. For example, the data might be transformed in Dataflow before being loaded into BigQuery. When would you use EL? The bottom line is that you should use EL only if the data is already clean and correct. Perhaps you have log files in Cloud Storage. You can extract data from files on Cloud Storage and load it into BigQuery's native storage. This is a simple REST API call. You can trigger this pipeline from Cloud Composer, Cloud Functions or via a scheduled query. You might even set it to work in micro batches, not quite streaming but near real time. Whenever a new file hits Cloud Storage, the Cloud Function runs, and the function invokes a BigQuery job. The data transfer service in BigQuery will also work here. Use EL for batch loading historical data or to do scheduled loads of log files. But let me emphasize, use EL only if the data is already clean and correct. ELT starts with EL, so the loading is the same and could work the same way. File hits Cloud Storage, function invokes BigQuery load, table appended to. The big difference is what happens next. The table might be stored in a private dataset and everyone accesses the data through a view which imposes data integrity checks. Or maybe you have a job that runs a SQL query with a destination table. This way, transformed data is stored in a table that everyone accesses. When do you use ELT? One common case is when you don't know what kind of transformations are needed to make the data usable. For example, let's say someone uploads a new image. You invoke the Vision API and back comes a long JSON message about all kinds of things in the image, text in the image, whether there's a landmark, a logo, what objects. What will an analyst need in the future? You don't know, so you store the raw JSON as is. Later, if someone wants to count the number of times a specific company's logos are in this set of images, they can extract logos from the JSON and then count them. Of course, this works only if the transformation that's needed can be expressed in SQL. In the case of the Vision API, the result is JSON and BigQuery SQL has support for JSON parsing, so ELT will work in this case.

#### Quality considerations

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509031

person: Now that we have looked at EL and ELT, let's look at some of the transformations you might want to do and how they can be done in BigQuery. To keep things precise, let's assume that our data processing needs all revolve around quality improvements. What are some of the quality related reasons why we might want to process data? The top row are characteristics of information. Information can be valid, accurate, complete, consistent and or uniform. These terms are defined in the science of logic, each is independent. For example, data can be complete without being consistent, it can be valid without being uniform. There are formal definitions for each of these terms that you can look up online. But the main practical reason for seeking them is shown in the second row, the problems they present in data analysis. It is one thing to seek each of the five badges for your data to have objectively good data quality. However, it is another thing when poor quality data interferes with data analysis and leads to incorrect business decisions. So the reason to spend time, energy, and resources detecting and resolving quality issues is that it can affect a business outcome. Thus, if data does not conform to your business rules, you have a problem of validity. For example, let's say that you sell movie tickets, and each ticket costs $10. If you have a $7 transaction, then you have a validity problem. Similarly, accuracy problems are due to data not conforming to objective truth. Completeness has to do with failing to process everything. Consistency problems are if two different operations ought to be the same but yield different results, and because you don't know what to trust, you can't derive insights from the data. Uniformity is when data values of the same column in different rows mean different things. The main causes of these problems are listed in the third row. We will explore methods of detecting each of these issues in data. Now you have found the problems, what do you do about them? ELT and BigQuery can often help fix many data quality issues. Here is an example. Imagine you plan to analyze data but there are duplicate records making it seem like one kind of event is more common, when in fact this is just a data quality issue. You cannot derive insights from the data until the duplicates are removed. So do you need a transformation step to remove the duplicates before you store the data? Maybe, but a simpler solution exists, to count unique records. You do, of course, have count distinct in BigQuery and you can use that instead. Similarly, a problem like data being out of range can be solved in BigQuery without an intermediate transformation step. Invalid data can be filtered out using a BigQuery view and everyone can access the view rather than the raw data.

#### How to carry out operations in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509032

In this lesson, we will look at various quality issues and talk through some BigQuery capabilities that can help you address those quality problems. We can use Views to filter out rows that have quality issues. For example, remove quantities less than zero using a WHERE clause. After you do a GROUP BY, you can discard groups whose total number of records is less than 10 using the HAVING clause. Think carefully about how you wish to treat nulls and blanks. A NULL is the absence of data. A BLANK is an empty string. Consider if you are trying to filter out both NULLS and BLANKS or only NULLs or only BLANKs. You can easily count non-null values using COUNTIF and use the IF statement to avoid using specific values in computations. For accuracy, test data against known good values. For example, if you have an order, you could compute the sub_total from the quantity_ordered and item_price and make sure the math is accurate. Similarly, you can check if a value that is being inserted belongs to a canonical list of acceptable values. You can do that with a SQL IN. For completeness, identify any missing values and either filter out, or replace them with something reasonable. If the missing value is NULL, SQL provides functions like NULLIF, COUNTIF, COALESCE, etc. to filter missing values out of calculations. You might be able to do a UNION from another source to account for missing months of data. The automatic process of detecting data drops and requesting data items to fill in the gaps is called “backfilling”. It is a feature of some data transfer services. When loading data, verify file integrity with checksum values (hash, MD5). Consistency problems are often due to duplicates. You expect that something is unique, and it isn’t, so things like totals are wrong. COUNT provides the number of rows in a table that contain a non-null value. COUNT DISTINCT provides the number of unique values. If they are different, then it means that you have duplicate values. Similarly, if you do a GROUP BY, and any group contains more than one row, then you know you have two or more occurences of that value. Another reason that you might have consistency problems is if extra characters have been added to the fields. For example, you may be getting timestamps, some of which may include a timezone. Or you have strings that are padded. Use string functions to clean such data before passing it on. What happens if you are storing some value in centimeters, and suddenly, you start getting the value in millimeters? Your data warehouse will end up with non-uniform data. You have to safeguard against this. Use SQL cast to avoid issues with data types changing within a table. Use the SQL FORMAT() function to clearly indicate units. And in general, document them very clearly. I hope that what you are coming away with is the idea that BigQuery SQL is very powerful and you can take advantage of this.

#### Shortcomings

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509033

In the previous lesson, we showed you some of the ways in which you can use SQL in an E-L-T pipeline to safeguard against quality issues. The point is that you don’t always need E-T-L. E-L-T might be an option even if you need transformation. However, there are situations where E-L-T won’t be enough. In that case, E-T-L might be what you need to do. What are the kinds of situations where it is appropriate? The first example - translating Spanish to English - requires calling an external API. This cannot be done directly in SQL. It is possible to use a BigQuery remote function, invoke the Cloud Translation API, and perform content translation. But this involves programming outside of BigQuery. The second example - looking at a stream of customer actions over a time window - is rather complex. You can do it with windowed aggregations, but it is far simpler with programmatic logic. So, if the transformations cannot be expressed in SQL or are too complex to do in SQL, you might want to transform the data before loading it into BigQuery. The reference architecture for Google Cloud suggests Dataflow as an E-T-L tool. We recommend that you build E-T-L pipelines in Dataflow and land the data in BigQuery. The architecture looks like this: Extract data from Pub/Sub, Cloud Storage, Spanner, Cloud SQL, etc. Transform the data using Dataflow, And have the Dataflow pipeline write to BigQuery. When would you do this? When the raw data needs to be quality-controlled, transformed, or enriched before being loaded into BigQuery. And the transforms are difficult to do in SQL. When the data loading has to happen continuously, i.e. if the use case requires streaming. Dataflow supports streaming. We’ll look at streaming in more detail in the next course. And when you want to integrate with continuous integration / continuous delivery (CI/CD) systems and perform unit testing on all components. It’s easy to schedule the launch of a Dataflow pipeline. Dataflow is not the only option you have on Google Cloud if you want to do E-T-L. In this course, we will look at several data processing and transformation services that Google Cloud provides: Dataflow, Dataproc, and Data Fusion. Dataproc and Dataflow can be used for more complex E-T-L pipelines. Dataproc is based on Apache Hadoop and requires significant Hadoop expertise to leverage directly. Data Fusion provides a simple graphical interface to build E-T-L pipelines that can then be easily deployed at scale to Dataproc clusters. Dataflow is a fully managed, serverless data processing service based on Apache Beam that supports both batch and streaming data processing pipelines. While significant Apache Beam expertise is desirable in order to leverage the full power of Dataflow, Google also provides quick-start templates for Dataflow to allow you to rapidly deploy a number of useful data pipelines. You can use any of these three products to carry out data transformation and then store the data in a data lake or data warehouse to support advanced analytics.

#### ETL to solve data quality issues

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509034

So now, let's look at using ETL to solve data quality issues. Unless you have specific needs, we recommended that you use Dataflow and BigQuery. What are a few needs that cannot be met easily with Dataflow and BigQuery? Low Latency and high throughput. BigQuery queries are subject to a latency on the order of a few hundred milliseconds and you can stream on the order of a million rows per second into a BigQuery table -- this used to be 100,000 rows, but recently it got raised to 1 million per project. The typical latency number quoted for BigQuery is on the order of a second, but with BI engine it is possible to get latency on the order of 100 milliseconds -- you should always check the documentation and the solutions pages for the latest values. If your latency and throughput considerations are more stringent, then Bigtable might be a better sink for your data processing pipelines. Reusing Spark pipelines. Maybe you already have a significant investment in Hadoop and Spark. In that case, you might be a lot more productive in a familiar technology. Use Spark if that’s what you know really well. Need for visual pipeline building. Dataflow requires you to code data pipelines in Java or Python. If you want to have data analysts and non-technical users create data pipelines, use Cloud Data Fusion. They can drag-and-drop and visually build pipelines. We’ll look at all these options briefly now and in greater detail in the remainder of this course. Dataproc is a managed service for batch processing, querying, streaming, and Machine Learning. It is a service for Hadoop workloads and is quite cost effective when taking into consideration eliminating the tasks related to running Hadoop on bare metal and taking on all of the related maintenance activities. It also has a few powerful features like auto-scaling and out-of-the-box integration with Google Cloud products like BigQuery. Cloud Data Fusion is a fully-managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines. You can use it to populate a data warehouse, but you can also use it for transformations and cleanup, and ensuring data consistency. Users, who can be in non-programming roles, can build visual pipelines to address business imperatives like regulatory compliance without having to wait for an IT team to write a Dataflow pipeline. Data Fusion also has a flexible API so IT staff can create scripts to automate execution. Regardless of which E-T-L is used -- Dataflow, Dataproc, Data Fusion -- there are some crucial aspects to keep in mind. First: Maintaining data lineage is important. What do we mean by Lineage? Where the data came from, what processes it has been through, and what condition it is in, are all lineage. If you have the lineage, you know for what kinds of uses the data is suited. If you find the data gives odd results, you can check the lineage to find out if there is a cause that can be corrected. Lineage also helps with trust and regulatory compliance. The other cross-cutting concern is that you need to keep metadata around. You need a way to track the lineage of data in your organization for discovery and identification of suitability for uses. On Google Cloud, Dataplex provides discoverability. But you have to do your bit by adding labels. Dataplex metadata can be viewed directly in BigQuery thereby simplifying the process of confirming data lineage. A label is a key-value pair that helps you organize your resources. In BigQuery you can attach labels to Datasets, Tables, and Views. Labels are useful for managing complex resources because you can filter them based on their labels. Labels are a first step towards a Data Catalog. Among the things that labels help with is Cloud Billing. If you attach labels to Compute Engine instances and to buckets and to Dataflow pipelines, then you have a way to get a fine-grained look at your Cloud bill because the information about labels is forwarded to the billing system, and so you can break down your billing charges by label. Data Catalog is a fully managed and highly scalable data discovery and metadata management service. It is serverless and requires no infrastructure to set up or manage. It provides access-level controls and honors source A-C-Ls for read, write, and search for the data assets; giving you enterprise-grade access control. Think of Data Catalog as a metadata-as-a-service. It provides metadata management service for cataloging data assets via custom APIs and the UI, thereby providing a unified view of data wherever it is. It supports schematized tags (e.g., E-num, Bool, DateTime) and not just simple text tags — providing organizations rich and organized business metadata. It offers unified data discovery of all data assets, spread across multiple projects and systems. It comes with a simple and easy-to-use search UI to quickly and easily find data assets; powered by the same Google search technology that supports Gmail and Drive. As a central catalog, it provides a flexible and powerful cataloging system for capturing both technical metadata (automatically) as well as business metadata (tags) in a structured format. One of the great things about the data discovery is that it integrates with the Cloud Data Loss Prevention API. You can use it to discover and classify sensitive data, providing intelligence and helping to simplify the process of governing your data. Data Catalog empowers users to annotate business metadata in a collaborative manner and provides the foundation for data governance. Specifically, Data Catalog makes all the metadata about your datasets available to search for your users, regardless of where the data are stored. Using Data Catalog, you can group datasets together with tags, flag certain columns as containing sensitive data, etc. Why is this useful? If you have many different datasets with many different tables — to which different users have different access levels — the Data Catalog provides a single unified user experience for discovering those datasets quickly. No more hunting for specific table names in the databases, which may not be accessible by all users.

#### Introduction to Building Batch Data Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/53/quizzes/509035

### Executing Spark on Dataproc

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509036

- In this module, we will discuss Dataproc, Google Cloud's managed Hadoop service, and in particular, Apache Spark. In this module, we'll cover the Hadoop Ecosystem, learn about running Hadoop on Dataproc, understand the benefits of Cloud Storage instead of HDFS, learn about optimizing Dataproc, and complete a hands-on lab with Apache Spark on Dataproc.

#### The Hadoop ecosystem

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509037

>> Let's start by looking at the Hadoop ecosystem in a little more detail. It helps to place the services you'll be learning about in historical context. Before 2006, big bata meant big databases. Database design came from a time when storage was relatively cheap and processing was expensive, so it made sense to copy the data from its storage location to the processor to perform data processing, then the result will be copied back to storage. Around 2006, distributed processing of big data became practical with Hadoop. The idea behind Hadoop is to create a cluster of computers and leverage distributed processing. HDFS, the Hadoop Distributed File System, stored the data on the machines in the cluster and MapReduce provided distributed processing of the data. A whole ecosystem of Hadoop related software grew up around Hadoop, including Hive, Pig and Spark. Organizations use Hadoop for on-premises big data workloads. They make use of a range of applications that run on Hadoop clusters, such as Presto, but a lot of customers use Spark. Apache Hadoop is an open source software project that maintains the framework for distributed processing of large datasets across clusters of computers using simple programming models. HDFS is the main file system Hadoop uses for distributing work to nodes on the cluster. Apache Spark is an open source software project that provides a high performance analytics engine for processing batch and streaming data. Spark can be up to 100 times faster than equivalent Hadoop jobs because it leverages in memory processing. Spark also provides a few for dealing with data including resilient distributed datasets and data frames. Spark in particular is very powerful and expressive and used for a lot of workloads. A lot of the complexity and overhead of OSS Hadoop has to do with assumptions in the design that existed in the data center. Relieved of those limitations, data processing becomes a much richer solution with many more options. There are two common issues with OSS Hadoop: tuning and utilization. In many cases, using Dataproc as designed will overcome these limitations. On-premises Hadoop clusters, due to their physical nature, suffer from limitations. The lack of separation between storage and compute resources results in capacity limits and an inability to scale fast. The only way to increase capacity is to add more physical servers. There are many ways in which using Google Cloud can save you time, money and effort compared to using an on-premises Hadoop solution. In many cases, adopting a Cloud based approach can make your overall solution simpler and easier to manage. Built-in support for Hadoop. Dataproc is a managed Hadoop and Spark environment. You can use Dataproc to run most of your existing jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know. Managed hardware and configuration. When you run Hadoop on Google Cloud, you never need to worry about physical hardware. You specify the configuration of your cluster and Dataproc allocates resources for you, you can scale your cluster at any time. Simplified version management. Keeping open source tools up to date and working together is one of the most complex parts of managing a Hadoop cluster. When you use Dataproc, much of that work is managed for you by Dataproc versioning. Flexible job configuration. A typical on-premises Hadoop setup uses a single cluster that serves many purposes. When you move to Google Cloud, you can focus on individual tasks, creating as many clusters as you need. This removes much of the complexity of maintaining a single cluster with growing dependencies and software configuration interactions. Running MapReduce directly on top of Hadoop is very useful, but it has the complication that the Hadoop system has to be tuned for the kind of job being run to make efficient use of the underlying resources. A simple explanation of Spark is that it is able to mix different kinds of applications and to adjust how it uses the available resources. Spark uses a declarative programming model. In imperative programming, you tell the system what to do and how to do it. In declarative programming, you tell the system what you want and it figures out how to implement it. You will be learning to work with Spark in the labs in this course. There is a full SQL implementation on top of Spark. There is a common data frame model that works across Scala, Java, Python, SQL and R. And there is a distributed machine learning library called Spark ML Lib.

#### Running Hadoop on Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509038

Next, we'll discuss how and why you should consider processing your same Hadoop job code in the cloud using Dataproc on Google Cloud. Dataproc lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. When compared to traditional, on-premises products, and competing cloud services, Dataproc has unique advantages for clusters of three to hundreds of nodes. There is no need to learn new tools or APIs to use Dataproc, making it easy to move existing projects into Dataproc without redevelopment. Spark, Hadoop, Pig, and Hive are frequently updated. Here are some of the key features of Dataproc. Low cost: Dataproc is priced at 1 cent per virtual CPU per cluster per hour, on top of the other Google Cloud resources you use. In addition, Dataproc clusters can include preemptible instances that have lower compute prices. You use and pay for things only when you need them, so Dataproc charges second-by-second billing with a one-minute-minimum billing period. Super-fast: Dataproc clusters are quick to start, scale, and shutdown, with each of these operations taking 90 seconds or less, on average. Resizable clusters: Clusters can be created and scaled quickly with a variety of virtual machine types, disk sizes, number of nodes, and networking options. Open source ecosystem: You can use Spark and Hadoop tools, libraries, and documentation with Dataproc. Dataproc provides frequent updates to native versions of Spark, Hadoop, Pig, and Hive, so there is no need to learn new tools or APIs, and it is possible to move existing projects or ETL pipelines without redevelopment. Integrated: Built-in integration with Cloud Storage, BigQuery, and Bigtable ensures data will not be lost. This, together with Cloud Logging and Cloud Monitoring, provides a complete data platform and not just a Spark or Hadoop cluster. For example, you can use Dataproc to effortlessly ETL terabytes of raw log data directly into BigQuery for business reporting. Managed: Easily interact with clusters and Spark or Hadoop jobs, without the assistance of an administrator or special software, through the Cloud Console, the Cloud SDK, or the Dataproc REST API. When you're done with a cluster, simply turn it off, so money isn‚Äôt spent on an idle cluster. Versioning: Image versioning allows you to switch between different versions of Apache Spark, Apache Hadoop, and other tools. Highly available: Run clusters with multiple primary nodes and set jobs to restart on failure to ensure your clusters and jobs are highly available. Developer tools: Multiple ways to manage a cluster, including the Cloud Console, the Cloud SDK, RESTful APIs, and SSH access. Initialization actions: Run initialization actions to install or customize the settings and libraries you need when your cluster is created. And automatic or manual configuration: Dataproc automatically configures hardware and software on clusters for you while also allowing for manual control. Dataproc has two ways to customize clusters; optional components and initialization actions. Pre-configured optional components can be selected when deploying from the console or via the command line and include: Anaconda, Hive WebHCat, Jupyter Notebook, Zeppelin Notebook, Druid, Presto and Zookeeper. Initialization actions let you customize your cluster by specifying executables or scripts that Dataproc will run on all nodes in your Dataproc cluster immediately after the cluster is set up. Here's an example of how you can create a Dataproc cluster using the Cloud SDK. And we're going to specify an HBase shell script to run on the clusters initialization. There are a lot of pre-built startup scripts that you can leverage for common Hadoop cluster setup tasks, like Flink, Jupyter and more. You can check out the GitHub repo link in the Course Resources to learn more. Let's talk more about the architecture of the cluster. A Dataproc cluster can contain either preemptible secondary workers or non-preemptible secondary workers, but not both. The standard setup architecture is much like you would expect on-premise. You have a cluster of virtual machines for processing and then persistent disks for storage via HDFS. You've also got your manager node VM (or VMs) and a set of worker nodes. Worker nodes can also be part of a managed instance group, which is just another way of ensuring that VMs within that group are all of the same template. The advantage is that you can spin up more VMs than you need to automatically resize your cluster based on the demands. It also only takes a few minutes to upgrade or downgrade your cluster. Google Cloud recommends a ratio of 60/40 as the maximum between standard VMs and preemptible VMs. Generally, you shouldn't think of a Dataproc cluster as long-lived. Instead you should spin them up when you need compute processing for a job and then simply turn them down. You can also persist them indefinitely if you want to. What happens to HDFS storage on disk when you turn those clusters down? The storage would go away too, which is why it's a best practice to use storage that's off cluster by connecting to other Google Cloud products. Instead of using native HDFS on a cluster, you could simply use a cluster on Cloud Storage via the HDFS connector. It's pretty easy to adapt existing Hadoop code to use Cloud Storage instead of HDFS. Change the prefix for this storage from hdfs// to gs//. What about Hbase off-cluster? Consider writing to Bigtable instead. What about large analytical workloads? Consider writing that data into BigQuery and doing those analytical work loads there. Using Dataproc involves this sequence of events: Setup, Configuration, Optimization, Utilization, and Monitoring. Setup means creating a cluster. And you can do that through the Cloud Console, or from the command line using the gcloud command. You can also export a YAML file from an existing cluster or create a cluster from a YAML file. You can create a cluster from a Terraform configuration, or use the REST API. The cluster can be set as a single VM, which is usually to keep costs down for development and experimentation. Standard is with a single Primary Node, and High Availability has three Primary Nodes. You can choose a region and zone, or select a "global region" and allow the service to choose the zone for you. The cluster defaults to a Global endpoint, but defining a Regional endpoint may offer increased isolation and in certain cases, lower latency. The Primary Node is where the HDFS Namenode runs, as well as the YARN node and job drivers. HDFS replication defaults to 2 in Dataproc. Optional components from the Hadoop-ecosystem include: Anaconda (Python distribution and package manager), Hive Webcat, Jupyter Notebook, and Zeppelin Notebook. Cluster properties are run-time values that can be used by configuration files for more dynamic startup options. And user labels can be used to tag the cluster for your own solutions or reporting purposes. The Primary Node Worker Nodes, and preemptible Worker Nodes, if enabled, have separate VM options, such as vCPU, memory, and storage. Preemptible nodes include YARN NodeManager but they do not run HDFS. There are a minimum number of worker nodes, the default is 2. The maximum number of worker nodes is determined by a quota and the number of SSDs attached to each worker. You can also specify initialization actions, such as initialization scripts that can further customize the worker nodes. And metadata can be defined so that the VMs can share state information. Preemptible VMs can be used to lower costs. Just remember they can be pulled from service at any time and within 24 hours. So your application might need to be designed for resilience to prevent data loss. Custom machine types allow you to specify the balance of Memory and CPU to tune the VM to the load, so you are not wasting resources. A custom image can be used to pre-install software so that it takes less time for the customized node to become operational than if you installed the software at boot-time using an initialization script. You can also use a Persistent SSD boot disk for faster cluster start-up. Jobs can be submitted through the cloud console, the gcloud command, or the REST API. They can also be started by orchestration services such as Dataproc Workflow and Cloud Composer. Don't use Hadoop's direct interfaces to submit jobs because the metadata will not be available to Dataproc for job and cluster management, and for security, they are disabled by default. By default, jobs are not restartable. However, you can create restartable jobs through the command line or REST API. Restartable jobs must be designed to be idempotent and to detect successorship and restore state. Lastly, after you submit your job you'll want to monitor it, you can do so using Cloud Monitoring. Or you can also build a custom dashboard with graphs and set up monitoring of alert policies to send emails for example, where you can notify if incidents happen. Any details from HDFS, YARN, metrics about a particular job or overall metrics for the cluster like CPU utilization, disk and network usage, can all be monitored and alerted on with Cloud Monitoring.

#### Cloud Storage instead of HDFS

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509039

person: Let's discuss more about using Google Cloud Storage instead of the native Hadoop file system or HDFS. Network speeds were slow originally. That's why we kept data as close as possible to the processor. Now, with petabit networking, you can treat storage and compute independently and move traffic quickly over the network. Your on-premise Hadoop clusters need local storage on its disk since the same server runs computes on stores jobs. That's one of the first areas for optimization. You can run HDFS in the Cloud just by lifting and shifting your Hadoop workloads to Dataproc. This is often the first step to the Cloud and requires no code changes. It just works. But HDFS on the Cloud is a subpar solution in the long run. This is because of how HDFS works on the clusters with block size, the data locality and the replication of the data in HDFS. For block size in HDFS, you're tying the performance of input and output to the actual hardware the server is running on. Again, storage is not elastic in this scenario, you're in the cluster. If you run out of persistent disk space on your cluster, you will need to resize even if you don't need the extra compute power. For data locality, there are similar concerns about storing data on individual persistent disks. This is especially true when it comes to replication. In order for HDFS to be highly available, it replicates three copies of each block out to storage. It would be better to have a storage solution that's separately managed from the constraints of your cluster. Google's network enables new solutions for big data. The Jupyter networking fabric within a Google data center delivers over one petabit per second of bandwidth. To put that into perspective, that's about twice the amount of traffic exchanged on the entire public internet. See Cisco's annual estimate of all internet traffic. If you draw a line somewhere in a network, bisectional bandwidth is the rate of communication at which servers on one side of the line can communicate with servers on the other side. With enough bisectional bandwidth, any server can communicate with any other server at full network speeds. With petabit bisectional bandwidth, the communication is so fast that it no longer makes sense to transfer files and store them locally. Instead, it makes sense to use the data from where it is stored. Inside of a Google data center, the internal name for the massively distributed storage layer is called Colossus. Under the network inside the data center is Jupyter. Dataproc clusters get the advantage of scaling up and down VMs that they need to do the compute while passing off persistent storage needs with the ultra-fast Jupyter network to a storage products like Cloud Storage, which is controlled by Colossus behind the scenes. A historical continuum of data management is as follows. Before 2006, big data meant big databases, database design came from a time when storage was relatively cheap, and processing was expensive. Around 2006, distributed processing of big data became practical with Hadoop. Around 2010, BigQuery was released, which was the first of many big data services developed by Google. Around 2015, Google launched Dataproc, which provides a managed service for creating Hadoop and Spark clusters and managing data processing workloads. One of the biggest benefits of Hadoop in the Cloud is that separation of compute and storage. With Cloud Storage as the backend, you can treat clusters themselves as ephemeral resources, which allows you not to pay for compute capacity when you're not running any jobs. Also, Cloud Storage is its own completely scalable and durable storage service, which is connected to many other Google Cloud projects. Cloud storage could be a drop-in replacement for your HDFS backend for Hadoop, the rest of your code would just work. Also, you can use the Cloud storage connector manually on your non-Cloud Hadoop clusters if you didn't want to migrate your entire cluster to the Cloud yet. With HDFS, you must over-provision for current data and for data you might have, and you must use persistent disks throughout. With Cloud Storage however, you pay for exactly what you need when you use it. Cloud Storage is optimized for large bulk parallel operations. It has very high throughput, but it has significant latency. If you have large jobs that are running lots of tiny little blocks, you may be better off with HDFS. Additionally, you want to avoid iterating sequentially over many nested directories in a single job. Using Cloud Storage instead of HDFS provides some key benefits due to the distributed service including eliminating bottlenecks and single points of failure. However, there are some disadvantages to be aware of, including the challenges presented by renaming objects and the inability to append to objects. Cloud Storage is at its core an object store, it only simulates a file directory. So directory renames in HDFS are not the same as they are in Cloud Storage, but new objects store oriented output committers mitigate this as you see here. Disk CP is a key tool for moving data. In general, you want to use a push-based model for any data that you know you will need while pull-based may be a useful model if there is a lot of data that you might not ever need to migrate.

#### Optimizing Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509040

Next, let's look at optimizing Dataproc. Where is your data and where is your cluster? Knowing your data locality can have a major impact on your performance. You want to be sure that your data's region and your cluster zone are physically close in distance. When using Dataproc you can omit the zone and have the Dataproc Auto Zone feature select a zone for you in the region you choose. While this handy feature can optimize on where to put your cluster it does not know how to anticipate the location of the data you're cluster will be accessing. Make sure that the Cloud storage bucket is in the same regional location as your Dataproc region. Is your network traffic being funneled? Be sure that you do not have any network rules or roots that funnel Cloud storage traffic through a small number of VPN gateways before it reaches your cluster. There are large network pipes between Cloud Storage and Compute Engine. You don't want to throttle your bandwidth by sending traffic into a bottleneck in you're google Cloud networking configuration. How many input files and Hadoop partitions are you trying to deal with? Make sure you are not dealing with more than around 10,000 input files. If you find yourself in this situation try to combine or union the data into larger file sizes. If this file volume reduction means that now you are working with larger datasets more than approximately 50,000 Hadoop partitions you should consider adjusting the setting fs.gs.block.size to a larger value accordingly. Is the size of your persistent disk limiting your throughput? Oftentimes when getting started with Google Cloud you may have just a small table that you want to benchmark. This is generally a good approach as long as you do not choose a persistent disk that assigns to such a small quantity of data, it will most likely limit your performance. Standard persistent disk scale linearly with volume size. Did you allocate enough virtual machines to your cluster? A question that often comes up when migrating from on-premises hardware to Google Cloud is how to accurately size the number of virtual machines needed. Understanding your workloads is key to identifying a cluster size. Running prototypes and benchmarking with real data and real jobs is crucial to informing the actual VM allocation decision. Locally, the ephemeral nature of the Cloud makes it easy to write size clusters for the specific task at hand instead of trying to purchase hardware upfront, thus, you can easily resize your cluster as needed. Employing job scoped clusters is a common strategy for Dataproc clusters

#### Optimizing Dataproc storage

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509041

Local HDFS is a good option if: Your jobs require a lot of metadata operations—for example, you have thousands of partitions and directories, and each file size is relatively small. You modify the HDFS data frequently or you rename directories. Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards. You heavily use the append operation on HDFS files. You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as in this example. You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation. In general, we recommend using Cloud Storage as the initial and final source of data in a big-data pipeline. For example, if a workflow contains five Spark jobs in series, the first job retrieves the initial data from Cloud Storage and then writes shuffle data and intermediate job output to HDFS. The final Spark job writes its results to Cloud Storage. Using Dataproc with Cloud Storage allows you to reduce the disk requirements and save costs by putting your data there instead of in the HDFS. When you keep your data on Cloud Storage and don't store it on the local HDFS, you can use smaller disks for your cluster. By making your cluster truly on-demand, you're also able to separate storage and compute, as noted earlier, which helps you reduce costs significantly. Even if you store all of your data in Cloud Storage, your Dataproc cluster needs HDFS for certain operations such as storing control and recovery files, or aggregating logs. It also needs non-HDFS local disk space for shuffling. You can reduce the disk size per worker if you are not heavily using the local HDFS. Here are some options to adjust the size of the local HDFS. Decrease the total size of the local HDFS by decreasing the size of primary persistent disks for the primary and workers. The primary persistent disk also contains the boot volume and system libraries, so allocate at least 100 GB. Increase the total size of the local HDFS by increasing the size of primary persistent disk for workers. Consider this option carefully— it's rare to have workloads that get better performance by using HDFS with standard persistent disks in comparison to using Cloud Storage or local HDFS with SSD. Attach up to eight SSDs to each worker and use these disks for the HDFS. This is a good option if you need to use the HDFS for I/O-intensive workloads and you need single-digit millisecond latency. Make sure that you use a machine type that has enough CPUs and memory on the worker to support these disks. And use SSD persistent disks for your primary or workers as a primary disk. You should understand the repercussions of geography and regions before you configure your data and jobs. Many Google Cloud services require you to specify regions or zones in which to allocate resources. The latency of requests can increase when the requests are made from a different region than the one where the resources are stored. Additionally, if the service's resources and your persistent data are located in different regions, some calls to Google Cloud services might copy all of the required data from one zone to another before processing. This can have a severe impact on performance. Cloud Storage is the primary way to store unstructured data in Google Cloud, but it isn't the only storage option. Some of your data might be better suited to storage in products designed explicitly for big data. You can use Bigtable to store large amounts of sparse data. Bigtable is an HBase-compliant API that offers low latency and high scalability to adapt to your jobs. For data warehousing, you can use BigQuery. Because Dataproc runs Hadoop on Google Cloud, using a persistent Dataproc cluster to replicate your on-premises setup might seem like the easiest solution. However, there are some limitations to that approach. Keeping your data in a persistent HDFS cluster using Dataproc is more expensive than storing your data in Cloud Storage, which is what we recommend. Keeping data in an HDFS cluster also limits your ability to use your data with other Google Cloud products. Augmenting or replacing some of your open-source-based tools with other related Google Cloud services can be more efficient or economical for particular use cases. Using a single, persistent Dataproc cluster for your jobs is more difficult to manage than shifting to targeted clusters that serve individual jobs or job areas. The most cost-effective and flexible way to migrate your Hadoop system to Google Cloud is to shift away from thinking in terms of large, multi-purpose, persistent clusters and instead think about small, short-lived clusters that are designed to run specific jobs. You store your data in Cloud Storage to support multiple, temporary processing clusters. This model is often called the ephemeral model, because the clusters you use for processing jobs are allocated as needed and are released as jobs finish. If you have efficient utilization, don't pay for resources that you don't use - employ scheduled deletion. A fixed amount of time after the cluster enters an idle state, you can automatically set a timer. You can give it a timestamp, and the count starts immediately once the expiration has been set. You can set a duration, the time in seconds to wait before automatically turning down the cluster. You can range from ten minutes as a minimum, to 14 days as a maximum at a granularity of one second. The biggest shift in your approach between running an on-premises Hadoop workflow and running the same workflow on Google Cloud is the shift away from monolithic, persistent clusters to specialized, ephemeral clusters. You spin up a cluster when you need to run a job and then delete it when the job completes. The resources required by your jobs are active only when they're being used, so you only pay for what you use. This approach enables you to tailor cluster configurations for individual jobs. Because you aren't maintaining and configuring a persistent cluster, you reduce the costs of resource use and cluster administration. This section describes how to move your existing Hadoop infrastructure to an ephemeral model. To get the most from Dataproc, customers need to move to an “ephemeral” model of only using clusters when they need them. This can be scary because a persistent cluster is comfortable. With Cloud Storage data persistence and fast boot of Dataproc, however, a persistent cluster is a waste of resources. If a persistent cluster is needed, make it small. Clusters can be resized anytime. Ephemeral model is the recommended route but it requires storage to be decoupled from compute. Separate job shapes and separate clusters. Decompose even further with job-scoped clusters. Isolate dev, staging, and production environments by running on separate clusters. Read from the same underlying data source on Cloud Storage. Add appropriate ACLs to service accounts to protect data. The point of ephemeral clusters is to use them only for the jobs' lifetime. When it's time to run a job, follow this process: Create a properly configured cluster. Run your job, sending output to Cloud Storage or another persistent location. Delete the cluster. Use your job output however you need to. View logs in Cloud Logging or Cloud Storage. If you can't accomplish your work without a persistent cluster, you can create one. This option may be costly and isn't recommended if there is a way to get your job done on ephemeral clusters. You can minimize the cost of a persistent cluster by: Creating the smallest cluster you can. Scoping your work on that cluster to the smallest possible number of jobs. And scaling the cluster to the minimum workable number of nodes, adding more dynamically to meet demand.

#### Optimizing Dataproc templates and autoscaling

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509042

The Dataproc Workflow Template is a YAML file that is processed through a Directed Acyclic Graph or DAG. It can create a new cluster, select from an existing cluster, submit jobs, hold jobs for submission until dependencies can complete, and it can delete a cluster when the job is done. It’s available through the gcloud command and the REST API. You can view existing workflow templates and instantiated workflows through the Google Cloud console as well. The Workflow Template becomes active when it is instantiated into the DAG. The Template can be submitted multiple times with different parameter values. You can also write a template inline in a gcloud command, and you can list workflows and workflow metadata to help diagnose issues. Here's an example of a Dataproc workflow template. First, we get all the things that need to be installed in the cluster using our startup scripts and manually echoing pip install commands like the one seen here to install matplotlib. You can have multiple startup shell scripts run like you see in this example. Next, we use the gcloud command for creating a new cluster in advance of running our job. We specify cluster parameters like the template to be used in our desired architecture and what machine types and image versions we want for hardware and software. After that, we need to add a job to the newly created cluster. In this example, we have a Spark job written in Python that exists in a Cloud Storage bucket that we control. Lastly, we need to submit this template itself as a new workflow template as you see with the last command. Dataproc autoscaling provides clusters that size themselves to the needs of the enterprise. Key features include: Jobs are “fire and forget”, There’s no need to manually intervene when a cluster is over or under capacity, You can choose between standard and preemptible workers, and You can save resources such as quota and cost at any point in time Autoscaling policies provide fine-grained control. This is based on the difference between YARN pending and available memory. If more memory is needed, then you scale up. If there’s excess memory, you scale down. Obey VM limits and scale based on scale factor. Dataproc autoscaling provides flexible capacity for more efficient utilization, making scaling decisions based on Hadoop YARN Metrics. It’s designed to be used only with off-cluster persistent data, not on-cluster HDFS or HBase. It works best with a cluster that processes a lot of jobs or that processes a single large job. It doesn’t support Spark Structured Streaming, a streaming service built on top of Spark SQL. It’s also not designed to scale to zero. So it’s not the best for sparsely utilized or idle clusters. In these cases it’s equally fast to terminate a cluster that’s idle and create a new cluster when it’s needed. For that purpose you would look at Dataproc Workflows or Cloud Composer, and Cluster Scheduled Deletion. One of the things that you want to consider when working with autoscaling is setting the initial workers. The number of initial workers is set from Worker Nodes, Nodes Minimum. Setting this value ensures that the cluster comes up to basic capacity faster than if you let autoscaling handle it. Because autoscaling might require multiple autoscale periods to scale up. The primary minimum number of workers may be the same as the cluster nodes minimum. There is a maximum that caps the number of worker nodes. If there is heavy load on the cluster, autoscaling determines it is time to scale up. The scale_up.factor determines how many nodes to launch. This would commonly be one node. But if you knew that a lot of demand would occur at once, maybe you want to scale up faster. After the action, there is a cooldown period to let things settle before autoscaling evaluation occurs again. The cooldown period reduces the chances that the cluster will start and terminate nodes at the same time. In this example, the extra capacity isn't needed. And there is a graceful decommission timeout to give running jobs a chance to complete before the node goes out of service. Notice there is a scale down factor. In this case it is scaling down by one node at a time for a more leisurely reduction in capacity. After the action, there is another cooldown period. And a second scale down, resulting in a return to the minimum number of workers. A secondary min_workers and max_workers controls the scale of preemptible workers.

#### Optimizing Dataproc monitoring

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509043

person: In Google Cloud, you can use Cloud Logging and Cloud Monitoring to view and customize logs, and to monitor jobs and resources. The best way to find what error caused a Spark job failure is to look at the driver output and the logs generated by the Spark executioners. Note, however, that if you submit a Spark job by connecting directly to the primary node using SSH, it's not possible to get the driver output. You can retrieve the driver program output by using the Cloud Console or by using G Cloud command. The output is also stored in the Cloud storage bucket of the Dataproc cluster. All other logs are located in different files inside the machines of the cluster. It's possible to see the logs for each container from the spark app Web UI, or from the history server after the program ends in the executer's tab. You need to browse through each Spark container to view each log. If you write logs or print to standard out or standard air in your application code, the logs are saved in the redirection of standard out or standard air. In a Dataproc cluster, Yarn is configured to collect all these logs by default, and they're available in Cloud Logging. Logging provides a consolidated and concise view of all logs so that you don't need to spend time browsing among container logs to find errors. This screen shows the login page in the Cloud Console. You can view all logs from your Dataproc cluster by selecting the cluster's name in the selector menu. Don't forget to expand the time duration in the time range selector. You can get logs from a Spark application by filtering by its ID, you can get the application ID from the driver output. To find logs faster, you can create and use your own labels for each cluster or for each Dataproc job. For example, you can create a label with the key environment or ENV as the value in the exploration and use it for your data exploration job. You can then get logs for all exploration job creations by filtering with the label environment with a value exploration in logging. Note that this filter will not return all logs for this job, only the resource creation logs. You can set the driver log level using the following G Cloud command: G Cloud, Dataproc, jobs, submit, Hadoop, with the parameter driver log levels. You set the log level for the rest of the application from the spark context, for example, Spark dot Spark context dot set log level. And for here, we'll just say the example is debug. Cloud Monitoring can monitor the cluster's CPU, disk, network usage and Yarn resources. You can create a custom dashboard to get up to date charts for these and other metrics. Dataproc runs on top of Compute Engine. If you want to visualize CPU usage, disk IO or networking metrics in a chart, you need to select a Compute Engine VM instance as the resource type, and then filter by the cluster name. This diagram shows an example of the output. To view metrics for Spark queries, jobs, stages, or tasks, connect to the spark applications Web UI.

#### Lab Intro: Running Apache Spark jobs on Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509044

person: Now it's time for our lab. In this lab, you'll be running Apache Spark jobs on Dataproc. Let's take a look at what you're going to do. First, you're going to migrate existing Spark job code to Dataproc. Then you'll be modifying your Spark jobs to use a different backend that's Cloud Storage instead of HDFS. Finally, you optimize Spark jobs to run on job specific clusters. Good luck.

#### Running Apache Spark jobs on Cloud Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509045

#### Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509046

person: Welcome to the end of the module. Let's do a brief recap. You saw how you can run your entire Hadoop ecosystem on the Cloud with Dataproc. We covered the advantages of separating compute and storage for cost efficiency and performance by using cloud storage instead of HDFS. Lastly, we discussed how you can optimize Dataproc by resizing your cluster as your needs change and enable smart features like automatically turning down the cluster after a certain period of nonuse.

#### Executing Spark on Dataproc

- https://www.cloudskillsboost.google/paths/16/course_templates/53/quizzes/509047

### Serverless Data Processing with Dataflow

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509048

Earlier in the course, you saw how to do batch data processing with Dataproc and other methods. Now it's time to introduce you to a key serverless tool that should be in your data engineering toolkit, Dataflow. This entire module will cover batch Dataflow pipelines and why Dataflow is a commonly used data pipeline tool on Google Cloud. Not to give away too much of the answer, but you can write the same code to do both batch and streaming pipelines with Dataflow. We'll cover streaming pipelines later. So the topics we will address are how to decide between Dataflow and Dataproc, why customers value Dataflow, Dataflow pipelines and Dataflow templates. Let's get started.

#### Introduction to Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509049

person: Let's start by exploring Dataflow in more detail. The reason Dataflow is the preferred way to do data processing on Google Cloud is that Dataflow is serverless. You don't have to manage clusters at all. Unlike with Dataproc, the auto scaling in Dataflow scales step by step, it's very fine grained. Plus, as we will see in the next course, Dataflow allows you to use the same code for both batch and stream. This is becoming increasingly important. When building a new data processing pipeline, we recommend that you use Dataflow. If on the other hand, you have existing pipelines written using Hadoop technologies, it may not be worth it to rewrite everything, migrate it over to Google Cloud using Dataproc and then modernize it as necessary. As a data engineer, we recommend that you learn both Dataflow and Dataproc and make the choice based on what's best for a specific use case. If the project has existing Hadoop or Spark dependencies, use Dataproc. Please keep in mind that there are many subjective issues when making this decision, and that no simple guide will fit every use case. Sometimes the production team might be much more comfortable with a DevOps approach where they provision machines than with a serverless approach. In that case too, you might pick Dataproc. If you don't care about streaming and your primary goal is to move existing workloads, then Dataproc would be fine. Dataflow, however, is our recommended approach for building pipelines. Dataflow provides a serverless way to execute pipelines on batch and streaming data, it's scalable to process more data, Dataflow will scale out to more machines, it will do this transparently. The stream processing capability also makes it low latency, you can process the data as it comes in. This ability to process batch and stream with the same code is rather unique. For a long time, batch programming and data processing used to be two very separate and different things. Batch programming dates to the 1940s and the early days of computing where it was realized that you can think of two separate concepts, code and data. Use code to process data. Of course, both of these were on punch cards. So that's what you were processing, a box of punch cards call a batch. It was a job that started and ended when the data was fully processed. Stream processing on the other hand is more fluid. It arose in the 1970s with the idea of data processing being something that is ongoing. The idea is that data keeps coming in and you process the data, the processing itself tended to be done in micro batches. The genius of beam is that it provides abstractions that unify traditional batch programming concepts and traditional data processing concepts. Unifying programming and processing is a big innovation in data engineering. The four main concepts are P transforms, P collections, pipelines and pipeline runners. A pipeline identifies the data to be processed and the actions to be taken on the data. The data is held in a distributed data abstraction called a P collection. The P collection is immutable. Any change that happens in a pipeline ingests one P collection and creates a new one. It does not change the incoming P collection. The action or code is contained in an abstraction called a P transform. P transform handles input, transformation, an output of the data. The data in the P collection is passed along a graph from one P transform to another. Pipeline runners are analogous to container hosts such as Google Kubernetes Engine. The identical pipeline can be run on a local computer, data center VM, or on a service such as Dataflow in the Cloud. The only difference is scale and access to platform specific services. The services the runner uses to execute the code is called a backend system. Immutable data is one of the key differences between batch programming and data processing. Immutable data where each transform results in a new copy means there is no need to coordinate access control or sharing of the original ingest data. So it enables or at least simplifies distributed processing. The shape of a pipeline is not actually just a singular linear progression but rather a directed graph with brunches and aggregations. For historical reasons, we refer to it as a pipeline, but a data graph or Dataflow might be a more accurate description. A P collection represents both streaming data and batch data. There is no size limit to a P collection. Streaming data is an unbounded P collection that doesn't end. Each element inside a P collection can be individually accessed and processed. This is how distributed processing of the P collection is implemented. So you define the pipeline and the transforms on the P collection and the runner handles implementing the transformations on each element distributing the work as needed for scale and with available resources. Once an element is created in a P collection, it is immutable, so it can never be changed or deleted. Elements represent different data types. In traditional programs, a data type is stored in memory with a format that favors processing. Integers in memory are different from characters which are different from strings and compound data types. In a P collection, all data types are stored in a serialized state as byte strings. This way, there is no need to serialize data prior to network transfer and deserialize it when it is received. Instead, the data moves through the system in a serialized state and is only deserialized when necessary for the actions of a P transform.

#### Why customers value Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509050

person: So we've discussed what Dataflow is, but why do data engineers value Dataflow over other alternatives for data processing? To understand that, it helps to understand a bit about how Dataflow works. Dataflow provides an efficient execution mechanism for Apache Beam. The Beam pipeline specifies what has to be done. The Dataflow services chooses how to run the pipeline. The pipeline typically consists of reading data from one or more sources, applying processing to the data and writing it to one or more sinks. In order to execute the pipeline, the Dataflow service first optimizes the graph by, for example, fusing transforms together. It then breaks the jobs into units of work and schedules them to various workers. One of the great things about Dataflow is that the optimization is always ongoing. Units of work are continually rebalanced. Resources, both compute and storage, are deployed on demand and on a per job basis. Resources are torn down at the end of a job stage or on downscaling. Work scheduled on a resource is guaranteed to be processed. Work can be dynamically rebalanced across resources. This provides fault tolerance. The watermarking handles late arrivals of data and comes with restarts, monitoring and logging. No more waiting for other jobs to finish. No more preemptive scheduling. Dataflow provides a reliable, serverless, job-specific way to process your data. To summarize, the advantages of Dataflow are, first, Dataflow is fully managed and auto configured. Just deploy your pipeline. Second, Dataflow doesn't just execute the Apache Beam transforms as is. It optimizes the graph, fusing operations, as we see with C and D. Also, it doesn't wait for a previous step to finish before starting a new step. We see this with A and the Group By Key. Third, autoscaling happens step-by-step in the middle of a job. As the job needs more resources, it receives more resources. You don't have to manually scale resources to match job needs. If some machines have finished their tasks and others are still going on, the tasks queued up for the busy ones are rebalanced out to the idle machines. This way, the overall job finishes faster. Dynamic work rebalancing in mid-job removes the need to spend operational or analyst resource time hunting down hotkeys. All this happens while maintaining strong streaming semantics. Aggregations, like sums and counts, are correct even if the input source sends duplicate records. Dataflow is able to handle late arriving records. Finally, Dataflow functions as the glue that ties together many of the services on Google Cloud. Do you need to read from BigQuery and write to BigTable? Use Dataflow. Do you need to read from Pub/Sub and write to Cloud SQL? Use Dataflow.

#### Building Dataflow pipelines in code

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509051

person: Let's look in greater detail at an example Dataflow pipeline. Here's how to construct a simple pipeline where you have an input PCollection and pass it through three PTransforms and get an output PCollection. The syntax is shown in Python. You have the input, the pipe symbol, the first PTransform, the pipe symbol, the second PTransform, et cetera. The pipe operator essentially applies the transform to the input PCollection and sends out an output PCollection. The first three times, we don't give the output a name, simply pass it on the next step. The output of PTransform_3, though, we save into a PCollection variable named PCollection_out. In Java, it is the same thing, except that, instead of the pipe symbol, we use the apply method. If you want to do branching, just send the same PCollection through two different transforms. Give the output PCollection variable in each case a name. Then you can use it in the remainder of your program. Here, for example, we take the PCollection_in and pass the collection first through both PTransform_1 then through PTransform_2. The result of the first case, we store as PCollection_out_1. In the second case, we store it as PCollection_out_2. What we showed you so far is the middle part of a pipeline. You already had a PCollection, and you applied a bunch of transforms, and you end up with a PCollection, but where does the pipeline start? How do you get the first PCollection? You get it from a source. What does a pipeline do with the final PCollection? Typically, it writes out to a sink. That's what we are showing here. This is Python. We create a PCollection by taking the pipeline object P and passing it over a text file in cloud storage. That's the read from text line. Then, we apply the PTransform called FlatMap to the lines read from the text file. What FlatMap does is that it applies a function to each row of the input and concatenates all the outputs. When the function is applied to a row, it may return zero or more elements that go to the output PCollection. The function in this case is the function called count_words. It takes a line of text and returns an integer. The output PCollection then consists of a set of integers. These integers are written to a text file in cloud storage. Because the pipeline was created in a with clause and because this is not a streaming pipeline, exiting the with clause automatically stops the pipeline. Once you have written the pipeline, it's time to run it. Executing the Python program on the previous slide will run the program. By default, the program is run using the default runner, which runs on the same machine where the Python program was executed. When you create the pipeline, you can pass in a set of options. One of these options is the runner. Specify that as Dataflow to have the pipeline run on Google Cloud. This example contains hard coded variables, which in most cases is not a preferred practice for programming at scale. Of course, normally you will set up command line parameters to transparently switch between local and cloud. Simply running main runs the pipeline locally. To run on cloud, specify cloud parameters.

#### Key considerations with designing pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509052

person: To design pipelines, you need to know how each step works on the individual data elements contained inside of a PCollection. Let's start with the input and outputs of the pipeline. First, we set up our Beam pipeline with beam. Pipeline and pass through any options. Here, we'll call the pipeline P. Now it's time to get some data as input. If we wanted to read a series of CSV files in Cloud Storage, we could use beam.io. ReadFromText and simply parse in the Cloud Storage bucket and file name. Note the use of an asterisk wild card can handle multiple files. If we wanted to read instead from a Pub/Sub topic, you would still use beam.io, but instead it's ReadStringsFromPubSub, and you'd have to parse in the topic name. What about if you wanted to read in data that's already in BigQuery? Here's how that would look. You'd prepare your SQL query and specify BigQuery as your input source and then parse in the query and source as a read function to Dataflow. These are just a few of the data sources from which Dataflow can read. But now what about writing to sinks? Take the BigQuery example but as a data sink this time. With Dataflow, you can write to a BigQuery table, as you can see here. First, you establish the reference to the BigQuery table with what BigQuery expects, your project ID, data set ID and table name. Then you use beam.io. WriteToBigQuery as a sink to your pipeline. Note that we are using the normal BigQuery options here for write_disposition. Here, we're truncating the table if it exists, meaning to drop data rows. If the table doesn't exist, we can create it if needed. Naturally, this is a batch pipeline if we're truncating the table with each load. You can also create a PCollection in memory without reading from a particular source. Why might you do this? If you have a small data set, like a lookup table or a hard coded list, you could create the PCollection yourself, as you can see here. Then we can call a pipeline step on this new PCollection just as if we sourced it from somewhere else.

#### Transforming data with PTransforms

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509053

person: Now that we have looked at how to get the data in, let's look at how we transform each data element in the PCollection with PTransforms. The first step of any map produced process is the map phase, where you're doing something in parallel. In the word length example, there is one length output for each word input, so the word dog would map to three for length. In the bottom graph example, the function my_grep returns each instance of the term it's searching for in the line. There may be multiple instances of the term in a single line in a one-to-many relationship. In this case, you may want my_grep to return the next instance each time it's called, which is why the function has been implemented with a generator using yields. The yield command has the effect of preserving the seed of the function so that the next time it's called, it can continue from where it left off. FlatMap has the effect of iterating over one-to-many relationships. The map example returns a key value pair. In Python, this is simply a two tuple for each word. The FlatMap example yields the line only for lines that contain the search term. ParDo is a common intermediate step in a pipeline. You might use it to extract certain fields from a set of raw input records or convert raw input into a different format. You might also use ParDo to convert process data into an output format, like table rows for BigQuery or strings for printing. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it. If your input PCollection contains elements that are of a different type or format than you want, you can use ParDo to perform a conversion on each element and output the result to a new PCollection. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection. You can use ParDo to perform simple or complex computations on every element or certain elements of a PCollection and output the results as a new PCollection. When you apply a ParDo transform, you need to provide code in the form of a DoFn object. A DoFn is a Beam SDK class that defines a distributed processing function. Your DoFn code must be fully serializable, item potent and thread safe. In this example, we're just counting the number of words in a line and returning the length of the line. Transformations are always going to work on one element at a time here. Here we have an example from Python which can return multiple variables. In this example, we have below and above, some cutoff in our data elements and return two different types, below and above, two different variables by referencing these properties of the results.

#### Lab Intro: Building a Simple Dataflow Pipeline

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509054

Person: Next, let's do a lab: A simple Dataflow pipeline to perform serverless data analysis using Python or Java. You can select which version of the lab you'd like to do.

#### A Simple Dataflow Pipeline (Python) 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509055

#### Serverless Data Analysis with Dataflow: A Simple Dataflow Pipeline (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509056

#### Aggregate with GroupByKey and Combine

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509057

Now, let's look at more capabilities of the Dataflow model. What do you do after the map phase? The unnamed phase is the shuffle phase where you group together like keys. This works on a PCollection of key-value pairs or two elements tuples. Groups by common key and returns a single key-value pair where the value is actually a group of values. The idea here is that we want to find all the zip codes associated with the city. For example, New York is a city and it may have one-zero-zero-zero-one and one-zero-zero-zero-two zip codes. You could first create a key-value pair and a ParDo and then group by the key. The resulting key-value pairs are simply two tuples. We do have to be aware of data skew on we're doing this. When the same example is scaled up in the presence of skewed data, the situation becomes much worse. Let's say that you're doing your GroupByKey, but your group has 1 million items in it. One million is not too big of a deal on modern hardware, but with one billion you're forcing all of those elements to go to a single work group to be counted. This could definitely run into some issues on the network. This is the same performance concern when doing high cardinality group by queries on billions of records in BigQuery. In this example, there are a million X values and only a thousand Y values. GroupByKey will group all of the X values on one worker. The worker will take much longer to do its processing on the million values than the other worker, which only has a thousand values to process. Of course, you're paying for the worker that sits idle waiting for the other worker to complete. Dataflow is designed to avoid efficiencies by keeping the data balance. You can help by designing your application to divide work into aggregation steps and subsequent steps and to avoid grouping or to push grouping towards the end of the processing pipeline. CoGroupByKey is very similar. It groups results across several PCollections by key. The result for each key is a tuple of the values associated with that key in each input collection. Now, we can move to the reduce phase. How do we calculate totals or averages or other aggregations on our PCollections? Combined is used to combine collections of elements or values in your data. Combine has variants that work on entire PCollections and some that combine the values for each key and PCollections of key-value pairs. CombineGloballlyfn reduces a PCollection to a single value by applying the FN or the function. CombinePerKey is similar to GroupByKey, but combines the values by a combined function or a callable that takes an iterable action such as sum or max. When you apply a combine transform, you must provide the function that contains the logic for combining the elements or values. There are pre-built combined functions for common numeric combination operations such as sum, min, and max. Simple combine operations such as sums can usually be implemented as a simple function. More complex combination operations might require you to create a subclass of a combine function that has an accumulation type distinct from the input and or output site. The combining function should be commutative and associative, as the function is not necessarily invoked exactly once on all values within a given key. Because the input data including the value collection may be distributed across multiple workers, the combining function might be called multiple times to perform multiple combining on subsets of the value collection. For more complex combined functions, you can define a subclass of combine function. You should use the combine function if the action needed requires a more sophisticated accumulator, must perform additional pre or post processing, might change the output type, or takes the key into account. A general combining operation consists of four operations. When you create a subclass of combine function, you must provide four operations by overriding the corresponding methods. Create accumulator creates a new local accumulator. In the example case taking a mean average, a local accumulator tracks the running sum of values. Combine is orders of magnitude faster than GroupByKey because Dataflow knows how to parallelize a combine step. The way that GroupByKey works, Dataflow can use no more than one worker per key. In this example, GroupByKey causes all the values to be shuffled so they are all transmitted over the network. And then there is one worker for the 'x' key and one worker for the 'y' key. Combine allows Dataflow to distribute a key to multiple workers and process it in parallel. In this example, CombineByKey first aggregates values and then processes the aggregates with multiple workers. Also, only 6 aggregate values need to be passed over the network. Combine is a Java interface that tells Dataflow that the combine operation (like Count) is both commutative and associative. This allows Dataflow to shard within a key vs. having to group each key first. As a developer, you can create your own custom Combine class for any operation that has commutative and associative properties. Flatten works a lot like a SQL UNION. It's a beam transform for PCollection objects that store the same data type. Flatten merges multiple PCollection objects into a single logical PCollection. Partition is also a beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections. You might use partition if, for example, you wanted to calculate percentages or quartiles and the top quartile has different processing than all the others.

#### Lab Intro: MapReduce in Beam

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509058

In this next lab, you'll practice creating and performing Map and Reduce operations on PCollections as part of your pipeline. You can choose between Python and Java. You'll first identify the Map and Reduce operations, then execute the pipeline, and lastly, modify command-line parameters.

#### MapReduce in Beam (Python) 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509059

#### Serverless Data Analysis with Beam: MapReduce in Beam (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509060

#### Side inputs and windows of data

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509061

Person: In this lesson, you'll learn about the role of side inputs and windows. In addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your do function can access each time it processes an element in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transform's do function while processing each element. Side inputs are useful if your ParDo needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime and not hard coded. Such values might be determined by the input data or depend on a different branch of your pipeline. Here's how side inputs work. This is an example in Python. This set of steps is actually a subgraph of our overall graph. It begins with words that run through the map function to get the length and then combine globally to compute the total lengths across the whole data set. So if we were trying to figure out if any given word is shorter or longer than the average word length, first we need to compute the average word length using these steps. But then this whole branch can be fed into this method. That's what creates the view which is static and then becomes available to all the worker nodes for later use. That is a side input you see here. Before we go to the next lab, here are a few notes about additional capabilities. Many transforms have two parts. One occurs item at a time until all items are processed, and another occurs after the last item is processed. One of the easiest analogies is the arithmetic mean. You can add up the value of each element and keep count. This is the accumulation step. After you have processed all the elements, you have a total of all the values read and a count of the number of values read. The last thing to do is divide the total by the count. This is fine so long as you know you have read the last item, but if you have an unbounded data set, there is no predetermined end, so you just keep adding and never break out of the loop and perform the division. The global window is not very useful for an unbounded PCollection, meaning streaming data. The timing associated with the elements in an unbounded PCollection is usually important to processing the data. An unbounded PCollection has no defined end or last element, so it can never perform the completion step. This is particularly important for GroupByKey and Combine, which perform the shuffle after end. The discussion about unbounded PCollections and Windows will be continued in the course on processing streaming data. The global window is a default, and here, you can see how you can set it with beam. WindowInto(window. GlobalWindows). So are streaming pipelines out of luck if they can't use the global window? No. You can use time-based windows which can be useful for processing data that comes in streaming at different times. We'll cover this in detail in the streaming course. For batch inputs, you can group by time as well. You can explicitly admit a time stamp in your pipeline instead of standard output. In this example, an offline access log is being read, and the date/time stamp is extracted and used for windowing. Here, we're using Windows to aggregate our batch data by time. Subsequent groups, aggregations and so forth are computed only within the time window. This example here uses a sliding window. As you can see, with beam. WindowInto(bean.window. SlidingWindows(60, 30)), which means capture 60 seconds worth of data but start a new window every 30 seconds. So for example, say you had all of your sales records, and you wanted to compute sales by day. You'd just extract that time stamp field that represents the time stamp. Then you would create fixed windows with a 1-day duration, and Dataflow automatically will compute the sum over each window to computer those totals. The main thing to remember here is that you can do this in batch. Discussion of streaming continues in the streaming data processing course.

#### Lab Intro: Serverless Data Analysis with Dataflow: Side Inputs

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509062

person: In this lab, you'll practice creating siphon pods to your dataflow pipeline. You once again have the choice to complete the lab using Python or Java. Specifically, you'll bring in data from BigQuery into your pipeline and then execute the job on Dataflow.

#### Serverless Data Analysis with Dataflow: Side Inputs (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509063

#### Serverless Data Analysis with Dataflow: Side Inputs (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509064

#### Creating and re-using pipeline templates

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509065

person: Next, we'll look at Dataflow Templates where you as a data engineer can create new templates for your team to leverage. You can also start from some of Google's pre-existing templates which we'll cover as well. Dataflow Templates allow users who don't have any coding capability to execute their Dataflow job. It enables the rapid deployment of standard types of data transformation jobs, removing the need to develop the pipeline code and removing the need to consider the management of components dependencies in the pipeline code. In the traditional workflow, the developer creates the pipeline in the development environment using the Dataflow SDK in Java or Python, and there are dependencies to the original language and SDK files. Whenever a job is submitted, it is reprocessed entirely or recompiled. There is no separation of developers from users, so the users basically have to be developers or have the same access and resources as developers. Dataflow Templates enable a new development in execution workflow. The templates help separate the development activities and the developers from the execution activities and the users. The user environment no longer has dependencies back to the development environment. The need for recompilation to run a job is limited. The new approach facilitates the scheduling of batch jobs and opens up more ways for users to submit jobs and more opportunities for automation. App developers, database administrators, analysts and data scientists can use Templates as a solution. You can also run them using the command-line tool or REST API as you see here. Simply specify the Cloud Storage location of your template that you already have. Alternatively, you can use the Google provided templates. After you create and stage your Dataflow template, execute the template with the Cloud Console, REST API, or gcloud command-line tool. You can deploy Dataflow Template jobs from many environments, including App Engine Standard Environment, Cloud Functions and other constrained environments. What if you wanted to create your own template? To create your own template, you'll add your own value providers. This is what parses the command-line or optional arguments to your template, and that is how users can specify optional arguments. Once a template file is created, you call it from an API. You might not have considered this before, but values like user options and input file that are compiled into your job, they aren't just parameters. They are compiled time parameters. To make these values available to non-developer users, they have to be converted to runtime parameters. These work through the Value Provider interface so that your users can set these values when the template is submitted. Value Provider can be used in IO, transformations and your functions. There are also static and nested versions of Value Provider for more complex cases. This is a Java example for creating your own template. Note that Value Providers are passed down throughout the whole pipeline construction phase. Sometimes we need to transform a value from what the user passes at runtime to what a source or sync expects to consume. Nested value providers meet this need. Each template has associated metadata with it upon creation. This will help your downstream users know what your template is doing and what parameters it expects. The metadata file is located in the same directory as your template and simply has the underscore metadata suffix to the name.

#### Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509066

person: Earlier in the course, you learned how to do batch processing of your Hadoop and Spark jobs using Dataproc. This is an ideal first step through the cloud for existing jobs. Simply run them on Dataproc, and they just work. You learned that Dataflow takes a lot of the cluster resizing and other management tasks and automates them for you as a true serverless product. Use Dataflow if you're writing new pipelines or if you're ready to rewrite and migrate your Hadoop jobs to faster processing with Apache Beam on Dataflow. You then saw how to build pipelines using Apache Beam, which is open source. For the pipelines to work, we created inputs with a Beam.io syntax and walked through how you can read CSV files from Cloud Storage, streaming message queues from Pub/Sub and structured data already living in BigQuery. We then looked at some key considerations when designing your pipeline. Recall that you should consider using combine when you can instead of GroupByKey, especially if your data is heavily skewed. This will prevent a single worker from being a bottleneck if you have a high cardinality data set. To do the actual transformations, you practiced writing PTransforms in your labs. Remember that the P in PTransforms and PCollections means parallel. Recall that the PCollection itself is immutable. Data is never processed in place. A new PCollection is always created, and the individual elements of a PCollection are massively distributed over many workers to perform the parallel transform. This is a whole map part of map reduce. For the reduce part of map reduce, we looked at aggregation functions like GroupByKey and combine. Keep in mind you can have multiple parallel parts of your pipeline combine into a single PTransform like in aggregation. The pipeline does not have to execute in serial unless you've set it up that way with dependencies. After that, you practiced with side inputs in your lab and how to create windows of data even for batch data sets. Lastly, you saw how to create and save new Dataflow templates for your team to use and where you can see Google's premade templates in our public GitHub.

#### Serverless Data Processing with Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/53/quizzes/509067

### Manage Data Pipelines with Cloud Data Fusion and Cloud Composer

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509068

Person: In this module, we will discuss how to manage data pipelines with Cloud Data Fusion and Cloud Composer. Specifically, we will look at how you can use Cloud Data Fusion to visually build data pipelines and how you can use Cloud Composer to orchestrate work between Google Cloud services.

#### Introduction to Cloud Data Fusion

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509069

Let’s start with an introduction to Cloud Data Fusion. Cloud Data Fusion provides a graphical user interface and APIs that increase time efficiency and reduce complexity. It equips business users, developers, and data scientists to quickly and easily build, deploy, and manage data integration pipelines. Cloud Data Fusion is essentially a graphical no code tool to build data pipelines. Cloud Data Fusion is used by developers, data scientists, and business analysts alike. For developers, Cloud Data Fusion allows you to cleanse, match, remove duplicates, blend, transform, partition, transfer, standardize, automate, and monitor data. Data scientists can use Cloud Data Fusion to visually build integration pipelines, test, debug, and deploy applications. Business analysts can run Cloud Data Fusion at scale on Google Cloud, operationalized pipelines, and inspect rich integration metadata. Cloud Data Fusion offers a number of benefits. Integrate with any data - through a rich ecosystem of connectors for a variety of legacy and modern systems, relational databases, file systems, cloud services, object stores, NoSQL, EBCDIC, and more. Increase productivity - If you have to constantly move between numerous systems to gather insight, your productivity is significantly reduced. With Cloud Data Fusion, your data from all the different sources can be pooled into a view like in BigQuery, Spanner, or any other Google Cloud technologies, allowing you to be more productive faster. Reduce complexity - through a visual interface for building data pipelines, code free transformations, and reusable pipeline templates. Increase flexibility - through support for on-prem and cloud environments, interoperability with the Open source software CDAP. At a high level, Cloud Data Fusion provides you with a graphical user interface to build data pipelines with no code. You can use existing templates, connectors to Google Cloud, and other Cloud services providers, and an entire library of transformations to help you get your data in the format and quality you want. Also, you can test and debug the pipeline and follow along with each node as it receives and processes data. As you will see in the next lesson, you can tag pipelines to help organize them more efficiently for your team, and you can use the unified search functionality to quickly find field values or other keywords across your pipelines and schemas. Lastly, we’ll talk about how Cloud Data Fusion tracks the lineage of transformations that happen before and after any given field on your dataset. One of the advantages of Cloud Data Fusion is that it's extensible. This includes the ability to templatize pipelines, create conditional triggers, and manage and templatize plugins. There is a UI widget plug-in as well as custom provisioners, custom compute profiles, and the ability to integrate to hubs.

#### Components of Cloud Data Fusion

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509070

The two major user interface components we will focus our attention on in this course, are the Wrangler UI for exploring data sets visually, and building pipelines with no code, and the Data Pipeline UI for drawing pipelines right on to a canvas. You can choose from existing templates for common data processing tasks like Cloud Storage to BigQuery. There are other features of Cloud Data Fusion that you should be aware of too. There's an integrated rules engine where business users can program in their pre-defined checks and transformations, and store them in a single place. Then data engineers can call these rules as part of a rule book or pipeline later. We mentioned data lineage as part of field metadata earlier. You can use the metadata aggregator to access the lineage of each field in a single UI and analyze other rich metadata about your pipelines and schemas as well. For example, you can create and share a data dictionary for your schemas directly within the tool. Other features, such as the microservice framework, allow you to build specialized logic for processing data. You can also use the Event Condition Action (ECA) Application to parse any event, trigger conditions, and execute an action based on those conditions.

#### Cloud Data Fusion UI

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509071

person: Managing your pipelines is easiest when you have the right tools. We'll now take a high level look at the Cloud Data Fusion UI as you saw in the component overview. Here are some of the key user interface elements that you will encounter when using Data Fusion. Let's look at each of them in turn. Under Control Center is the section for applications, artifacts and a data set. Here you could have multiple pipelines associated with a particular application. The Control Center gives you the ability to see everything at a glance and search for what you need, whether it's a particular data set, pipeline or other artifact, like a data dictionary, for example. Under the Pipeline section, you have a developer studio. You can preview, export, schedule a job or project. You also have a connector and a function palette and a navigation section. Under the Wrangler section, you have connections, transforms, data quality, insights and functions. Under the Integration metadata section, you can search, add tags and properties and see the data lineage for field and data. The Hub allows you to see all the available plugins, sample use cases and prebuilt pipelines. Entities include the ability to create pipelines, upload an application, plugin, driver, library and directives. There are two components in Administration, management and configuration. Under management, you have services and metrics. Under configuration, you have namespace, compute profiles, preferences, system artifacts and the REST client.

#### Build a pipeline

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509072

>> Now that we've looked at the components in the UI, we'll discuss the process of building a data pipeline. A pipeline is represented visually as a series of stages arranged in a graph. These graphs are called DAGs, or Directed Acyclic Graphs, because they flow from one direction to another, and they cannot feed into themselves. Acyclic simply means not a circle. Each stage is a node. And as you can see here, nodes can be of a different type, you may start with a node that pulls data from Cloud Storage, then passes it on to a node that parses a CSV. The next node takes multiple nodes, has an input and joins them together before passing the joined data to two separate data sink nodes. As you saw in our previous example, you can have multiple nodes fork out from a single parent node. This is useful because you may want to kick off another data processing work stream that should not be blocked by any processing on a separate series of nodes. You can combine data from two or more nodes into a single output in a sink. In Cloud data fusion, the studio is the user interface where you author and create new pipelines. The area where you create nodes and chain them together in your pipeline is your canvas. If you have many nodes in a pipeline, the Canvas can get visually cluttered, so use the mini map to help navigate around a huge pipeline quickly. You can interact with the canvas and add objects by using the Canvas control panel. When you're ready to save and run the entire pipeline, you can do so with the pipeline actions toolbar at the top. Don't forget to give your pipeline a name and description, as well as make use of the many pre-existing templates and plugins so you don't have to write your pipeline from scratch. Here, we've used a template or data pipeline batch, which gives us the three nodes you see here to move data from a Cloud storage file, process it in a wrangler and output it to BigQuery. You should make use of preview mode before you deploy and run your pipeline in production to ensure everything you run will run properly. While a pipeline is in preview, you can click on each node and see any sample data or errors that you will need to correct before deploying. After deployment. You can monitor the health of your pipeline and collect key summary stats of each execution. Here, we're ingesting data from Twitter and Google Cloud and parsing each tweet before loading them into a variety of data sinks. If you have multiple pipelines, it's recommended that you make liberal use of the tags feature to help you quickly find and organize each pipeline for your organization. You can view the start time, the duration of the pipeline run and the overall summary across runs for each pipeline. You can quickly see the data throughput at each node in the pipeline simply by interacting with the node. Note the compute profile used in the Cloud. Clicking on a node gives you detail on the inputs, outputs and errors for that given node. Here, we are integrating with the speech to text API to process audio files into searchable text. You can track the individual health of each node and get useful metrics like records out per second, average processing time, and Max processing time, which can alert you to any anomalies in your pipeline. You can set your pipelines to run automatically at certain intervals. If your pipeline normally takes a long time to process the entire data set, you can also specify a maximum number of concurrent runs to help avoid processing data unnecessarily. Keep in mind that Cloud data fusion is designed for batch data pipelines. We'll dive into streaming data pipelines in future modules. One of the big features of Cloud data fusion is the ability to attract the lineage of a given field value. Let's take this example of a campaign field for double click data set and track every transform operation that happened before and after this field. Here, you can see the lineage of operations that are applied to the campaign field between the campaign dataset and the double click dataset. Note the time this field was last changed by a pipeline run and each of the input fields and descriptions that interacted with the field as part of processing it between datasets. Imagine the use cases if you've inherited a set of analytical reports and you want to walk back upstream all of the logic that went into a certain field. Well, now you can.

#### Explore data using wrangler

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509073

person: We've discussed the core components, tools and processes of building data pipelines. Now we'll look at using Wrangler to explore the data set. So far in the course, we have focused on building new pipelines for our data sets. That presumes we know what the data is and what transformations need to be made already. Oftentimes, a new data set still needs to be explored and analyzed for insights. The Wrangler UI is the cloud data fusion environment for exploring new data sets visually for insights. Here, you can inspect the data set and build a series of transformation steps called directives to stitch together a pipeline. Here's what the Wrangler UI looks like. Starting from the left, you have your connections to existing data sets. You can add new connections to a variety of data sources like Google cloud storage, BigQuery or even other cloud providers. Once you specify your connection, you can browse all of the files and tables in that source. Here, you can see a cloud storage bucket of demo data sets and all the CSV files of customer complaints. Once you've found an example data set like customers.csv here, you can explore the rows and columns visually and view sample insights. As you explore the data, you might want to create new calculated fields, drop columns, filter rows or otherwise wrangle the data. You can do so using the Wrangler UI by adding new directives to form a data transformation recipe. When you're happy with your transformations, you can create a pipeline that you can then run at regular intervals.

#### Lab Intro: Building and executing a pipeline graph in Cloud Data Fusion

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509074

- Now it's time for you to practice building and executing a pipeline graph in Cloud Data Fusion. In this lab, you will connect Cloud Data Fusion to a couple of data sources, apply basic transformations, join two data sources, and write data to a sink.

#### Building and Executing a Pipeline Graph with Data Fusion 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509075

#### Orchestrate work between Google Cloud services with Cloud Composer

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509076

person: The next big task for managing data pipelines is to orchestrate the work across multiple Google Cloud services. For example, if you have three cloud data fusion pipelines, and two ML models that you wanted to run in a certain order, you need an orchestration engine. In this module, we'll look at using Cloud Composer to help out with tasks like that. Cloud Composer will control the Google Cloud services that we need to run. What Cloud Composer is simply a serverless environment on which an open source workflow tool runs. That workflow tool is called Apache airflow, which is an open source orchestration engine. The heart of any workflow is D.A.G. As you saw with cloud data fusion, You're also building D.A.G.s with Apache Airflow as you see here. What's happening in this particular D.A.G. are four tasks that update our training data, export it, retrain our model, and we deploy it. You can tell your D.A.G. to do pretty much anything you need it to do. Here, it's sending tasks to BigQuery, cloud storage, and Vertex AI. But yours could orchestrate among four completely different services.

#### Apache Airflow environment

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509077

person: Let's preview the actual Cloud Composer environment. Once you use the command line or Google Cloud web UI to launch a Cloud Composer instance, you'll be met with a screen like this. Keep in mind that you can have multiple Cloud Composer environments, and with each environment, you can have a separate Apache Airflow instance, which could have zero to many DAGs. An important note here is that sometimes you'll be required to edit environment variables for your workflows, like specifying your specific Google Cloud project account. Normally, you will not do that at the Cloud Composer level but on the actual Apache Airflow instance level. Again, generally, you're only on the Cloud Composer page here to create new environments before you launch directly into the Airflow web server. To access the Airflow admin UI, where you can monitor and interact with your workflows, you'll click on the link underneath Airflow webserver. The second box you see is the DAGs folder, which is where the code of your actual workflows will be stored. The DAGs folder for each airflow instance is simply a cloud storage bucket that is automatically created for you when you create your Cloud Composer instance. Here is where you upload your DAG files written in Python, and bring your first workflow to life in Airflow.

#### DAGs and Operators

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509078

>> Now that you're familiar with the basic environment setup, it's time to discuss your primary artifact, which is your DAG and the operators you're using to call whichever services you want to send tasks to. First, airflow workflows are written in Python, you'll have one Python file for each DAG. For example, here we have simple _load_dag.py in our DAG folder Cloud Storage bucket, and you can see a preview of what the DAG file looks like. Don't worry about reading the code, we'll go into that later. It's sufficient enough for now to just know that there are a series of user created tasks in each DAG file that invoke predefined operators, like this task, which uses the data flow Python operator and is given the task ID of Process Delimited and Push. We'll go over creating a DAG file and its components a little later. Once you've uploaded the Python file to the DAG's folder, you can navigate back to the airflow web server, and under DAGs, you'll see the DAG you created with code represented visually as a directed graph with nodes and edges. You'll remember that the Python code that defined a task we called Process Delimited and Push is now a node in our graph here. Let's explore a bit more of the airflow Web UI. You can see that this particular workflow is called GCS to BigQuery Triggered and it has three tasks when it runs. One, process delimited and push. I just happen to know from that Python file you saw earlier that this task invokes a Dataflow job to read in a new CSV file from a Cloud Storage bucket, processes it and writes the output to BigQuery. Two, success move to completion, which moves the CSV file from an input Cloud Storage bucket to a process store completed Cloud Storage bucket for archiving. Or three, if the pipeline fails partway, the file is moved to the completion bucket but tagged as failure. This is an example of a DAG which isn't strictly sequential. There, a decision is made to run one node or a different one based on the outcome of the parent note. But regardless of the size and shape of your workflow DAG, one common thread for all workflows is the common operators used. If the DAG itself is how to run the workflow, first do step one, then either move to step two or three, the operators specify what actually gets done as part of the task. In this simple example, we're calling on the Dataflow Python operator and the general Python operator. Those are by no means the only operators. So let's pause here and look at all the operators at our disposal to achieve our goal of automatic retraining and deployment of our ML model. Airflow has many operators which you can invoke the tasks you want to complete. Operators are usually atomic in a task, which means generally you only see one operator per task. This list of all services that airflow can orchestrate to is taken directly from the Apache Airflow documentation. Let's take a look at the ones that are likely most relevant to us as data engineers. As you might have guessed, we'll certainly be making use of the BigQuery operators since our workflows depend on the data that is fed into them through Cloud storage and BigQuery. Here's a list of the specific operators that we can invoke in a task to call on the BigQuery service for querying and other data related tasks. You'll be mainly working with the first three in this course, but I encourage you to skim the resource link on all the operators so you can get a feel for what is possible. Once we have our training data in a good place, the next logical step in our workflow is to retrain and redeploy our model. In the same DAG file, after the BigQuery operators complete, we can make a service call through a Vertex AI operator to kick off a new training job and manage our model like incrementing the version. You might have noticed that your airflow DAG can have operators that send tasks out to other Cloud providers. This is great for hybrid workflows where you have components across multiple Cloud platforms, or even on premise. Apache Airflow is open source and continually adds more operators to other services. So be sure to check out the list and the documentation periodically if you're waiting for a new service to be added. Here, you see four tasks: T1, T2, T3, and T4, and four operators corresponding to four Google Cloud services. The names should look familiar, and you can probably start to guess what this pipeline does at a high level just by reading them in order. The first two are concerned with getting fresh model data from a BigQuery dataset and into Cloud Storage for consumption by our ML model later in the pipeline. In the lab you're going to work on later, the dataset will be the one you're already familiar with, the Google Analytics news articles sample dataset. Let's see the parameters the BigQuery operator takes. The BigQuery operator allows you to specify a SQL query to run against a BigQuery dataset. In this quick example, we're parsing in a query which returns the top 100 most popular Stack Overflow posts from the BigQuery public dataset for a specified date range you see there in the Where clause. Notice anything different about the filters in the Where clause? Yes, they are parameters. In this case, for max date, and min date, you can parameterize pieces of the SQL statements like what we did here to only return posts for January 2018 with min query date, and max query date. What is really powerful is that you can even make the parameters dynamic instead of the static ones shown here, and have them be based on the DAG schedule date, like macros.ds_adds-7, which is a week before the DAG scheduled run date. The next two operators handle retraining the model by submitting a job to the machine learning engine and then deploying the updated model to App Engine. At the end of almost all DAG files, you'll find the actual order in which we want these operators to run. This is what the D in DAG is for, Directed. For our example, T2 or task two won't run until T1 has completed. This is what gives our graph the dependencies of a workflow. I can probably guess what you're thinking at this point, you could build some cool branching of multiple child nodes per upstream parent, and that is totally possible. Just don't forget to comment your code so you know where one branch begins, where it's going, and what tasks are involved. As a tip, after you load your DAG file into the DAG folder, you can see the visualization of your DAG in the airflow UI as a directed graph, a Gantt chart or a list if you want. Reviewing the visual representation of the ordered tasks will help you confirm your tasks are ordered properly.

#### Workflow scheduling

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509079

>> Now that you're familiar with the Cloud Composer and Apache Airflow environments, and the basics of building a list of tasks for Google Cloud services in your DAG, it's time to discuss a really important topic: workflow scheduling. As we hinted at earlier, there are two different ways your workflow can be run without you sitting there manually clicking run DAG. The first and most common is a set schedule or periodic run of a workflow, like once a day at 6:00am, or weekly on Saturdays. The second way is trigger based. Like if you wanted to run your workflow whenever new CSV data files were loaded into a Cloud Storage bucket, or if new data came in from a Pub Sub topic you've subscribed to, then navigate to the DAGs tab to view the existing workflows that you have Python DAG files for. Here we have two DAGs, the bottom one composer sample simple greeting has a daily schedule. But why is this top DAG missing a schedule? How would it ever get run? The answer is the fact that it's not on a set schedule at all. It's event driven. The driver of when this workflow runs is a Cloud function that we create. In the next lesson, we'll actually create our own Cloud function that watches a Cloud Storage bucket for new CSV files. If you want to go the regular scheduled route, you can specify the schedule_interval in your DAG code, like what you see here. By the way, clicking on the schedule of one day here in the UI won't allow you to edit it there, but instead, will take you to the history of all the runs for that workflow. As you saw earlier in this course, there are two general patterns for ETL workflows: event triggered or push. As in, you push a new file to Cloud Storage and your workflow kicks off, or pull, which is where airflow at a set time could look in your Cloud storage folder, and take all the contents that are found for its workflow run. We can use Cloud Functions to create our event driven or push architecture workflow. I mentioned triggering on events within a Cloud Storage bucket, which you can also trigger based on HTTP requests, Pub Sub, FireStore, Firebase, and more as you see here. Generally, push technology is great when wanting to distribute transactions as they happen. Stock tickers and other types of financial institution transactions are very important when it comes to push technology. How about disasters and notification? Again, important. For ML workflows, where your upstream data doesn't arrive at a regular pace, like get all the transactions at the end of each day, consider experimenting with a push architecture. Your final lab, since it's based on regular Google Analytics, news article data will be a pull architecture, but I've added in an optional lab for you to get practice with Cloud Functions and event-driven workflows for those interested. So let's talk through it more now. For our example, let's assume we have a CSV file or a set of files loaded to Cloud Storage. So we'll choose a Cloud storage trigger for our function. Then we specify an event type, finalize, create new files, and a bucket to watch. As part of the Cloud function, we need to create the actual function we want called in JavaScript. The good news is most of this code for triggering airflow DAGs in a function is all boilerplate for you to copy from as a starting point. Here, we specify a name for our function called Trigger DAG, then we tell it where your airflow environment is to be triggered and which DAG in that airflow environment. In this case, it's looking for one called GCS to BigQuery triggered. Keep in mind, you can have multiple workflows or DAGs in a single airflow environment, so be sure you specify the correct DAG underscore name to trigger. Then we have a few constants that we are provided, which construct the airflow URL that we're going to trigger a post request to, as well as who's making the request and what the body of the request is. Lastly, the Trigger DAG function makes the actual request against the airflow server to kick off a workflow DAG. Once you have the Cloud function code ready in your index.js file and the metadata about the function in package.json, which contains code dependency and versioning information, you still need to specify which function you actually want executed. In this case, we created one called Trigger DAG. So we just copy that down. I'll also save you about 20 minutes of frustration and tell you that the function to execute box is case sensitive. So all capital letters, D-A-G is different from capital D, lowercase A and G. And there you have it. Your Cloud function has been created, and is actively watching your Cloud storage bucket for file uploads. But how can you be sure everything is working as intended? For that, check out the next topic on monitoring and logging.

#### Monitoring and Logging

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509080

person: By this point, we've got our environment set up with our DAGs running at a predefined schedule or with triggered events. The last topic we'll cover before you practice what you've learned in your labs is how to monitor and troubleshoot your cloud functions and Airflow workflows. One of the most common reasons you'll want to investigate the historical runs of your DAGs is in the event that your workflow simply stops working. Note that you can have it auto retry for a set number of attempts in case it's a transient bug, but sometimes you can't get your workflow to run at all in the beginning. In the DAG runs, you can monitor when your pipelines run and in what state, like success, running or failure. The quickest way to get to this page is clicking on the schedule for any of your DAGs from the main DAGs page. Here we have five successful runs over 5 days for this DAG, so this one seems to be running just fine. Back on the main page for DAGs, we see some red, which indicates trouble with some of our recent DAG runs. Speaking of DAG runs, you'll note the three circles below, which indicate how many runs passed, are currently active or have failed. It certainly doesn't look good for 268 runs failed and zero passed for this first DAG. Let's see what happened. We click on the name of the DAG to get to the visual representation. It looks like the first task is succeeding, judging by the green border, but the next task, success-move-to-completion is failing. Note that the lighter pink color for the failure-move-to-completion node means that node was skipped. So reading in to this a bit, the CSV file was correctly processed by Dataflow in the first task, but there was some issue moving the CSV file to a different cloud storage bucket as part of task two. To troubleshoot, click on the node for a particular task and then click logs. Here you will find the logs for that specific Airflow run. I search for the word error and then start my diagnosis there. Here, this was a pretty simple error where it was trying to copy a file from an input bucket to an output bucket, and the output bucket didn't exist or was named poorly. Another tool in your toolkit for diagnosing Airflow failures is the general Google Cloud logs. Since Airflow launches other Google Cloud services through tasks, you can see and filter for errors for those services in Cloud Logging as you would debugging any other normal application. Here I filtered for Dataflow step errors to troubleshoot why my workflow is failing. It turns out that I had not changed the name of the output bucket for the CSV file, so after the file was processed by Dataflow as part of step one, it dumped the completed file back into the input bucket, which triggered another Dataflow job for processing and so on. You might be wondering, if there's an error with my cloud function, my Airflow instance would never have been triggered or issued any logs at all since it was unaware we were trying to trigger it, and you're exactly right. If you're using cloud functions, be sure to check the normal Google Cloud Logs for errors and warnings in addition to your Airflow logs. In this example, each time I upload a CSV file to my cloud storage bucket hoping to trigger a cloud function and then my DAG, I get an error message that includes expected to export function named trigger DAG. Remember way back when I said cloud functions were case sensitive? Looking for a function with capital DAG doesn't exist if it's capital D, lowercase A and G, so be sure to be mindful when setting up your cloud functions for the first time.

#### Lab Intro: An Introduction to Cloud Composer

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509081

Person: Let's now take some time to practice what you learned in this module by creating an environment in Cloud Composer, running a DAG in Airflow, and analyzing the results. The objectives of this lab are to use the Cloud Console to create a Cloud Composer environment, view and run a DAG in the Airflow web interface, and view the results of the wordcount job in storage.

#### An Introduction to Cloud Composer 3

- https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509082

#### Manage Data Pipelines with Cloud Data Fusion and Cloud Composer

- https://www.cloudskillsboost.google/paths/16/course_templates/53/quizzes/509083

### Course Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/53/video/509084

person: You've made it to the end of this course on building batch data pipelines. Let's recap what you've learned. In this course, we covered the different methods of loading data into your data lakes and warehouses. Remember ELT versus ETL and when to use each. ELT of extract, load, transform is a common pattern for when the transformations you want on the data set are minor and then can be handled after you load the data. An example is loading the data into BigQuery first and then doing SQL on the raw data and storing the transformed version as a new table. On the other hand, ETL, or transforming before loading, is common when you have data that needs more complex transformations, where the sheer volume of data makes it better to do those transformations before loading. That's where you would want to use a batch pipeline like Dataflow. Next, we discussed how to transform data in BigQuery using SQL, and you saw the common operations you can perform to ensure your structured data is ready for insights. Lastly, you practiced building batch pipelines in the serverless way with Dataflow. In our Hadoop module, we discussed dataproc in detail. You learned that you can lift and shift your existing Hadoop workloads to the cloud with no code changes, and they will just work. However, once in the cloud, there were additional optimizations you could make, like using cloud storage for cluster storage instead of HDFS for greater efficiency and cost savings. We then used Dataflow to build batch data pipelines using Dataflow templates and by writing them ourselves. Recall that the fundamental unit of logical data in the pipeline is the PCollection, which stands for a parallel collection. Dataflow will automatically split up your data set into many pieces and farm out the processing across as many worker VMs as it needs to complete the job. Dataflow is a serverless application, which means you will have some control over the maximum number of workers, the actual processing work and the autoscaling of workers up and down as the demand requires. In our module on managing data pipelines with Google Cloud, we discussed two new tools: Cloud Data Fusion and Cloud Composer. Recall that Cloud Data Fusion allows data analysts and ETL developers to wrangle data and build pipelines in a visual way. The technology then builds and executes the pipelines on a runner. With Cloud Data Fusion, you also get access to the lineage of each data field, which is the series of any transformation logic that happened before and after the field reaches your data set. The second tool we covered is Cloud Composer as a workflow orchestrator. Cloud Composer is managed Apache Airflow and allows you to, at a high level, command Google Cloud services in a DAG to perform complex operations. These DAGs can be user scheduled or event driven with cloud functions. Recall the example where when a new CSV was uploaded to our cloud storage bucket, a cloud function was triggered to start a Dataflow pipeline for processing and then sync the data into BigQuery. Congratulations on completing building batch data pipelines on Google Cloud. Building resilient streaming analytics systems on Google Cloud is the third course of the data engineering on Google Cloud core series and is covered next. We hope to see you there.

### Course Resources

#### Building Batch Data Pipelines on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/53/documents/509085

### Your Next Steps

## 06: Building Resilient Streaming Analytics Systems on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/52

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564976

Damon: Welcome to Building Resilient Streaming Analytic Systems on Google Cloud, I'm Damon and I am a Technical Curriculum Developer at Google. Building resilient streaming analytic systems on Google Cloud is the third course of the data engineering on Google Cloud core series and it covers building resilient streaming analytic systems. Those systems allow organizations to make accurate and timely decisions from data points generated in real-time. This course discusses what streaming data processing is, how it fits in your overall big data architecture, win streaming data processing make sense and what Google Cloud technologies and products you can choose from to build your own resilient streaming analytic solutions. Here's how the course is broken down. We start off with what streaming data is and the challenges associated with processing streaming data. Like variable volumes and latency. Next we look at using pub/Sub, Dataflow and BigQuery to help us ingest, process and derive insights from data as it streams in. We dive into each product and learn about its streaming capabilities. Then we look at big table when higher throughput is a requirement. Finally, we review some of BigQuery's advance analytic capabilities like GIS functions and ways to improve query performance.

### Introduction to Processing Streaming Data

#### Processing streaming data

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564977

This module discusses what stream processing is, how it fits into a big data architecture, when stream processing makes sense, and also, the challenges associated with streaming data processing. As this module is all about streaming, I’ll be discussing that part of the reference architecture. Data typically comes in through Pub/Sub, then that data goes through aggregation and transformation in Dataflow. Then you use BigQuery or Bigtable depending on whether the objective is to write aggregates or individual records coming in from streaming sources. Let’s look at streaming ideas first. Why do we stream? Streaming enables us to get real-time information in a dashboard or another means to see the state of your organization. In the case of the New York City Cyber Command, Noam Dorogoyer stated the following: “We have data coming from external vendors, and all this data is ingested through Pub/Sub, and Pub/Sub pushes it through to Dataflow, which can parse or enrich the data,” If data comes in late, especially when it comes to cybersecurity, it’s no longer valuable, especially during an emergency. So, from a data engineering standpoint, the way we constructed the pipeline is to minimize latency at every single step. If it’s maybe a Dataflow job, we designed it so that as many elements as possible are happening in parallel so at no point is there a step that’s waiting for a previous one. ” The amount of data flowing through the Cyber Command varies each day. Dorogoyer said that on weekdays during peak times, it could be 5 or 6 terabytes. On weekends, that can drop to 2 or 3 terabytes. As the Cyber Command increases visibility across agencies, it will deal with petabytes of data. Security analysts can access the data in several ways. They run queries in BigQuery or use other tools that will provide visualizations of the data, such as Looker Studio. Streaming is data processing on unbounded data. Bounded data is data at rest. Stream processing is how you deal with unbounded data. A streaming processing engine provides: low latency, speculative or partial results, the ability to flexibly reason about time, controls for correctness, and the power to perform complex analysis. You can actually use streaming to get real-time data warehouses and then create a dashboard of real-time information. For example, you could see in real-time the positive versus negative tweets about your company's product, use it to detect fraud, use for gaming events, or for finance back office apps, such as stock trading, anything dealing with markets, etc. So, when you look at the challenges associated with streaming applications, you’re talking about the 4 V’s, variety, volume, velocity, and veracity. First, data could come in from a variety of different sources and in various formats. Imagine hundreds of thousands of sensors for self-driving cars on roads around the world. The data is returned in various formats such as number, image, or even audio. Now consider point-of-sale data from a thousand different stores. How do we alert our downstream systems of new transactions in an organized way with no duplicates? Next, let’s increase the magnitude of the challenge to handle not only an arbitrary variety of input sources, but a volume of data that varies from gigabytes to petabytes. You’ll need to know whether your pipeline code and infrastructure can scale with those changes or whether it will grind to a halt or even crash. The third challenge concerns velocity. Data often needs to be processed in near-real time, as soon as it reaches the system. You’ll probably also need a way to handle data that arrives late, has bad data in the message, or needs to be transformed mid-flight before it is streamed into a data warehouse. And the fourth major challenge is veracity, which refers to the data quality. Because big data involves a multitude of data dimensions resulting from different data types and sources, there’s a possibility that gathered data will come with some inconsistencies and uncertainties. Challenges like these are common considerations for pipeline developers. The three products you are going to examine here are: Pub/Sub, which will allow you to handle changing and variable volumes of data, Dataflow, which can assist in processing data without undue delays, and BigQuery, which you will use for your ad-hoc reporting, even on streaming data. Let’s take a look at the steps that happen. First, some sort of data is coming in, possibly from an app, a database, or an Internet of Things, or IoT. These are generating events. Then, an action takes place. You are going to ingest those and distribute those with Pub/Sub. This will ensure that the messages are reliable. This will give you buffering. Dataflow, then is what aggregates, enriches, and detects the data. Next, you will write into a database of some kind, such as BigQuery or Bigtable, or maybe run things through a Machine Learning model. For example, you might use this streaming data as it is coming in to train a model in Vertex AI. Then, finally, Dataflow or Dataproc could be used for batch processing, backfilling, etc. So, this is a pretty common way to put things together in Google Cloud.

#### Introduction to Processing Streaming Data

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/564978

### Serverless Messaging with Pub/Sub

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564979

person: Now that we have a good understanding of the process of streaming data let's dive into Pub/Sub to see how it works. We'll start by understanding how Pub/Sub works and is used in decoupling systems. Then we'll discuss the distribution of messages and different patterns through push and pull delivery models. Finally, we'll focus on actual implementation, looking at how things are set up in Pub/Sub. This will include a hands-on lab where you will publish streaming data into Pub/Sub.

#### Introduction to Pub/Sub

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564980

As we begin this module, I would ask you to keep your mind open to new ways of doing things. Pub/Sub does streaming differently than probably anything you have used in the past. It may be a different model than what you have seen before. Pub/Sub provides a fully managed data distribution and delivery system. It can be used for many purposes. It is most commonly used to loosely-couple parts of a system. You can use Pub/Sub to connect applications within Google Cloud, and with applications on premise or in other clouds to create hybrid Data Engineering solutions. With Pub/Sub the applications do not need to be online and available all the time. And the parts do not need to know how to communicate to each other, but only to Pub/Sub, which can simplify system design. First, Pub/Sub is not software; it is a service. So, like all of the other serverless services we have looked at, you don’t install anything to use Pub/Sub. Pub/Sub client libraries are available in C#, GO, Java, Node.js, Python, and Ruby. These wrap REST API calls which can be made in any language. It is also highly available. Pub/Sub offers durability of messages. By default it will save your messages for seven days in the event your systems are down and not able to process them. Finally, Pub/Sub is highly scalable as well. Google processes about 100 million messages per second across their entire infrastructure. This was actually one of the use cases for Pub/Sub early on at Google. To be able to distribute the search engine and index around the world because we keep local copies of search around the world, as you might imagine, in order to be able to serve up results with minimal latency. If you think about how you would architect that, we are crawling the entire world wide web, so we need to send the entire world wide web around the world. Either that or we would need to have multiple crawlers all over the world, but then you have data consistency problems; they would all be getting different indexes. Therefore, what we do is use Pub/Sub to distribute. As the crawler goes out, it grabs every page from the world wide web, and we send every single page as a message on Pub/Sub and it gets picked up by all local copies of the search index so it can be indexed. Currently, Google indexes the web anywhere from every two weeks, which is the slowest, to more than once an hour, for example on really popular news sites. So, on average, Google is indexing the web three times a day. Thus, what we are doing is sending the entire world wide web over Pub/Sub three times a day. This should explain how Pub/Sub is able to scale. Pub/Sub is a HIPAA-compliant service, offering fine-grained access controls and end-to-end encryption. Messages are encrypted in transit and at rest. Messages are stored in multiple locations for durability and availability. You control the qualities of your Pub/Sub solution by the number of publishers, number of subscribers, and the size and number of messages. These factors provide tradeoffs between scale, low latency, and high throughput. How does Pub/Sub work? The model is very simple. The story of Pub/Sub is the story of two data structures, the Topic and the Subscription. Both the Topic and the Subscription are abstractions which exist in the Pub/Sub framework independently of any workers, subscribers, etc. The Pub/Sub client that creates the Topic is called the Publisher. And the Pub/Sub client that creates the Subscription is called the Subscriber. In this example, the Subscription is subscribed to the Topic. To receive messages published to a topic, you must create a subscription to that topic. Only messages published to the topic after the subscription is created are available to subscriber applications. The subscription connects the topic to a subscriber application that receives and processes messages published to the topic. A topic can have multiple subscriptions, but a given subscription belongs to a single topic. In essence, it’s an enterprise message bus. So, how does it work? As you see here, there’s an HR Topic that relates to New Hire Events. For example, a new person joins your company and this notification should allow other applications that need to be notified about a new user joining to subscribe and get that message. What applications could tell you that a new person joined? One example is the Company Directory. This is a client of the Subscription also called a Subscriber. However, Pub/Sub is not limited to one Subscriber or one Subscription. Here there are multiple Subscriptions and multiple Subscribers. Maybe the Facilities System needs to know about the new employee for badging, and the accounting provisioning system needs to know for payroll. Each Subscription guarantees delivery of the message to the service. These subscriber clients are decoupled from one another and isolated from the publisher. In fact, we will see later that the HR System could go offline after it has sent its message to the HR Topic, and the message will still be delivered to the subscribers. These examples show one Subscription and one Subscriber. But you can actually have more Subscribers for a single Subscription. In this example, the Badge Activation System requires a human being to activate the badge. There are multiple workers, but not all of them are available all the time. Pub/Sub makes the message available to all of them. But only one person needs to fetch the message and handle it. This is called a Pull Subscription. The other examples are Push Subscriptions. Now, a new contractor arrives. Instead of entering through the HR System, they go through the Vendor Office. The same kinds of actions need to occur for this worker. They need to be listed in the company directory. Then, Facilities need to assign them a desk. Account provisioning needs to set up their corporate identity and accounts. And the badge activation system needs to print and activate their contractor badge. A message can be published by the Vendor Office to the HR Topic. The Vendor Office and the HR System are entirely decoupled from one another but can make use of the same company services. You can see from this illustration how important Pub/Sub is. Therefore, it gets the highest priority. When you receive messages from a subscription with a filter, you only receive the messages that match the filter. The Pub/Sub service automatically acknowledges the messages that don't match the filter. You can filter messages by their attributes. In the example, the filter sorts messages between those with the name attribute and the value of "com" and those without. You don't incur egress fees for the messages that Pub/Sub automatically acknowledges. You incur message delivery fees and seek-related storage fees for these messages. You can create a subscription with a filter using the Google Cloud console, the gcloud command-line tool, or the Pub/Sub API. You have learned generally what Pub/Sub does. Next, you will learn how it works and many of the advanced features it provides.

#### Pub/Sub Push versus Pull

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564981

person: From the HR analogy previously, you saw how Pub Sub works and how it's used in decoupling systems. Let's now discuss the technical details associated. We'll start by understanding the distribution of messages and different patterns. Here, the different colors represent different messages. The first pattern is just a basic straight through flow, where one publisher publishes messages into a topic, which then get consumed by the one subscriber through the one subscription. The second pattern is fan-in or load balancing, multiple publishers can publish the same topic, and multiple subscribers can pull from the same subscription, leveraging parallel processing. What you see here are two different publishers sending three different messages all on the same topic. That means the subscription will get all three messages. The third pattern is fan out, where you have many use cases for the same piece of data, and all data is sent to many different subscribers. As you can see here, we have two subscriptions. So both are going to get the messages, both the red message and the blue message. Pub Sub allows for both push and pull delivery. In the pull model, your clients are subscribers, and will be periodically calling for messages, and Pub Sub will just be delivering the messages since the last call. In the pull model, you're going to have to acknowledge the message as a separate step. So what you see here is we initially make the call to the subscribers, it pulls the messages, it gets a message back, and then separately, it acknowledges that message. The reason for this is because the pull queues are often used to implement some kind of queueing system for work to be done. So you don't want to acknowledge the message until you firmly have the message and have done the processing on it. Otherwise, you might lose the message if the system goes down. Therefore, we generally recommend you wait to acknowledge until after you have gotten it. In the pull model, the messages are stored for up to seven days. In the push model, it actually uses an HTTP endpoint. You register a web hook as your subscription, and Pub Sub infrastructure itself will call you with the latest messages. In the case of push, you just respond with status 200OK for the HTTP call, and that tells Pub Sub the message delivery was successful. It will actually use the rate of your success responses to self-limit so that it doesn't overload your worker. The way the acknowledgments work is to ensure every message gets delivered at least once. What happens is when you acknowledge a message, you acknowledge on a per subscription basis. So if you have two subscriptions, you have one acknowledge and the other one doesn't. The one that acknowledged will continue to get the messages. Pub Sub will continue to try to deliver the message for up to seven days until it is acknowledged. There is a replay mechanism as well that you can rewind and go back in time and have it replay messages. But in any case, you will always be able to go back seven days. You can also set the acknowledgment deadline and do that on a per subscription. So if you know that on average it takes you 15 seconds to process a message in your work queue, then you might set your acknowledgment deadline to 20 seconds. This will ensure it doesn't try to redeliver the messages. Configuring a topic with message retention gives you more flexibility, allowing any subscription attached to the topic to seek back in time and replay previously acknowledged messages. Topic message retention also allows a subscription to replay messages published before a subscription was created. Snapshots are utilized to make replay highly efficient. If topic message retention is enabled, storage costs for the messages retained by the topic are to be built to the topic's project. Subscribers can work individually or as a group. If we have just one subscriber, it is going to get every message delivered through that subscription. However, you can set up worker pools by having multiple subscribers sharing the same subscription. In this case, it is going to distribute the message, so one and three go to subscription one and two goes to subscription two. And it is just random based on when it pulls from messages throughout the day. In the case of a push subscription, you only have one web endpoint so you will only have one subscriber typically, but that one subscriber could be an App Engine standard app or cloud run container image which auto scales. So it is one web endpoint but it can have auto scale workers behind the scenes and that is actually a very good pattern.

#### Publishing with Pub/Sub code

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564982

person: With the theoretical background covered in the previous lesson, let's now shift our focus on actual implementation. How are things set up in Pub/Sub? Let's look at a little bit of code now. This example is using the client library for Pub/Sub. If we want to publish the message, we first create the topic. Then, we can publish the topic on the command line. More commonly, it will be done in code. Here, we get a publisher client, create a topic, and publish the message. Notice the letter B in front of my first message. This is because Pub/Sub just sends raw bites. This means that you aren't restricted to just text. You can send other data, like images, if you wanted to. The limit is 10 Megabytes. There are also extra attributes that you can include in messages. In this example, you see author='dylan'. Pub/Sub will keep track of those attributes to allow your downstream systems to get metadata about your messages without having to put it all in the message and parse it out. So instead of serializing and deserializing, it will just keep track of those key value pairs. Some of them have special meaning. We will see some of those shortly. To subscribe with Pub/Sub using the pull method, the code is similar. Select the topic, name the subscription. This is a pull subscription, so we will define a callback. When you are doing a pull subscription, it looks like this. You can pull the messages from the command line. You will see this in the lab. By default, it will just turn one message, the latest message. But there is a --limit you can set. Maybe you want 10 messages at a time. You can try that in the lab. You can also batch publish messages. This just prevents the overhead of the call for individual messages on the publishing side. This allows the Pub/Sub publishing engine to wait and send 10 or 50 at a time. This increases efficiency. However, if you are waiting for 50 messages, this means the first one now has latency associated with it. So it is a trade-off in your system. What do you want to optimize? But in any case, even if you batch publish, they still get delivered one at a time to your subscribers. We will practice this technique in the lab. Here's how you would set the batch in Python code. Explore the documentation for other command options and settings. If messages have the same ordering key and are in the same region, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them. When the Pub/Sub service redelivers the message with an ordering key, the Pub/Sub service also redelivers every subsequent message with the same ordering key, including acknowledged messages. If both message ordering and a dead letter topic are enabled on a subscription, the ordering may not be true, as Pub/Sub forwards messages to dead letter topics on a best effort basis. To receive the messages in order, set the message ordering property on the subscription you receive messages from. Please note that receiving messages in order might increase latency. You can set the message ordering property when you create a subscription using the Cloud Console, the gcloud command-line tool, or the Pub/Sub API. Pub/Sub is also going to help us with streaming resilience or buffering. What happens if your systems get overloaded with large volumes of transactions, like Black Friday? What you really need is some sort of buffer or backlog, so that you can feed messages only as fast as the systems are able to process them. Pub/Sub has this as a built-in capability. Let's recap this, then. When you look at the example on this slide, in the example on the left, and overload of arriving data causes a traffic spike. This overdrives the resources of the application, as illustrated by the smoke. One solution to this problem is to size the application to handle the highest traffic spike, plus some additionally capacity as a safety buffer. This is not only wasteful of resources, which must be retained at top capacity, even when not being used, but it provides a recipe for distributed denial of service attack by creating an upper limit at which the application will cease to behave normally and will exhibit nondeterministic behavior. The solution on the right uses Pub/Sub as an intermediary, receiving and holding data until the application has resources to handle it, either through processing the backlog of work or autoscaling to meet the demand. Erroneous records may cause your pipeline to get stuck or fail outright. I highly recommend implementing a dead letter queue and error logging to prevent these failure modes. These can help catch problems in user code and/or data shape. The idea behind exponential back off is to add progressively longer delays between retry attempts. After the first delivery failure, Pub/Sub will wait for a minimum back off time before retrying. For each consecutive failure on that message, more time will be added to the delay, up to a maximum delay. The maximum and minimum delay intervals are not fixed, and should be configured based on local factors to your application. Cloud Audit Logs maintains three audit logs for each Google Cloud project, folder, and organization: Admin activity, data access, and system event. Admin activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Data access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Admin activity audit logs are always written. You can't configure or disable them. Data access audit logs are disabled by default because they can be quite large. They must be explicitly enabled to be written. Pub/Sub reports metrics to Cloud Logging, including Pub/Sub topic and Pub/Sub subscription, which you can monitor against service quota utilization in a dashboard, and for setting notifications and alerts. Authentication is provided by service accounts. You can also directly authenticate users by their user accounts and their identity is reported in audit logs. But user account authentication is not recommended. Access control is provided by IAM.

#### Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564983

Person: When working with Pub/Sub, there are a few things to keep in mind. First, data may be delivered out of order. But that doesn't mean you have to process the data that way. You can write an application that handles out-of-order and replicated messages. This is different from a true queueing system. In general, they will be delivered in order, but you can't rely on that with Pub/Sub. This is one of the compromises made for scalability, especially since it's a global service. We have a mesh network, so a message might take another route. And if it happens to be a slower route, you could have an earlier message arriving later. For example, you wouldn't use this to implement a chat application, because it will be awkward when messages arrive out of order. Therefore, we will handle ordering using other techniques. Finally, we need to be ready for duplication. You can use data flow in conjunction with Pub/Sub to solve some of the problems we just discussed. Dataflow will de-duplicate messages based on the message ID, because in Pub/Sub, if a message is delivered twice, it will have the same ID in both cases. BigQuery can also be used for this purpose, but has limited capabilities. Dataflow will not be able to order in the sense of providing exact sequential order of when messages were published. However, it will help you deal with late data. Using Pub/Sub and Dataflow together allows you to get a scale that wouldn't be possible otherwise. In the next module, you will look at Dataflow's streaming capabilities in greater detail.

#### Lab Intro: Publish Streaming Data into Pub/Sub

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564984

Person: Now let's practice publishing streaming data into Pub/Sub. In this lab, you create a Pub/Sub topic and subscription and simulate San Diego traffic data into Pub/Sub.

#### Streaming Data Processing: Publish Streaming Data into PubSub

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/564985

#### Serverless Messaging with Pub/Sub

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/564986

### Dataflow Streaming Features

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564987

Person: In the previous module, we discussed using Pub/Sub to receive streaming data from a variety of sources. Now we are ready to process it or prepare it for further analysis. Let us see how Dataflow can help you to do this. More specifically, let's look at Dataflow's streaming features. In this module, we'll discuss some of the challenges associated with streaming data and then the different windowing capabilities provided by Dataflow.

#### Streaming data challenges

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564988

Person: Let's start by identifying some of the challenges associated with processing streaming data. Dataflow, as we already know, provides a serverless service for processing batch and streaming data. It is scalable and for streaming has a low latency processing pipeline for incoming messages. We've discussed that we can have bounded and unbounded collections, so now we are examining an unbounded pipe that results from a streaming job. All of the things we've done thus far, like branching merging, we can do as well with Dataflow for streaming pipelines. However, now every step of the pipeline is going to act in real time on incoming messages, rather than in batches. What are some of the challenges with processing streaming data? One challenge is scalability-- being able to handle the volume of data as it gets larger and/or more frequent. The second challenge is fault tolerance. The larger you get, the more sensitive you are to going down unexpectedly. The third challenge is the model being used--streaming or repeated batch. Another challenge is timing or the latency of the data. For example, what if the network has a delay or a sensor goes bad and messages can't be sent? Additionally, there is a challenge around any kind of aggregation you might be trying to do. For example, if you are trying to take the average of data, but it is in a streaming scenario. You cannot just plug values into the formula for an average-- the sum from one to n-- because n is an ever-growing number. So in a streaming scenario, you have to divide time into windows, and we can get the average within a given window. This can be a pain if you have ever had to write a system like this, you can imagine it can be difficult to maintain windowing, time, roll threads, etcetera. The good news is that Dataflow is going to do this for you automatically. In Dataflow, when you are reading messages from Pub/Sub, every message will have a timestamp that is a Pub/Sub message timestamp. And then you will be able to use this timestamp to put the data into the different time windows and aggregate all of those windows. Message ordering matters and there may be a latency between the time that a sensor is read and the message is sent. You may need to modify the timestamps if this latency is significant. If you want to modify a timestamp and have it based on some property of your data itself, you can do that. Every message that comes in for example, the sensor provides its own date timestamp as part of the message. Element Source adds a default date timestamp, or DTS, which is the time of entry to the system rather than the time the sensor data was captured. A PTransform extracts the date timestamp from the data portion of the element and modifies the DTS metadata so the time of data capture can be used in window processing. Here's the code used to modify the date timestamp by replacing the message timestamp with a timestamp from the element data. If Pub/Sub IO is configured to use custom message IDs, Dataflow de-duplicates messages by maintaining a list of all the custom IDs it has seen in the last 10 minutes. If a new message's ID is in the list, the message is assumed to be a duplicate and discarded.

#### Dataflow windowing

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564989

person: Next, let's look at Dataflow windowing capabilities. This is really Dataflow's strength when it comes to streaming. Dataflow gives us three different types of windows: fixed, sliding, and sessions. Fixed windows are those that are divided into time slices. For example, hourly, daily, monthly. Fixed time windows consist of consistent, non-overlapping intervals. Sliding windows are those that you use for computing. For example, give me 30 minutes' worth of data and compute that every 5 minutes. Sliding time windows can overlap. For example, in a running average. Sliding windows are defined by a minimum gap duration, and timing is triggered by another element. Session windows are defined by a minimum gap duration, and the timing is triggered by another element. Session windows are for situations where the communication is burst-y. It might correspond to a web session. An example might be if a user comes in and uses four to five pages and leaves. You can capture that as a session window. Any key in your data can be used as a session key. It will have a time-out period, and it will flush the window at the end of that time out period. Here's how we can set these different types of windows in Python. In the fixed time window example, we can use the functions beam. WindowInto and window. FixedWindows with argument 60 to get fixed windows starting every 60 seconds. In the second example, with the sliding time window, we use window. SlidingWindows with argument 30 and 5. Here, the first argument refers to the length of the window, that is 30 seconds. And the second argument refers to how often new windows open, that is 5 seconds. Finally, we have the example of a session window. We use windows. Sessions with an argument of 10, multiplied by 60 to define a session window with time out of 10 minutes. That is, 600 seconds. How does windowing work? All things being equal, this is how windowing ought to work. If there was no latency, if we had an ideal world. If everything was instantaneous, then these fixed time windows would just flush at the close of a window. At the very microsecond at which is become 8:05, a 5-minute window terminates and flushes all of the data. This is only if there is no latency. But in the real world, latency happens. We have network delays, system backlogs, processing delays, Pub/Sub latency, et cetera. So when do we want to close the window? Should we wait a little bit longer than 8:05? Maybe a few more seconds? This is what we call the watermark, and Dataflow keeps track of it automatically. Basically, it is going to keep track of the lag time, and it is able to do this, for example, if you are using the Pub/Sub connector because it knows the time of the oldest, unprocessed message in Pub/Sub. And then it know the latest message it has processed through the Dataflow. It then takes this difference, and that is the lag time. So what Dataflow is going to do is continuously compute the watermark, which is how far behind we are. Dataflow ordinarily is going to wait until the watermark it has computed has elapsed. So if it is running a system lag of 3 or 4 seconds, it is going to wait 4 seconds before it flushes the window, because that is when it believes all of the data should have arrived for that time period. What then happens to late data? Let's say it gets an event with a time stamp of 8:04, but now it is 8:06. It is 2 minutes late, 1 minute after the close of the window. What does it do with this data? The answer is, you get to choose that. The default is just to discard it. But you can also tell it to reprocess the window based on those late arrivals. Beams default windowing configuration tries to determine when all data has arrived based on the type of data source, and then advances the watermark past the end of the window. This default configuration does not allow late data. The default behavior is to trigger at the watermark. If you don't specify a trigger, you are actually using the trigger after watermark. After watermark is an event time trigger. We could also apply any other trigger using event time. The message's time stamps are used to measure time with these triggers. But we could also add custom triggers. If the trigger is based on processing time, the actual clock, real time, is used to decide when to omit results. For instance, you can decide to omit exactly every 30 seconds, regardless of the time stamps of the messages that have arrived to the window. After count is an example of a data-driven trigger. Rather than omitting results based on time, here we trigger based on the amount of data that has arrived within the window. The combination of several types of triggers opens a world of possibilities with streaming pipelines. We may omit some results early, using after processing time, and then again at the watermark, when data is complete, and then for the next five messages that arrive late, after the watermark. Now we know different techniques to handle accumulation late arrival data. We also know that triggers are used to initiate the accumulation, and watermarks help in deciding the lag time and related corrective actions for computing accumulations. The code in this example creates a sample trigger. As you can see in the code, we are creating a sliding window of 60 seconds, and it slides every 5 seconds. The function after watermark method gives us details about when to trigger the accumulation. The code uses two options. First, early or speculative figuring, which is set to 30 seconds. Second, late for each late-arriving item. The second code segment demonstrates the composite trigger. The composite trigger will get activated either after 100 elements are available for accumulation, or every 60 seconds irrespective of watermark. This code segment uses a fixed window of 1 minute's duration. This is how the window reprocesses. This late processing works in Java and Python. When you set a trigger, you need to choose either accumulate mode or discard mode. This example shows the different behaviors caused by the intersection of windowing, triggers, and accumulation mode.

#### Lab Intro: Streaming Data Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564990

Person: In this lab, you use Dataflow to collect traffic events from simulated traffic sensor data made available through Pub/Sub, process them into an actionable average, and store the raw data in BigQuery for later analysis. You will learn how to start a Dataflow pipeline, monitor it, and lastly, optimize it.

#### Streaming Data Processing: Streaming Data Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/564991

#### Dataflow Streaming Features

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/564992

### High-Throughput BigQuery and Bigtable Streaming Features

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564993

Welcome to the module: High-Throughput BigQuery and Bigtable Streaming Features. In this module, you will learn about the analysis of streaming data and the throughput constraints associated with it. These analysis systems are primarily responsible for analyzing data in real time and helping make timely business decisions. We're going to talk about both BigQuery and Bigtable.

#### Streaming into BigQuery and visualizing results

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564994

person: In this first lesson, we'll discuss the streaming of data into BigQuery and using Google Data Studio to visualize results. Streaming data is not added to BigQuery via a load job. There is a separate BigQuery method called Streaming Inserts. Streaming Inserts allows you to insert one item at a time into a table. New tables can be created from a temporary table that identifies the schema to be copied. Usually the data is available within seconds. The data enters a streaming buffer where it is held briefly until it can be inserted into the table. Data availability and consistency are considerations. Candidates for streaming or analysis are applications that are tolerant of late or missing data or data arriving out of order or data that is duplicated. The stream can pass through other services, introducing additional latency and the possibility of errors. Since streaming data is unbounded, you need to consider the streaming quotas. There is both a daily limit and a concurrent rate limit. You can find more information about these in the online documentation. You can disable best effort de-duplication by not populating the insert ID field for each row inserted. When you do not populate insert ID, you get much higher streaming ingest quotas for the U.S. region, one million per second versus 500,000 inserts per second. This leads to a pertinent question, "When should you ingest a stream of data rather than use a batch approach to load data?" The answer is, when the immediate availability of the data is a solution requirement. And the reason, well, in most cases, loading batch data is not charged. Loading streaming data is a charged activity, so use batch loading or repeating batch loading rather than streaming, unless that real time is a requirement of the application. Here's an example of the code used to insert streaming data into a BigQuery table. In this example, the message body has already been decoded. In a full example, a step would be required to extract the appropriate message elements to be inserted. After streaming into a BigQuery table has been initiated, you can review the data in BigQuery by querying the table receiving the streaming data. When working with data in BigQuery, including streaming data, you can use Data Studio to explore the data further. After you execute a query, you can choose Data Studio from the explore data options to immediately start creating visualizations as part of a dashboard. This is the Data Studio home page. There are two ways to create a new report from scratch. Select blank report in the templates panel in the middle of the screen, or click the create button in the navigation pane on the left of the screen. Note that you can have any or all of these data sources in a single Data Studio report. In addition to the Google connectors, there is an increasing list of partner connectors to choose from as well. Since Data Studio reports can be shared, you should be aware of the ramifications of adding a data source. When you add a data source to a report, other people who can view the report can potentially see all the data in that data source. And anyone who can edit the report can use all the fields from any added data sources to create new charts with them. Click add to report. Having selected a dataset, you can specify what elements of the dataset you wish to visualize. This includes selecting the dimensions and metrics that you want to use from the available fields of your dataset. The edit data source picker in front of the data source name can be selected to edit the dataset fields. Easily change your data table view to a chart by clicking chart in the properties panel and selecting a chart type from the options provided. You can edit the style of your chart or even change your chart type selection later. You can also revert back to a data table view. You can also add separate charts by selecting add a chart from the toolbar, resize the components on the canvas to arrange data tables and different chart types as required. In the same way that you defined dimensions and metrics earlier, do the same for your chart by adding selections from the available fields list. Tip: the sequence of the fields under metric will determine the order in which the data is displayed in the chart. Use the drag feature to easily change the sequence of the fields. Give your report a name. Since Data Studio is based on Google Drive, note that you can have duplicate file names. Click the view toggle button to view the end-user version of the report. And here's your report. Notice, it looks very similar to when you were editing it, but as a viewer you can't modify the report. When a viewer mouses over the chart, they're able to view live data. In this example, the viewer is able to see that in the year 2000 there were 31 natural disasters attributable to extreme temperature. Note that users cannot edit your reports unless you give them permission. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine. BI Engine is a fast, in-memory analysis service that is built directly into BigQuery and is available to speed up your business intelligence applications. Historically, BI teams would have to build, manage and optimize their own BI service and OLAP cubes to support reporting applications. Now with BI Engine, you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and service as a fast, in-memory, intelligent caching service that maintains state.

#### Lab intro: Streaming Data Processing: Streaming Analytics and Dashboards

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564995

Person: In this streaming analytics and dashboards lab, you will connect to a BigQuery data source from Google data studio and create reports and charts to visualize the BigQuery data.

#### Streaming Data Processing: Streaming Analytics and Dashboards

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/564996

#### Generate Personalized Email Content with BigQuery Continuous Queries and Gemini

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/564997

#### Streaming Analytics and Dashboards

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/564998

#### High-throughput streaming with Bigtable

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/564999

So far, we looked at how to do queries on data even if it's streaming in using BigQuery, and displaying the data using Looker Studio. BigQuery is a very good general purpose solution, something that would work in most cases. But every once in a while, you will come across a situation where the latency of BigQuery is going to be problematic. In BigQuery, the data that's streaming in is available in a matter of seconds, but sometimes you will want lower latency than that. You will want your information to be available in milliseconds or microseconds. You may also have run into issues where the throughput of BigQuery may not be enough and you may want to deal with a higher throughput. So what we will be looking at next is how to handle such throughput or latency requirements when BigQuery is not enough. Where do you go? We will talk about Bigtable, which is suited to high-performance applications. We will look at how to design for Bigtable, specifically how to design schemas, how to design the row key from Bigtable. We will look at how to ingest data into Bigtable. To use Bigtable effectively you have to know a lot about your data and how it will be queried up-front. A lot of the optimizations happen before you load data into Bigtable. Bigtable is ideal for applications that need very high throughput and scalability for non-structured key/value data, where each value is typically no larger than 10 MB. Bigtable is not well suited for highly structured data, transactional data, small data volumes less than 1 TB, and anything requiring SQL Queries and SQL-like joins. Here are a few examples of data engineering requirements that have been solved using Bigtable. Machine learning algorithms frequently have many or all of these requirements. Applications that use marketing data, such as purchase histories or customer preferences. Applications that use financial data such as transaction histories, stock prices, or currency exchange rates. Internet of Things - IoT data, such as usage reports from meters, sensors, or devices. Time-series data, such as resource consumption like CPU and memory usage over time for multiple servers. Bigtable is most often used in real-time lookup capacity for an application where high-throughput is a necessity. Bigtable stores data in a file system called Colossus. Colossus also contains data structures called Tablets that are used to identify and manage the data. And metadata about the Tablets is what is stored on the VMs in the Bigtable cluster itself. This design provides amazing qualities to Bigtable. It has three levels of operation. It can manipulate the actual data. It can manipulate the Tablets that point to and describe the data. Or it can manipulate the metadata that points to the Tablets. Rebalancing tablets from one node to another is very fast, because only the pointers are updated. Bigtable is a learning system. It detects "hot spots" where a lot of activity is going through a single Tablet and splits the Tablet in two. It can also rebalance the processing by moving the pointer to a Tablet to a different VM in the cluster. So its best use case is with big data -- above 300 GB -- and very fast access but constant use over a longer period of time. This gives Bigtable a chance to learn about the traffic pattern and rebalance the Tablets and the processing. When a node is lost in the cluster, no data is lost. And recovery is fast because only the metadata needs to be copied to the replacement node. Colossus provides better durability than the default 3 replicas provided by HDFS. Bigtable stores the actual data elements in tables. And to begin with, it is just a table with rows and columns. However, unlike other table-based data systems like spreadsheets and SQL databases, Bigtable has only one index. That index is called the Row Key. There are no alternate indexes or secondary indexes. And when data is entered, it is organized lexicographically by the Row Key. The design principle of Bigtable is speed through simplification. If you take a traditional table, and simplify the controls and operations you allow yourself to perform on it, then you can optimize for those specific tasks. It is the same idea behind RISC - Reduced Instruction Set Computing - Simplify the operations. And when you don't have to account for variations, you can make those that remain very fast. In Bigtable, the first thing we must abandon in our design is SQL. This is a standard of all the operations a database can perform. And to speed things up we will drop most of them and build up from a minimal set of operations. That is why Bigtable is called a NoSQL database. The green items are the results you want to produce from the query. In the best case you are going to scan the Row Key one time, from the top-down. And you will find all the data you want to retrieve in adjacent and contiguous rows. You might have to skip some rows. But the query takes a single scan through the index from top-down to collect the result set. The second instance is sorting. You are still only looking at the Row Key. In this case the yellow line contains data that you want, but it is out of order. You can collect the data in a single scan, but the solution set will be disordered. So you have to take the extra step of sorting the intermediate results to get the final results. Now think about this. What does the additional sorting operation do to timing? It introduces a couple of variables. If the solution set is only a few rows, then the sorting operation will be quick. But if the solution set is huge, the sorting will take more time. The size of the solution set becomes a factor in timing. The orderliness of the original data is another factor. If most of the rows are already in order, there will be less manipulation required than if there are many rows out of order. The orderliness of the original data becomes a factor in timing. So introducing sorting means that the time it takes to produce the result is much more variable than scanning. The third instance is searching. In this case, one of the columns contains critical data. You can't tell whether a row is a member of the solution set or not without examining the data contained in the critical column. The Row Key is no longer sufficient. So now you are bouncing back and forth between Row Key and column contents. There are many approaches to searching. You could divide it up into multiple steps, one scan through the Row Keys and subsequent scans through the columns, and then perhaps a final sort to get the data in the order you want. And it gets much more complicated if there are multiple columns containing critical information. And it gets more complicated if the conditions of solution set membership involve logic such as a value in one column AND a value in another column, or a value in one column OR a value in another column. However, any algorithm or strategy you use to produce the result is going to be slower and more variable than scanning or sorting. What is the lesson from this exploration? To get the best performance with the design of the Bigtable service, you need to get your data in order first, if possible, and you need to select or construct a Row Key that minimizes sorting and searching and turns your most common queries into scans. Not all data and not all queries are good use cases for the efficiency that the Bigtable service offers. But when it is a good match, Bigtable is so consistently fast that it is magical. Here is an example using airline flight data. Each entry records the occurrence of one flight. The data include city of origin and the date and time of departure, and destination city and date and time of arrival. Each airplane has a maximum capacity, and related to this is the number of passengers that were actually aboard each flight. Finally, there is information about the aircraft itself, including the manufacturer (called the make), the model number, and the current age of the aircraft at the time of the flight. In this example, the Row Key will be defined for the most common use case. The Query is to find all flights originating from the Atlanta airport and arriving between March 21st and 29th. The airport where the flight originates is in the Origin field. And the date when the aircraft landed is listed in the Arrival field. If you use Origin as the Row Key, you will be able to pull out all flights from Atlanta -- but the Arrival field will not necessarily be in order. So that means searching through the column to produce the solution set. If you use the Arrival field as the Row Key, it will be easy to pull out all flights between March 21st and 29th, but the airport of origin won't be organized. So you will be searching through the arrival column to produce the solution set. In the third example, a Row Key has been constructed from information extracted from the Origin field and the Arrival field -- creating a constructed Row Key. Because the data is organized lexicographically by the Row Key, all the Atlanta flights will appear in a group, and sorted by date of arrival. Also, with the word “arrival” present in the key there is no need to verify that the timestamp is for the Arrival rather than the Departure. Using this Row Key you can generate the solution set with only a scan. In this example, the data was transformed when it arrived. So constructing a Row Key during the transformation process is straightforward. Bigtable also provides Column Families. By accessing the Column Family, you can pull some of the data you need without pulling all of the data from the row or having to search for it and assemble it. This makes access more efficient. The most common query is for the current arrival delay from Atlanta. That will involve averaging flight delays over the last 30 minutes. Hence, origin arrival. We want this at the top of the table, hence the reverse timestamp or RTS. You can reverse timestamps by subtracting the timestamp from your programming language's maximum value for long integers, such as Java's java.lang. Long. MAX VALUE, for example LONG MAX timestamp.millisecondsSinceEpoch. By reversing the timestamp, you can design a row key where the most recent event appears at the start of the table instead of the end. As a result, you can get the N most recent events simply by retrieving the first N rows of the table. When you delete data, the row is marked for deletion and skipped during subsequent processing. It is not immediately removed. If you make a change to data, the new row is appended sequentially to the end of the table, and the previous version is marked for deletion. So both rows exist for a period of time. Periodically, Bigtable compacts the table, removing rows marked for deletion and reorganizing the data for read and write efficiency. Distributing the writes across nodes provides the best write performance. One way to accomplish this is by choosing row keys that are randomly distributed. However, choosing a row key that groups related rows so they are adjacent makes it much more efficient to read multiple rows at one time. In our airline example, if we were collecting weather data from the airport cities, we might construct a key consisting of a hash of the city name along with a timestamp. The example row key shown would enable pulling all the data for Delhi, India, as a contiguous range of rows. Whenever there are rows containing multiple column values that are related, it is a good idea to group them into a column family. Some NoSQL databases suffer performance degradation if there are too many column families. Bigtable can handle up to 100 column families without losing performance. And it is much more efficient to retrieve data from one or more column families than retrieving all of the data in a row. There are currently no configuration settings in Bigtable for compression. However, random data cannot be compressed as efficiently as organized data. Compression works best if identical values are near each other, either in the same row or in adjoining rows. If you arrange your row keys so that rows with identical data are adjacent, the data can be compressed more efficiently. Bigtable periodically rewrites your table to remove deleted entries, and to reorganize your data so that reads and writes are more efficient. It tries to distribute reads and writes equally across all Bigtable nodes. In this example, A, B, C, D, E are not data, but rather pointers or references and cache, which is why re-balancing is not time-consuming. We are just moving pointers. Actual data is in tablets in the Colossus file system. Based on the learned access patterns, Bigtable re-balances data accordingly, and balances the workload across the nodes. With a well-designed schema, reads and writes should be distributed fairly evenly across an entire table and cluster. However, in some cases, it is inevitable that certain rows will be accessed more frequently than others. In these cases, Bigtable will redistribute tablets so that reads are spread evenly across nodes in the cluster. Note that ensuring an even distribution of reads has taken priority over evenly distributing storage across the cluster. In 2019 Spotify ran the largest Dataflow job ever at the time with Bigtable "...used as a remediation tool between Dataflow jobs in order for them to process and store more data in a parallel way, rather than the need to always regroup the data" By using Bigtable, Spotify was able to break down Dataflow jobs into smaller components — and reusing core functionality — and was able to speed up jobs and make them more resilient.

#### Optimizing Bigtable performance

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565000

person: We will look now at how you can further optimize Bigtable performance. There are several factors that can result in slower performance. The table schema is not designed correctly. It's essential to design a schema that allows reads and writes to be evenly distributed across the Bigtable cluster. Otherwise individual nodes can get overloaded, slowing performance. The workload isn't appropriate for Bigtable. If you are testing with a small amount, less than 300 gigabytes of data, or for a very short period of time, seconds rather than minutes or hours, Bigtable won't be able to properly optimize your data. It needs time to learn your access patterns, and it needs large enough shards of data to make use of all the nodes in your cluster. The Bigtable cluster doesn't have enough nodes. Typically, performance increases linearly with the number of nodes in a cluster. Adding more nodes can therefore improve performance. Use the monitoring tools to check whether a cluster is overloaded. The Bigtable cluster was scaled up very recently. While nodes are available in your cluster almost immediately, Bigtable can take up to 20 minutes under load to optimally distribute cluster workload across the new nodes. The Bigtable cluster uses HDD disks. Using HDD disks instead of SSD disks means slower response times and a significantly lower cap on the number of read requests handled per second, 500 QPS for HDD disks versus 10,000 QPS for SSD disks. There are issues with the network connection. Network issues can reduce throughput and cause reads and writes to take longer than usual. In particular, you'll see issues if your clients are not running in the same zone as your Bigtable cluster. Because different workloads can cause performance to vary, you should perform tests with your own workloads to obtain the most accurate benchmarks. This is an example of some of the numbers that are possible in terms of throughput. With 100 nodes, you can handle 1 million queries per second. Throughput scales linearly well into the hundreds of nodes. A higher throughput means more items are processed in a given amount of time. If you have larger rows, then fewer of them will be processed in the same amount of time. In general, smaller rows offer higher throughput and therefore are better for streaming performance. Bigtable takes time to process cells within a row, so if there are fewer cells within a row, it will generally provide better performance than more cells. Finally, selecting the right row key is critical. Rows are sorted lexicographically. The goal when optimizing for streaming is to avoid creating hot spots when writing, which would cause Bigtable to have to split tablets and adjust loads. To accomplish that, you want the data to be as evenly distributed as possible. Reading delays adding to processing delays leads to response time. Replication for Bigtable enables you to increase the availability and durability of your data by copying it across multiple regions or multiple zones within the same region. You can also isolate workloads by routing different types of requests to different clusters. Use gcloud bigtable clusters create to create a cluster of Bigtable replicas. If a Bigtable cluster becomes unresponsive, replication makes it possible for incoming traffic to failover to another cluster in the same instance. Failovers can be either manual or automatic depending on the app profile an application is using and how the app profile is configured. The ability to create multiple clusters in an instance is valuable for performance, as one can be for writing and the replica cluster exclusively for reading. Bigtable also supports automatic failover for high availability. The generalizations of isolate the right workload, increase number of nodes, and decrease row size and cell size will not apply in all cases. In most circumstances, experimentation is the key to defining the best solution. A performance estimate is given in the documentation online for write-only workloads. Of course, the purpose of writing data is to eventually read it, so the baseline is an ideal case. At the time of this writing, a 10-node SSD cluster with 1-kilobyte rows and a write-only workload can process 10,000 rows per second at a 6-millisecond delay. This estimate will be affected by average row size, the balance and timing of reads distracting from writes and other factors. You will want to run performance tests with your actual data and application code. You need to run the tests on at least 300 gigabytes of data to get valid results. Also, to get valid results, your test needs to perform enough actions over a long enough period of time to give Bigtable the time and conditions necessary to learn the usage pattern and perform its internal optimizations. Key Visualizer is a tool that helps you analyze your Bigtable usage patterns. It generates visual reports for your tables that break down your usage based on the row keys that you access. Key Visualizer automatically generates hourly and daily scans for every table in your instance that meets at least one of the following criteria: During the previous 24 hours, the table contained at least 30 gigabytes of data at some point in time. During the previous 24 hours, the average of all reads or all writes was at least 10,000 rows per second. The core of a Key Visualizer scan is the heat map, which shows the value of a metric over time broken down into contiguous ranges of row keys. The X-axis of the heat map represents time, and the Y-axis represents row keys. If the metric has a low value for a group of row keys at a point in time, the metric is cold, and it appears in a dark color. A high value is hot, and it appears in a bright color. The highest values appear in white.

#### Lab intro: Streaming Data Processing: Streaming Data Pipelines into Bigtable

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565001

Person: Next is the second hands-on lab for this module: Streaming Data Pipelines into Bigtable. In this lab, you will launch a Dataflow pipeline to read from Pub/Sub and write into Bigtable, and open an HBase shell to query the Bigtable database.

#### Streaming Data Processing: Streaming Data Pipelines into Bigtable

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/565002

#### High-Throughput Streaming with Bigtable

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/565003

### Advanced BigQuery Functionality and Performance

#### Module introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565004

person: In this final module of the building resilience streaming systems on Google Cloud course you'll learn about some of the advanced features of BigQuery. In this module you'll learn about analytic Window functions and the use of width clauses to make complex queries more manageable. We'll introduce you to some of the GIS functions built into BigQuery. And finally we'll share best practices to consider for BigQuery performance.

#### Analytic window functions

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565005

person: Let's start by looking at how to use analytic window functions for advanced analysis. BigQuery like other databases has built-in functions to allow for rapid calculation of results. These include window functions to support advanced analysis. Three groups of functions exist standard aggregations, navigation functions, and ranking and numbering functions. The Count function is a frequently used and self-explanatory function. Other standard aggregation functions are listed here with more detail and how to properly use them available in the documentation. The LEAD function will return a value for a subsequent row in relation to the current row. In the example, the next bike rental time is listed along with the current rental row. Navigation functions generally compute some value expression over a different row in the window frame from the current row. Here are a few commonly used navigation functions. Rank returns the ordinal one based rank of each row within the ordered partition. In the example, each station's duration is returned in ranked order descending, the longest duration returned first. This example shows the ranking of employees by tenure using the start date within each department. First the rows are partitioned by department then ordered by start date. And then finally ranked. This is the SQL code used to perform the rank operation from the ranking of employees by tenure, example on the previous slide. Addition, ranking and numbering exists for specific use cases. The example seen here are frequently used when determining relationships between the rows of data rather than by an external measurement. WITH clauses are instances of a named subquery in BigQuery. WITH clauses are an easy way to isolate SQL operations and make complex queries more manageable.

#### GIS functions

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565006

person: BigQuery has many built-in geographic information system or GIS features, you'll learn about some of them in this lesson. In the example shown, a zip code is used to determine how many bike stations are within one kilometer of the zip code and have at least 30 bikes available. ST_Geog point and ST_ DWithin are used together to pinpoint the stations of interest. ST simply means spatial type. ST_DWithin is used in conjunction with the geospatial boundaries of U.S. zip codes. The latitude and longitude of the bike stations join together in ST_Geog point to create a geospatial object and the value of 1000 for 1000 meters, which is one kilometer as the distance between the objects, zip code boundary and station point. This will return only those within one kilometer. ST_Geog point creates a geospatial object in well-known text or WKT from values provided within the database. In this case, we use latitude and longitude. If the latitude and longitude are provided in JSON format, the function ST_Geog from GeoJSON can be used to generate a geospatial object. To allow for quick testing of geospatial data Google Cloud provides the lightweight BigQuery Geo Viz application. This application will allow rendering of GIS data with minimal configuration. As mentioned earlier, ST_Geog point is used to create a geospatial object from relevant data. The image shows the exact coordinates of the ID values on a map of London. ST_MakeLine and ST_MakePolygon are two additional geospatial functions that can be used to overlay information on a map to help highlight relationships in the data. As mentioned in our earlier example, ST_DWithin can be used to determine the relative location of two points or objects. This image shows cities that are all within 150 kilometers linear distance from Terre Haute, Indiana. The functions ST_Intersects, ST_Contains and ST_CoveredBy allow reporting on the overlay or co-location of geospatial objects.

#### Demo: GIS Functions and Mapping with BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565007

person: Welcome to another BigQuery demo. Here we're looking at some advanced BigQuery functions, specifically some Geographic Information System functions, or GIS functions. Whatever you think of latitude and longitude data, there's some really neat built-in GIS functions and some mapping capabilities built into BigQuery and some really cool plug-ins as you're going to see. So first, we need a really cool dataset. I'm going to be using the BigQuery Public Dataset on London Bike Share. This is millions of bike rides happening around the city of London, where they're coming from, where they're going to because the city of London has bike commuting stations that are fixed, that people can just rent bikes from and then take them from between places. So first off, what are the tables that we're going to be looking at? You've got your cycling stations, which is just the fixed locations around London that has just basic information about where you can rent the bikes, and really important part here I just want to cover later is it has a latitude and longitude, which means we can put it on a map, which is awesome. And then we need some activity of who's renting the bikes, how long are they spending on the bikes, how fast they're going. We'll actually show you how to impute that in a pretty easy, straight-line fashion. That's cycle_hire. So our end goal is, we're going to create a map for the first problem, is to find the fastest bike commuters inside of London. Those people who have the fastest average kilometers per hour, because we're in the UK, we use kilometers per hour, going from a starting station to a ending station. Average, fastest average speed for rides that must be more than 30 minutes, so people who are really getting a workout in. So I'm going to copy this first seemingly long query, and I'm going to break down every part of it for you before we run it so it all makes sense. So paste it in there, in to BigQuery, and we have a couple different tables. I'm going to get Alec to explore the tables first. So we have the stations themselves, so I want to pull the stations up and how many stations do we have? And the details, we have 778 stations, assuming that each row represents an individual station. Very small amount of data. Here's what the data actually looks like. Station's got an ID. Really important for GIS, it's got latitude and longitude. These data types, as you can see, for latitude and longitude, are floats. They're not GIS data types yet. Just because we have latitude and longitude does not mean it's a geographic point yet. I'm going to show you the very easy function called ST_GeogPoint, geography point. It's going to turn those floats in to a geographic point that you can then put on a map. So you've got lat and long, which is great. That's pretty much all we need from here because we can get the name of the station which is useful for our map label as well . So, how do I get this raw data and get it in to a good GIS format? That's the stations, the activity is in the cycle hires. Rentals table and you get what you might expect for a transactional table. How long the bike was in duration for, primary key, rental ID, need to account on those, get how many renters per station. A same bike can be rented more than once, which is interesting for bike maintenance. And then, where it was started and where it ended, but honestly you don't need these from the bikes table because you already have the station ID and a lot more information because the latitude and longitude for that station is not in here, so we can kind of ignore the ending station there. But, we do have the when it was rented and when the rental ended, with the end date there. So, let's talk about at this query. I call this staging because I like to get my data in a good format. This is the preprocessing before we actually go. So, what do we actually do? With the width clause, the width clause says, "Hey, get this named subquery, everything below here, between these two parenthesis, lines one through 34, is a named subquery. Technically you could stuff all of this in to the from clause if the below table, but it won't be really readable. What are we actually doing? So, I'm basically saying select as a struct. A struct is kind of like pre-jointed table, it's just a simple container. I do it for readability, to basically say, "All of these fields, I want you to prepend or prefix it with the word stating. " So it will be starting.name or starting.point, because you're going to have a lot of similar sounding column names because you have a starting station and a ending station. I prefer to use a struct just for that reason. So, I've got a lot of information about where it came from. The station name. We've used the ST Geography Point to turn the longitude and the latitude in to an actual geographic point, which BigQuery natively supports. Some other information about the station, like how many docks, bike docks, does it have, when the station was installed, lots of other good information in there as well. So I'm actually going to throw just a quick limit on this query. You can see, I mean, if you look at the cycle hire tables, it is a very, very large table. This is 24 million records. And, let's throw just a limit on there to see what some of this data actually looks like. So running this. And again, we're doing the joins -- we're doing two joins actually because we're going to do kind of like a self-join to basically get at the both the starting and ending station. Here's what it is actually going to look like in the end. And again, this is -- you can store this table if you want. I'm going to talk a little bit more about why you may or may not want to do that. We have this starting.name for the station, and then that's where the bike started from, that's how many bikes are at that dock, that's when the station was installed. Where did it go? It went from New Springs Garden Walk to Waterloo Place, St. James. Point to point, and everything that starts with bike.whatever -- that's the bike struct, has this information. Here's the line, so I'll show you if you actually scroll down in the query you can get some really interesting data with Geographic Information Systems functions, GIS functions. You can basically say, "Give me two points and I'll give you the straight line distance between them. " ST_Distance. "Also, give me two points and I'll draw a line string for you," if you're going to visualize this later on and map. So the distance is going to output in meters, for ST_Distance, and ST_MakeLine basically says, "Hey, if you're a GIS visualizer, here's a line string," So you can see that the bike distance is 2,184 meters between those two stations I just mentioned, and this is the point to point to actually draw that line as you're going to see when we actually dump this out in to a GIS visualizer a little bit later. So, we have, again, we're looking for average speed. So we got the bike duration in -- I think it's seconds, 1,980 seconds, and we have the distance that it covered, so we can get the average speed from there on out, and that's exactly what the latter half of this query actually does. So, we have all that information to stage that data. I dump it in and with clause you can see what it's doing. Yes, you can dump it into a table, but you'll lose the benefit if you're filtering on WHERE clauses. The WHERE clause here, as I noted, later on down inside of the query here allows you to do what's called a automatic predicate pushdown. It's a really cool, almost magical, part of BigQuery where yes, you can store all this data inside of a permanent materialized table, but if your users are constantly just filtering for a very small sliver of that later on, like we're filtering for the duration that's greater than 30 minutes, than what BigQuery can do is it can take this WHERE clause filter and actually, while -- before it actually executes that, as part of the query execution plan, bring that up in to the WHERE clause, you're not going to see this on the UI or anything like that, and then filter that data before it gets processed. So again, the argument against materializing this out to a permanent table is your users might be continuously filtering on this, so allow them the benefit of predicate pushdown. But honestly, it really just depends on your user case. So once we have all that raw data, the actual query that does the average speed is pretty easy. We want to return some basic dimensional information, the starting station, ending station, and distance in kilometers since it's going from meters to kilometers, rounded to just two decimal places. This is the line the trip has made, just uniting together, matching together all those lines. It's essentially how to do a distinct on the trip line for our use case. And, the total trips that were made between those two stations, that's just the count there, and then this is literally just the very simple average of the bike distance over the duration. So, the distance, meters over second, that's going to give you your speed. We actually want that in kilometers per hour, so you're taking meters divided by thousand, and then second divided by 60 to get minutes divided by 60 to get in to hours, and that's how you get to kilometers per hour, where bikes have at least 30 minute durations and the stations themselves have at least 100 trips between them and order by the fastest average pace first limit 100, so let's go ahead and run that. I should have been giving this explanation while this query is running because I think churning through all this data, I'm creating all this geographic information systems point. It's probably going to take at least -- You know, I think, when I ran this last, it was 30 to 60 seconds. So while that's running, that is going to be the aver speed. And, boom, here are the results. We don't have to wait for ours to finish. It actually looks like between Finlay Street and Fulham, and King Edward Street and St. Paul, the distance is 9 kilometers. And folks on a bike share -- And again, this is the average. This isn't even the max. The average people are putting out is about 10 miles an hour, 16.6 kilometers per hour, and that's 103 trips in this dataset. So, once you have the distance to tripline these geographic points, isn't it better to visualize this type of information point to point using a map? So you can use this kind of open tool, BigQuery Geography Viz, to just plot your BigQuery map data. So, let's see. This actually finished in 22 seconds, and we actually, I'm going to save these results somewhere. So, let's say, I'm just going to dump them to a BigQuery table. Let's see, do I have a cool one? I'm going to use business. And then, I'll call this just GIS demo, or something like that, and boom. Dump them in to a table. Oh, it's on the EU server, so I have to create a dataset in the EU. So, let me create a dataset in the EU because that's where this data wants to live. And let's see, create a dataset. First of all, let me disable turning off cache. So hopefully, I won't have to rerun that whole query again. We will do a GIS demo, will be the dataset ID. Perfect. And then, we'll say that it's in the European Union, because that's where this London data exists, and for the BigQuery poll dataset. And then let's hope -- Did it save it in cache? No because I think I just saved it to cache. Oh, well, we'll wait 22 seconds for that to actually come in to play, and then we'll save that as a table. Once you have that as a table, one of the cool things that you can do is inside of this BigQuery Geo Viz, you just literally say, "Hey, here's my projects." So let's get a project ready while we have this -- oh, it's already done. It's pretty fast. And, let's save the results, BigQuery table. I have GIS visualization. We'll just call this demo for speed, and then we'll dump it in to there. And my project, that's fine, GIS demo. Here it is. Here's the demo table. I'm just going to query this just so I have that query give me everything that from that table. So for this given project, how BigQuery GIS works is just coming Appspot project publicly available. You paste in your project ID and then, much like inside of BigQuery, you just dump the query in to the query editor that you see here, and it brings a little bit more on screen. Oh, I've got to make sure that I'm opening this in the same incognito window, so let me actually open this here. And I'm going to authorize it, yes, as my Qwiklabs account that I already have here, allow it to view and manage the data. Now I can paste it as a student because previously it was trying to do it from my Google e-mail address. And, boom, dump that in to there. And now let's get some GIS data in here, shall we? So, oh, what's this magical wizard things looks? I'm going to run this processing location, auto-select, sure. Go nuts. Whoa. Look at that. That's awesome. It automatically recognized, "Hey, you've got some lines inside of here," and it immediately zoomed us in onside of London which is super cool. So we've got this. Now let's look at the data. Geometry column, tripline, excellent. I want to style it though. I want the larger lines to represent a faster average speed trips. So I think inside of my demo, what do we have? What do we have? Yeah. Down here, it's the stroke weights, which is the weight of the line. We want to get a linear function with average kilometers per hours. Let me just try to make that. So we want the stroke weight, which is the width of the line, to be data driven, sure. And, it's in a linear function, and it's going to be the average kilometers per hour, so we want the thicker lines there, and then we want this to be -- I honestly just messed around with this and basically said these are all the different ranges that you can have. You can zoom out on this. I ended up. I played with this a little bit and found that if you did -- What did I do? The domain itself is -- This is just because you want the bigger lines to show up a little bit better, but honestly a lot of this is half an art and a science. So I'm just copying from what I did earlier, and you can see, as we zoom in to the map, you find that largest line, that is the highest average speed, so I'm going to go ahead and click on that, and this is exactly what we saw. So it started on Finlay Street, went to King Edward Street, the distance is 9 kilometers. There's 103 trips that made that and the average speed of all them was 16.6. So we went full circle and, again, if you reduce this, if you increase this, this is just changes the weights of those lines as well. How big -- how much of a disparity, deviation, rather, between the sizing of those as well. So you can see, not only was that one pretty fast, but there was two other ones that were pretty fast in commuting there as well. And you can make all sorts of assumptions and interpretations about why this one was the fastest route. Maybe people were just, "Hey, we don't want to go around town and explore a lot," because you can imagine not all these bikes are just going from station to station. Maybe the ones in the inner city of London are tourists that take a long time. Maybe they're going superfast. But they take a long, long, long, long, long, actual distance because what the distance that we're measuring here is just literally in a straight line distance between stations. So maybe one assumption is if you're riding from Fulham to wherever this was ending up, that most likely you're just going point to point. You're not using the bike as, like, a touring route as well. Again, just all assumptions until you actually go in and take a look at the data, maybe interview some of the folks who are starting from station to station there, and see that maybe the majority of them are actually just bike commuters instead of tourists. So, that's a basic recap. You just went from a raw dataset converted with GIS functions, and then ultimately visualize it in BigQuery Geo Viz. Nice work. So in our demo, you saw me use ST_Distance, but that's just one of the many GIS functions supported by BigQuery. Here's an example where we take New York City bike station locations against zip code boundaries to see how many stations are inside of that zip code polygon with ST_DWITHIN. A major takeaway is that all Latin long values should be converted with ST Geography Point as a WKT, or a well-known text, which is much more efficient to store and query from. In the demo, we covered ST_MakeLine, but you can also make polygon areas as well, as you see here with these three points in the triangle. Here's an example in which queries, which point are within 150 kilometers of another given point, and then draws those lines to match against that intersection. Again, these are not driving time estimates, but simple straight lines. You can get pretty advanced with GIS, like seeing if locations intersect with ST_Intersects, and all the other functions that are listed here. If this interests you, I'd really encourage you to read up on the GIS documentation and other examples from BigQuery and see how you can make some pretty awesome GIS insights and maps with your data.

#### Performance considerations

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565008

This lesson is a recap on BigQuery performance and pricing topics. The goal for virtually every information system is to promote fast and smart decisions. Here are a few best practices to consider: Use Dataflow to do the processing and data transformations. Create multiple tables for easy analysis. Use BigQuery for streaming analysis and dashboards, and store data in BigQuery for low cost, long-term storage. Also, create views for common queries. Exploring a dataset through SQL is more than just writing good code. You need to know what destination you’re heading towards and the general layout of your data. Good data analysts will explore how the dataset is structured even before writing a single line of code. People often analyze data and develop a schema at the beginning of a project and never revisit those decisions. The assumptions they made at the beginning may have changed and are no longer true. So they attempt to adjust the downstream processes without ever reviewing and considering changing some of the original decisions. Look at the data. Perhaps it was evenly distributed at the start of the project but as the work has grown, the data may have become skewed. Look at the schemas. What were the goals then? Are those the same goals now? Is the organization of the data optimized for current operations? Stop accumulating work that could be done earlier. Analogy: dirty dishes. If you clean them as you use them, the kitchen remains clean. If you save them, you end up with a sink full of dirty dishes and a lot of work. There are five key areas for performance optimization in BigQuery and they are: Input and output - how many bytes were read from disk? Shuffling - how many bytes were passed to the next query processing stage? Grouping - how many bytes were passed through to each group? Materialization - how many bytes are written permanently out to disk? Lastly, Functions and UDFs, how computationally expensive is the query on your CPU? There’s an old Silicon Valley saying: "Don't scale up your problems. Solve them early while they are small." Here's a cheat sheet of best practices that you should follow. Don't select more data columns than you need, that means avoid SELECT * at all costs when you can. If you have a very large dataset, consider using approximate aggregation functions instead of regular ones. Next, make liberal use of the WHERE clause at all times to filter data. Then, don't use an ORDER BY on a wide clause or sub-queries that you have, only apply ORDER BY as the last operation that you will perform. For joins, put the larger table on the left if you can, that'll help BigQuery optimize it and how it does its joins. If you forget, BigQuery will likely do those optimizations for you so you might not even see any difference. You can use wildcards in table suffixes to query multiple tables, but try to be as specific as possible as you can with those wildcards. For your GROUP BYs, if you're grouping by the names of every Wikipedia author ever, which means high distinct values or high cardinality, that's a bad practice or an anti-pattern. Stick to low unique value group bys. It is also important to understand data distribution to avoid data skew. Low cardinality means less values by key, but these values may occur very often, for example a status column has values of "available", "do not disturb", and "offline". The value "available" might occur 5,000 times but the value "do not disturb" might occur just 10 times. Lastly, use partition tables whenever you can. If you create a large, multi-stage query, each time you run it, BigQuery reads all the data that is required by the query. Intermediate table materialization is where you break the query into stages. Each stage materializes the query results by writing them to a destination table. Querying the smaller destination table reduces the amount of data that is read. In general, storing the smaller materialized results is more efficient than processing the larger amount of data. The analogy is air travel from Sunnyvale, California, USA to Japan. There is one direct flight. Or a series of four shorter connecting flights. The direct flight has to carry the fuel for the entire journey. The connecting flights only need enough fuel for each leg of the trip. The total fuel used in landing and taking off (an analogy for storing the intermediate tables) was less than the total fuel used for carrying everything in the entire journey. Here is a tip: Compare costs of storing the data with costs of processing the data. Processing the large dataset will use more processing resources. Storing the intermediate tables will use more storage resources. In general, processing data is more expensive than storing data. But you can do the calculations yourself to establish a breakeven for your particular use case. A different way to check how many records are being processed is by clicking on the Explanation tab in the BigQuery UI after running a query. We started with 313 million rows and filtered down to 100 as our output result. The query stages represent how BigQuery mapped out the work required to perform the query job. Approximate Functions are a great way to improve performance. The: APPROX_COUNT_DISTINCT function returns an approximate result for the COUNT(DISTINCT expression). The result is less accurate but it performs much more efficiently. An easy way to understand the performance of your BigQuery operations is through Cloud Monitoring, a default component of every Google Cloud project. These charts show Queries in Flight and Slot Utilization for the period of September 2 to October 14.

#### Lab Intro: Optimizing your BigQuery Queries for Performance

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565009

person: In this lab you'll optimize your BigQuery queries for performance. Specifically, you'll use BigQuery to minimize input and output from your queries. You'll cash results from your previous queries, learn about performing efficient joints, avoid overwhelming single workers with your query and lastly use approximate aggregation functions. Good luck.

#### Optimizing your BigQuery Queries for Performance 2.5

- https://www.cloudskillsboost.google/paths/16/course_templates/52/labs/565010

#### Cost considerations

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565011

person: Storage pricing is based on the amount of data stored in your tables when it is uncompressed. The size of the data is calculated based on the data types of the individual columns. Active storage pricing is prorated per megabyte per second. If a table is not edited for 90 consecutive days, it is considered long-term storage. The price of storage for that table automatically drops by approximately 50 percent. There is no degradation of performance, durability, availability or any other functionality. If the table is edited, the price reverts back to the regular storage pricing and the 90-day timer starts counting from zero. Anything that modifies the data in a table resets the timer, including loading data into a table, copying data into a table, writing query results to a table, using the data manipulation language, using the data definition language, streaming data into the table. All other actions do not reset the timer, including querying a table, creating a view that queries a table, exporting data from a table, copying a table to another destination table, and patching or updating a table resource. Because you don't get to see the VMs running behind the scenes, BigQuery exposes slots to help you manage resource consumption and costs. BigQuery automatically calculates how many slots are required by each query depending on query size and complexity. The default slot capacity and allocation work well in most cases. You can monitor slot usage in Cloud Monitoring. Candidate circumstances where additional slot capacity might improve performance are solutions with very complex queries on very large datasets with highly concurrent workloads. You can read more about slots in the online documentation or contact a sales representative. Fixed-rate pricing is $10,000 per 500 slots per month. A 25 percent discount is offered for customers choosing a term length of at least 1 year, $7,500 for 500 slots. Capacity is sold in increments of 500 slots with a current minimum of 500 slots. Flex Slots are an option available for you to purchase BigQuery slots for short durations. Flex slots allow you to purchase BigQuery slots for short durations, as little as 60 seconds at a time. A slot is the unit of BigQuery analytics capacity. Flex slots let you quickly respond to rapid demand for analytics and prepare for business events such as retail holidays and app launches. Flex slots give BigQuery reservation users immense flexibility without sacrificing cost, predictability or control. Flex slots are priced at $0.04 per slot per hour, and are available in increments of 100 slots. It usually takes just a few minutes to deploy Flex slots in BigQuery reservations. Once deployed, you can cancel after just 60 seconds and you will only be billed for the seconds Flex slots are deployed. You can seamlessly combine Flex slots with existing annual and monthly commitments to supplement steady-state workloads with bursty analytics capability. For many businesses, specific days or weeks of the year are crucial. Retailers care about Black Friday and Cyber Monday. Gaming studios focus on the first few days of launching new titles. And financial services companies worry about quarterly reporting and tax season. Flex slots enables such organizations to scale up their analytics capacity for the few days necessary to sustain the business event and scale down thereafter, only paying for what they consumed. There are a number of considerations for flat-rate pricing. Flex slots are a special commitment type. The commitment duration is only 60 seconds. You can cancel Flex slots any time thereafter. You're charged only for the seconds your commitment was deployed. Flex slots are subject to capacity availability. When you attempt to purchase Flex slots, success of this purchase is not guaranteed. However, once your commitment purchase is successful, your capacity is guaranteed until you cancel it. Monthly commitments cannot be canceled for 30 days after your commitment is active. After the first 30 calendar days, you can cancel or downgrade at any time. If you cancel or downgrade, the charges are prorated per second at the monthly rate. For example, you cannot cancel on day 29. If you cancel during the first second of day 31, you're charged for 30 days and 1 second. And if you cancel at the mid-point of the third month, you're charged 50 percent of your monthly rate for that month. Prior to the anniversary of your commitment date, you can choose to renew for another year, or convert it to a monthly or Flex commitment. If you move to the monthly rate, you can cancel any time and you're charged per second at the monthly rate. For example, if you renew for another year after your annual commitment date, you enter into a new annual commitment and you continue to be charged the yearly commitment rate. Also, if you don't renew for another year after your annual commitment date, you can cancel at any time and charges are prorated per second at the monthly rate. If you determine you need more BigQuery slots, you can purchase additional increments of 500. However, doing so will create a new commitment. When you purchase a flat-rate plan, you specify the allocation of slots by location. To use slots in multiple locations, you much purchase slots in each location. A project can use either flat-rate or on-demand pricing. If you have multiple projects in a given location, you can choose which projects use flat-rate pricing and which projects use on-demand pricing. Lastly, to discontinue a flat-rate pricing plan, you must cancel or downgrade your commitment, but only after the initial commitment period, 30 days or 1 year. BigQuery doesn't support fine-grained prioritization of interactive or batch queries. To avoid a pileup of BigQuery jobs and timely execution, estimating the right BigQuery slots allocation is critical. Currently, BigQuery times out any query taking longer than 6 hours. If one query is executing within BigQuery, it has full access to the amount of slots available to the project or reservation -- by default, 2,000. If we suddenly execute a second query, BigQuery will split the slots between the two queries, with each getting half the total amount of slots available -- in this case, 1,000 each. This subdividing of compute resources will continue to happen as more queries are executed. This is a long way of saying, it's unlikely that one resource-heavy query will overpower the system and steal resources from other running queries. In flat-rate pricing, organizations have a fixed number of slots. Concurrency is fair across projects, users and queries. That is, if you have 2,000 slots and two projects, each project can get up to 1,000 slots. If one project uses less, the other project will be able to use all of the remainder. If you have two users in each project, each users will be able to get 500 slots. And if each of the two users of the two projects runs two queries, they'll each get 500 slots. This is a long way of saying, they won't likely degrade performance by adding projects. Note that if you want to prioritize one project over another, you can set up a hierarchical reservation. Let's say you have an ETL project that is somewhat lower priority than your dashboarding project. You can give the ETL project 500 slots as a sub-reservation, and the dashboarding project will be in the outer one. If both projects are fully using their reservations, the ETL project can never get more than 500 slots. When one project is lightly used, the other project will be able to take the remaining slots.

#### BigQuery advanced functionality and performance considerations

- https://www.cloudskillsboost.google/paths/16/course_templates/52/quizzes/565012

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/16/course_templates/52/video/565013

person: You've reached the end of this course on building resilient streaming analytic systems on Google Cloud. Let's recap what you've learned. We started off with what streaming data is and the challenges associated with processing streaming data. We said it was important to be able to ingest fairing amounts of data, because you could have spikes in your data. It is important to be able to deal with unexpected delays because latency is a fact of life. We want to be able to derive real-time insights from the data, even as the data is streaming in. In order to do that we look at the architecture that consisted of ingesting the data with Pub/Sub, processing the data in stream using Dataflow and streaming it into BigQuery for a durable storage and interactive analysis. We also spent time talking about how Bigtable is a better solution when a much higher throughput is desired. And finally we went back to BigQuery to look at some of its advanced analysis capabilities with window functions and GIS functionalities, as well as reviewed ways to optimize query performance. Congratulations on completing building resilience streaming analytics system on Google Cloud. Smart analytics, machine learning and AI on Google Cloud is the fourth and final course of the data engineering on Google Cloud Core series and is covered next, we hope to see you there.

### Course Resources

#### Building Resilient Streaming Analytics Systems on Google Cloud

- https://www.cloudskillsboost.google/paths/16/course_templates/52/documents/565014

### Your Next Steps

## 07: Serverless Data Processing with Dataflow: Foundations

- https://www.cloudskillsboost.google/paths/16/course_templates/218

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520329

Hi, and welcome to the first installment of the Serverless Data Processing with Dataflow series, Dataflow Foundations, My name is Mehran Nazir, and I am a product manager with Dataflow. The Serverless Data Processing with Dataflow Course Series builds on the concepts covered in the Data Engineering specialization. We introduced core Dataflow principles when exploring how to build batch data pipelines on Google Cloud. We also covered streaming basics concepts like windowing, triggers, and watermarks while learning how to build resilient streaming systems using Dataflow. This course series expands on those concepts with three additional courses: Foundations, which will cover the fundamentals of the Apache Beam and Dataflow model. Developing Pipelines, which will provide a comprehensive review of the Apache Beam SDK. And Operations, which will equip learners with the tools to run your Dataflow pipelines at scale. In this course, we will do a deep dive on Foundations. Let’s review the outline for the Dataflow Foundations course. First, we will do a quick refresh on the Apache Beam programming model and Google’s Dataflow managed service. Next, we will learn about the Beam Portability Framework, which allows users to write pipelines in their preferred programming language and run on their desired execution engine. In the next module, we will learn about Dataflow’s premium backends that separate compute and storage for maximum performance. We will then explore how IAM, quotas, and permissions work together to enable Dataflow pipelines. Finally, we will review the main security features that are available with Dataflow and how to implement them. To conclude, we will summarize the main concepts covered in the Foundations course.

#### Beam and Dataflow Refresher

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520330

Federico: Hi, I'm Federico Patota, a cloud consultant here at Google. In this section, we will revisit some concepts on the relationship between Apache Beam and Dataflow, and we will see why customers value Dataflow so much. Apache Beam is an open source unified programming model to define both batch and streaming processing pipelines. To create a pipeline, you can use the Beam SDK of the language of your choice to build a program that defines your data-processing pipeline. Beam SDKs use the same classes to represent both batch and streaming data sources, and the same run forms to operate on that data. We will talk more about this in the next course on developing pipelines. A pipeline can be run locally on your computer, remotely on a virtual machine in a data center, or by using the services of a cloud provider. To decide which will be the engine powering your pipeline, you need to specify a runner. Each runner has its own configuration, and it is associated with a backend service. As you might now already, Dataflow is one of the runners available in Apache Beam. It is a fully-managed data processing service with automated provisioning and management of processing resources. Dataflow includes resource autoscaling and dynamic work rebalancing to maximize resource usage and automatically optimize your pipeline execution. It is part of the Google Cloud ecosystem and uses horizontal service like logging and monitoring. Dataflow allows you to separate computing storage resources. We will cover this more in detail in another module of this course.

### Beam Portability

#### Beam Portability

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520331

Federico: Hi and welcome back. Now you will learn about Beam portability. This module is made up of four sections. In this video, we talk about the Beam portability framework. To better understand the purpose of Beam portability, we can start with the vision behind it. The Beam vision is to provide a comprehensive portability framework for data processing pipelines, one that allows you to write your pipeline once in the programming language of your choice and run it with minimal effort on the execution engine of your choice. With Apache Beam, you can define your pipeline in popular languages like Java, Python, Go, SQL. With Beam, you also have the flexibility to move your data processing pipeline from your own premise environment to Dataflow on Google Cloud or any other clouds. There is no vendor lock-in. At the same time, we believe that Dataflow offers the most compelling experience across all of the runners. The portability framework is a language-agnostic way of representing and executing Beam pipelines. It introduces well-defined, language neutral data structures and protocols between the SDKs and the runners. This interoperability layer is called Portability API and enables you to use the language of your choice with the runner of your choice, thus ensuring that SDKs and runners can work with each other uniformly. Moreover, because docker containerization is used, you can customize the execution environment running on the worker nodes of the back end service. Portability brings several additional benefits. Let's review them together. With portability, every runner can work with every supported language. Containerization allows us a configurable, hermetic worker environment. You can have multi-language pipelines and cross-language transforms because of the language-agnostic representation of pipelines and the isolated environment of each operation. And all of this will bring you a faster delivery of new features available in the SDKs because, with portability, every time a new functionality is added to a supported language, it will automatically be available to all DRs.

#### Runner v2

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520332

person: Let's talk about Dataflow Runner v2. To use the portability features mentioned earlier, you must use the Dataflow Runner v2. This new version of the Dataflow Runner uses a more efficient and portable work architecture, based on the Apache Beam portability framework. As we will discuss in detail in a moment, it supports custom containers, multi-language pipelines and cross-language transforms. This runner is packaged together with the Dataflow Shuffle service and Streaming Engine that you will see more in depth in the next module. To enable it, please refer to Dataflow official documentation.

#### Container Environments

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520333

Federico: In this video, we'll look at container environments. The Beam SDK runtime environment can be containerized with Docker to isolate it from other runtime systems. Each user operation has an associated environment in which to execute. Typically, supported SDKs provide a default environment that you can further customize. Because of containerization, you can benefit from ahead-of-time installation. You can include arbitrary dependencies, and even further customization is possible. Now, let's see how you can run your pipeline with custom containers. To use this feature, you need to have the Apache Beam SDK version 2.25.0 or later installed. If you want to test your pipeline locally, you will also need to have Docker installed. To create a custom container image, create a Docker file in which you specify the Apache Beam image as the parent image. Then add your own customizations. After creating your custom Docker file, you need to build the image and push it to a container registry. To do so, you need to specify your project, the name of the image repository, the tag that you want to associate with your image, and the image registry host name. Then you can use either Cloud Build or Docker to build the image and push it to a container registry like GCR.IO. Finally, you can launch your Dataflow job by referencing the regular parameters and the location of the custom container image.

#### Cross-Language Transforms

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520334

Person: In this video, we look at cross-language transforms. With a language agnostic representation of pipelines and the possibility to specify the environment of each operation, you are no longer limited to a single language in a given pipeline. Portability makes it possible for you to run multi-language pipelines that leverage the respective benefits of the individualized case. For example, you can now write a Python pipeline while using I/O connectors that were only available in Java, or if you want to use a TensorFlow extended block of code for a machine learning model in your Java pipeline, you can now use a cross-language transform. Let's go through an example together. The code you see represents a part of a Python streaming pipeline. The ReadFromKafka transform imported from the apache_beam.io.kafka module is a cross-language transform implemented using the Beam Java SDK. Under the hood, to make Java transforms available to a dataflow Python pipeline, The Apache Beam Python SDK starts up a local Java service on your computer to create and inject the appropriate Java pipeline fragments into your Python pipeline. Then the SDK downloads in stages the necessary Java dependencies needed to execute these transforms, and at run time, the dataflow workers will execute the Python and Java code simultaneously to run your pipeline.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/218/quizzes/520335

### Separating Compute and Storage with Dataflow

#### Separating Storage and Compute with Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520336

Federico: Now, let's discuss how to separate compute and storage with Dataflow. This module contains four sections. In this video, we talk about Dataflow. Dataflow allows you to execute your VM pipelines on Google Cloud. There are several reasons why customers love Dataflow so much. First, it's because it is fully managed and autoconfigured. Second, Dataflow optimizes the graph execution by fusing operations efficiently and by not waiting for previous steps to finish before starting a new one unless there is a dependency involved. Third, autoscaling happens step by step in the middle of a pipeline job. As a job needs more resources, it receives them automatically. You don't have to manually scale resources to match job needs, and you don't pay for VM resources that aren't being used. Dataflow will turn down the workers as the job demand decreases. All of this happens while maintaining strong streaming semantics Aggregations like sums and counts are correct, even if the input source sends duplicate records. As we mentioned in a previous course, Dataflow can also handle later-arriving records with intelligent watermarking. Now, let's talk about how to separate compute and storage and save money and time with Dataflow shuffle service, Dataflow streaming engine, and flexible resource scheduling.

#### Dataflow Shuffle Service

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520337

person: In this video, we look at the Dataflow Shuffle service. A shuffle is a Dataflow-based operation behind transforms such as GroupByKey, CoGroupByKey, and Combine. The Dataflow Shuffle operation partitions and groups data by key in a scalable, efficient, fault-tolerant manner. Currently, Dataflow uses a shuffle implementation that runs entirely on worker virtual machines and consumes worker CPU, memory, and persistent disk storage. The service-based Dataflow Shuffle feature available for batch pipelines only moves the shuffle operations out of the worker VMs and into the Dataflow service backend. With the Dataflow Shuffle service, you will have faster execution time of batch pipelines for the majority of the job types. The worker nodes will benefit from a reduction in consumed CPU, memory, and persistent disk storage resources, and your pipelines will have better autoscaling because the worker nodes VMs no longer hold any shuffle data, and can therefore be scaled down earlier. Also, because of the service, you will get better fault tolerance. An unhealthy VM holding Dataflow Shuffle data will not cause the entire job to fail, which would happen without the feature. See the Dataflow official documentation to learn how to enable the Dataflow Shuffle service for your batch pipelines.

#### Dataflow Streaming Engine

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520338

person: In this video, we look at the dataflow streaming engine. Just like shuffle component in batch, the streaming engine offloads the window state storage from the persistent disks attached to worker VMs to a back-end service. It also implements an efficient shuffle for streaming cases. Luckily, no code changes are required. Worker nodes continue running your user code and implements data transforms and transparently communicate with a streaming engine to source state. With the dataflow streaming engine, you will have a reduction in consumed CPU, memory, and persistent disk storage resources on the worker VMs. Streaming engine works best with smaller worker machine types like n1-standard-2, and does not require persistent disks beyond a smaller worker boot disk. This leads to a lower resource and quota consumption. With streaming engine, your pipeline will be more responsive to variations to incoming data volume. Finally, you will have improved supportability, since you don't need to redeploy your pipelines to applied service updates. To activate dataflow streaming engine, see dataflow official documentation.

#### Flexible Resource Scheduling

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520339

person: Now let's talk about Flexible Resource Scheduling, or in short, FlexRS. FlexRS helps you reduce the cost of your batch processing pipelines because you can use advanced scheduling techniques in the Dataflow Shuffle Service and leverage a mix of preemptible and normal virtual machines. When you submit a FlexRS job, the Dataflow service places the job into a queue and submits it for execution within six hours from job creation. This makes FlexRS suitable for workloads that are not time-critical, such as daily or weekly jobs that can be completed within a certain time window. As soon as you submit your FlexRS job, Dataflow records a job ID and performs an early validation run to verify execution parameters, configurations, quota and permissions. In case of failure, the error is reported immediately, and you don't have to wait for a delayed execution. To enable FlexRS, please refer to Dataflow official documentation.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/218/quizzes/520340

### IAM, Quotas, and Permissions

#### IAM

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520341

Omar: Hello, there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we talk about the different IAM roles, quotas, and permissions required to run Dataflow, Google's batch and streaming analytic service based on Apache Beam. In this video, we learn how IAM provides access to the different Dataflow resources. You have your Beam code, and now you want to run it on Dataflow. Let us look at what happens when your Beam code is submitted. When the pipeline is submitted, it is sent to two places. The SDK uploads your code to Cloud storage and sends it to the Dataflow service. The Dataflow service does a few things. It validates and optimizes the pipeline, it creates the Compute Engine virtual machines in your projects to run your code, it deploys the code to the VMs, and it starts to gather monitoring information for display. When all that is done, the VMs will start running your code. At each of the stages we mentioned-- user submission of code, Dataflow validating the pipeline, and the VM running-- IAM plays a role in determining whether to continue the process. We will briefly explain how IAM comes into play at each stage. Three credentials determine whether a Dataflow job can be launched. The first credential that is checked is the user role. When you submit a code, whether you are allowed to submit it is determined by the IAM role set to your account. On Google Cloud, your account is represented by your email address. For example, when I submit a Dataflow job, it is done via Omar@mysuccessfulcompany.com. Three user roles can be assigned to each user or group. Each role is made up of a set of permissions that determine how much access each user or group has to the different Dataflow resources. The first role you can assign to a user or group is the Dataflow viewer role. If you want a user or group to be able to only view Dataflow jobs, assign them the Dataflow viewer role. This role prevents submitting, updating, and cancelling jobs. It allows users who have the role to only view Dataflow jobs either in the UI or by using the command line interface. The next role you can assign to a user or group is the Dataflow developer role. This role is ideal for a person who is responsible for managing pipelines that are running. For a job to run on Dataflow, the user must be able to submit the job to Dataflow, stage files to cloud storage, and view the available Compute Engine quota. If a user only has the Dataflow developer role, they can view and cancel jobs that are currently running, but they cannot create jobs because the role does not have permissions to stage the files and view the Compute Engine quota. You can use the Dataflow developer role as a building block to compose custom roles. For example, if you also want to be able to create pipelines, you can create a role that has the permissions from the Dataflow developer role plus the permissions required to stage files to a bucket and to view the Compute Engine quota. The last role you can assign to a user or group is the Dataflow admin role. Use this role to provide a user or group with the minimum set of permissions that allow both creating and managing Dataflow jobs. The Dataflow admin role allows a user or group to interact with Dataflow and stage files in an existing Cloud storage bucket and view the Compute Engine quota. The second credential Dataflow uses is the Dataflow service account. Dataflow uses the Dataflow service account to interact between your project and Dataflow. For example, to check project quota, to create worker instances on your behalf, and to manage the job during job execution. When you run your pipeline on Dataflow, it uses the service account service- @dataflow-service-producer-prod . iam.gserviceaccount.com. This account is automatically created when the Dataflow API is enabled. It is assigned the Dataflow service agent role and has the necessary permissions to run a Dataflow job in your project. In our job overview diagram, the Dataflow service account is responsible for the interaction happening here between your project and Dataflow. The last credential used to run Dataflow jobs is the controller service account. The controller service account is assigned to the Compute Engine VMs to run your Dataflow pipeline. By default, workers use your project's Compute Engine default service account as the controller service account. This service account, <project-number>-compute @developer.gservices.com, is automatically created when you enable the Compute Engine API for your project from the API's page in the Google Cloud console. The Compute Engine default service account has broad access to your project's resources, which makes it easy to get started with Dataflow. However, for production workloads, we recommend that you create a new service account with only the roles and permissions that you need. At a minimum, your service account must have the Dataflow worker role and can be used by adding the service account email flag when launching a Dataflow pipeline. When using your own service account, you might also need to add additional roles to access different Google Cloud resources. For example, if your job reads from BigQuery, your service account must also have a role like the BigQuery Data Viewer role. Let us review. In our job overview diagram, where would the controller service account be? Here.

#### Quotas

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520342

In the second section of this module, we look at the quotas to consider when running Dataflow. Let’s get started! One of the quotas that Dataflow consumes is CPU. CPU quota is the total number of virtual CPUs across all of your VM instances in a region or zone. Any Google Cloud product that creates a Compute Engine VM, such as Dataproc, GKE, or AI Notebooks, consumes this quota. CPU quota can be viewed in the UI on the IAM Quota page. For example, right now, I am consuming 219 CPUs in the northmaerica-northeast1 region. Say you want to start a Dataflow job with 100 workers. If the VM size selected is n1-standard-1, meaning 1 CPU core per VM, the CPU usage will be 100. If the VM size selected is n1-standard-8, that would mean 800 CPUs are needed. If the limit is 600, the job will display an error because the CPU limit has been exceeded. Another quota to consider is the number of in-use IP addresses in each region. The in-use IP address quota limits the number of VMs that can be launched with an external IP address for each region in your project. Like the CPU quota, this quota is shared across all Google Cloud products that create VMs with an external IP address. When you launch a Dataflow job, the default setting is for the VM to launch with an external IP address. Jobs that access APIs and services outside Google Cloud require internet access. However, if your job does not need to access any external APIs or services, you can launch the Dataflow job using internal IPs only, which saves money and conserves the In-use IP address quota. In our next module, we will show you how to launch VMs with internal IPs only. Unlike the CPU quota, the in-use IP address quota is independent of the machine type; there is no difference between launching 150 n1-standard-1s vs 150 n1-standard-8s. In the slide image here, the In-use IP address limit for a few regions is 575. In the previous slide for CPU quota, the maximum number of CPUs per region was 600. When you launch a Dataflow job, the more restrictive quota takes precedence. Let us look at quotas for persistent disks. You can choose between two different types of Persistent Disks when running Dataflow jobs. You can launch jobs with either legacy Hard Disk Drives or modern Solid State Drives. Each disk type has a limit per region that can be used. For example, in the image shown here, Google Cloud products in my project that use HDDs in northamerica-northeast1 are consuming 23.5 TB of disk space out of the available 102.4TB To specify the disk type, set the worker_disk_type flag to the prefix shown in the image, and end it with either pd-ssd or pd-standard. Use Pd-standard for Hard Disk Drives and pd-ssd for Solid State Drives. In the slide example, we set the disk type to SSD using both Python and Java. When you launch a batch pipeline, the ratio of VMs to PDs is 1:1. For each VM, only one persistent disk is attached. For jobs running shuffle on worker VMs, the default size of each persistent disk is 250 GB. If the Batch job is running using Shuffle Service, the default PD size is 25 GB. Recall that Dataflow Shuffle moves the shuffle operation out of the worker VMs and into the Dataflow service backend, which is why the default persistent disk size attached to the VM is smaller. Note that you can use the disk_size_gb flag to override the default persistent disk size for batch pipelines using either shuffle on VM or Dataflow Shuffle. Streaming pipelines, however, are deployed with a fixed pool of Persistent Disks. Each worker must have at least 1 persistent disk attached to it, while the maximum is 15 persistent disks per worker instance. As with Batch jobs, Streaming jobs can be run either on the worker VMs or on the Dataflow backend. When you run a job using the Dataflow backend, the feature that is used is Dataflow's Streaming Engine. Streaming Engine moves pipeline execution out of the worker VMs and into the Dataflow service backend. For jobs launched to execute in the worker VMs, the default persistent disk size is 400 GB. Jobs launched using Streaming Engine have a persistent disk size of 30 GB. Just like with Batch pipelines, these default persistent disk limits can be overridden using the disk_size_gb flag. It is important to note that the amount of disk allocated in a streaming pipeline is equal to the max_num_workers flag. For example, if you launch a job with 3 workers initially and set the maximum number of workers to 25, 25 disks will count against your quota, not 3. To set the maximum number of workers that a pipeline can use, use the --max_num_workers flag. This cannot be above 1000. When you launch a streaming job that does not use Streaming Engine, the flag --max_num_workers is required. For streaming jobs that do use Streaming Engine, the --max_num_workers flag is optional. The default is 100.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/218/quizzes/520343

### Security

#### Data Locality

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520344

Omar: Hey there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we talk about the different ways you can enhance security while running Dataflow. We discuss four security features in this module. Let us get started with data locality. Data locality ensures that all data and metadata stay in one region. When you launch a Dataflow job, a backend exists in a Google-managed project that deploys and controls your pipeline. As we discussed in the IAM module, the Dataflow service account communicates between your project and the Dataflow backend. The Dataflow backend exists in a few regions across the globe and can be different from the region in which your workers run. What metadata is transferred between your project and the regional endpoint? There are regular health checks from the workers, workers requesting a work item and the regional endpoint responding with a work item, the worker item status, and autoscaling events. Unexpected events are also transferred to the regional endpoint. For example, unhandled exceptions in user code, jobs that fail to launch because of permissions, worker item failures, and errors from another related system, such as Compute Engine. These items are stored at the regional endpoint and are visible to you on the Dataflow UI, along with any other info you see in the UI, such as pipeline parameter values, job name, job ID, and start time. There are a couple of reasons for specifying a regional endpoint. The first is to support your project's security and compliance needs. For example, if you work for a bank in certain countries, regulatory rules mandate that data does not leave the country of operation. These rules can be met by specifying the regional endpoint. You can also specify a regional endpoint to minimize network latency and network transport costs. If your pipeline sources, syncs, and staging locations are all in the same region, you will not be charged for network egress because all the info remains in the same region. If you have a pipeline with workers in northamerica-northeast and its regional endpoint is set to us-central1, your network egress charge will increase because of the metadata that is transferred between your project and the regional endpoint. In the next couple of slides, we will show you how to specify the regional endpoint you want the Dataflow service to run in. If you want to use a supported regional endpoint and have no zone preference within the region, specify the regional flag only. In this case, the regional endpoint automatically selects the best zone based on available capacity. In Apache Beam 2.15 and greater, specifying this flag is mandatory. If you need worker processing to occur in a specific zone of a region that has a regional endpoint, specify both region and worker zone flags. Use the region flag to specify the regional endpoint, and use the worker zone flag to specify the specific zone within that region. If you need worker processing to occur in a specific region that does not have a regional endpoint, specify both region and worker region flags. Use the region flag to specify the supported regional endpoint that is closest to the region where the worker processing must occur. Use worker region flag to specify a region where worker processing must occur. Compared to the scenarios on the two previous slides, specifying a different region for the regional endpoint and the worker has the protentional to create greater latency. It is important to note that even if no regional endpoint exists in a region you want your data to be kept in, only metadata is transferred. Your application data stays in that region.

#### Shared VPC

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520345

person: Another security feature you can use with Dataflow is shared VPC. Dataflow can run in networks that are either in the same project or in a separate project which we call the host project. When a network exists in a host project, we call the networking setup shared VPC. Shared VPC lets organization admins delegate administrative responsibilities, such as creating and managing instances, to others while maintaining centralized control over network resources like subnets, routes, and firewalls. When set to run in a shared VPC, Dataflow works in either a default or a custom network. The default network is the one automatically set by Google Cloud when you create a project. A custom network is one where you create the network and define the regions and the subnets in the network. When setting the number of workers to use, remember to have enough IP addresses available. For example, if you have a subnet with a /29 subnet and no other VMs running in it, the maximum number of Dataflow workers that you can launch is four. Finally, the Dataflow service account needs the Compute Network user role in the host project on either a project level or a subnet level. We show the difference between using the network and subnetwork flags here. In the Python example, the Dataflow service account has a compute network role set at the project level, and the user wants to deploy to the default network. In the Java example, the Dataflow service account's permissions are defined at the subnet level, and the user is launching the job in a custom subnet.

#### Private IPs

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520346

person: Another security feature you can use with Dataflow is disabling external IP usage. This blocks the workers from accessing the internet, thus securing your data processing infrastructure. By not using public IP addresses for your Dataflow workers, you also lower the number of public IP addresses you consume against your in-use IP address quota. With public IPs turned off, you can still perform administrative and monitoring tasks on Dataflow. By default, the Dataflow service assigns workers both public and private IP addresses. When you turn off public IP addresses, the Dataflow pipeline can access resources only in the following places: another instance in the same VPC network, a shared VPC network, or a network with VPC network peering enabled. If your pipeline is communicating with other Google services and APIs and is in a custom network, Private Google Access must be enabled for the subnetwork your worker will be launched in. If you disable Private Google Access and have no other way of reaching the internet, such as Cloud NAT, the VM instances can no longer reach Google Cloud APIs and services. To use private IPs only, two flags need to be added. The first flag to specify is either the network or subnetwork the workers should run in. The second flag, no_use_public_ips, lets Dataflow know that you want to launch the workers with internal IP addresses only.

#### CMEK

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520347

person: The last security feature we look at is CMEK. CMEK stands for customer managed encryption key. During a Dataflow job's lifecycle, different storage locations are used to store data. When a Dataflow job is created, a cloud storage bucket is used to store binary files containing pipeline code. A cloud storage bucket is also used to temporarily store export or import data. While the job is running, persistent disks attached to Dataflow workers are used for persistent disk-based shuffle and streaming state storage. If a batch job is using Dataflow Shuffle, the backend stores the batch pipeline state during execution. If a job is using Dataflow Streaming Engine, the backend stores the streaming pipeline state during execution. By default, when data is stored in any of these locations, a Google-managed key is used to encrypt the data. CMEK allows you to encrypt data at rest using one of your symmetric keys stored in Google Cloud key management system. This means that you can use CMEK in any of the data storage locations mentioned. When your pipeline starts and the data is loaded into the worker memory, data keys used in key-based operations, such as windowing, grouping, and joining, will be decrypted using your CMEK keys. For an additional layer of security, you can hash or transform the key. Job metadata is encrypted with Google encryption. Job metadata includes the following: user-supplied data, such as job names, job parameter values, and pipeline graphs, and system generated data, such as job IDs and IP addresses of workers. Using CMEK requires both the Dataflow service account and the Controller Agent service account to have the cloud KMS CryptoKey Encrypter/Decrypter role. To use CMEK, two flags need to be specified. First, specify the cloud storage path for Dataflow to stage temporary files created during the execution of the pipeline using the temp location flag. Second, specify the location of the key in Google's key management service using the Dataflow KMS key flag. When you launch a job that uses CMEK, the region for your key and the regional input for your Dataflow job must be the same. Global or multiregional keys will not work. The bucket selected to temporarily store data must also be in the same region as the key. If you override the pipeline's worker region or zone to a different region than the region associated with your keys, regional keys will not work.

#### Setup IAM and Networking for your Dataflow Jobs

- https://www.cloudskillsboost.google/paths/16/course_templates/218/labs/520348

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/218/quizzes/520349

### Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/218/video/520350

Mehran: Hi, it's Mehran again. Congratulations, you've made it to the end of the foundation's course of serverless data processing. What data flow? Let's recap what you've learned. In this course, we started with the refresher of what Apache Beam is and its relationship with dataflow. Apache Beam is an open source programing model with the unified approach for batch and streaming pipelines so that you don't have to manage multiple data processing architectures, no more land architectures. Data flow is a fully managed, distributed data processing engine for Apache beam pipelines integrated within the Google cloud. Ecosystem data will automates the provisioning and orchestration of worker machines and uses techniques like horizontal auto scaling in dynamic work rebalancing to achieve the lowest total cost of ownership. Next, we talked about the Apache beam vision and the benefits of the beam portability framework. Thanks to the interoperability layer introduced by the Portability API, digital pipelines can leverage new features such as custom containers and multi-language pipelines. The Beam Portability Framework achieves the vision that a developer can use their favorite programing language with the preferred execution backend. Dataflow is runner review to implementation offers this portable architecture on dataflow, it can be enabled via pipeline option without rewriting a single line of code. Another aspect that we looked at is how do you feel allows you to separate, compute and storage while saving money? The shuttle service helps Batz pipelined scale seamlessly to hundreds of terabytes without any testing required by offloading the shuttle operation from worker VMS onto the dataflow service back end. Because operations are carried out on a service back end, pipelines consume less CPU memory and persistent storage. The same principle applies for streaming pipelines with streaming engine, which offloads Windows state storage from the persistent disk attached to workers onto a back end service, this significantly improves auto scaling and data latency and also reduces the resource footprint for your streaming pipeline. We finish this module with flexible resource scheduling or flex hours that can help you save in costs for your pipelines by using a combination of preemptive VMS and our shuffle back end. And the best thing about all these features is that you can enable them for your pipeline to that rewriting a line of code, all you need to do is pass a new parameter when you deploy the pipeline. We reviewed how identity access and management tools interact with your data flow pipelines. We learn about different predefined rules for data flow users and the different service accounts used to run data for pipelines. Then we looked at which quotas apply to data flow jobs. Specifically, VCP use IPS and persistent disks. Every capacity planning exercise it involves data flow. Workloads should involve estimating consumption needs for these resources. Lastly, we looked at how to implement the right security model for your use case on dataflow. We learn how to comply with the locality requirements by specifying the region in zone parameters in your job, and we learn about how to run dataflow jobs with various VPC configurations. We learn how to prevent data exfiltration by disabling public IPS on your dataflow workers. We ended our discussion on security by covering data encryption on data flow. By default, all data is encrypted with Google managed keys, but data integration with cloud key management service allows you to bring your own encryption keys to ensure the maximum level of security. I hope you enjoy this course and you'll be able to put into practice what you've learned.

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/218/documents/520351

### Your Next Steps

## 08: Serverless Data Processing with Dataflow: Develop Pipelines

- https://www.cloudskillsboost.google/paths/16/course_templates/229

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577115

Hi, and welcome to the second installment of the Serverless Data Processing with Dataflow series, Developing Pipelines on Dataflow. My name is Mehran Nazir, and I am a product manager with Dataflow. If you’ve been following the data engineering progression thus far, you’ve learned about different Google Cloud services you can use for your data processing needs. You might have chosen to go deeper on Dataflow, Google Cloud’s unified batch and stream processing engine that’s serverless, fast, and cost-effective. If you’ve taken the Dataflow Foundations course, the first part of this course series, you likely understand Dataflow’s IAM, quotas and security model, and also have a conceptual grasp of the Beam Portability Framework and how Dataflow separates compute and storage with Shuffle and Streaming Engine. If you remember, there are three ways to launch a Dataflow pipeline: Launching a template using the Create Job Wizard in Cloud Console. You don’t have to write code with this option—all you have to do is select your desired template from a drop-down menu, fill out a few fields, and your job can be deployed. We covered this workflow briefly in the Building Batch Pipelines course in the data engineering curriculum. 2. Authoring a pipeline using the Apache Beam SDK and launching from your development environment. This can mean writing a pipeline using the Java SDK in an interactive development environment (IDE) like IntelliJ, or using a read-eval-print-loop workflow with the Python SDK using a Jupyter notebook. We introduced the building blocks of the Apache Beam SDK in the data engineering course. 3. Writing a SQL statement and launching it in the Dataflow SQL UI. Dataflow SQL lets you launch Dataflow jobs using the familiar semantics of SQL, and includes streaming extensions that allow you to express logic for handling data in real time. In this second installment of the Dataflow course series, we are going to be diving deeper on number 2 (developing pipelines using the Beam SDK) and will dedicate one module to number 3 (Dataflow SQL). Developing your pipelines using the SDK allows you to tap into the full suite of possibilities afforded by the Beam model, and is often the choice of our most advanced users. Let’s take a look at what we’ll be covering in the Developing Pipelines with Dataflow course. We will first spend some time refreshing the concepts covered in earlier courses. More specifically, we will be reviewing the building blocks of the Beam programming model. We will then review watermarks and triggers, introduced in our Building Resilient Streaming Analytics Systems course and expanded upon in this course. Next, we will review sources and sinks, which represent the “Extract” and the “Load” of your Extract-Transform-Load (or ETL) pattern. From there, we will introduce schemas, which give developers a way to express structured data in their Beam pipelines. In the next module, we will cover state and timers. These powerful primitives unlock new use cases by giving developers fine-grained control over in-flight data. After we have laid the foundations of the Beam SDK, we will discuss best practices and review common patterns that maximize performance for your Dataflow pipelines. We will dive into two domain-specific languages, SQL and DataFrames. We’ll explore how SQL is implemented with Beam and Dataflow, then examine Beam DataFrames, an API that gives developers a similar interface to the popular pandas open-source project. Our last module will cover Beam notebooks, an interface for Python developers to onboard onto the Beam SDK and develop their pipelines iteratively in a Jupyter notebook environment. We’ll wrap up the course with a summary of all of the concepts covered.

### Beam Concepts Review

#### Beam Basics

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577116

Israel: Hello, my name is Israel Herraiz, and I work as a strategic cloud engineer at Google. In this video, you will learn the main concept of Apache Beam and how to apply them to write your own data processing pipelines. Let's start with the main concerns of Apache Beam. The genius of Beam is that it provides instructions that unify traditional batch programing concepts and stream processing concepts. Unifying batch programming and [indistinct] processing is a big innovation in data engineering. The four main concepts are, Beam transforms P collections, pipelines, and pipeline runners. ♠A pipeline identifies the data to be processed and the actions to be taken on the data. The data is held on a distributed data instruction called a P collection. A P collection is immutable. Any change that happens in a pipeline receives one P collection as input and creates a new P collection as output. It doesn't change the incoming P collection. The actions are contained in an instruction called a P transform. A P transform handles input, transformation and output of the data. The data in a P collection is passed along the graph from one P transform to another. Pipeline runners are analagous to container hosts, such as Kubernetes Engine. The integral pipeline can be run on a local computer, in a virtual machine, in a data center or in a service in the cloud, such as Dataflow. The only differences are scale and access to platform specific services. For instance, Google Cloud Storage. Imutable data is one of the key differences between batch programing and testing processing. The assumption in the von Neumann architecture was that data would be operated on and change in place. This was very memory efficient, and this made sense when memory was expensive and scarce. So making a copy of data was expensive. Nowadays, in distributed systems, imitable data where each form results in a new copy means that there is no need to coordinate access, control or sharing of the original ingested data. So it enables, or at least it simplifies distributed processing.

#### Utility Transforms

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577117

Apache Beam comes with a set of transforms that you can use as the building blocks of your pipeline. Let's learn about those transforms. By combining these blocks, you can build a complex process in logic that is applied at scale by Dataflow. ParDo lets you apply a function to each one of the elements of a P collection. GroupByKey and Combine are similar. With GroupByKey, you put all the elements with the same key together in the same worker. If your group is very large or the data is very skewed, you have a so-called hotkey and you're going to apply a commutative and associative operation, you can use Combine instead. Combine will make the transformation in a hierarchy of several steps. For large groups, this will have much better performance than GroupByKey. GroupByKey let you join two P collections by a common key. You can create a left or right, outer join, inner join and so on using GroupByKey. Flatten also receives two or more input P collections and fuses them together. But please do not confuse flattened with joins or with GroupByKey. If two P collections contain exactly the same type, they can be fused together in just one P collection using the Flatten transform. However, with joins with GroupByKey, you have two P collections, but typically with different value types that share a common key. Partition is in a way the opposite of Flatten. It divides your P collection into several output P collections by applying a function that assigns a group to ID each element in the input P collection.

#### DoFn Lifecycle

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577118

person: One of the main features of Butterbean is the richness of possibilities that you can implement in a Pardoo and they do function. Pardoo seems like a simple map or a filter, but it is actually a very powerful and versatile transform. We will not always need all this power and versatility. Bin offers and convenience versions of Pardieu! Transforms for these situations. If you need to feel better or just map or flat map the elements of a collection or add keys or extract keys or values, you can use this higher level, more convenient transforms. Just don't forget that the functions offered you very powerful possibilities. Let's see those more detail elements in a collection are processing bundles. The division of the collection in the bundle is arbitrary and selected by the runner. This allows the runner to choose an appropriate middle ground between persistent results after every element and having to retract everything. If there is a failure, for example, a streamlined runner may prefer to process and commit small bundles, and the match runner may prefer the process. Larger bundles when processing and values. A single bundle may contain several different keys, and the function has several methods that can be overdriven to control how your code interacts with each day to bundle the main method, this process where each one of the elements is transformed. But there are other methods, other call at different moments during the life cycle of the function. These methods enable you to control how the data bundles are processed in the function in combination with side inputs and outputs. This opens a myriad of possibilities for writing your functions. Let's see how these methods work. When a worker starts, it creates an instance of the function right after creating that Eastnor instance, it calls the setup method. This method is called once per worker. This is a good place to start. Objects such as data connections, network connections or any other kind of helper process that will be used with all the data Rundle's. Every time the function receives a new data bundle, the runner calls the start bundle method of the function. This is a good place to start tracking your data bundle if you need to. For instance, for instance, Marable's or matrix purposes. After I start the bundle for every element, the runner will call the process method of the function. This is where the transition takes place. For that transform the process method may redistribute or receive side inputs from the process method. You can also update the state and this will be shown in the state and damaged sections later in this course. If you define Demerse, this may be called more than once per bundle, depending on the value of the timer. They stayed on timer sections, covered this in more detail to once they do function, transforms the last element of the bundle. The runner calls the method finish one, though this method is a good place to do match calls for. For instance, if you're advocating an external system, if the function is, I think finally when all the data bundles are processed and the worker is not needed anymore, the runner calls the teardown method. If you started any connection in your setup method, this is the method where you should close those connections. Beware when we did in a standard estate, in your function as a generic rule, always mutator state using state variables rather than class members, the runner may recycle you the function or process. The same bundle in different workers for redundancy do not moutet external state from your process method ensure that any state variable is clear in the bundle method. Otherwise they could contain state for the previous bundle. And remember, a bundle may contain several keys. So Estoril State maps based on that key.

#### Serverless Data Processing with Dataflow - Writing an ETL pipeline using Apache Beam and Dataflow (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577119

#### Serverless Data Processing with Dataflow - Writing an ETL Pipeline using Apache Beam and Dataflow (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577120

#### Quiz 1 - Beam Concepts Review

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577121

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577122

### Windows, Watermarks, and Triggers

#### Windows

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577123

person: Hi, it's me again. Israeli strategic engineer at Google, in this video, you would learn about how to process data in streaming with data flow. For that, there are three main concepts that you need to learn how to group data and windows, the importance of watermarks to know when the window is ready to produce results and how you can control when and how many times the window will emit output. Let's start talking about the windows. It is likely that your first experience with data processing pipelines is processing data, much, much pipelines are often run on schedule. For instance, why are they so? They produce fresh results with frequency running batch pipelines with a certain frequency is also a way to Chank data. So when we have large amounts of data, we can divide the processing and handle all the data by doing batches in situations like this. What you probably need a extremely pipeline. It is likely that your data is not discretionary. Despite being processing matches, the batches are artificially split to simplify the processing of data. If your data is not distortionary, how party will lets you handle it as a stream of continuous data. However, dealing with a string is not only a matter of continuity and making a split to process data. There are other inherent problems to processing data. One of the main problems you have to deal with when processing processing data is the lack of order. Imagine a situation where you are processing events coming from the mobile application. One of your routers here, Shaunessy Green Square. I started using the application athon at 8:00 in the morning. You receive some messages in your pipeline, but then another user shown here at a yellow hexagon did the same. But this user was driving the subway in a tunnel with no phone coverage when the user returned to the surface and they phone let you get the message with Sandile. But the wait may be worse. Yet another user, the second blue here, was using your fantastic application while flagging on a very long transcontinental flight using their mobile phone in airplane mode. This user enabled the phone signal when they arrive at the destination and suddenly you start getting more messages that were produced at 8:00 in the morning, but that you are only seen hours later. How can you deal with out of the data and how can you make a split to process data? The answer to both is windows. But these windows are not just simple groups or batches of data. Let's see where. So a window is just a way to divide it in groups in order to do it, and that's what happens with the data when the wind divides data into time based, finite chunks. Windows are required when doing aggregations of about unbounded data is being primitives such as a group bickie or combinat. However, you can also do aggregations within a state and time without having to use a window instrument pipelines. There are two dimensions of time processing time and event time in processing time. Data flow assigns the current timestamp to every new message in event time. We use instead the tiny stamp of the messages, as it was said in the original source when the message was produced. If you get messages by processing time, this is the same as micro matching messages that were produced around the same time. If they arrive out of order will be assigned to different batches. Processing done is fine, depending on the kind of calculations you want to perform. But when time enables you to apply a more complex aggregation logic to the data in Aventine, messages are grouped together depending on the systems generated at the source, not depending on the moment of their arrival. For instance, one message may be late and arrive very closely to another on time message. These two messages belong to different Windows Barrat arriving at approximately the same time, Dataflow reads the messages. Direness Times determines that one of the messages was actually late and assigns it back to the proper window, assuming the windows is still open or waiting for later. By doing this, we can record the order and groups of data as they were producing the source, even if they arrive out of order to flow. This is a very powerful feature of a streaming pipeline. And here in lies the possibilities of doing complex and sophisticated calculations in streaming pipelines, even in the case of out of order delivery. Butterbean includes three different types of windows that are available by default, fix is Liveing and sessions. We can also create custom window types. Fix windows are those that are divided into tiny slices. For example, hourly, daily, monthly, fixed time windows consists of consistent, non overlapping, overlapping intervals, sliding down windows, also representing intervals in the data stream. However, sliding down windows may overlap. For example, each window we make up captured 60 seconds worth of data, but a new window will start every 30 seconds. The frequency with which a sliding windows begin is called a period. A typical application of a sliding windows will be to calculate a moving average session based windows capture bars of user activity. Session windows are defined by a minimum gap duration, and the timing is triggered by another element. Such and windows are data dependent windows that are not known ahead of time. You need to look at the data to figure that out. Examples are intercessions you should never to and website, etc..

#### Watermarks

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577124

person: With windows, you decide where you put the message, but you now need to make another additional decision. When is the window going to meet the results? At the first glance, you may decide just to meet the results when the window closes. This is a very intuitive is you have a fixed window, but in other situations, like a session windows, it might not be so obvious. In addition to this, you will also receive late data. So you need to decide how to trigger output in the case of laded. But how would they define if they are windowing by Aventine, your messages will be within the boundaries of the window. How do you decide if a message is laid or that you have waited long enough for LAYTH data? This is where the concept of a watermark becomes useful. Let's focus on how windows work when there is no latency and no later data, in an ideal world, if there were no latency and everything was instantaneous, then these fixed windows would just flash at the close of the window at the very microsecond that the time it to begins, a one minute window terminates and flashes all the data. But this is only if there is no latency. But in the real world, in assuming pipelines, the order of our data will be altor. Even if you receive data in perfect order when it is processed in the pipeline, in a distributed system, different messages will take different processing times and that order will be lost. How can you decide that the window can be closed if the data is out of order? How can you be sure that no further and order messages will be received in estimated pipelines that are two dimensions of time? The relationship between the two defines what it is called the watermark. The watermark is the relationship between the processing timestamp and the event. DINNERSTEIN The processing timestamp is the moment the message arrives at the pipeline. Ideally, the both should be the same with no delays. However, this rarely happens. There are always delays, latencies and so on. Any message that arrives before the watermark is set to be Everleigh. This happens too, if it arrives right after the watermark is said to be on time and if it arrives later, then it is late to date. So the watermark is what defines whether a message to circulate the watermark can be calculated because it depends on messages we have not yet seen. So data flow estimates the watermark as the oldest is timestamp waiting to be processed. This estimation is continuously updated with every new message that is received. Now, why do you need to keep two dimensions of paint and a watermark definition? Let's see how watermarks help decide when a window is complete and you can proceed with your calculations. In a real war setting, data will always arrive with Stalactite, this lag time is the difference between when the data was suspected and when the data is actually arriving these days. On Friday, the expectation is what we call the watermark. They will keep track of the lack of every message and will try to predict the value of the watermark that they lack in the future. When the Dennis stamp of the last message is after or add the value of the watermark, then it means that the window can be considered complete. Any message received after this moment will be considered late. In this example, data one is late because it is arriving much later than when it was expected that it's arriving much later than the watermark. So data is only late when it compared to the watermark. It doesn't make sense to talk about later data unless we have a watermark. Data flow will wait until the watermark is trespass to close the window while it actually waits for some additional time as a form of buffer. But after that, the windows flash and the result is omitted. Any message coming after this moment will be considered late. You will need to make a decision about what to do with the data. The default behavior is to drop late date, but as you will see in the trigger section, you can choose to wait for data and limit results again if there are any later messages. When you run a stream pipeline in the floor, the jump into the flow contains some details about the watermark values. The data freshness metric is actually related to the watermark of your input data. When you are processing fresh data, the data value decreases when the wind is close and those messages will are considered now fully. Process data has not waited until it has been processed by the law. In these situations, the watermark will be close to real time. They the freshness is the difference between real time and the stamp of the oldest message waiting to be processed, the watermark is actually a tiny stamp of the message that has not been processed yet. So they the freshness is a measurement of how far the oldest messages is far from the current moment when you see a monotonically increase in value. It means that data has the weight of the input for more time waiting to start to be processed. There could be two reasons for the additional weight. It could be because the pipeline is busy processing messages, or it could be because the input has increased very quickly and data is accumulating at the input. Or it could be because of how can we distinguish between both situation for that system? Latency is a useful metric system. Latency measures the time it takes to fully process a message. This includes any weighting down in the input source. If for some reason the pipeline needs more time to process a message, then system latency will increase. For instance, because the pipeline is missing, processing a complex message. When seasonless latency keeps increasing and data freshness keeps increasing to it means that the pipeline cannot process more messages until it does not finish processing the current messages. You are not necessarily receiving a lot of more messages. The pipeline is just taking longer to process the current messages. But if Sistan latency remains constant or reduces and does not monotonically increase and data freshness value is monotonically increasing, that could be because there are many more messages at the input. For instance, we have received like a peek at the input the pipeline gives processing data at the same pace. So latency doesn't increase system latency. But unless data flow adds more workers to the pipeline, it will not be able to catch up with the input peak and the freshness increases. If you are running without the scaling in this situation, data flow will spin up more workers to process this additional data for the. So although we don't get actual watermark values from the floor with the freshness and Lattanzi metrics, we can written about the situation of our pipeline and diagnose if we are getting more input or if the pipeline is busy doing more calculations, dataflow itself uses these metrics to decide when to upscale or downscale to our data the amount of resource use to the actual demand of data processing. The ideal situation for a streaming pipeline is to have both a stable data freshness and then latency values if they have monotonically increases and latency doesn't increase that evidence that you're receiving more input data data flow will spin up new workers because of the size of the backlog. To be processed is increasing. If latency increases and data freshness is stable, then messages are taking more time in the pipeline to be processed. CPU usage will probably increase, so data flow will spin up new workers. But you may also see other situations that increase latency, but no super cheap usage. For instance, if you are accessing an external service or API and the service may be overloaded and taking a long time to respond to ship usage will not be high. But the latency will increase in that situation. Outas killing would not create more workers. They would be useless anyways to accelerate the pipeline in that situation. And if both metrics monotonically increase, then your backlog is increasing and your pipeline is also taking more time to process A16 messages. All those should create more workers to adapt to the increasing demand.

#### Triggers

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577125

>> You have seen how the water mark is useful to have an idea of data completeness and how to use metrics to see how the water mark is evolving, the default behavior is to trigger the results when the water mark is sparse. But that can be often a very long time. Do you have to wait that long before you can see the results coming out of your windows? No, you don't. Three years are useful to define in precise detail when we want to see the results of our window. Let's see how things work. By using three years, you can control the Lattanzi to produce a result or you can ensure the data completeness before you emit a result or a combination of both triggers can be based on Aventine, for instance, immediate results after 30 seconds as measured by the messages, timestamps or on processing time. For instance, Emet results every 30 seconds as measured by the workers clock, regardless of their messages thunderstorms. And they can also be based on data, for instance, and meet some results after seeing 25 messages. Or you can use any combination of the above three years with a composite trigger, we can employ and implement a very complex logic for deciding when and how many times to trigger the results of our windows. The default behavior is to trigger the watermark. So if you don't specify a trigger, you are actually using the trigger after watermark after watermark is an event time trigger. We could also apply any other triggers event time. The message is that are used to measure time with these triggers, but we could also add custom triggers if the trigger is based on processing time. The actual clock real time is used to decide when to meet the results. For instance, you could decide to meet exactly every 30 seconds, regardless of the timestamps of the messages that have arrived. The window after count is an example of a data driven trigger rather than immediate results based on time here with trigger based on the amount of data that has arrived within the window. The combination of several types of triggers, openside worth of possibilities with a streaming pipeline, we may need some results using after processing time and then again at the watermark when data's complete and then for the next five messages that arrive late after the watermark. In summary, you can integrate to make sure that Doumitt results early, which is to say with minimal latency, or you can use them to make sure that you process data and that your results include all the relevant messages, even if those messages are delayed. Or you can combine the two conditions. For instance, make sure that the immediate results early and late repeat the calculation and new results when data is complete. But when you hand me the results several times, how will that aperture have been rapid? The calculation, you can actually control that. Let's talk about accumulation modes. When you trigger the the window several times, you have to decide on the desired accumulation mode. There are two accumulation modes in a batch of accumulate and this car with accumulate every time you trigger it again in the same window. The calculation is just repeated with all the messages that have been included in the window so far with this car. Once some messages have been used for a calculation. Those messages are discarded. If new messages arrive later and there is a new trigger, the result will only include the new messages and those messages will be discarded again. Sure, there will be any additional triggers later. Let's see how these models work with an example. This example is using fixed windows of 10 minutes, but you don't want to wait so long until you see results. So the trigger is set to Aventine every couple of minutes in the first trigger, the window has only seen two messages and we immediately at the containing just two messages by the ten. Then by the time the next trigger fires up, the window has received four more messages. Now the trigger amidst the list, again containing the previous messages and the new messages. The third trigger, again, includes all the previous messages and the new messages. If your window is very wide, using accumulate as the accumulation mode may consume considerable resources as the accumulated output has to be stored when the window is still open, the windows and the message here are the same as in the previous slide. But now we have said the accumulation mode to discard every new trigger will only use the new messages that the window has received so far. And once the result is emitted, it will discard those messages. If the calculation you need to make with the windows is associative and commutative, you can safely update that calculation. Using this commode without any loss of accuracy in the output storage where you store the partial results, you should be able to aggregate the partial results to get the actual calculation value. The main advantage of using the discard mode is that the performance will not suffer even if you use a very wide window, because no state, no accumulation is stored for very long, only until the next trigger is released. Let's see how the specified triggers in a butterbean, these examples are in Python, in the example at the top, the pipeline triggers more than once per window. There will be a trigger 30 seconds after opening the window and then again once the watermark is reached after that, for every late message within the first two days, there will be an additional three or so. Every window will produce more than one output. Bear that in mind when designing Utøya index sample at the bottom, the window is not trigger the watermark, but whenever the window sees a hundred messages or 60 seconds have passed, whichever happens first. The trigger only computes that dealt us. Once it really is calculated, the previous messages are discarded because the window allows for two days to wait for little data. The trigger will produce output if we have left messages within two days after the watermark. So even if we are not triggering the watermark, the watermark is still important. You may have heard, by the way, that the Python SDK for Baturin doesn't support setting a value for allowed liveness. That was the case sometime ago, but they're now allowed. The place is fully supported in Python, in dataflow and other runit. These are the same examples, but in Java here, you need to specify the type of the collection we are assuming this extreme battery could be actually any other type, even a custom class. The API is different than in the case of Python, more adapted to the customs of Java. But the concepts are exactly the same as those shown in the producers like you have learned how to process detainee stimming with a Butterbean instrument is not only about the continuity of data, it has other important features as well. The most prominent lack of order when receiving data windows can help with that window by event time lets you recover the natural order of the data. But you can also use processing time, which is more like MacRobertson to omit results in a window. Watermarks are important to know if our data is complete or whether we should still wait for more data. If data arrives after the watermark, that data will be considered late. That's not a big deal with triggers. We can decide how to deal with data. Remember that Couston triggers. Let you decide when to meet results early, when the data is complete and so on, and the window may have several triggers.

#### Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577126

#### Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577127

#### Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577128

#### Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577129

#### Quiz 2 - Windows, Watermarks Triggers

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577130

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577131

### Sources and Sinks

#### Sources & Sinks

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577132

Welcome to the Sources and Sinks for Dataflow module. My name is Wei Hsia and I’m a Customer Engineer for Google Cloud. In this module, you will learn about what makes sources and sinks in Dataflow. The module will go over some examples of TextIO, FileIO, BigQueryIO, PubsubIO, KafKaIO, BigtableIO, AvroIO, and Splittable DoFn. The module will also point out some useful features associated with each I/O. In this video, we talk about sources and sinks. In a data pipeline, there’s generally an input and an output. In Beam, these are called sources and sinks. A source is when you read input data into a Beam pipeline. Sources generally appear at the beginning of a pipeline—but that doesn’t necessarily need to be the case, as you will see later in this module. A sink is where you would write output data from your Beam pipeline. A sink is a PTransform that performs a write to the specified destination. A PTransform is an operation that takes an input and provides an output. A common output for a sink is PDone, which signals that the branch of the pipe is done. A bounded source is a source that reads a finite amount of input. This is commonly associated with batch processing. A bounded source will be responsible for splitting up the work of reading an input into bundles. Bundles are groupings of elements in the pipeline for a unit of work. The bounded source will also provide estimates to the service and number of bytes to be processed. Because the input is finite, there is a known start and a known end. If the bundles can be broken down into smaller chunks, Dataflow will dynamically rebalance work to achieve better performance. An unbounded source is a source that reads from an unbounded amount of input. An unbounded source is commonly associated with streaming. Checkpoints allow for the ability to bookmark where the data has been read in the source, which means that data that has been processed in the stream doesn’t need to be re-read. Watermarks from sources can provide the point in time estimates for a piece of data. Finally, some unbounded sources, for example PubsubIO, have the ability to pass a record ID to allow deduplication of messages. Dataflow will keep track of the message IDs for 10 minutes and automatically discard the record if duplicated. Sinks are often PTranformations that write data to an end system. You can check the code out for the various sinks to see this. In general, sinks will emit a PDone value to signify the completion of the transform. There are some, such as BigtableIO, that also allow you to continue processing data after the success, as you will see later. If you have a need for continued processing, you can also write your own PTransform sink to write out the data and have an output. Apache Beam is open-source, so there are many developments contributed by the community and by Google. There are a lot of various open source I/Os, and the list continues to grow. For an updated list of various I/O connectors, refer to the official Apache Beam documentation page.

#### TextIO & FileIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577133

TextIO and FileIO are used for sources and sinks when you need to work with text or files, respectively. Let's dive into an example in Java for TextIO. This is a simple read using TextIO. There are variant different read methods available for TextIO. Now let's take a look at a couple of Python examples. In the first example, you provide a file name as the first step and pass that into the second step, which uses the file name to read from a file. This is an example where the second step is also a source and not necessarily the first step. For the second example, you're reading from a file by passing in the file name. Use the method that best suits your needs. Here's an example of FileIO in Java using a file match pattern. The match keyword and the argument filepattern will allow you to search for a pattern as seen here. You're able to use the filename and other metadata as part of the read. In the Python example shown here, you're mapping the file to the variable x and using the object to access the contents and the metadata. File pattern matching is useful when you need to grab a range of files. Beam FileIO is also able to continuously monitor a location for a particular pattern. This Java example will monitor the location every 30 seconds for an hour to see if new files match the pattern provided. This pattern is useful for times when you have files flowing in but have a sliding window in which they will arrive. Here's another example, but rather than providing the filenames, you read off a message queue such as Pub/Sub using PubsubIO. Certain systems like Cloud Storage have the native ability to trigger a message to Pub/Sub on metadata changes. The message is then parsed to return the filename for the subsequent step that can now target a file for a read. This method lets you read a stream of files. Contextual I/O provides many mechanisms to enhance the behavior of text reading. Historically, when more complicated TextIO reads are required, you are relegated to using FileIO. Contextual I/O lets you use TextIO for those use cases. For example, you're able to return things such as ordinal position or read multi-line CSV records. This is an example of a sink written in Java using TextIO to write your output to a file. You can write to many different file systems and object stores. Here is another example of a simple write using TextIO, this time in Python. Dynamic destinations allow you to determine the sink destination at run time. You can invoke this with the writeDynamic function in Java. This example allows you to use the transaction type to determine the file name. In this Python example, you take the dynamic destinations a step further by writing to different sinks depending on the characteristics of the data. In this example, the record type determines the destination file type. Dynamic destinations are useful when you are unsure of the specific destination at run time. In these examples, you can expand the range of your destinations without altering the code.

#### BigQueryIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577134

BigQueryIO is a useful connector for BigQuery, a scalable and serverless data warehouse. In this example, you read from BigQuery and use a standard SQL statement to retrieve results from BigQuery. When using standard SQL, Dataflow will submit the query to BigQuery and first retrieve metadata. BigQuery will then export the results to a temporary staging location in Cloud Storage and then Dataflow will read the contents from there, prioritizing throughput. Once the data is read from Cloud Storage, you can then map the results to be used in the data pipeline. The BigQuery Storage API is built to facilitate consumption from distributed processing frameworks, such as Beam. This Storage API allows you to achieve very high throughput when reading from BigQuery. To invoke the Storage API read method, use the DIRECT_READ method. In this example, you are using the column projection feature, withSelectedFields, to reduce the number of columns accessed by BigQuery. There is also a simple predicate filter that you can pass through for row filtering by invoking the withRowRestriction clause. You can write to BigQuery with BigQueryIO. There are multiple ways to specify schema, but using the built-in schema functionality is a quick and efficient way to specify it. You will be exploring schemas in more depth later on in this course. In this Java example, you are utilizing dynamic destinations to route your writes to various tables in BigQuery. Dynamic destinations allow you to write to multiple destinations-- in this case tables-- with varying schemas as well. BigQueryIO can write to BigQuery with streams or batches. Your streaming job can write to BigQuery in batches by windowing your data and using the FILE_LOADS method. By default, your streaming job will default to the streaming write method. Here's a Python example of a BigQuery dynamic destination write. You can define the function to return the destination you would like the data routed to.

#### PubsubIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577135

Pub/Sub is Google’s highly scalable and robust messaging service. Dataflow and Pub/Sub often go hand in hand, and you can connect them using PubsubIO. In this Java example, you are reading from a Pub/Sub topic using PubsubIO. This read method automatically creates a subscription when the Dataflow job is deployed, and is destroyed upon termination of the job. If you would like to have a subscription remain upon termination of the job, create a subscription and use the fromSubscription method. Dataflow PubsubIO automatically acknowledges the messages when the data is durably persisted in Dataflow, in other words when it is materialized in the service. By default, Pub/Sub’s message timestamp is used for windowing features. It is possible to reassign that value to take advantage of other timestamps. One example would be if you had a reported timestamp from the source, versus the published timestamp in the message. You could use the reported timestamp to compute your windows and other calculations. A common scenario in any data pipeline is the ability to capture failures. You want to be able to capture these failures so that you can act upon them. One method is to use a dead letter queue. A dead letter queue is a queue where you divert messages that meet one or more criteria. This could be a failure or a piece of data that you may need a second look at. Here’s an example of reading from a Kafka system and publishing it to Pub/Sub. There are two scenarios and you can see the failed messages are diverted to a second topic.

#### KafkaIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577136

KafkaIO is an unbounded source and is generally used for streaming. You're able to use checkpoints with Kafka to bookmark your read so you can resume where you left off. Kafka topics are where streams of records are stored. You can choose which topics to subscribe to with KafkaIO by submitting the parameter withTopics and a list of topics of interest. Alternatively, you can use withTopic to submit a read for a single topic. KafkaIO is built in Java, but Beam has a concept of cross-language transforms. Python's KafkaIO module uses the cross-language transform to enable KafkaIO on Python. The cross-language transformation enables transforms, like sources and sinks, across multiple software development kits, or SDKs. You can see in this example that the call uses a Python function from a library that implements the external calls.

#### BigtableIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577137

Bigtable is a scalable, NoSQL database service by Google. Both Bigtable and Dataflow are designed for high throughput and scalability. BigtableIO serves as the module that will communicate between Bigtable and Dataflow. You are able to include a row filter when reading from Bigtable. This allows you to filter out rows based on criteria. You can invoke this with the withRowFilter clause and passing the filter criteria. Another important aspect of NoSQL databases is the ability to use the index. withKeyRange will allow you to enable a prefix scan on the index to quickly arrive at the desired prefix. There are times where you will want to continue on a pipeline after a sink rate has completed. BigtableIO has this ability by triggering the Wait.on function in Beam by sending a signal to the Wait function. This enables you to continue with an additional transformation after a write has completed.

#### AvroIO

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577138

Avro is a file format that is self-describing. It's a very popular format for big data. AvroIO allows you to read and write to that file type. Avro files provide the schema and the data so the files can be self-describing. You can use the built-in functions from AvroIO to retrieve the schema into your Beam pipeline. You can also use wild cards to read multiple files as seen in this Python example.

#### Splittable DoFn

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577139

Splittable Do Functions, or DoFns, enhance the sources and generic Do Function capabilities. Sources going forward will be able to utilize splittable Do Functions for both streaming and batch. This brings Beam's unified batch and streaming programming model closer to fruition. Splittable Do Functions are a generalization of a Do Function that gives it core capabilities of a source, splittability, and the ability to report back the metrics that you learned about earlier in this module, such as progress. Progress and other metrics then allow you to know how far along a bundle is and how far it has to go. This enables the ability for the work to be split into multiple bundles. The splittable Do Function also keeps the syntax, flexibility, modularity, and ease of coding from the Do Function syntax. When you read a file, the splittable Do Function allows you to set the restrictions, such as a sequence of blocks, on where the files are read to. Splittable Do Functions allow you to build custom sources with ease. The function is a Do Function with additional parameters, such as RestrictionTracker, as shown in this Java example. You need to define an initial restriction that will create a restriction describing a complete unit of work. This is shown with the function "def initial_restriction" in this pipeline example. One way to accelerate your development of a Dataflow pipeline is to refer to the open source code as a basis for your code. There are many examples in Python and in Java in the links provided. Thanks for listening.

#### Quiz 3 - Sources & Sinks

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577140

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577141

### Schemas

#### Beam schemas

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577142

David: Hi there, my name is David Sabater, and I work as outbound product manager for data analytics at Google Cloud. In this model, I'm going to introduce Beam schemas and also provide some code examples. This model about schemas is part of the DataFlow developing pipelines course. Let us start introducing schemas before we look at some examples. A P collection must consist of elements of the same type. For example, it could consist of many JSON objects or of other available object types like byte stream, also known as PlainText, Avro or protocol buffer. To Beam, these collections of types are blocks that are passed between transforms. However, to support this [indistinct] processing, Beam needs to be able to encode each individual element-- for example, as a byte stream-- so elements can be read and passed around to distributed workers. Common Beam sources can produce JSON, Avro, Proto [indistinct], or database raw objects. All of these types have well-defined structures: structures that can often be determined by examining the type. Even within an SDK pipeline, simple Java protos or [indistinct] equivalent structures in other languages are often used as [indistinct] types. These are also have a clear structure that you can infer by applying a custom coder and inspecting the class. As we have seen in our previous slide, the types of records being processed typically have an obvious structure. By understanding the structure of a pipeline's records, we can provide much more concise APIs for data processing. And actually, database folks have known this since the '70s, using schemas. Schemas to the rescue. Most structured records share some common characteristics which can be represented as schemas. They can be subdivided into separate name fields and values. Fields usually have string names, but sometimes, as in the case of index, [indistinct] have numerical indices instead. There is a finite list of primitive types that a field can have. These often match primitive types in most programing languages: int, long, string, and so on. Often a field type can be marked as optional, sometimes referred to as nullable or requited. Often records have a nested structure. A nested structure occurs when a field itself has two fields. So the type of the field itself has a schema. These structure records have some commonly feature array or map type fields. Now, in order to take advantage of schemas, your P collection must have a schema attached to it. Often the source itself will attach a schema to the P collection. For example, when using Avro IO to read Avro files, the source can automatically infer a Beam schema from the average schema and attach that to the Beam P collection. However, not all sources produce schemas. In addition, Beam pipelines often have intermediate stages and types, and those also can benefit from the expressiveness of schemas.

#### Code examples

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577143

person: Let's now show some code examples. Let's show with an example how to use these small, concise APIs for typical data processing examples. We'll start by filtering purchases based on geolocation using longitude and latitude coordinates. Please note that these examples only cover the Java SDK. For Python, SQL and [indistinct] APIs are used instead and will be introduced later in the course. Feel free to check out the [indistinct] documentation on the Apache Beam website for guidance on using schemas in Python as well. We can see how concise the code for filtering is in both cases. This is possible thanks to features introducing Java SDK 8 to filter streams with lambdas. It's easy to understand the logic we are trying to implement with and without using schemas. Now let's see a more verbose example to implement joins in the Java SDK. Let's talk about joining datasets using this example. We're going to join transactions with purchases from an online system. It's important to point out that one transaction always has one or more purchases so we can use an inner join. We want to calculate the total purchases per transaction grouped by user ID. We can see how expressive the solution is when using schemas compared with actual Java without schemas, focusing on the business logic instead of having to embed steps to [indistinct] types, right? Schemas make your code more readable and easier to manage--whoo-hoo.

#### Serverless Data Processing with Dataflow - Branching Pipelines (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577144

#### Serverless Data Processing with Dataflow - Branching Pipelines (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577145

#### Quiz 4 - Schemas

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577146

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577147

### State and Timers

#### State API

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577148

Israel: Hello, my name is Israel Herraiz, and I work as a strategic cloud engineer at Google. In this video, you would learn about state and timers, two powerful features that you can use in your DoFn to implement stateful transformations. Let's start learning about the state API. Apache Beam pipelines can aggregate data with two main types of transformers, using GroupByKey or Combine. ParDos cannot do aggregations. This is because normally ParDos' states transform. You can map from one to zero, one to one, or one to many elements, but you cannot aggregate them together. Or can you? For instance, if you want to count the number of messages you have seen per key, is it possible to do that with a ParDo? Apache Beam comes with additional possibilities for ParDos: stateful transformations. You can have a state variables that can be reused across elements to do any kind of calculation that requires accumulating a state from several different messages. With stateful ParDos, there are many aggregations that can be implemented without having to use a combiner or a GroupByKey. State is maintained per key. So the input should be part of key values. For streaming pipelines, the state is also maintained per window. The state can be read and mutated during the processing of each one of the elements. The state is local to each one of the transformers. For instance, two different keys process, and two different workers are not able to share a common state, but all elements in the same key can share a common state. In this ParDo, we are receiving squares and we are doing a element by element transform to circles. We are also calculating the total area of the square scene so far. To do this calculation, we are accumulating the partial results in a state variable. The state variables are parsed through the process element as any other variable and can be safely mutated from within the process element method. You might be wondering that if you create your own DoFn class, why not using class members assisted variables? In case of reprocessing for instance, because of errors, dataflow takes care of making sure that the mutation of the state is consistent and safe. It will discount any mutation that has not been fully processed. If you use normal class members for keeping that state, you would have to implement that logic to discount that and ensure that your state mutation is safe and consistent. And that's actually very complex. Finally, in step three, after reaching the limit, the state values read will produce an output circles using the accumulated state value triangles. We could have just produced the state values as output, but in general we can produce any output that requires that value. In this example, the pipeline is calling an external service to enrich the data that is being processed. If you are running a large pipeline in data flow, accessing an external service for every element that you are processing in the ParDo can be problematic. You would be making millions of calls per second from hundreds of different workers. It is very easy to overwhelm an external service in that situation. How can you overcome this problem? The state variables allow you to batch the request by accumulating elements in a buffer and making batch calls to the external services, that, for example, you can make a call every 10 messages. Let's see the code for this example. This example is in Python. There are two state variables: buffer state and count state. These are passed through the process method of your DoFn as additional input arguments. In the buffer state, the DoFn is adding new elements right after the DoFn increases the count. When the count surpasses the max buffer size, the call to the external service is made. In this example, we have omitted this call for simplicity. It is important to remember to clear the state once it has been used. The DoFn will not finish entirely unless the whole state has been clear. This is the same example, but in Java. In Java, you need to annotate the state variables using state ID. And they are also passed as additional input arguments to the process method of the DoFn. Again, we have omitted the actual call to the external service for the purpose of simplicity. Now, both these code and the Python version shown in the previous slide have a problem. For the last message that we receive for the last messages that we receive, what if the buffer does not reach this max buffer size? The DoFn would keep that state indefinitely. And the DoFn would never finish. How can you solve that problem? Let's introduce the concept of timers.

#### Timer API

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577149

person: In addition to the state API, you can also use timers in your stateful transformations. The combination of both state and timers enables you to get rich and complex stateful transformations. Let's see how state and timers work together. Timers are used in combination with state variables to ensure that the state is clear at regular intervals of time. As with the case of windows and triggers, we can define timers either in event time or processing time. Event time timers depend on the watermark value. Processing time timers depend on the clock of the workers, and not on any timestamp included in the messages that are being processed. With a timer, we can solve the problem of a state being kept indefinitely. When the DoFn is processing the last messages of the last bundle, it is likely that the last buffer will not reach the threshold value set in the code. Also, if messages are coming in slowly, it may take a long time for the buffer to fill up. In both situations, you probably will want to produce some results rather than wait for a long time until more messages show up. Timers are useful for that. Let's revisit our example, but now let's add a timer to avoid having the state wait indefinitely. This example is in Python. The logic for the state is the same as in the previous examples, but now the DoFn has also an event time timer. When the watermark processes the value of allowed lateness, the timer expires. Then the state will be processed and clear, even if the buffer has not reached the limit size yet. By using a timer, you ensure that your state is not kept indefinitely and that the DoFn will finish even if no new messages are coming. Without the timer, the DoFn will have to be waiting indefinitely until the size of the buffer reaches the limit. Note that you need to have expiry method and annotated with on timer. This is the method that is called when the timer expires. This is the same example, but in Java. You need to use the timer ID annotation to create a timer, and the on timer annotation for the method that is called when the timer expires. The logic is the same as in the Python example. The timer expiry method ensures that the bundles are processed, even if the count does not reach the minimum count size or if the messages are coming in very slowly. In summary, this is that DoFn implemented with the examples. In addition to the two state variables, the DoFn has now a timer, which is called when the watermark goes over a certain value of allowed lateness. This is the typical pattern combining a state variable and timers. Remember that you have two options for timers: event time and processing time. You should use event time timers when you want to do the callback based on the watermark and the timestamps of your data, that is, when data is a state. Event time timers will be influenced by the rate of progress of the input data. Processing time timers will expire regardless of the progress of your data. The timer will trigger at regular intervals. Either event or processing time timers can be used for the example shown in the previous slides. You need to decide whether you want to fire always at regular intervals or depending on the pace of progress of your data and use the timers accordingly.

#### Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577150

person: The previous videos, you have learned about how you can use state and timers to implement stateful transformations. Remember, there are two types of timers available in a [inaudible]. Processing time timers are good for implementing timeouts. And event time timers are good for output based on data completeness. The time in processing time is relative to the previous messages and you will have periodic outputs based on that relative time. Event time timers are based on the timestamps of the messages being processed. If you want to make sure that you are emitting output when data is complete and you don't expect more data, then use event timers. A word of warning with event timers. Always make sure to clear the state after emitting the output. If you leave the state behind, then the function will keep waiting for new data and that will consume resources in your pipeline. In summary, for short and predictable latencies with maybe incomplete results, use processing time. For complete outputs with possible high latency, use event timers. Depending on the kind of state that you want to accumulate, you can use a different type of state variables. Value state is genetic. It can hold any kind of value of any type. If you want to add several elements, use a bag state for a more efficient pipeline. Bag will return the objects that were added previously, but with no guarantee of order. Appending objects to a bag is very fast. For any kind of aggregation that is associative and commutative, it is better to use the combining state. And if you are going to maintain a set of key values, a dictionary or map, use map state. With a map, you have random access given a key. Map state is more efficient than other state variables for retrieving specific keys. Finally, the set state, available in the patching programming model, but not supported in data flow. You may use the bag state for similar purposes instead. In summary, state and timers open a lot of new possibilities for due functions. You could implement domain-specific triggering of result not only based on time, but based on anything you may think of. There are also applications for slowly changing dimensions when you keep a dimension table and only a reference to every dimension in a large collection of data. How do you update your large collection of data when the dimension changes? Yes, with state and timers. Joins in a streaming or joining all the elements of a graph with all elements of another graph, so-called biclique. You can also apply the state and timers to implement such a join logic. In any situation where you need fine control on how the aggregation of the elements is done, state and timers allow you to implement a precise and complex logic. In general, any workflow that should be applied per key can be expressed as state and timers. State and timers are a very powerful feature of [inaudible]. You can implement complex logic in a due function and do much more than just map and filters. We could almost say that the limit of the difference is you can do with state and timers is imagination.

#### Quiz 5 - State and Timers

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577151

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577152

### Best Practices

#### Schemas

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577153

Ajay: Hi, my name is Ajay. I'm a strategic cloud engineer at Google. And now you have covered different building blocks of a DataFlow pipeline, like Beam basic concepts, windows, watermarks and triggers and their usage in creating data processing pipelines; different sources and sinks supported in DataFlow; Beam schemas for processing structured data; pipeline state and timers. In this chapter, we'll take a deep dive into some of the best practices involved in DataFlow. We will begin with the introduction of Beam schemas. We will explore how, using schemas, we can process structured data more efficiently. Then we'll explore best practices for handling unprocessable or erroneous records in a pipeline. We will also cover some best practices around error handling and generation of POJOs, also known as plain old Java objects. We will wrap up this section with an overview of DAG optimization and ways to exploit the life cycle of DoFn to do batch processing. Let's start by looking into Beam schemas. As we have discussed in previous chapters, a schema describes a type in terms of fields and values. Each field is named and has a type. Schemas can be nested arbitrarily and can contain repeated or complex fields as well. When you use schemas in DataFlow jobs, you make your code more readable and easier to manage. Also, it allows the DataFlow service to make optimizations behind the scenes as it is aware of the type and structure of data being processed. For example, the DataFlow service optimizes the encoder and decoder required for [indistinct] and deceleration of data as it moves from one phase to another. Here is an example of using schemas in Java and Python SDK. Each code snippet shows an example of a class with the name, purchase, and in Java and Python respectively. It has five fields: user ID, item ID, shipping address, cost cents, and transactions.

#### Handling un-processable data

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577154

person: Next section is handling un-processable or erroneous records in a pipeline. When working in a real world use case, we should design our pipelines to handle data that is not in the desired or expected format. Erroneous records may cause our pipeline to fail if not handled properly. When faced with erroneous records, rather than just log the issue, send the [inaudible] to a persistent storage medium, such as BigQuery or Cloud storage to handle them separately. Take--use double tags to access multiple outputs from the resulting P collection. Here we can see the [inaudible] code to implement the dead-letter sink pattern. Consider wrapping user code inside a process element function with a try-catch block. Inside the try-catch block, avoid logging every error exception, as it may overwhelm the whole pipeline. Especially when presenting your [inaudible], increases, instead, send the erroneous records to an alternative dead-letter sink. Line 11 in this snippet shows the erroneous records is being sent to a side output using dead-letter tag. Finally, it is written to a different sink at line 15.

#### Error handling

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577155

The next section covers error handling. In this section, we'll explore best practices to handle errors and exceptions in a Dataflow pipeline. How should I handle errors and exceptions in my Dataflow pipeline? As you might already know, errors and exceptions are part of any data processing pipeline. To write a performant, fault-tolerant pipeline, it is important to handle them appropriately. Always read the user code in DoFn functions with a try-catch block. Handle the different exceptions according to their severity. In the exception block, rather than just log the issue, send the raw data out as SideOutput into a storage medium such as BigQuery or Bigtable using a String column to store the raw, unprocessed data. You can use tuple tags to write output to multiple sinks. In case of erroneous records, you can use tuple tags to send data to a dead letter queue.

#### AutoValue code generator

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577156

person: In this section, we will explore some of the utility classes that the Beam SDK provides for generating POJOs, which stands for Plain or Java Object. Overall, Apache Beam schemas are the best way to represent objects in a pipeline because of the intuitive way they allow you to work with structure data. However, there are still places where a POJO is needed while developing pipelines in Java, for example, when dealing with key value objects or handling the object state. Hand-building POJOs require you to code appropriate overrides for the equal and hash code matters, which can be time consuming and error-prone. You can end up with inconsistent applications easily. To avoid this, use the AutoValue class builder to generate POJOs. This ensures that the necessary overrides are covered and lets you avoid the potential errors introduced by hand-coding. AutoValue is heavily used within the Apache Beam code base, so familiarity with it is useful if you want to develop Apache Beam pipelines on dataflow using Java SDK. AutoValue can also be used in concert with Apache Beam schemas if you add on @DefaultSchemas annotation. For more information, see "Creating Schemas" in reference section in the end. For more information on AutoValue, see AutoValue docs in the reference section. Remember, this is only applicable to Java-based pipelines.

#### JSON data handling

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577157

person: In this section, we'll explore best practices involving handling JSON data. Processing JSON strings in dataflow is a common need, for example, when processing click stream information captured from web applications. To process JSON strings, you often need to convert them into either rows or plain old Java objects, also known as POJOs, for the duration of the pipeline processing. The Apache Beam built-in transform JsonToRow is a good solution for converting JSON strings to rows. If you need to convert JSON strings to a POJO using AutoValue, register a schema for the type by using the @DefaultSchema annotation. Then, use the Convert utility class so you end up with code similar to the following code snippet. The structure of JSON data may change frequently. Use Deadletter pattern to handle unsuccessful messages resulting from unexpected structures or schemas. For more details, refer to Queueing Unprocessable Data for further analysis.

#### Utilize DoFn lifecycle

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577158

person: DoFns play an important role in dataflow pipelines. They allow users to transform each input element. In this section, we'll explore how we can reference the lifecycle of DoFn objects for micro batching if required. It's common to invoke external APIs as part of your pipeline. While working on big data use cases, it is easy to overwhelm an external service endpoint if you make a single call for each element flowing through the system, especially if you haven't applied any reducing functions. If you remember what we covered in the Beam concepts review module, you will remember this is what the lifecycle of a DoFn looks like. We recommend batching calls to external systems by leveraging @StartBundle and @FinishBundle lifecycle elements. The code snippets here shows surer code to override @StartBundle and @FinishBundle functions of DoFn. For micro batching, you can initialize or reset the batch in @StartBundle and commit it in the @FinishBundle function. Remember, depending on runner implementation, @StartBundle and @FinishBundle may be called multiple times to process more than one bundle. It is important to reset variables appropriately while using lifecycle functions of DoFn.

#### Pipeline Optimizations

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577159

person: This is our last section on dataflow best practices. Here we'll explore a few things that we should keep in mind while designing our dataflow pipelines. Let's look into some general guidelines we should consider while developing dataflow pipelines. Whenever possible, filter data early in the pipeline and move any steps that reduce data volume up in your pipeline. This will reduce the overall data volume flowing through the pipeline, enabling efficient use of the pipeline resources. This includes placing them about window operations as well, even though the window transform itself does nothing more than DAG element in preparation for the next aggregation step in the DAG. Data collected from external systems often needs cleaning, since a single message can suffer from multiple issues that needs correction. Think carefully about the direct acyclic graph or DAG you will need. If an element contains data with multiple effects, you must ensure that the elements flows through all of the appropriate transforms whenever possible. Applied data transformation serially to let the Dataflow service optimize data for you. Whenever transformations are applied serially, they can be merged together in single stage, enabling them to be processed in the same worker nodes and reducing costly IO network operations. If your pipeline interacts with external systems, look out for back pressure and external systems. May be a key value store like BigTable or [indistinct] used for lookups in a pipeline or an [indistinct] sink your pipeline writes to. It is recommended that you ensure the appropriate capacity of external systems to avoid back pressure issues. Enabling auto scaling for Dataflow pipelines is also a good idea. If for some reason your [indistinct] system is backlogged, your Dataflow pipeline can scale down instead of underutilizing pipeline resources. In this module, we started with intro to Beam schemas. We discussed its usefulness while dealing with structured data. Then we looked into best practices for handling unprocessable or erroneous records. Next, we covered best practices around error handling and generation of modules. We wrapped up this session with overview of DAG optimizations, and ways to exploit lifecycle of [indistinct] to do batch processing. Thanks for joining.

#### Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577160

#### Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577161

#### Quiz 6 - Best Practices

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577162

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577163

### Dataflow SQL and DataFrames

#### Dataflow and Beam SQL

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577164

Hi again, my name is David Sabater Dinter and I work as Outbound Product Manager for data analytics at Google Cloud. This module is part of the Dataflow Developing Pipelines course, and we are going to introduce two new APIs to represent your business logic in Beam, SQL and Dataframes. Let’s start with SQL, available in our Dataflow runner and also within the Beam SDK. You might remember this slide from the Schemas section, where one of the key takeaways was that by understanding the structure of a pipeline’s records, we can provide much more concise APIs for data processing. Do you remember what API is used by Database folks since the ‘70s? SQL, that’s right. Have you ever wondered why? Let’s discuss briefly. SQL reduces boilerplate code, and is easier to understand by implementing simple SQL statements describing your transformations. It can also automatically optimize the pipeline execution: SQL planners can actually optimize on every execution, probably more so than our handwritten code. And workers are also able to perform further optimizations. SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system. Or, more relevant here, for stream processing in a relational data stream management system. The data is accessed through relational algebra, where, for example, projections are used to pick a subset of the columns to query, filter to apply certain conditions to the rows being returned, and finally apply aggregation via the group by clause. It includes also syntax to operate on nested structures. Don’t worry about reading the code! This is just to show you how verbose it can be to write a join in the Java SDK. You might remember the example that was used to introduce schemas in the other section. The important part is to understand what a join is. Fundamentally, this is joining two input datasets to obtain one output dataset. Let’s now show why Dataflow SQL can help you to implement your data processing pipelines. See the amount of lines of code required to implement a join in the Java SDK, without using schemas and SQL. Most of the extra code is required to annotate types, mapping key/values, and so on. If we use the Scala SDK with Scio, developed by Spotify and available as open source Apache license, we can reduce the code verbosity significantly, thanks to lambdas and type inference features from Scala. With SQL, we are providing the ability to translate the business logic written in SQL back to Apache Beam primitives, to be executed in our Dataflow serverless service in a scalable and overall concise way. What are we trying to do here? We can see in this slide how these primitives are derived and chained together in a Directed Acyclic Graph to execute our logic. Before we go into detail about Dataflow SQL, let’s first distinguish some important components associated with the personas and their user journeys or how they interact with the service. A data analyst will typically start interacting with historical data in the BigQuery UI, running SQL statements on historical data to test their hypothesis about what happened, typically referenced as batch data. After testing on historical data, they will ideally want to test the same business logic in the form of SQL statements over real-time data this time. Switching to the Dataflow SQL UI to test the same logic over real-time data involves very few changes—nice! Lastly, once the data analyst is happy with the logic, they would be able to pass those SQL statements to the data engineer, who will be able to implement them with little change in the form of SQLTransforms inside the Beam Java SDK. Please note that Beam SQL and Dataflow SQL are effectively identical, but while Beam SQL offers a programmatic interface, Dataflow SQL also offers a UI interface. Let’s walk through this journey together. First we begin with the common denominator, which is Beam SQL. It allows a Beam user to query bounded and unbounded PCollections with SQL statements, also referred to as querying data in batch and streaming mode. Your SQL query is embedded using SQLTransforms, an encapsulated segment of a Beam pipeline similar to PTransforms. You can freely mix SQLTransforms and other PTransforms in your pipeline if needed. It also supports User-Defined Functions. Beam SQL includes the following dialects to interpret SQL statements, which we will cover later: Apache Calcite SQL, and Google ZetaSQL. Finally it integrates Schema PCollections and supports windowing when aggregating unbounded data. As mentioned earlier, Beam SQL supports two dialects to understand the relational algebra: The Beam Calcite SQL is a variant of Apache Calcite, a dialect widespread in big data processing, compatible with Apache Flink SQL for example. Beam Calcite SQL is the default Beam SQL dialect and supports Java UDFs among other mature features. Beam ZetaSQL is more compatible with BigQuery, so it’s especially useful in pipelines that write to or read from BigQuery tables, for example when using the Dataflow SQL UI writing to BigQuery. Now let's talk about Dataflow SQL. Dataflow SQL integrates with Apache Beam SQL and supports a variant of the ZetaSQL query syntax, using SQLTransforms in a Dataflow Flex template (but all transparent to you, the user!). You can actually write your SQL logic through the UI or gcloud client command line. ZetaSQL provides the same dialect as BigQuery Standard SQL. And lastly, it can optionally be used as a long-running batch engine. As you may remember from the personas we described, one of the core use cases for Dataflow SQL is to help data analysts query streaming data using a common language, SQL. A typical use case will: Select from Pub/Sub, Join with batch data, Aggregate some metrics over a particular Window, And finally publish to BigQuery or Pub/Sub topic. It’s also worth mentioning that the Dataflow SQL is not only restricted to GCP-native services like BigQuery or Pub/Sub. We are also planning to integrate with many others like Kafka and Bigtable. As we described earlier, data analysts can use their existing SQL skills to develop and run streaming pipelines from the BigQuery web UI. You don't need to set up an SDK environment or know how to program in Java or Python. By using the familiar BigQuery UI, one could easily join streams such as Pub/Sub with snapshotted data sets. BigQuery tables are an example, but Kafka and Bigtable are also coming soon as already mentioned. You can query your streams at static datasets with SQL by associating schemas with objects such as tables, files, and Pub/Sub topics. You create a job and specify the output location, for example writing your results into BigQuery tables for analysis and dashboarding. This is just as simple as selecting Dataflow as the execution engine for SQL statements, using the BigQuery web UI and your destination dataset and table. Remember always to ensure the regional endpoint, sources and destination are within the same region if possible. You can directly access the Dataflow UI to monitor the underlying Dataflow job running your query. In case you want to launch Dataflow SQL jobs programmatically instead of relying on the UI, there is also an option to use the gcloud command tool for authoring jobs through the Dataflow SQL CLI. The Dataflow SQL interface is integrated with gcloud to give you that capability through your command-line interface. Finally, as described in the user journeys, the data engineer would be able to apply the SQL logic implemented by the data analyst, within existing pipelines. In this case, note the use of PCOLLECTION as a table name. Named Tuples can also be accessed by name. And before closing this section, you can see here a simplified version of the Dataflow Template being submitted when using the Dataflow SQL UI. Data engineers are free to actually implement a similar template to be able to encapsulate all that logic from the data analysts, in a more programmatic and scalable way.

#### Windowing in SQL

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577165

person: Let's move now to see how we can apply streaming concepts like windowing in SQL. We're going to see how we can implement the different types of windows for aggregations as we stream data. The first one you will remember from previous sections is the tumbling or fixed window. We can see in this gray how the tumble term is included to incorporate that type of windowing. Then for hopping or sliding windows, the same approach can be done using the hop term. In this case, including two different intervals. One defining the length of the window and the other when the new window begins. Lastly, we can see how session windows will be implemented in SQL. In this case, session will be used, including the interval of time that determines when a new session window must be created.

#### Beam DataFrames

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577166

person: In the last but not least part of this model, let's discover how we can also leverage another Appia used widely data frames the Apache beam Pythonesque provides a data frame API for working with panels like data frame objects. The feature lets you convert a collection to a data frame and then interact with the data frame using the standard methods available on the panels. Data from API like the example in the slides, adding up total prices grouped by a recipe. The data from API is built on top of the panels implementation and Panesar different methods are invoked on subsets of the data sets. In parallel, the big difference between beam data frames and Panesar difference is that operations are deferred by the Beam API to support the beam parallel processing model. You can think of beam data frames as a domain specific language for beam pipelines similar to beam cycle data frames is a DSL built into the Beam Python SDK. Using these DSL, you can create pipelines without referring standard beam constructs like produce and Combine Purkiss. The Beam Data Frame API is intended to provide access to a familiar programing interface within a beam pipeline. In some cases, the data from API can also improve pipeline efficiency by deferring to the highly efficient Becta responder's implementation. Let's introduce the first primitive group by the more primitive group Bickie Combined. Pearcy and Thibeault combined effect are significantly more verbose and less intuitive. You've already seen some examples of these. When we cover schemas with the Java SDK, a group, a group, sorry, a group by operation involves some combination of a combination of a splitting the object, applying a function and combining the results. These can be used to group large amounts of data and compute operations on these groups using an arbitrary expression like the example above producing the expected results. It's also possible to use the data from API by a function to the two data frame transform data frame. Transform is similar to sequel's transform from the beam cycle DSL that we introduced before where sequel Transform translates a sequel query to a P transform a data frame. Transform is a pittance from the plays function that takes on returns data frames. Are they different? Transform can be particularly useful if you have a standalone function that can be called both on beam and an ordinary PARNAS data frames data frame, transform, can accept and return multiple collections by name and by keyword as shown in the following examples. These is last slide demonstrates how simple it is to convert Pikul collections to beam data frames and vice versa. Beam data frames are deferred like the rest of the Beam API, as a result, there are some limitations on what you can do with beam data frames. Compare to the standard policy implementation. Again, because all operations are deferred, the result of a given operation might not be available for contraflow or interactive visualizations. For example, you can compute some, but you can't branch. And the result? Result columns must be computable without access to data, for example, you can't use transpose. Also, big elections are inherently unordered, so panis operations that are sensitive to the ordering of rows are unsupported, for example, or other sensitive operations such as shift comix, a Kumin head and tail are not supported with being data frames. Competition doesn't take place until the pipeline runs. Before that, only the shape or a schema of the result is known, meaning that you can work with the names and types of the columns, but not the result data itself. However, we can see that HelloWallet example in data processing counting words. We first need to map the source data to a schema to be able to see these more expressive APIs. We then need to convert to data frame before we can apply a group by function to aggregate the sum by word to obtain the word count. And lastly, like in pangas data frame, we can directly say the results with the two kesby method. Finally, data frames can also be converted back to schema collections.

#### Serverless Data Processing with Dataflow - Using Dataflow SQL for Batch Analytics (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577167

#### Serverless Data Processing with Dataflow - Using Dataflow SQL for Streaming Analytics (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/229/labs/577168

#### Quiz 7 - Dataflow SQL & DataFrames

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577169

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577170

### Beam Notebooks

#### Beam Notebooks

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577171

>> Hi, my name is Rosa Orkney and I'm a Google cloud developer advocate for Google Earth dataflow. And today, as you can see on the agenda, we're going to be looking at being notebook's and running them on the dataflow service. We think about the way that we would normally develop a Pache pipeline. The SDK allows us to declaratively described the pipeline that we would like the service to run. As we can see in this example here with the store sales and the online sales, which comes and describes a direct, slightly graphic computation attack. Once we've done this, we actually submitted to the service and the service goes ahead and runs that pipeline for us. The this is fantastic for production use cases because it allows the runner to be able to do fancy optimizations like dataflow fusion, where it can collapse stages together and make it very efficient for the processing that's going to happen. However, it's not the best experience when you're first developing your pipeline because we often get into this right submit job re waiting for logs, waiting for print statements. This is especially true when we're still exploring our data where we would not only like access to the data as we purchased it, but access to the data as our transforms are transforming that data now with Apache Beam. We do have the interactive runner that's available and the interactive runner allows us to be able to get access to the pipeline. So specifically, it gives us access to the intermediate results that are available from our transformations, allowing us to do exploration and the next stages of development. Importantly, as you would expect with Apache Beam, the interactive runner also works with batch and stream sources. So we no longer have to mock out or objects as we're doing our development. We can actually, when needed, run directly against the real data, even if that source is unbounded and industry. So going on to the you know, how we get to run, this will look at the being notebook's and on this slide you actually see the various steps we need to take with the data flow service UI to be able to set up a notebook environment. So we start the data flow notebooks that gives us access to creating new instances. And the nice thing there is that once a new instance is being created, which is the host for our notebooks, all the libraries and things that we need for them already in place so we can immediately start doing our development. The notebook's environment also comes with some ready-Made examples, which are great for exploration and learning, but also very useful for Kosmidis specifically. We'll talk a little bit more about one of the examples, the word count example, and we won't reproduce it completely in this session. That's for you to hopefully do once you start exploring the notebooks directly. But just to take a few snippets and some of the options that will be available when you do start making use of the notebooks. So in this slide, we can actually see some of the transformations in that word. Count example. First, we need to read from our inbounded source, which we do with read from pops up. So this puts it into the words collection. We then apply a fixed window to those elements, a fixed window of 10 seconds. This puts it into the window word collection. And finally we do a count. Now, if we were doing this without the interactive runner at this point, we'd have the fourth transform, would do some lobbying or turn it converted to a sink where we can then look at the data because we are using the interactive running, we can access intermediate results. How do we do need a way to tell the system when to stop reading from this unbounded source of information? And the way that we do that is with a couple of options from the interactive runner, which is the IB options, either recording duration to be set or recording size limit. The duration gives a fixed amount of time for the interactive runner to record data, and the recording size limit gives us a fixed amount of bytes to read. The latter is very useful when you're working with a real stream of information where you might have a very large volume of data and you don't want all of that to be put into your notebook as you're doing the experimentation. The other important factor is that we have the option to actually reuse the stream of information that we've gathered or to get fresh data. So the reuse is useful because then we were exploring data and manipulating it or working with the same data set in terms of looking at those collections and looking at that intermediate result. I did not show allows us to visually see the information, as we can see in this slide example here. The options include window info will also give us some extra metadata about each element. For example, the event time and the window that that data belongs to. Visualization is obviously very useful, but we also want to be able to use this data and manipulate it directly within our code, for that we can use it to collect, which allows us to then output this into a Penders data frame, which then we can do all our manipulation against if we wanted to actually do further visualization of data with graphs, etc.. The notebook comes in built with a feature via IB show, which is to set visualize data to true. This gives us access to the UI that you're seeing here and you can do various visual exploration of the information with these core primitives. We can now start getting out of that right. Submit to service lifecycle when we are dealing with our data. But obviously once we complete our development, we then want to be able to submit the job to the service and we want to do that with as little as code as possible. So finally, until the gain from development to production, there's very little we actually need to do the code, because at the end of the day, we're just writing a Beam SDK code for this whole process. We just need to enable running on the service by importing the data from runner, by providing options, the pipeline options, for example, the project and the staging directory, etc. And then we just do run it on pipeline, which will submit that job to the service. So hopefully with this, you've had a nice overview of what will be available when we when you start working with the notebooks and thank you.

#### Quiz 8 - Beam Notebooks

- https://www.cloudskillsboost.google/paths/16/course_templates/229/quizzes/577172

#### Module Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/229/documents/577173

### Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/229/video/577174

person: You've reached the end of the developing pipelines with dataflow course. Let's do a brief recap of what we've covered. We started by reviewing core Apache beam concepts, defining key terms like pipelines, Peak Collection's P transforms and Rutter's. We also covered utility transformers like Pardoo Group Bickie and Flatting included with the breakdown of the life cycle of a new fund. We can put these pieces together to build basic beam pipelines. We looked at how windows, watermarks and triggers were together to deal with streaming data with the flexibility of the B model. You can decide how your pipeline emits results and manages late arriving data. These concerns can help you translate your business logic into a streaming pipeline that will deliver Real-Time Insights for your applications and end users data for jobs. Read from sources and right to Sync's. And we covered a wide range of issues available to you to the BPM SDK from Textile and File Eyo for text and file says respectively to Google Cloud Io's like McQuary Pub, Sub and Big Table. We also covered popular open source countries like Kafka, IO and Avro. These sources and sinks almost all have their own nuances, so it's important to reference the documentation to review what tuning Premraj available to you for your use case. Your organization might also need to build their own connectors for proprietary Io's with split-Level do funds, you can write your own source that leverages the utilization benefits of distributed processing to maximize throughput schemas. Help express the structure of your data in the language of beam, making your code easier to manage and more efficient to run. State and timers provide a way for developers to manage Purkey State, which gives more fine grained control over aggregations by manipulating the state of inflight data and controlling when data is processed. Using timers, you can effectively enable any use case you can imagine, no longer limited by the limitations of Pardieu in group bickies. We combine all of these building blocks in the best decade to develop pipelines that are executed on the data for service. We share a number of best practices based on years of experience, working with engineers across a wide range of use cases, some of the highlights include implementing a dead letter. Q Which can ensure that your pipeline does not stall indefinitely if it encounters corrupted input data. Devising an air handling strategy for your due funds handle JSON data using beams built in JSON utility transforms. Bache calls to external APIs so you don't disrupt external services and employing various pipeline optimization techniques that are discussed in more detail in the module. We explore an alternative way to launch data for pipelines using sequel. Dataflow school provides an interface integrated into the big queerer UI to select your sources, write a school segment with streaming extensions that describe your windowing logic, then write to a big query table for further analysis. However, if you want to invoke data for jobs via SQL programmatically, we also offer a command line interface to do just that. If you want to integrate sequel into your handcrafted Beahm pipeline, you can do that with Beahm sequel. We introduced the beam data frames, which allows you to convert a collection to a data frame and interact with it using the standard methods available in the popular Panda's data frame API. If you are a Python developer, data scientist, this API can offer a familiar entry point into beam data flow that looks like your existing toolkit. We finished the course by covering Beahm notebooks, which merges the Beam Python SDK with the Jupiter Lab interface, enabling a completely different way of operating beam pipelines. The interactive runner that is deployed on beam notebooks allow you to inspect intermediate P collections so that you can validate your transformations before you launch a pipeline onto the data service. Bime Notebook's also contains source recording features that allow developers to prototype pipelines that read from unbounded data sources a BMW VM can be launched directly from the console UI. If you're just starting with the Python STK, the beam notebook is the place to start. It comes preloaded with several tutorials and walked through to the SDK offering a learning path that is available and no other SDK. In summary, Apache beam data flow offers a compelling platform for all your data processing needs and without the fear of vendor lock in. We're excited to see what applications you built with the concepts from this course.

### Your Next Steps

## 09: Serverless Data Processing with Dataflow: Operations

- https://www.cloudskillsboost.google/paths/16/course_templates/264

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567914

Mehran: Hi, and welcome to the third installment of the Dataflow series: Dataflow Operations. My name is Mehran Nazir, and I'm a product manager with Google Cloud Dataflow. You have arrived at the final course of the Dataflow course series, which seeks to provide you all of the skills needed to build your modern data platform on Dataflow. In the Foundations course, we learned about the building blocks of Dataflow, including Shuffle, Streaming Engine, Flexible Resource Scheduling, and Beam portability. We also covered horizontal integrations with Dataflow, including IAM, quotas, and security features. We then move to Developing Pipelines, which explored how you can turn your business logic into a Dataflow Pipeline. We reexamined the building blocks of the Beam SDK, introduced advanced features like state and timers, reviewed best practices, and concluded with a deep dive on SQL, data frames, and notebooks. In the last installment of the Dataflow course series, we will introduce the components the Dataflow operational model. These lessons will help ensure that your data platform is stable and resilient to unanticipated circumstances. Let's review the outline for the Dataflow Operations course. First, we'll explore Dataflow monitoring, which includes a walkthrough of the various screens of the Dataflow console experience. Next, we'll discuss the logging and Error Reporting integrations, a critical piece of the Dataflow Operations stack. We will review our recommended approach for troubleshooting and debugging Dataflow Pipelines, then explore common causes for Pipeline errors. From there, we will do a thorough examination of performance optimization techniques for Dataflow Pipelines. This module will help you get the most out of your Dataflow jobs. We will discuss testing and continuous integration/continuous deployment, otherwise known as CI/CD, with Dataflow which will help you safely test and roll out changes to your Pipelines. We will move on to reliability with Dataflow Pipelines and discuss methods for building systems that are resilient to corrupted data and data center outages. The final module of this course will cover Flex Templates, a feature that helps data engineering teams standardize and reuse Dataflow Pipeline code. Many operational challenges can be solved with Flex Templates. We will conclude the course with a recap of all the key lessons from the modules. Let's get started.

### Monitoring

#### Job List

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567915

Omar: Hey, there, my name is Omar Ismail, a solutions developer at Google Cloud. You have launched your Apache Beam pipeline onto Dataflow, and now you want to view its usage. What monitoring resources exist? There is a Dataflow monitoring page which you will learn to use for both batch and streaming jobs. Dataflow also integrates with cloud monitoring. Cloud monitoring is mostly used as a back end to build the Dataflow UI, and you can use it as well to build your own custom dashboards. Let's start by looking at the job list page in Dataflow. This is the first page you see when you visit Dataflow. The job list page shows all the jobs that have run over the last 30 days. If we click on the running button... We see only the jobs that are running. In my project, I currently have two. if we toggle back to show all jobs and enable sorting... the columns become sortable. For example, I can sort by the start time... or by pipeline status. Let's switch off sorting... and view how filtering works. Click into the filter text box, and you can choose any of the columns to filter by. I'm going to select status. And that shows me all the available statuses. Let me select all the ones that failed. I can use more than one filter and set its relationship to the previous one. "And" is implied by default. So if you want your condition to be an "or," you'll need to select the "or" option. I am not going to set an "or." Instead, I'm going to filter by name... And find jobs that have the word "Wikipedia" in them. Imagine a use case where you want to check the status of all of your failed jobs that ran with the word "Wikipedia" in them. Instead of having to add that filter every time, you can bookmark that link and revisit those jobs every time. If I take the link in the URL... Copy it... Open a new tab... And then paste in go... We can see that it takes us back to the filtered view that we had. We can then go ahead and add the link as a bookmark.

#### Job Info

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567916

person: Now that we have seen where all the data flow jobs are listed, let us explore how we can monitor one job. The first page you see when you click on a job is the job graph page. Let's go over what we see on the screen. To the right, we have the job Info panel. This shows basic metadata about the job, such as the regional endpoint being used, the worker location, and the encryption type. Below that, we see the number of resources that are in use. And below that, we see the parameters used to run the pipeline.

#### Job Graph

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567917

person: Now let's focus on what we see on the job graph. In the middle of the page, we see a visual representation on the steps from my Beam code. I am running a batch pipeline that reads data from a BigQuery table, reshuffles it, and writes it to a cloud storage bucket in TensorFlowRecord format. The records are split across training, validation, and test sets. In Beam, some steps are made up of substeps. We can see these by expanding each step. As this is a batch job, steps are executed sequentially. The next part should not start until the one before it finishes. DataFlow optimizes your pipeline for both streaming and batch pipelines. Part of the optimization includes fusing multiple steps in your pipeline into single steps. If we press on any step, we can see how DataFlow splits each stage into a number of optimized stages. Some stages will be shared between different steps. For example, if I press on the RecordToExample step and view its stages, and then press on ReshuffleResults and view its stages, we can see they share a common stage. When that stage starts, the UI will show both RecordToExample and ReshuffleResults running. If we were running a streaming job, all the stages and steps would run concurrently. Pressing on each step not only shows us the optimized stages, but also throughput info for each step across time. Below that, we see the total number of elements added and the estimated size. Another metric available at each step and substep is the wall time. This shows the total amount of time by the assigned workers to run each step. This can be a useful metric to look at when you want to see where your workers are spending the most amount of time. Eventually, the batch job will complete. In batch jobs, as we are dealing with a known amount of data, jobs do get completed. Once a job finishes, all steps should be marked with a green check mark, as shown here. If a job fails, the steps that failed will be shown in red with an error symbol. As streaming jobs process unbounded collections, there is no completion time for the job unless you cancel or drain it. Beam lets you set custom metrics for your pipeline. The metrics class has three methods that can be used: counter, distribution, and gauge. The counter method lets you increment and decrement any variable or event you are interested in checking. The distribution method is not a histogram, but tracks for you the count, minimum, maximum, and the mean. The gauge method lets you see the latest value of their variable you set it to track. Please review our public docs to see which custom metric types are supported in DataFlow. The DataFlow UI displays any custom metric on the right pane of the Job Graph page. For example, this is a pipeline run of the Beam Java word count example. On the DataFlow UI, we see the custom metrics associated with the job here. We count the number of empty lines and the length distribution of each line.

#### Job Metrics

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567918

person: The other page available on the Dataflow UI is the Job Metrics tab. This shows us time series data for our job. This page varies between batch and streaming. Let's look at the Job Metrics for the BigQuery two tensor flow records batch pipeline we ran earlier. The first graph shows the number of workers that ran across the lifetime of the job with auto scaling enabled. At certain points during the job, we can see that the Dataflow service decided that more workers were needed to increase the job throughput. The green line shows how many workers are needed, and the blue line shows the current number of workers. There will be a small time gap between the two as each new worker needs time to spin up and for work to be assigned to it. The second graph shows the throughput for each sub step versus time. Recall that your beam steps are made up of sub steps. And here we see the throughput of each one. In the Job Graph tab, we discussed how batch pipelines do not run all the steps concurrently. We can see that on this graph here. The first hump shows the records being read, and the second one shows the records being partitioned and saved to Google Cloud Storage. The third graph shows each CPU utilization percentage. In our job run, we see the all workers reached near 100% CPU utilization. A healthy pipeline should have all the workers running around the same CPU utilization rate. If you see that a couple of your workers are running at 100% and the rest of the workers have low utilization, your pipeline is likely unhealthy and suffering from an uneven distribution of workload. Some beam operations like group by key cannot be split across workers. Each worker will be assigned a range of keys to group. If your data is heavily skewed, one worker could end up doing all the work while the others do nothing. On the CPU utilization graph, we see this as a couple of workers having a high CPU utilization, while the others have low CPU utilization. The last graph in batch pipelines is the worker error log count. As the name suggests, this shows the number of log entries from the workers that had a level of error. In batch jobs. If processing an element fails four times in a row, the whole batch pipeline fails. Let us now look at a streaming pipelines Job Metrics page. This is for a pipeline I ran that reads from Pub/Sub and syncs to BigQuery. Just like batch pipelines, there are graphs for auto scaling throughput, CPU utilization, and worker error log count. In addition to these graphs, there are a few graphs for streaming jobs. Let us start with the first two, the data freshness and system latency graphs. These graphs are great to measure the health of a streaming pipeline. The data freshness graph shows the difference between real time and the output watermark. The output watermark is a timestamp where any time step prior to the watermark is nearly guaranteed to have been processed. For example, if the current time is 9:26 a.m., and the data freshness graphs value at that time is six minutes, that means that all elements with a timestamp of 9:20 a.m. or earlier have arrived and have been processed by the pipeline. The system latency graph shows how long it takes elements to go through the pipeline. If the pipeline is blocked at any stage, the latency will increase. For example, imagine our pipeline reads from Pub/Sub, does some beam transformation on the elements, then syncs them into Spanner. Suddenly, Spanner goes down for five minutes. When this happens, Pub/Sub won't receive confirmation from Dataflow that an element has been sunk into Spanner. This confirmation is needed for Pub/Sub to delete that element. As there is no confirmation, the system latency and data freshness graphs will both rise to five minutes. Once the Spanner service comes back, all the elements will be written into Spanner and data flow will confirm that with Pub/Sub, returning the system latency and data freshness graphs to normal. In addition to the data freshness and system latency graphs, streaming jobs can also have an input and output metrics at the bottom of the metrics page. Input metrics and output metrics are displayed if your streaming Dataflow job has read or written records using Pub/Sub. In my case, I only had Pub/Sub as an input so I can only see input metrics. If I have more than one Pub/Sub source or sink, I can view the metrics of any one of them by clicking on the drop down and choosing the Pub/Sub source or sink I want. In my case, I only have one Pub/Sub source and that is my subscription name data flow fund. The first graph we talked about is the request per second graph. Requests per seconds is the rate of API request to read or write data by the source or sink over time. If this rate drops to zero or decreases significantly for an extended period relative to your expected behavior, then the pipeline might be blocked performing certain operations, or there is no data to read. If this happens, you should review steps that have a high system watermark to see where the blockage is happening. Also, examine the worker logs for errors or indications that slow processing is occurring. The second graph is the response errors per seconds by error type graph. Response errors per second by type error is the rate of failed API requests to read or write data by the source or sink over time. If errors occur frequently and repeatedly, see what they are and cross reference them to the specific error code documentation on Pub/Sub error codes. For all pipelines, you can restrict the timeline for the graphs and logs using the time selector tool. Right now I have a job that has been running for a few hours. How do I focus on a specific time interval? This is where the time selector tool comes in, and I'll show you how to use it. Open the time selector tool by pressing on the button showing the current time range selected. This will open a drop down menu, you can select a time range for the charts and logs ranging from hours to the maximum lifetime of the pipeline. You can even choose a custom time range by setting the start and end time you want to view. Let's click the max time for the pipeline to see how the graphs change across the pipeline's entire time. And press apply to see the change. Keep an eye on the data freshness and system latency graphs. At the beginning of our run, the pipeline had a lot of data to read. If I bring the cursor near the peak of the data freshness graph, we can see the pipeline was approximately 16 hours behind wall time when it started. This is because I first sent data to a Pub/Sub subscription for 16 hours before starting the pipeline. If I want to zoom into a specific time period from the graph, I press on the start point I am interested in and drag and hold to the end of the time period I am interested in. Once I released the pointer all the graphs will be zoomed into the time range highlighted. If I want to exit the zoomed view, I press on the Reset Zoom button at the top.

#### Metrics Explorer

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567919

person: We've explored the DataFlow UI and its monitoring capabilities. Let's look at how Cloud Monitoring Metrics Explorer can be used with DataFlow. Cloud Monitoring Metrics Explorer is a sink into which all the DataFlow metrics we've seen are exported. You can explore DataFlow metrics using the Metrics Explorer and build custom dashboards to view them. The available metrics range from plotting the job status to custom metrics that you create. Here is an example of a custom dashboard using Cloud Monitoring. This dashboard shows the data watermark leg across all pipelines that start with a specific name. While other metrics can be used, it depends on what you want to measure. Some metrics that can be used are shown on this page. For example, if you want to see if your job failed, set is_failed to greater than 0 and filter by job name. If you want to see if a dependency is failing, set up a counter to measure the number of times the dependency is called and plot the results using the user counter metric. You may have noticed that most of the graphs we looked at had a Create Alerting Policy button. Cloud Monitoring gives you the ability to create alerts and be notified when a certain metric crosses a specific threshold. Where can this be useful? In streaming pipelines, if an element fails to get processed, it is retried indefinitely. Streaming pipelines have no concept of failure unless you specifically cancel or drain a job. You can catch indefinite retries by setting an alert if system latency increases over a predefined value. Every time an alert is triggered, an incident and a corresponding event are created. If you specified a notification mechanism in the alert, such as an email or SMS, you will also receive a notification. The alerting policies provided are on a per-pipeline basis, but you can build your own custom alerting policy grouping more than one pipeline using Cloud Monitoring. This is the end of this module. You should now be able to: navigate the DataFlow job details UI, interpret job metric graphs to diagnose pipeline regressions, and set up alerts on DataFlow jobs using Cloud Monitoring.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567920

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567921

### Logging and Error Reporting

#### Logging

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567922

Omar: Hello there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we learn about the Logs panel at the bottom of the Job Graph and Job Metrics pages, as well as the centralized error reporting page. Let's get started with logging. I've created and canceled a streaming pipeline that I designed not to work so we can see some error logs. I set the sync to a bucket I do not have access to. How can I get logging info about this job? Let's start by expanding the Logs panel at the bottom of the page. The first tab is the job logs tab. These are messages that are from the data flow service. We can filter to show a minimum log level. For example, if I want to see logs with a severity of error and above, I would select error. If I'm looking for a specific message, I can type it in the filter text box. The next tab I can click is the Worker Logs tab. Worker logs are from the VMs running the worker. Just like in the Job Logs tab, we can filter by log level and by message. If I want to see logs from a specific step, or any of its sub steps, I click on the sub step or step in the Jobs Graph page. Here I have selected the WriteShardedBundlesToTempFiles sub step that is part of the TextIO.Write step. If I want to return to looking at the general log view, I click on the whitespace outside of this step. This returns us to the Worker Logs tab. We now move on to the Diagnostics tab. This tab shows the frequency of each error across time in your entire project, as well as when it was first seen and last seen. Clicking on the error takes you to the error reporting page, which provides more detailed information. We talk about the error reporting page in the next module. Not only does a Diagnostics tab shows errors coming from user exceptions, it also provides job insights for you. During the pipeline's life, Dataflow analyzes the logs that are part of your job and highlights important ones in the Diagnostics tab. A few of these are listed on this slide. If the jar file provided to the worker were missing required classes, you will get a worker JAR file misconfiguration error message. The Diagnostic tab also shows if the worker VM had to kill a process or shut down due to the JVM crashing. If your code is running a step that is taking a long time to perform operations, you might see a lengthy operation message in the Diagnostic tab. If the slow processing is due to a hotkey, then the Diagnostic tab will show you that a hotkey was detected. In streaming scenarios, your pipeline might fail to process if you are grouping a huge amount of data without using a combined transform, or are producing a large amount of data from a single input element. If this happens, the Diagnostic tab will tell you that the commit key request exceeds the size limit. Finally, if there was a high rate of log messages from the job, and some of them were not sent to cloud logging, throttling logger worker will appear in your Diagnostic tab. In this example, I have a batch job that failed. Upon checking the Diagnostic tab, I can see that the JVM crashed due to memory pressure. Without sifting through the logs, I can find the cause of failure just by looking at the Diagnostics tab. If your pipeline reads from or loads data into BigQuery, there is one more tab that can be viewed. This is the BigQuery Jobs tab, and it can be used for troubleshooting and monitoring BigQuery jobs that are part of your pipeline. This tab will appear if you are using Beam 2.24 and larger and have the BigQuery admin role. Your Beam code can either read an entire BigQuery table or issue a query to read parts of a table. When the former is used, BigQuery exports the table as a JSON file to GCS using an extract job. When the latter is used, BigQuery exports the selected rows as JSON files to GCS using a query job. If either of the two read methods are used, they will appear in the BigQuery jobs tab. The BigQuery IO supports two methods of inserting data into BigQuery, load jobs and streaming inserts. By default, the BigQuery IO uses load jobs when you sync bounded p collections and streaming inserts when you sync unbounded p collections. Only load jobs will appear in the BigQuery Jobs tab. Let's look at a batch job that read and wrote data using BigQuery. The pipeline I ran read from a BigQuery table with stats on tornadoes computed the number of tornadoes in each month and outputted the results to a different BigQuery table. The pipeline read from BigQuery using an extract job and wrote to BigQuery using a load job. Let's view them in the BigQuery Jobs tab. First, select the location to pull BQ jobs from. BigQuery jobs run in the same location as the data set they read from or write to. Let's retrieve the jobs. Depending on how many BigQuery jobs the pipeline run, it may take a few minutes to retrieve the job list. As my job was quick, the jobs are retrieved almost immediately. As we can see, the pipeline ran two BigQuery jobs. The extract job read the BigQuery table and exported the results to GCS, and the load job wrote to BigQuery via GCS as well. If I want more detailed information about each job, I can press on the command line button, and a pop-up window will appear, showing a G Cloud command to run. Let us run it in Cloud Shell and view the results. Some of the statistics available are the destination URL, the table we are reading from, and the length of time the job took to run. We can also see how many bytes were read, the timeline of the job, and if it ran in a BigQuery reservation.

#### Error Reporting

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567923

person: Now let's talk about the Error Reporting page. In the last module, we showed the different types of tabs available in the logs pane. One of them was the diagnostic tab, which shows you a list of frequently occurring errors for your job. pressing on the error takes you to the Error Reporting page. Error Reporting aggregates and displays errors produced in your running cloud services. For Dataflow, Error Reporting shows you your most frequent or new errors that occur across all the Dataflow pipelines in your project. You can see how many times the error occurred across a specific timeframe by pressing on the different time ranges on the top right. You can view the jobs where the error occurred. We can see the error occurred in a couple of Dataflow jobs that I ran. If we scroll down, we can see the full stack trace for the error. If your company has an issue tracker, you can link this error reporting page to it by pasting your issue tracker link here. Once you've seen an error and have it handled, you can change its status from open to either acknowledged, resolved or muted. This is the end of this module. You should now be able to use the Dataflow logs and diagnostic widgets to troubleshoot pipeline issues.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567924

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567925

### Troubleshooting and Debug

#### Troubleshooting workflow

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567926

Omar: Hey there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we'll learn how to troubleshoot and debug Dataflow Pipelines. We will begin by looking at the general troubleshooting workflow for Dataflow. The general troubleshooting workflow involves two steps: first, checking for errors in the job, and second, looking for anomalies in the Job Metrics tab. Let us begin by looking at how to check for errors in a job. The most common first step is to look in the Dataflow Jobs page and notice the status of the job in question. If the job is in the failed state, we can then click into the job and dive deeper into the root cause. It is also important to note that not all problematic jobs will be in a failed state, it is possible that certain problematic jobs are still in the running state. In the Job Graph view page, the most common place to look for errors is the error notification above the job graph. If the job failed, then you will also see the individual step in the job graph that failed. More detailed error messages can be found by expanding the Log section, as shown below. Click the open icon to view the full logs in Cloud Logging. Cloud logging provides a simple UI to filter and search for logs within the job. The next part of this section involves looking for anomalies in the job using the Job Metrics tab. Data freshness and system latency are good indicators of the performance of a streaming Dataflow job. Increasing data freshness indicates that the Pipeline workers are unable to keep up with the data being ingested into the Pipeline. Increasing system latency indicates that a certain work item within the Pipeline is taking a long time to get processed. For all Dataflow jobs, the CPU utilization graph is a good indicator of the parallelism in a job and can also indicate if a job is CPU-bound. The latter half of the CPU graph shown here is a good example of limited parallelism in a Pipeline where only one or fewer workers have a high CPU utilization with others close to zero.

#### Types of troubles

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567927

person: Let's move on to the different types of troubles seen in data flow pipelines. Typically, a failed or problematic Apache Beam pipeline run can be attributed to one of the following causes; one, graph or pipeline construction errors when building the job graph. Two, errors in job validation by the Dataflow service. Three, exceptions during pipeline execution. And finally, slow running pipelines or lack of output that affect the performance of the pipeline. Let's start by looking at the first type of trouble. These types of errors occur while Apache Beam is building your Dataflow pipeline, and validating the beam aspects as well as the input/output specifications of your pipeline. These errors can typically be reproduced using the direct runner and can be tested against using unit tests. Keep in mind that no job will be created on the Dataflow service if there is an error while building the pipeline. The example shown here depicts an error where the pipeline code is performing an illegal operation that is checked and caught while building the code on the beam side. This message should be visible in the console or terminal window where you ran your Apache Beam pipeline. Let's move on to the second type of job troubles. Once the data flow service has received your pipelines graph, the service will attempt to validate your job. This validation includes the following, making sure the service can access your jobs associated cloud storage buckets for file staging and temporary output. Checking the required permissions in your Google Cloud project, making sure the service can access input and output sources such as files. If your job fails the validation process, you'll see an error message in the data flow monitoring interface, as well as in your console or terminal window if you are using blocking execution. The error displayed is an example of a situation in which the pipeline code passed its validation, but the pipeline was rejected by Dataflow due to lack of permissions in the project where the job was tried to run. These errors cannot be reproduced with the direct runner. They require the Dataflow runner and potentially the Dataflow service. To iterate quickly and protect against regression, build a small test that runs your pipeline or a fragment of it. Since the error does not depend on scale, run it on a tiny amount of data, so isn't costly. Let's move on to the third type of job troubles. While your job is running, you may encounter errors or exceptions in your worker code. These errors generally mean that the do functions in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job. Exceptions in your user code are reported in the Dataflow monitoring interface. You can investigate these exceptions using the general troubleshooting workflow described in the beginning of this module. The above screenshot shows that using cloud logging to see the error from the data flow interface gives us a more detailed stack trace on the exception. Consider guarding against errors in your code by adding exception handlers. For example, if you'd like to drop elements that fail some custom input validation done in a ParDo, handle the exception within your DoFn and drop the elements or return it separately. More details on this can be found in the best practices module of the developing pipelines with Dataflow course. You can also track failing elements in a few different ways. You can log the failing elements and check the output using cloud logging. You can check the Dataflow worker and worker startup logs for warnings or errors related to work item failures. And finally, you can have your ParDo write the failing elements to an additional output for later inspection. It is important to note that batch and streaming pipelines have different behaviors and handle exceptions differently. In batch pipelines, the Dataflow service retries failed tasks up to four times. In streaming pipelines, your failed job may stall indefinitely. You will need to use other signals to troubleshoot your job. High data freshness, job logs, cloud monitoring, metrics for pipeline progress, and error accounts. Let's move on to the final type of job troubles. Multiple factors such as pipeline design, data shape, interactions with sources, sinks, and external systems can affect the performance of a pipeline. These will be covered in further detail in the performance module of this course. The user interface provides useful information to debug performance problems at a step level. Use the user interface to identify expensive steps. The step info section can provide useful information including wall time, input elements, input bytes, output elements, and output bytes. Wall time for a step provides the total approximate time spent across all threads in all workers on the following actions: initializing the step, processing data, shuffling data, and ending the step. The input element count is the approximate number of elements that the step received, and the estimated size provides the total volume of data that was received. Similarly, the output element count is the approximate number of elements produced by the step, and the estimated size provides the total volume of data that was produced. This is the end of this module. You should now be able to use a structured approach to debug Dataflow pipelines and examine common causes for pipeline failures.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567928

#### Serverless Data Processing with Dataflow - Monitoring, Logging and Error Reporting for Dataflow Jobs

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567929

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567930

### Performance

#### Pipeline Design

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567931

Hi, I am Ajay, I am a Strategic Cloud Engineer at Google. In this module, we will discuss performance considerations we should be aware of while developing batch and streaming pipelines in Dataflow. We will begin with some general pipeline design considerations, then we will discuss effects of data shape on performance of a pipeline. Next we will see impact of external systems on a dataflow pipeline. In the end we will wrap up this section with insights of Dataflow specific performance optimization options. Let’s begin with pipeline design decisions which impacts performance of a pipeline. We might sometimes underestimate simple considerations that are critical to a pipeline’s performance. Filtering data early might be considered one such option. It is recommended to place transformations that reduce the volume of data as high up on the graph as possible. This includes placing them above window operations, even though the Window transform itself does nothing more than tag elements in preparation for the next aggregation step in the DAG. Choose coders that provide good performance. For example, in the Java SDK, do not use SerializableCoder. Choose a more efficient coder, for example ProtoCoder or Schemas. Pro Tip: Encoding and decoding are a large source of overhead. Thus, if you have a large blob but only need part of it, you could selectively decode just that part. For example, 'com.google.protobuf.FieldMask' in Protobufs enables reading specific bits of information without deserializing whole blob. If your pipeline has large windows aggregating large volumes of data, you can create smaller Window + Combine patterns before the main sliding window to reduce the volume of elements to be processed when the window slides. Runners may support fusion as part of graph optimization. Such optimizations can include fusing multiple steps or transforms in your pipeline's execution graph into a single phase. There are a few cases in your pipeline where you may want to prevent the Dataflow service from performing fusion optimizations. Before we discuss graph optimization further let’s first understand what a fanout transformation is. In a fanout transformation a single element can output hundreds or thousands of times as many elements. For example in this diagram, a single input element (key1 and value1) outputs multiple output elements. The primary example where fusion is not desirable is a large fanout transform. To prevent Fusion you can insert a Reshuffle after your first ParDo. The Dataflow service never fuses ParDo operations across an aggregation. Alternatively you can pass your intermediate PCollection as a side input to another ParDo. The Dataflow service always materializes side inputs. One of the common service ticket resolutions for performance comes from that old favorite: too much logging! In the Dataflow runner, logs are sent from all workers to a central location in Stackdriver. A thousand machines, all pushing hundreds of logs per second, can cause massive back pressure! Log.info should almost always be avoided against PCollection element granularity. These will rarely be useful in logs. Log.error should also be carefully considered. Using a dead letter pattern followed by a count per window of 5 minutes may be better suited for reporting data errors.

#### Data Shape

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567932

In this section we will discuss the effects of data shape on a pipeline’s performance. Unique characteristics of data you are processing also effects the performance of Dataflow pipelines. For example, data skew often results in unbalanced data processing. As discussed in the “Serverless Data Processing with Dataflow” course, operations like groupByKey merge multiple PCollections into one. During a GroupByKey or combine operation, keys will be shuffled to workers. All values related to one key will be sent to the same machine throughout the process. This can be a problem when the data you are processing is skewed. For example, columns used as keys that are @nullable often end up being hot keys. To mitigate the hot key issue, we can use one of the following three techniques. The first one is to use the helper API “withFanout(int)”. This allows for the definition of intermediate workers before the final combine step. Another similar API is withHotKeyFanout(Sfn). It is available for Combine.perkey and allows for a function to determine intermediate steps. Using Dataflow Shuffle for batch or Streaming Service also alleviates this issue. The Dataflow shuffle or streaming service offloads the shuffle operation to a backend service. This means that shuffle operation is not constrained by resources available on a single worker machine. The Dataflow service makes it easier to detect and surface hot keys. To do so, set “hotKeyLoggingEnabled” flag to true. Enabling this flag will print the specific key that is your bottleneck, which can help Dataflow developers to implement custom logic for that specific key. Without the flag, Dataflow will print if they think they've detected a hot key, but cannot reveal what that key is. Key space used in your pipeline also has an impact on its performance. For example, the maximum amount of parallelism is determined by the number of keys. More machines will not be able to do any more work if key space is limited. Below are some general guidelines regarding key space: Too few keys is bad for performance. Limited keyspace will make it hard to share workload, and per-key ordering will kill performance. Too many keys can be bad too as overhead starts to creep in. If the key space is very large, consider using hashes separating keys out internally. This is especially useful if keys carry date/time information. In this case you can "re-use" processing keys from the past that are no longer active, essentially for free. Pro Tip! If windows are distinct, the window can be added as part of the key to shard work across more workers. Adding the window to the key improves the ability of the system to parallelize processing since those keys can now be processed in parallel on different machines since they are now recognized as unrelated.

#### Source,  Sinks & external systems

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567933

SPEAKER: In this section, we'll discuss the impact of external systems on a Dataflow pipeline. In the Dataflow service, most sources and sinks abstract the user from the need to deal with read stage parallelism. Sometimes this hides underlying issues that impact a pipeline's performance. For example, if you are reading gzip files via TextIO, gzip files can't be read in parallel. A single thread will deal with each file. This will have three negative effects. First one is that only one machine can do the Read I/O operation. After the read stage, all few stages will need to run on the same worker that read the data. In any shuffle stage, a single machine will need to push all the data from the file to all other machines. This single-host network becomes the bottleneck. Switch to uncompressed files while using TextIO or switch to compressed Avro format. Beam runners are designed to be able to rapidly chew through parallel work, pin up many threads across many machines to achieve this goal. This can easily swamp an external system. This is an issue for both batch and streaming pipelines. The effect on the external system are often more pronounced in batch or during backlog processing in a streaming pipeline. To alleviate this issue, make use of a batch mechanism in the call to external system and use a mechanism like group into batches, transforms, or start bundle and finish bundle. You should also provision external services to handle the peak volume of the data flow pipeline. While working in cloud, it's sometimes easy to forget the impact of the simple choices we make while developing applications. Co-location is one such aspect. Using services and resources from the same region usually means relatively lower latency for interservice communication. This lower latency may result in significant performance gains, especially when the pipeline involves significant interaction with external services like BigQuery, Bigtable, or any other service outside of Dataflow.

#### Shuffle and streaming engine

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567934

Now let’s explore some Dataflow-specific performance optimization options. Dataflow Shuffle is the base operation behind Dataflow transforms such as GroupByKey, CoGroupByKey, and Combine. The Dataflow Shuffle operation partitions and groups data by key in a scalable, efficient, fault-tolerant manner. Currently, Dataflow uses a shuffle implementation that runs entirely on worker virtual machines and consumes worker CPU, memory, and persistent disk storage. The service-based Dataflow Shuffle feature, available for batch pipelines only, moves the shuffle operation out of the worker VMs and into the Dataflow service backend. The service-based Dataflow Shuffle has the following benefits: Faster execution time of batch pipelines for the majority of pipeline job types. A reduction in consumed CPU, memory, and persistent disk storage resources on the worker VMs. Better autoscaling, since VMs no longer hold any shuffle data and can therefore be scaled down earlier. Better fault tolerance. An unhealthy VM holding Dataflow Shuffle data will not cause the entire job to fail, as would happen if not using the feature. Dataflow Shuffle and the Streaming Engine feature offloads the window state storage operation from the persistent disks (PDs) attached to workers, to a backend service. It also implements an efficient shuffle for streaming cases. The Dataflow Shuffle service is applicable to batch pipelines, while the Streaming Engine service is built for streaming pipelines. No code changes are required to get the benefits of these features. Worker nodes continue running your user code that implements data transforms, and transparently communicate with the Streaming or Shuffle engine to store the pipeline state. Many scalability and autoscaling issues can be resolved by enabling Shuffle and Streaming Engine for your batch and streaming pipelines, respectively. This is the end of this module. You should now be able to: Understand performance considerations for pipelines, and Consider how the shape of your data can affect pipeline performance.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567935

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567936

### Testing and CI/CD

#### Testing and CI/CD Overview

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567937

Hi! My name is Vince Gonzalez, and I am a data engineer with Google Cloud. This module will introduce frameworks and features available to streamline your CI/CD workflow for Dataflow pipelines. In this module, we'll cover an overview of testing and CI/CD. We'll discuss unit testing your Beam pipelines, integration tests, artifact building, and considerations around deploying your pipelines. Let's get into the overview. Software engineers are no stranger to application lifecycle management. After all, that is how we keep applications fresh and up to date. Dataflow pipelines are no different. Dataflow pipelines are authored according to well-understood best practices within software engineering. First, Dataflow pipelines need a comprehensive testing strategy. So we should be implementing unit tests, integration tests, and end-to-end tests to ensure that our pipeline behaves as we expect. The approach to deployment should also be well structured. A haphazard rollout can result in corrupted data being written to the sink or disruptions to your downstream applications. Finally, data engineers should strive to validate changes made to pipeline logic, and have a rollback plan if there is a bad release. While all these considerations are similar to general application development, there are some key differences to point out. Data pipelines often aggregate data, and this makes them stateful, in that they must accumulate the result of some aggregation over time. This means that if you need to update your pipeline, you need to consider any state that may exist in the pipeline you're updating. We'll discuss this in more detail later, but when you change your pipeline, you'll need to account for existing state, as well as any changes to the pipeline logic and topology. Changes you make need to be compatible with the pipeline you're updating. If they are not, you will have to devise alternate migration strategies that might require reprocessing data. If you do roll out a bad configuration, you could be dealing with more than just an unpleasant experience for end users. If your pipeline makes non-idempotent side effects to external systems, you will have to account for those effects after a rollback. This raises the stakes for ensuring safe releases. Now that we understand some of the challenges that come with testing and deploying data processing applications, let’s take a look at what testing and CI/CD look like with Beam and Dataflow. Testing in Beam is summed up well by this diagram. You can read this from the center out, starting with the Beam pipeline itself, and some hand crafted test inputs, then moving to other PTransforms and DoFn subclasses, before considering integration testing, which involve real data sources and sinks. So let's talk about Unit tests. All pipelines revolve around transforms, and the lowest level we typically deal with in Beam is the DoFn. Since these are essentially functions, we validate their behavior with unit tests that operate on input datasets. They produce output datasets that we validate with assertions. Similarly, we can provide test inputs to the entire pipeline, which might contain our DoFns as well as other PTransforms and DoFn subclasses. We also assert that the results of the entire pipeline are what we expect. For system integration tests, we incorporate a small amount of test data using the actual I/Os. This should be a small amount of data, since our goal is to ensure the interaction with the IOs produces the expected results. Finally, end-to-end tests use a full testing dataset, which is more representative of the data our pipeline will see in production. Whatever tooling you're using in your CI/CD testing environment, you'll make use of the Direct Runner, which runs on your local machine, and your production runners, which run on the cloud service of your choice, like Dataflow. The Direct Runner will be used for local development, unit tests, and small integration tests with your data sources. You'll use your production runner when it's time to do larger integration tests, when you want to test performance, and when you want to test pipeline deployment and rollback. More broadly, the CI/CD lifecycle looks something like this. It's iterative, and moves through a cycle of development, building artifacts and testing, followed by deployment. In the development part of the lifecycle, we write our code, executing unit tests locally using the direct runner and executing integration tests using the Dataflow runner. As we develop and test, we're committing to source repositories along the way. These commits and pushes trigger the continuous integration system to compile and test our code in an automated manner, using Cloud Build or a similar CI system. Once the builds complete successfully, artifacts are deployed, first to a preproduction environment where end-to-end tests are run. If these succeed, we deploy to our production environment.

#### Unit Testing

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567938

person: Now that we've gotten the overview of the lifecycle, let's take a look at unit testing in some detail. Unit tests are fundamental to all software development, and beam is no exception. We use unit tests in Beam to assert behavior of one small testable piece of your production pipeline. These small portions are usually either individual DoFns or PTransforms. These tests should complete quickly, and they should run locally with no dependencies on external systems. To get started with unit testing in Beam Java pipelines, we need a few dependencies. Beam uses JUnit 4 for unit testing. And here you can see an example of the dependency section of a Maven POM for a Beam pipeline. In a regular Beam pipeline, the pipeline object represents your pipeline. Test pipeline is a class included in the beam SDK specifically for testing transforms. So when writing tests use TestPipeline in place of Pipeline where you would create a Pipeline object. Unlike Pipeline.create, TestPipeline.create handles setting pipeline options internally. PAssert is another class provided as part of Beam that lets you check the output of your transforms. Assertions on the contents of a PCollection are incorporated into the pipeline. These assertions can be checked no matter what kind of pipeline runner is used. PAssert becomes part of your pipeline alongside the transforms. PAssert works on both local and production runners. Putting this all together, unit tests help you ensure the correct functioning of your pipeline. Your pipeline is made up of DoFns sub classes and composite transforms, which might combine multiple DoFns. Unit tests lets you provide known input to your DoFns and composite transforms, then compare the output of those transforms with a verified set of expected outputs. The Apache Beam SDK provides a JUnit rule called TestPipeline for unit testing individual transforms like your DoFns subclasses, composite transforms, like your PTransform subclasses, and entire pipelines. You can use TestPipeline on a Beam pipeline runner such as the direct runner or the Dataflow runner to apply assertions on the contents of PCollection objects using PAssert as shown in the code snippet here. In this minimal example, we instantiate a TestPipeline, then create a test PCollection containing some data, then we assert that the PCollection contains the data we expect in any order. One thing to keep in mind when you're developing your pipeline is that it's an anti-pattern to design your pipeline too much around anonymous subclasses of DoFn. Anonymous subclasses make it impossible to test the correctness of the transform without duplicating the code in the test, which will quickly become a challenge to maintain. Anonymous classes are also not as reusable as named subclasses would be. So prefer named subclasses to anonymous ones. When we compare the right-hand code block to the anti-pattern in the left block, we see that the DoFns are now named subclasses. In other words, rather than putting the DoFn code in line in our ParDo, we create an instance of the Transform. Named subclasses are easily testable, so we can validate their behavior independently without having to execute the entire pipeline. In addition to the functionality of our transforms, we can and should test our assumptions about how window transforms will behave. Beam provides a create.timestamped method which can be used to create timestamped elements in a testing PCollection. You can manipulate the timestamp directly as we do in this example by adding the window duration to the timestamp of the last element. Then in the right-hand code block, you can see that we apply fixed windows of window duration and perform a count on the windowed elements. We can then assert that the resulting PCollection contains the windowed calculations we expect. Note that windowing takes place in both batch and streaming pipelines. Testing how your window transforms behave is useful in both types of pipelines. Speaking of streaming pipelines, let's talk about how to test those. Test stream is a testing input that generates an unbounded PCollection of elements advancing the watermark and processing time as elements are emitted. After all of the specified elements are emitted, test stream stops producing output. Each call to a TestStream.builder method will only be reflected in the state of the pipeline after each method before it has completed and no more progress can be made by the pipeline. The pipeline runner must ensure that no more progress can be made in the pipeline before advancing the state of the test stream. For streaming pipeline tests, we'll use TestStream to create a pipeline object that enables you to model the effect of element timings. Let's take a look at what this looks like in code. To use TestStream you first create a TestStream instance. Then you add timestamped elements to it. You can manipulate the timestamps in the TestStream by adjusting the timestamp object and manipulate the position of the watermark using an instant object. Advancing the watermark to infinity closes all windows so that you can perform your windowed calculation and assert on the result. Test stream is supported by the direct runner and the Dataflow runner. Use both to carry out your streaming pipeline tests. We can also test more complex streaming interactions. In this example, we're asserting on the presence of certain elements in particular panes of a window. We also ensure that for all the panes none have less than three elements or more than five.

#### Integration Testing

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567939

person: Now that we've gotten through the overview of unit testing, let's take a look at integration testing in some detail. An actual pipeline reads from two data sources and writes to BigQuery. Integration tests create a smaller amount of data, and assert that the output of the transforms are what we expect. For large integration tests, we work with data on closer to a production scale. To do this, we can clone data from a production project to a non production project. This diagram looks at a batch pipeline reading from two sources on cloud storage and writing to BigQuery. We can use the storage transfer service to copy cloud storage data. We can copy a BigQuery data set or even work with the production data set is read only. Let's take a look at large integration testing for streaming pipelines. One of the nice things about streaming data sources like cloud pub sub is that you can easily attach extra subscriptions to a topic. This comes at an extra cost. But for any major updates, you should consider cloning the production environment and running through the various lifecycle events. To clone the pub sub stream, you can simply create a new subscription against the production topic. You may also consider doing this activity on a regular cadence, such as after you have had a certain number of minor updates to your pipelines. The other option this brings is the ability to carry out AB testing. This can be dependent on the pipeline and on the update. But if the data you're streaming can be split, for example, on entry to the topic, and the syncs can tolerate different versions of the transforms, then this gives you a great way to ensure everything goes smoothly in production. And integration tests, we typically test the entire pipeline without sources and sinks. In this example, we see a p transform subclass called weather stats pipeline that summarizes integers representing weather data. We create a test pipeline instance and test weather stats pipeline by creating a p collection of integers and asserting that the result of the pipeline transformations match the data we expect.

#### Artifact Building

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567940

person: In this section we'll review artifact building. Specifically, we'll cover how we should package Apache Beam with the rest of our pipeline artifacts. Apache Beam uses semantic versioning. Version numbers use the form major dot minor dot incremental and are incremented as follows. Major versions are incremented for incompatible API changes. Minor versions are incremented for new functionality added in a backward incompatible manner. Finally, the incremental version is incremented for forward compatible bug fixes. Build artifacts necessary for your Java pipelines are available on Maven Central. There are lots of packages available for beam and you'll usually need more than just the core. The build system has Maven or Gradle. Here we're showing snippets of a Maven palm. It's typical that you'll need to pull in more than just the core. Notice here that we're pulling in other dependencies for the Dataflow runner and for GCP IOs. We recommend that you use beam 2.26 and higher Versions from 2.226 use the Google Cloud libraries bomb to specify Google related library versions which reduce the potential for dependency conflicts.

#### Deployment

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567941

Having tested, built, and established our environment, let's talk about deployment. We’ll look at three distinct stages of the pipeline lifecycle: deployment, in-flight actions, and termination. Let’s begin with deployment. There are two ways to deploy your Dataflow job. We can use a direct launch, in which we run the pipeline directly from the development environment. For Java pipelines this means running the pipeline from Gradle or Maven. For Python, this means running the python script directly. This method can be used for both batch and streaming pipelines. We can also use templates. Templates allow us to launch a pipeline without having access to a developer environment. Templates are built and deployed as an artifact on Cloud Storage, and can be used for both batch and streaming pipelines. Separating the development and execution environments makes it easy to automate your Dataflow deployments and enable non-technical users. If you're using an external scheduler, like Airflow, you'll be able to use Airflow's built-in support for Dataflow, which calls a template when invoked. You would supply your pipeline options via the operator arguments. More generically, if you're orchestrating the deployment via CLI tooling, you can provide the options via gcloud commands. You might notice that Dataflow SQL is not on this list. Dataflow SQL is a special case of a templated deployment—it’s actually a user interface built on top of a flex template. When we're deploying our pipeline for the first time, we need to submit the pipeline to the Dataflow service with a unique name. This name will be used by the monitoring tools when you look at the monitoring page in the console. Next, we’ll move onto pipeline actions that can be taken on in-flight pipelines. These actions are only available to streaming pipelines, since batch jobs can simply be relaunched. As a streaming pipeline processes data, it accumulates state. It's useful to have ways to preserve the state so that we can manage changes to our pipeline without the risk of permanent data loss. We can use snapshots for this. With snapshots, we can save the state of an executing pipeline before launching a new pipeline. This way, if we need to roll our pipeline back to a previous version, we can do that without losing the data processed by the version being rolled back. Since streaming pipelines are long-running applications, we’re likely to need to modify our pipeline from time to time. Now that we’ve saved the state of the running pipeline with a snapshot, we can safely update it. When you update a job on a Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated job ID. If for any reason you are not happy with how the replacement job is running, you can roll back to the prior version by creating a job from a snapshot. Let’s explore these two actions a little more deeply. Dataflow snapshots are useful for several scenarios. As mentioned, Dataflow Snapshots provides a copy of intermediate state of your pipeline at the moment the snapshot is taken. You can use that snapshot to validate a pipeline update, or use it as checkpoint for you to roll back your pipeline in the event of an unhealthy release. You can also use Snapshots for backups and recovery use cases. We’ll explore this in more depth in the Reliability module. Lastly, Snapshots create a safe path for migrating pipelines to Streaming Engine. If you want to reap the benefits of smoother autoscaling and superior performance, you can take a snapshot and create a job from that snapshot. The new job will run on Streaming Engine. The flip side of this is that jobs created from Snapshots cannot be run with Streaming Engine disabled. Let’s see how we use Dataflow Snapshots. We’ll cover two Snapshot workflows: creating snapshots, and creating a job from a snapshot. We can create a snapshot in the UI or using the CLI. We can navigate to the Job Details page of the pipeline of interest. You’ll see a Create Snapshot button to initiate the snapshot. After you click on the button, you’ll be prompted to select if your snapshot will or will not be created with a source snapshot. If you are using Pub/Sub, creating a snapshot with source will allow you to create a coordinated snapshot between your unread messages and accumulated state. This makes it easier to roll back your pipeline to a known point in time. The pipeline will pause processing while the snapshot is being taken. Depending on how much state is buffered, it could take a matter of minutes. We recommend planning to take snapshots during periods of the day when latency can be tolerated, such as non-business hours. You can also create snapshots using the CLI or API. These methods allow you to automate snapshots of your Dataflow pipelines, which lends itself nicely to scheduling snapshots on a weekly cadence. To create a job from a snapshot, we have to pass in two extra parameters, as seen in the sample command here. We have to enable Streaming Engine with the --enableStreamingEngine flag. Secondly, we pass in the Snapshot ID into the createFromSnapshot parameter. If you are creating a job from the snapshot for a modified graph, the new graph must be compatible with the prior job. We’ll discuss update compatibility shortly. Now that we’ve snapshotted our pipeline, we’re ready to update our pipeline. There are various reasons why you might want to update your Dataflow job: One is to enhance or otherwise improve your pipeline code. Another is to fix bugs in your pipeline code. You might also want to update your pipeline to handle changes in the data format. Finally, you might want to change your pipeline to account for version and other changes in your data source. To update your pipeline, you’ll need to do a couple of things. First, you need to pass the "update" and "jobName" options when you submit the new pipeline. You’ll have to set jobName to the name of the existing pipeline, or else the old job will not be replaced. This tells Dataflow that you're updating the job, rather than deploying a new pipeline. Second, if you added, removed, or changed any transform names, you'll need to tell Dataflow about these changes by providing a transformNameMapping. The replacement job will preserve any intermediate state data from the prior job. Note, however, that changing the windowing or triggering strategies will not affect data that's already buffered or already being processed by the pipeline. "In-flight" data will still be processed by the transforms in your new pipeline. Additional transforms that you add in your replacement pipeline code may or may not take effect, depending on where the records are buffered. Updates can also be triggered via the API. This can enable continuous deployment contingent on other tests passing. When you update your job, the Dataflow service performs a compatibility check between your currently running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job. This means that some changes are not possible with streaming update. Let’s review the most common compatibility breaks. Modifying your pipeline without providing a transform mapping will fail the compatibility check. When you update a job, the Dataflow service tries to match your transforms from your prior job to your new job so that intermediate state data for each step can be fully processed. If your changes have renamed or removed any steps, you will have to provide a transform mapping so that Dataflow can match the state. Adding or removing side inputs will also cause the check to fail. Changing coders. The Dataflow service isn’t able to serialize or deserialize records if your updated job uses different data encoding. Running your job with a new zone and a new region will also cause your compatibility check to fail. Your replacement job must run in the same zone in which you ran your prior job. Lastly, removing stateful operations. Dataflow fuses multiple steps together for efficiency, but if you’ve removed a state-dependent operation from within a fused step, the check will fail. If your pipeline requires any of these changes, we recommend draining your pipeline, then launching a new job with the updated code. Now that we’ve discussed actions you can take on your streaming pipeline, we’ll discuss two ways that you can terminate your pipeline. First, we start with drain. Selecting drain will tell the Dataflow service to stop consuming messages from your source and finish processing all buffered data. After the last record is processed, the Dataflow workers are torn down. This action is only applicable to streaming pipelines. Secondly, we can cancel the job. Using the Cancel option ceases all data processing and drops any intermediate, unprocessed data. We can cancel both batch and streaming jobs. Let’s take a closer look at both of these options. We can terminate our job from the UI. When you navigate to the Job Details page of your job, you will find a Stop button in the menu bar. You’ll be prompted between two options for stopping your job: canceling your job, or draining your job. Let’s explore each of these options. When we drain the pipeline, it will stop pulling data from the source, and it will finish processing data that has already been read into the pipeline. This provides an advantage from cancelling your job outright, since no record is dropped. When you relaunch your streaming job, it will continue processing unacknowledged messages from your source. However, when a streaming pipeline is drained, the watermark is moved to infinity, which closes all windows. Closing all the windows in this way will result in incomplete aggregations, since draining the pipeline will not wait for open windows to be closed before stopping pulling from the source system. Consider the impact of incomplete aggregations on downstream systems when draining your pipelines. Beam attaches a PaneInfo object that provides information about the pane an element belongs to, as every pane is implicitly associated with a window. You can use PaneInfo to identify incomplete windows and choose to write that data elsewhere, which can save you the hassle of reconciling incomplete aggregations with your production dataset. When you cancel a job, Dataflow will immediately begin shutting down the resources associated with your job. The pipeline will stop pulling and processing data immediately, which means you may lose any data that was still being processed when the pipeline was canceled. If your use case can tolerate data loss, then cancelling your job will fit your purpose. So, to summarize the lifecycle of a streaming pipeline, let's review our deployment options. First, if it's the first time deploying the pipeline, there's no existing state to consider. So you just deploy the pipeline. If there's an existing pipeline, and you want to update it, you should take a snapshot of your pipeline. This ensures that you have a working state you can revert your pipeline to if you observe an issue with your new deployment. Once you’ve taken a snapshot, you’re ready to update your job. You need to account for any changes to the names of the pipeline's transformations by providing the mapping from old names to the new. If the updated pipeline is compatible, the update will succeed, and you'll get a new pipeline in place of the old, without losing the state of the previous version of the pipeline. If the update is not possible, then you’ll need to choose between drain and cancel options. If you can replay the source, then you can choose to cancel the pipeline, which will drop any in-flight data. You can then deploy the new pipeline, and replay the data from the source. Note that we cannot use a Dataflow snapshot if the pipeline modifications are not update compatible, but taking a snapshot of the source with a Pub/Sub snapshot, you can minimize unnecessary reprocessing. If replay is not possible, then you can drain the pipeline. This will not lose data, but you may end up with incomplete aggregations in your output sink. Your downstream business logic should inform the appropriate approach to handling this. Once the pipeline has been drained, you can relaunch the pipeline. This is the end of the module. You should now be able to: Execute test approaches with your Dataflow pipeline, Snapshot and update your Dataflow pipelines, And drain and cancel your Dataflow pipelines.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567942

#### Serverless Data Processing with Dataflow - Testing with Apache Beam (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567943

#### Serverless Data Processing with Dataflow - Testing with Apache Beam (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567944

#### Serverless Data Processing with Dataflow - CI/CD with Dataflow

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567945

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567946

### Reliability

#### Introduction to Reliability

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567947

Federico: Hi, I'm Federico Patota, a cloud consultant here at Google. In this module, we will learn how to implement reliability in Dataflow pipelines. There are different approaches for reliability based on the type of pipeline you are running. Batch jobs are simple. If a batch job does not launch or if it fails during execution, you can always rerun the job. Source data is not lost and partial data written to sinks can be rewritten, if it was written at all. Streaming jobs, on the other hand, are more complex. Streaming jobs are continuously processing data and behave like a long-lived application. Thus, reliability is of the utmost importance. You must be vigilant for various failure modes, and when a failure inevitably occurs, you must act fast to minimize data loss and downtime. Most of the reliability best practices in this specific module are for streaming pipelines. In particular, the second half of this module focuses on disaster recovery and high availability configurations. With that being said, lessons covered in the monitoring and geolocation sections are also relevant for your batch workloads. We can classify the pipeline's failures in two broad categories, failures related to user code and data shapes, and failures caused by outages. Software bugs are a reality of software engineering, including data processing applications. Transient errors and corrupted data can impact your data processing jobs. So it's important to know how to mitigate the adverse effects that can be produced unintentionally by software bugs. Dataflow sits at the center of multiple Google Cloud services. That also means that Dataflow is susceptible to various outage modalities, including service outages, zonal outages, and regional outages. If a network service is down, Dataflow will likely be impacted by it. Similarly, if Compute Engine instances are inaccessible in a particular zone or region where Dataflow workers are running, the data processing jobs will be affected. Since Dataflow is often connected between different parts of a customer application, running on GCP, users need to be especially vigilant for any service disruptions. In the following modules, we will discuss different strategies to mitigate the risk of these incidents and increase the reliability of your Dataflow workloads.

#### Monitoring

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567948

person: Now we will discuss how a good monitoring strategy can improve the reliability of your Dataflow workloads. First, we start with a reminder that for batch jobs, tasks with failing items are retried up to four times. After four failures, the job will fail. As mentioned before, a batch job can be rerun with little fear of data loss or interruption of existing services, as long as this is run within a user desired service level objective or resolution. For streaming pipelines, failing work items will be retried indefinitely. The rest of this module, we will discuss techniques to prevent your streaming workloads from being stuck forever. Erroneous records may cause your pipeline to get stuck or fail outright. As described in previous modules, we highly recommend implementing a dead-letter queue, and error logging to prevent these failure modes. This can help catch problems in your code or in data shapes. This code snippet shows an example of a pattern written in Java, There are a few things to know here. We wrapped user code inside a process element function with a try catch block. Inside of the catch block, we do not log every error or exception, as it may overwhelm the whole pipeline. Instead, we send the erroneous record to an alternative dead-letter sink. We use tuple tags so that we can write to multiple outputs in the resulting p collection. This helps us write to downstream p collections as well as send raw data to a persistent storage medium like BigQuery or cloud storage, so that we can inspect them offline. To maximize the reliability of your workloads, it is essential to implement a robust monitoring and alerting strategy. Monitoring and alerting policies can help you catch issues with your data processing before they bring down production systems. It lets you combine different types of metrics and observe important Service Level Indicators or SLS of pipeline performance. If you are comparing those SLIs against acceptable threshold, monitoring can give you critical insights for early detection of potential issues. Dataflow provides a web based monitoring interface that can be used to view and manage jobs. You can create metrics based alerts with a couple of clicks. We've covered this in our monitoring module. In addition, data flows integration with cloud monitoring provides extensive flexibility for pipeline monitoring. You can collect custom metrics that point to health conditions that are relevant for your use case, like the number of erroneous records that have been detected. The possibilities are endless with Dataflow's monitoring integration. For batch workloads, you might be interested in the overall runtime of your job. If the job runs on a recurring schedule, you might want to ensure that the job completes successfully within a given period of time. Some variance in pipeline execution time is expected across runs due to a variety of factors, but if they are violating your service level objectives, or SLOs, you need to be notified right away. With cloud monitoring, you can track the elapsed time for a job and create an alert that goes off if the elapsed time exceeds a threshold that is equivalent to your SLO. This can be entirely done using its integration with Dataflow. For streaming pipelines, you want steady and sustained data processing. Dataflow provides standard metrics like data freshness and system latency that make it easy to track whether your pipeline is falling behind. You can create an alert with a couple of clicks from the Dataflow monitoring UI that will be triggered if this selected metrics fall behind the specified threshold. This is an example of a simple alerting policy. You can combine it with custom metrics or with other statistics to determine the failure condition that matters to your workload. These alerts are essential for improving the reliability posture of your Dataflow pipelines.

#### Geolocation

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567949

person: In this section we'll talk about how selecting the location of your data for processing can impact the reliability of your pipeline. Dataflow is a regional service. When a user submits a job to a regional endpoint without explicitly specifying end zone, the Dataflow service routes the job to a zone in the specified region based on resource availability. In other words, Dataflow will pick the best zone for the job based on available capacity. If you explicitly specify a zone, you will not get this benefit. If a job submission fails due to a zone issue, retrying without explicitly specifying a zone will usually fix the issue. This is a helpful technique in the event of a zonal outage. Note that you cannot change the location of a job after you got started. If it is a streaming job, you will have to drain or cancel the pipeline first before launching it again. This applies to when you relaunch a job in the same region without the zone specified or if you choose to relaunch a job in an entirely different region. When thinking about the locations of your Dataflow job, there are three elements to be aware: Your sources, your processing, and your sinks. You should always locate your resources in the same region. For an additional layer of reliability, you can also elect to use multiregional sources and sinks. Services like Google Cloud Storage, BigQuery, and Pub/Sub provide geo-redundant options that make your data seamlessly accessible in multiple regions. Dataflow processing can only occur in one region. But in the event of a regional outage, using multiregional sources and sinks allows you to move your data processing to a different region without suffering from performance penalty. You should try to avoid any configurations that have critical cross-region dependency. If you have a pipeline that has a critical dependency on services from multiple regions, your pipeline is likely to be affected by a failure in any of those regions. For example, a pipeline that is reading from my Cloud Storage bucket in us-central1 and writing for BigQuery table in us-east4 could go down if either one of those two regions are down.

#### Disaster Recovery

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567950

SPEAKER: In this video, we will look at disaster-recovery methods with Dataflow. These methods only apply to streaming pipelines. Data is your most prized asset, which is why it is essential to have a disaster-recovery strategy in place for your production systems. One way is to take snapshots of your data source. This capability is supported in many popular relational databases and data warehouses. But what if you are using a messaging application? Pub/Sub offers this capability. You can implement the disaster-recovery strategies with two features-- Pub/Sub Snapshots, which allows you to capture the message acknowledgment state of a subscription, and Pub/Sub Seek, which allows you to alter the acknowledgment state of messages in bulk. If you are using this strategy, you will have to reprocess messages in the event of a pipeline failure. This means you will have to consider how to reconcile this in your data sink and the duplicate sum records that have been written twice. Let's go over what we need to do to use Pub/Sub Snapshot to support our disaster-recovery requirements. First, you should take snapshots of the Pub/Sub subscription. To do this, you can use the Command Line Interface, CLI in short, or the Cloud console. After your Pub/Sub snapshot has been created, you can stop and drain your Dataflow pipeline. You can do this using the command line interface or in the Job Details page of the Dataflow UI. Once your pipeline has stopped processing messages, you can use Pub/Sub's Seek functionality to revert the acknowledgment of messages in your subscription. Again, you can achieve this using the command line tool. Finally, you are ready to resubmit your pipeline. You can launch your pipeline using any of the ways that you use to deploy your Dataflow job, either directly from your development environment or by using the command line tool to launch a template. The example on this slide shows a sample command for a templated job that is being launched with the command line interface. An important caveat to consider is that Pub/Sub messages have a maximum data retention of seven days. This means that, after seven days, a Pub/Sub Snapshot no longer has any use for your stream processing. If you choose to use Pub/Sub Snapshots for your disaster recovery, we recommend that you take snapshots weekly, at a minimum, to ensure that you do not lose any data in the event of a pipeline failure. Using Pub/Sub Snapshots in conjunction with Seek is a good starting point. But when you are using Pub/Sub and Dataflow for your streaming analytics, there are important things to consider. When you use Pub/Sub Seek to restart your data pipeline from a Pub/Sub Snapshot, messages will be reprocessed. This creates a few challenges. First, you might observe duplicate records in your sink. The amount of duplication depends on how many messages were processed between the time of when the Snapshot was taken and the time the product line was terminated. In addition to that, data that has been read by your pipeline but yet to be processed and written to sink will need to be processed over again. Remember that Dataflow acknowledges a message from Pub/Sub when it has read the message not when the record has been written to the sink. This presents a challenge for pipelines with complex transformation logic. For example, if your pipeline is processing millions of messages per second and goes through multiple processing steps, having to reprocess the data represents a significant amount of lost compute. Lastly, if your pipeline has implemented exactly once processing, windowing logic will be interrupted when you drain and restart your pipeline. Since you have to lose the buffered state when you drain your pipeline, you must conduct a tedious reconciliation exercise if exactly once processing is a requirement for your use case. Luckily, Dataflow also has Snapshot capabilities. If you recall, we introduced Dataflow Snapshots as a useful tool for testing and rolling back updates to streaming pipelines in our testing and CI/CD module. Dataflow Snapshots can also be used to offer disaster-recovery scenarios. Since Dataflow Snapshots save streaming pipeline's state, we can restart the pipeline without reprocessing in-flight data. This saves you money whenever you have to restart your pipeline. Moreover, you can restore your pipeline much faster than using the Pub/Sub Snapshots and Seek strategy. This ensures that you have minimal downtime. Dataflow Snapshots can be created with a corresponding Pub/Sub source Snapshot. This helps you coordinate the Snapshot of your pipeline with your source. In other words, you can pick up your processing where you left off when you restart the pipeline. This saves you the hassle of having to manage Pub/Sub Snapshots. Let's take a look at how we can use Dataflow Snapshots for disaster-recovery scenarios. Our first step involves creating the Snapshot of the Dataflow pipeline. We can do this directly in the UI with the Create Snapshot button in the Menu bar. You will be prompted to create a Snapshot with or without sources. If your pipeline is using Pub/Sub, we recommend that you select the With Sources option. You can also create a Snapshot using the command line interface. Next, we need to stop and drain your Dataflow pipeline. This is also possible in both the UI and using the command line interface. Lastly, we create a new job from the Snapshot. This is accomplished by passing in the Snapshot ID into a parameter when you deploy your job from your deployment environment. Since Dataflow Snapshots, like its Pub/Sub counterpart, has a maximum retention of seven days, we recommend scheduling a coordinated Dataflow and Pub/Sub Snapshot at least once a week. This means that, if your pipeline goes down, you have a point in time in the past seven days from which you can restart processing, ensuring you can almost always avoid any data-loss scenario. You can use Cloud Composer or Cloud Scheduler to schedule this weekly snapshot. Snapshots are located in the region of the origin job. When you create a job from a Snapshot, you must launch the job in the same region. This is useful for zonal outages. If a zone goes down, you can relaunch the job from a Snapshot in a different zone in the same region. This protects your workloads against zonal outages. However, Dataflow Snapshots cannot help you migrate in a different region in the event of a regional outage. The best action to take in that event is to wait for the region to come back online or to relaunch the job in a new region without a Snapshot. If you've taken a Snapshot, though, you can ensure that your data is not lost.

#### High Availability

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567951

person: In this final section we will look at different high availability configurations for your dataflow pipelines. This section only applies to streaming pipelines. High availability is a hard requirement for some use cases. If you are processing financial transactions or identifying cybersecurity threats in an event stream, there are very real external risks if your pipeline goes down. your specific needs should be assessed when considering how to implement a highly available architecture of your own. When considering high availability, you need to take three factors into consideration. First downtime. How much downtime can your operation tolerate without breaking business continuity? Many organizations define recovery time objectives, or RTO to articulate this upper link. Second data loss. How much of your data can your application afford to lose in the event of an outage? ET managers will often use the term recovery point objectives or RPOs to describe this requirement. Third, cost. Running in a highly available configuration doesn't come for free, and it is important to consider how much your business is willing to pay to ensure that their data pipelines reach sufficient reliability standards. Now that we've discussed the consideration, let's look at a couple of possible configurations on dataflow. You can choose to make redundant sources that are available in multiple regions. In this example architecture, you can maintain two independent subscriptions in two different regions that are reading from the same topic. If a regional outage occurs, you can start a replacement pipeline in the second region and have the pipeline consume data from the backup subscription. If a region goes offline, you can start a new pipeline in a different region immediately to continue processing. Your application might drop data in the process as the intermediate data in the original pipeline will be dropped. However, you can replay the backup subscription to an appropriate time to keep data loss at a minimum if you're coordinating pub/sub snapshots between the two subscriptions. Using a multi-regional sync can also ensure that your new pipeline will be able to write to the sync without degrading latency. Downstream applications must know how to switch to the running pipelines output. Since only the source data is duplicated, it is more cost efficient than other alternative high availability configurations. If your application cannot tolerate data loss, run duplicate web pipelines in parallel in two different regions. Your pipelines will consume the same data from two different subscriptions, process data using workers in different regions, and write to multi-regional sinks in each location. This architectures provides geographical redundancy and fault tolerance. We have already discussed how pub/sub's global architecture makes setting up redundant subscriptions in different regions really easy. Dataflow workers can only work in one zone per job. By running parallel pipelines in separate Google Cloud regions, you can insulate your jobs from failures that affect a single region. Using multi-regional storage locations for your data syncs is not a requirement, but provides you one extra degree of fault tolerance. Applications that feed from the process data sets must have a way to switch to the running pipelines output. In the example above, we are running pipelines in two different continents, America and Europe. However, this would be the approach if you were running on the same continent, but in different regions. For example, running redundant pipelines in US Central one and US East one. This architecture basically offers you zero downtime, even where we'll have multiple instances of your pipeline running. Similarly, as your data is being processed in multiple regions, data loss is extremely unlikely. However, since you are duplicating resources across the entire stack, this approach is the most expensive high availability configuration. This is the end of this module. You should now be able to take snapshots of your data fill pipeline for disaster recovery requirements.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567952

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567953

### Flex Templates

#### Classic templates

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567954

Hi, my name is Prathap. I am a Cloud Data Engineer at Google. Welcome to Dataflow Flex Templates module. In this module, we will cover: Review of Dataflow templates, What are flex templates? How to create a flex template, and Google-provided templates. Let’s start with classic templates. You might already know about Dataflow templates and why are they required, as covered in our earlier course “Serverless Data Processing with Dataflow.” Let us do a quick recap: In normal cases, when a developer executes a Beam pipeline, the SDK stages all the pipeline dependencies on Google Cloud Storage and calls the Dataflow Jobs API to create a job by passing a job request object. To launch a pipeline, this workflow requires runtime dependencies to be installed, which can be challenging for non-technical users. This dependency also limits using cloud-native services like Cloud Scheduler for scheduling. Dataflow templates enable the separation of the development activities and the execution activities. The initial version of templates are now referred to as classic templates. With classic templates, the developer stages the pipeline as a template file on Google Cloud Storage. Now users can launch the pipeline, referring to the template file without the need for any runtime dependencies. This new approach facilitates more opportunities for automation and reusability of pipelines across the teams.

#### Flex templates

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567955

SPEAKER: Classic templates have a couple of challenges-- ValueProvider support for beam I/O transforms and lack of support for dynamic DAG. Let us check each of them in detail. Pipeline options or compile time parameters. To make these options available to users, while launching templates, pipeline options have to be converted to runtime parameters. The ValueProvider interface allows the templates to accept runtime parameter values provided by end users. ValueProvider interface support has been added for several Google and non-Google I/Os, but there are few open source I/Os that lack ValueProvider support, which means that pipelines developed using these I/Os cannot be converted to classic templates. In classic templates, the pipeline graph gets built when the developer converts the pipeline into a template. Due to this, the shape of the graph cannot be changed based on user-provided options. An example would be if you developed a pipeline that consumes events from Pub/Sub and would like to load these events either to BigQuery or cloud storage, depending on the user's choice at runtime. With classic templates, since the dynamic selection of sync node at template launch time is not possible, you need to create two separate variations of this template, one for writing to BigQuery and the other for cloud storage. To address these challenges, Google has built the next generation of templates, referred to as flex templates. With flex templates, the pipeline developer packages the pipeline artifacts into docker image and stages the image on Google Container Registry. In addition, the developer creates a metadata specification file on cloud storage. Users can launch a template referring to a metadata spec file stored on cloud storage by passing appropriate parameter values. Behind the scenes, the template launcher service reads the metadata spec file, downloads the docker image, and invokes the pipeline using user-supplied values. It's important to note that, with flex templates, the job graph is generated when the end user launches the templates, whereas with classic templates, the graph is generated when the templates are created. This distinction makes flex templates more flexible than classic templates.

#### Using flex templates

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567956

SPEAKER: Now, let us check out how to create a flex template and run the Dataflow job from a flex template. Turning a Dataflow pipeline into a flex template is easy and straightforward. First, create a metadata file indicating the pipeline parameters. Next, run the flex-template build gcloud command. Let us cover these steps in detail. Create a metadata.json file with details like the pipeline name, description, and parameter details. You can also specify regexes for any parameters that need to be validated against user-supplied values before the template is launched. This fail-fast approach avoids the overhead of launching a job that may potentially fail due to incorrect parameter values. The gcloud dataflow flex-template build command can be used to build a flex template. In this command, you will provide the path to store the Docker image and template specification file. Refer to the metadata file created in previous step. Specify the artifact details-- such as .jar file, in this example-- and entry point for execution. Executing this command will package the pipeline artifacts into a Docker image, pushes the image into Google Container Registry, creates a template spec file on Cloud Storage containing the image URL and parameter details. For detailed instructions, check out our public documentation. Flex templates can be launched through several channels, like Google Cloud console, gcloud, REST API, or Cloud Scheduler. To launch a flex template from the Google Cloud console, select the Custom Template from Dataflow Template dropdown. Enter the Cloud Storage path referring to a template specification file. Provide the pipeline options, and run the job. To launch a flex template from gcloud, use the flex-template run command by referring to the template specification file and passing the template parameters as required by the pipeline. Alternatively, you can also run the template with a REST API request. Remember that classic templates and flex templates use different endpoints. Flex templates can also be scheduled using the native Cloud Scheduler. In this example, the template has been scheduled to run every 30 minutes. This table provides the detailed comparison of classic and flex templates. Google recommends using flex templates for any Dataflow pipeline that you would like to reuse.

#### Google provided templates

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567957

SPEAKER: We have reached the final section of our module, Google-provided Templates. Google readily provides a large collection of templates to Dataflow users. The good news is that you can use them without writing a single line of code. These templates can be used for transferring data between different systems. You can also add simple transformations through a JavaScript user-defined function. Google has also open sourced all the templates, with the full code available on GitHub. This repository also serves as a starting point for Dataflow developers to learn best practices for writing and testing beam pipelines. With active community support, we encourage you to contribute either new templates or enhancements to existing templates. Similar to user-developed templates, Google-provided templates can be launched through Console, G Cloud, REST API, or Scheduler. You can create a job using one of the templates by clicking on Create Job from Template option on the Dataflow job screen. Choose the appropriate template, provide the pipeline options, and run the job. You can also find usage instructions in the information pane on the right-hand side by clicking on Open Tutorial. When you select a template, you can also view the graph on the right-hand side for all classic templates. For flex templates, the graph is not rendered, as the final graph might change based on the user options. Based on whether graph is displayed or not, you can also determine if the template is a classic template or a flex template. Google-provided templates are classified into streaming, batch, and utility templates. The list provided here represents a sample of available templates. Some of the popular streaming templates are Pub/Sub to BigQuery and Data Masking Using DLP, whereas in batch, the templates like BigQuery export to Parquet and Spanner export to Cloud Storage are widely used. Utility templates like Streaming Data Generator are helpful to generate synthetic records used during proof of concept or for evaluating performance benchmarks. You've reached the end of Flex Templates Module. Now you are ready to convert any Dataflow pipeline into a flex template and share with others.

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/264/quizzes/567958

#### Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Java)

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567959

#### Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Python)

- https://www.cloudskillsboost.google/paths/16/course_templates/264/labs/567960

#### Additional Resources

- https://www.cloudskillsboost.google/paths/16/course_templates/264/documents/567961

### Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/264/video/567962

Mehran: Congratulations, you've made it to the end of the operations course. In the final installment of the data flow series, you're ready to build a modern data platform now. Before we do that, let's summarize the main concepts we covered in each of the modules in the operations course. We started this course with the walkthrough of the data flow monitoring experience. We learn how to use the jobs list page to filter for jobs that we want to monitor or investigate. We looked at how the job graph job info and job metrics tabs collectively provide a comprehensive summary of your Dataflow job. Lastly, we learn how we can use Dataflow integration with metrics Explorer for creating alerting policies for data flow metrics. We explored two important integrations in the data flow, operational toolkit, logging and error reporting. The logging panel helps you sift through job and work logs provides a diagnostics tab that surfaces errors, you can click through to the error reporting interface, investigate the frequency of these errors, and examine the full stack traces of these errors. We use the monitoring logging and error reporting capabilities and incorporated them into our recommended troubleshooting workflow, which leverages data flows integrated Error Reporting in jobs metrics tab. We then reviewed four common modes of failure for data flow, failure to build the pipeline, failure to start the pipeline and data flow, failure during pipeline execution, and performance issues. Performance is a key consideration for any data engineer operating a data processing system. In this module, we review how pipeline design can impact your performance that topology, coders, windows and logging that you implement can have adverse impacts on your pipeline performance if not taken into careful consideration the shape of your data, specifically if your keyspaces skewed can cause worker imbalances and cause under utilization for your pipeline. Your Dataflow pipeline will interact with sources, sinks and external systems. A well-tuned pipeline will take the limitations and constraints of these pieces into account. Lastly, shuffle and streaming engine can help offload data storage from worker attached disks onto a highly scalable back end that will deliver performance benefits to your pipeline. As your data requirements evolve, so do your data flow pipelines. A robust Dataflow architecture implements testing at various abstraction layers, starting with the do functions at the lowest level, then P transforms, then pipelines. And finally, for entire end to end systems. Dataflow's continuous integration continuous deployment model requires using the direct runner to validate your pipeline in a local environment, followed by testing it on a production runner before it is pushed to production. Beam provides helpful functions like p assert, test, pipeline and test stream to help implement this testing architecture. Dataflow offers features such as update, drain snapshots and cancel so that you can adjust the deployment of your streaming pipelines as needed. Next, we discussed how to implement reliability best practices for your Dataflow pipelines. Monitoring dashboards and alerts can help notify you when your system is encountering a bottleneck. And using dead letter queues and error logging can prevent pipelines from going down when corrupted data enters the pipeline. Protecting your pipelines from zonal and regional outages require thoughtfulness about how you specify the location of your sources, sinks, and Dataflow job, but data loss can be mitigated with pub sub and data flow snapshots. High Availability can be implemented by running redundant pipelines in different zones or regions. Our last module is covers flex templates, which makes it easy to share and standardized data flow pipelines for your organization. Templates allow you to call data flow pipelines by making an API call without the fuss of installing runtime dependencies in your development environment. Google offers a variety of templates directly in Cloud Console, which allows you to launch Dataflow job without writing a single line of code. Flex templates offers advantages over classic Dataflow templates, and are encouraged for all templating needs. To conclude, data flow offers a whole suite of features that makes it easy to manage your data processing system. This operational toolkit will help you focus your efforts on insights, not infrastructure, and ensure that you can spend your time creating value for your customers not keeping the lights on. We're excited to see what your organization builds on data flow.

### Your Next Steps

## 10: Build a Data Warehouse with BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/624

### Build a Data Warehouse with BigQuery

#### Creating a Data Warehouse Through Joins and Unions

- https://www.cloudskillsboost.google/paths/16/course_templates/624/labs/566707

#### Creating Date-Partitioned Tables in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/624/labs/566708

#### Troubleshooting and Solving Data Join Pitfalls

- https://www.cloudskillsboost.google/paths/16/course_templates/624/labs/566709

#### Working with JSON, Arrays, and Structs in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/624/labs/566710

#### Build a Data Warehouse with BigQuery: Challenge Lab

- https://www.cloudskillsboost.google/paths/16/course_templates/624/labs/566711

### Your Next Steps

## 11: Build a Data Mesh with Dataplex

- https://www.cloudskillsboost.google/paths/16/course_templates/681

### Build a Data Mesh with Dataplex

#### Dataplex: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/16/course_templates/681/labs/554913

#### Tagging Dataplex Assets

- https://www.cloudskillsboost.google/paths/16/course_templates/681/labs/554914

#### Implementing Security in Dataplex

- https://www.cloudskillsboost.google/paths/16/course_templates/681/labs/554915

#### Assessing Data Quality with Dataplex

- https://www.cloudskillsboost.google/paths/16/course_templates/681/labs/554916

#### Build a Data Mesh with Dataplex: Challenge Lab

- https://www.cloudskillsboost.google/paths/16/course_templates/681/labs/554917

### Your Next Steps

## 12: Boost Productivity with Gemini in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1169

### Boost Productivity with Gemini in BigQuery

#### Use Case

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/video/568616

data enthusiasts have you ever wished to have an intelligent AI assistant by your side available 24/7 this smart companion can generate queries based on natural language review and debug Sequel and visualize insights if this sounds like your dream too great news Gemini and big query can turn these dreams into reality hi everyone my name's Diane and this is Tate hi there happy to be here we work for a digital native company called datab beans where we solve business challenges for our clients with data and AI our newest client is coffee on Wheels a company that sells coffee from food trucks across cities globally we hope to help them transform operations by using data and AI we recently met with be the manager at coffee on Wheels to understand the business requirements be envisions a captivating dashboard for the future of food trucks which achieves three main goals enhance sales by identifying top and underperforming menu items and suggesting Innovative Replacements manage customer Relationships by analyzing customer reviews and generating proper feedback and optimize truck Logistics by determining real-time truck locations based on weather events traffic and customer preferences just like equipping every truck with an intelligent GPS wow cool I'm excited too but we should probably discuss some challenges first good call I often struggle with a cold start of unfamiliar data like those in coffee on Wheels it's like being a detective with overwhelming Clues but not knowing where to begin totally get it I'm in the same boat when it comes to data preparation I normally spend a significant amount of time typically over 60% cleaning data and designing pipelines time I'd rather spend on analysis another challenge is code development to stay current with data and AI Technologies I constantly need to learn new syntax and libraries I wish I had a 247 code assistant to help me generate code and explain them without judgment troubleshooting is also so crucial after coding I wish I had a go-to person to help with code review optimization debugging and suggestions couldn't agree more as a visual thinker I dream of starting a data project by sketching the workflow from data Discovery query formulation to insights creation these challenges are common among data practitioners let's explore how Gemini in big query addresses these issues and boosts productivity by using AI join us in this course to discover the three major goals that Gemini and big query helps you achieve explore and prepare data assist code development and discover and visualize workflow each lesson comes with a Hands-On lab providing you ample opportunities to practice and reinforce your learning can't wait let's start

#### Explore and Prepare Data

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/video/568617

let's begin with data exploration and preparation starting a new project is exciting but data exploration can be challenging often feel like a detective with a mountain of Clues but no clear Direction let's see how Gemini and big query can help Gemini powered tools like insights and table Explorer unlock valuable discoveries from data even for non-technical users with no coding experience insights automatically create queries from table metadata making data exploration a breeze here's how it works analyze metadata insights first scans your tables metadata like column names and descriptions to grasp the data's Essence generate queries inspired by this metadata insights then creates a list of helpful queries tailored to your tables these ready to use queries hold the key to unveil the hidden patterns explore and refine you dive into these queries and modify them as needed run and analyze you run the query analyze the results and uncover the insights cool how about table Explorer table Explorer is your visual tool to explore individual tables here's the process select Fields you can choose up to 10 fields in a table to focus on generate interactive cards table Explorer then generates interactive cards which show the most common values to give you a quick overview explore and filter you can click on values to drill down for more specific results create queries table Explorer automatically creates a query based on your actions apply you can apply the query directly or copy it to the query editor for further customization and execution think of both insights and table Explorer as tour guides of your data to help you with a cold start navigate through mountains of information and discover hidden patterns and paths amazing this also opens a door for non-technical users like B and her team equipped with the tools they often get deeper insights from the data than we do by leveraging their field expertise aside from the advantages are there any precautions I should take when using these tools excellent question remember that both tools are limited to working with single tables at a time this means for now they are not capable of performing ing complex operations such as joining multiple tables together well that's a great start the next real challenge lies in data preparation it's a timeconsuming and often tedious task that eats up over 60% of my time like many other data practitioners can't agree more big query uses Gemini to build low code visual AI augmented data preparations let's check out how it can be used for data cleaning and pipeline development for for data cleaning and transformation Gemini and big query provides two options first it automatically generates suggestions on the sidebar for tasks like filtering standardizing and fixing missing data you can directly apply the code if desired second you can direct Gemini to perform certain tasks by using natural language prompts for example ask Gemini to remove duplicate rows or convert data to a specific format how about automating a data pipeline Gemini in big query dramatically enhances ETL extract transform load it uses generative Ai and ml to Define Transformations it also ensures data quality with rule-based logic and anomaly detection plus it visualizes the data pipeline as a graph making it easy to understand the preparation process this speeds up the ETL development workflow in this demo let's see how insights and table Explorer in big query can help you explore data let's say you're curious about the order data start by clicking on the table then select insights and generate insights it might take a few minutes for big query to scan the metadata and create insights once it's done you'll have a list of query questions tailored to this table want to find out most popular menu items by quantity sold check out this automatically generated query code feel free to give it a try by clicking copy to query and then run here are the results want to rank the top three items however don't know how to code no worries just ask Gemini to do it for you type in the hashtag sign followed by a simple prompt find top three items accept the code and run it that's it you can bring the tab over to the left pane for a better view feel free to go through all these autogenerated insights explain them run them and modify them to your needs now let's check out the other tool table Explorer this one helps you dive deeper into each table let's say you're interested in the location table click on the table then select table Explorer and choose the fields you want once you save it you'll get an interactive card for each field showing the most common values if you're interested in certain values choose them and big query will automatically generate the query code for for you then you can run and modify the code as needed this is just a quick overview of the major features of insights and table Explorer feel free to explore more on your own cool can I try it myself absolutely here's a Hands-On lab for you to explore tools like insights and table Explorer hey Diane up for a quick quiz before the lab hit me with it what does insights do in big query a automatically generates queries to explore data B visualizes the relationships between tables C automatically transforms the data format or D generates interactive cards for each table H I think it's a you got it how can I use the help of Gemini in big query to write code and solve more complex tasks let's tackle that topic in the next lesson

#### Explore Data with Gemini in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/labs/568618

#### Assist Code Development

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/video/568619

in the previous lesson I learned how to explore and prepare data using Gemini and big query but how can it assist more complex code development like joining multiple tables think of Gemini and big query as a coding partner it aids in every step of code development and troubleshooting I found it particularly helpful recently want to hear more absolutely in terms of code development you can prompt with natural language to generate and complete code there are mult multiple ways to access code assistant the first option is to use the SQL generation tool button click the SQL generation tool button and type in your natural language prompt in the popup window you can also use natural language prompts start a line with a hashtag sign type natural language prompts directly into the query editor and press enter you may need to wait a few seconds SQL code will then be automatically generated press tab to accept the generated query note that once the query is generated you can further modify the code you know what else Gemini can do explain code I asked it about group by and SQL and it gave me a breakdown of its purpose how it works in my query the summary the result and even more examples it was like having a personal SQL tutor this would be extremely helpful I'm often hesitant to ask my colleagues for help because they're usually preoccupied and I don't want them to question my competence totally by the way how does Gemini handle troubleshooting similar to the code development you can prompt Gemini to review code optimize for better performance identify errors and debug code can Gemini help with both SQL and python in big query yes it can however please be patient with python assistance as Gemini is still learning awesome feels like I can use natural language to get anything any tips on how to prompt effectively absolutely Diane here are a few best practices for prompting Gemini in big query first Clarity is key choose Precision over ambiguity State your request clearly for example instead of prompting tell me about sales a more clear prompt is show me a breakdown of total sales by product category for the last quarter context matters provide any relevant background information to help Gemini understand your request instead of using what's the trend use what's the trend of monthly active users for our mobile app over the past year second ask direct questions ask one question at a time be concise avoid overloading Gemini with too much information third give explicit instructions explain key terms and algorithms for example sales equals the column unit price times the column quantity layout steps and the order fourth refine and iterate you may need to experiment a lot learn from feedback got it all in all practice makes perfect prompts true in this demo let's see how Gemini can be used in big query to develop transform explain debug and troubleshoot code let's say you are interested in finding orders specifically you want to find the total number of orders for each menu item start by creating a new query using the SQL generation tool with this prompt find the total number of orders for each menu item the results include menu item name price and size click generate review the generated SQL and click insert cool your query is added to the query workspace and ready to run run the query fantastic you get the results these results are helpful but notice the results aren't in any alphabetical order you can fix that using the transform feature select the query click the Gemini button select transform in the popup enter order the results as menu item ascending click generate review the transformed query and click insert and then run the query awesome your transformed query now alphabetizes the results you can also use Gemini to EXP explain queries or parts of a larger query imagine a teammate provides you with a custom query they wrote but you don't fully understand it you then select the query and click the Gemini button then you select the explain this query option super Gemini provides a detailed explanation in the chat window and you get a summary at the bottom you can also use Gemini to debug and troubleshoot your code by selecting the code and using prompts in the chat window let's say you're interested in locations and you want to know the total number of locations in each City and then find the top five cities with the highest number of locations you created this SQL code but you get an error when you run it let's troubleshoot it access the Gemini chat panel enter the first part of the prompt why am I getting this error table location must be qualified with a data set use shift enter or shift return on Mac to create a new line for the prompt copy and paste the query you want to troubleshoot into this new line and then click Send prompt excellent Gemini generates an explanation of the error suggests a fix and provides a key takeaway you use these to troubleshoot and resolve the error this is just a quick overview of the major features feel free to explore more on your own impressive I can't wait to try it out myself great here's a Hands-On lab where you can practice using Gemini and bigquery for code development up for another challenge I'm ready which prompt would you use to identify best products in coffee on Wheels a find popular products on the market B find top three products C find top three products by profit sales minus costs or D find top three products worst three trucks and top three locations C for sure a is too broad and against rule number one B does not explain the meaning of top products which can be different criteria such as sales profits and customer likes this is against rule number three of giving explicit instructions D asks multiple questions in one prompt which is against rule number two it also doesn't explain the exact meaning which is also against rule number three so C is perfect accurate and concise to ensure I'll guess I'll try all of them in the lab to see how Gemini interacts good idea I learned a lot thanks Tate for sharing one more question I'm a visual thinker so is it possible to sketch the workflow visually that's precisely what data canvas is for let's check it out in the next lesson

#### Develop Code with Gemini in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/labs/568620

#### Discover and Visualize Workflow

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/video/568621

Gemini and big query seems powerful in code development and troubleshooting while this may sound ambitious as a visual thinker is it possible to start a data project by sketching the workflow from end to end that's data canvas a new feature in big query with data canus an Innovative tool driven by generative AI users can visualize a workflow from data search to query formulation to Insight generation with natural language let's walk through the five primary features offered by data canvas find data when you have an abundance of data identifying the most relevant information becomes the first step with data canvas you can search data with either natural language or keywords examine tables schemas details and previews and add them to the canvas as necessary generate code you can then join multiple tables formulate query code by using natural language prompts edit and run the query create insights with simple clicks you can easily visualize your results with charts and generate comprehensive summaries visualize workflow each of the above steps can be visualized as part of the workflow on data canvas you can further modify the canvas by adding new nodes tables and joins conveniently the mini map in the top right corner offers a bird's eye view of the entire canvas aiding navigation when zoomed in share and collaborate most important importantly you can share the canvas and collaborate with others you can also export it to a notebook furthermore you can export data to CSV and sheets charts to PNG files and looker Studio this is super powerful in this demo let's see how data canvas can be used to visualize and design queries let's say you want to know the total revenue generated by City sips roaming Cafe in 2024 and then want to compare this to other locations in bar chart start by accessing data canvas from Big query Studio then search for the coffee on Wheels data set select the location orders and Order items tables click join you see the tables are visually Associated now with each other but not actually joined you do however have a sample prompt to join these tables click Send input awesome a query is created review this query that joins these tables and run the query the resulting table shows orders for all locations with each menu item item price and item total this new table is helpful and you can use the data in the table to get total revenue for the location by using the query these results feature with this feature you can query the results using natural language prompts or write your own SQL enter the prompt what is the total revenue gener generated for the city sips roaming cafe location in 2024 include the location name and the total revenue in the results round the total revenue field to two decimal places and click Send input a query is generated and then you run it note that this query is running on the last nodes output not all the raw tables themselves fantastic the total revenue for City sips in 2024 is displayed with two decimal places in the results if you wanted to edit the query you could change the natural language prompt or modify the SQL code itself what if you want to compare the total revenue of this location to other locations how can you visualize this start by returning to the node for the join tables hover over the branch another node option at the bottom of the node results click query these results a new note appears good now enter the prompt what is the total revenue generated for all locations in 2024 include the location name and the total revenue in the results round the total revenue field to two decimal places click Send input the query is created and you click run notice this query is similar to the previous one but with this one you are not limiting the location name field to City's roaming Cafe so all locations are included in the results with total revenue click visualize and select the create bar chart option a simple bar chart with total revenue for each location appears however the chart has many locations included so it's hard to do the comparison you can fix this by transforming the default query with this prompt create a vertical bar chart with the total revenue generated for all locations in 2024 include the location name on the X AIS and the total revenue on the Y AIS in the results starting with the location with the highest revenue use a unique bar chart color for City's roaming cafe location and click Send input excellent the result is a comparison of the total revenue for this location with other locations with this chart we can clearly infer City sips is the lowest performing location by total revenue this is just a quick overview of the major features feel free to explore more on your own amazing can't wait to try it myself of course here's a Hands-On lab where you can practice with the major features of data canvas perfect data canvas is like a playground for data enthusiasts it's got so much potential for boosting productivity and I can't wait to explore all the ways of using it what do you think can you imagine any specific use cases where data campus could be especially helpful I'd love to hear your ideas was

#### Use Data Canvas to Visualize and Design Queries

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/labs/568622

#### Summary

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/video/568623

wow quite an exciting Journey thank you Tate for sharing your experience anytime a quick recap of what I've learned in this course we focused on three main goals that Gemini and big query enables exploring and preparing data assisting code development and discovering and visualizing workflow each goal can be further divided into tasks and the tools to support them for example insights and table Explorer are used for data exploration Gemini for code development and troubleshooting and data canvas for workflow Discovery and visualization as data practitioners we can leverage natural language to instruct Gemini and big query for various tasks thus prompting Gemini and big query with precise instructions becomes a crucial skill in this course we explored prompting principles in big query emphasizing clarity directness explicitness and iteration note that this is only the beginning of using Gemini to enhance productivity in big query as we speak new features are in development for example using AI to optimize operations these include recommendations for partitioning and clustering query performance enhancement and data process optimization and don't forget while the course certainly teaches you the basics of using Gemini and big query it's only the tip of the iceberg more AI potentials are waiting for you to discover it's your turn to incorporate Gemini into your daily work and explore its multifaceted role discover its capabilities as an AI assistant a coding buddy and a subject matter expert true the course is intended to spark your curiosity and inspire you to think creatively about how to leverage this powerful AI assistant in your daily work expect to walk away with more questions and answers in the best possible way we hope you enjoyed this course as much as we do if you are interested in learning how to incorporate Gemini models to achieve your own gen tasks please check out our next course on the learning path work with Gemini models in big query links are included in the reading list thank you be sure to check out our other Google Cloud courses for continued learning

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/quizzes/568624

#### Reading

- https://www.cloudskillsboost.google/paths/16/course_templates/1169/documents/568625

### Your Next Steps

## 13: Work with Gemini Models in BigQuery 

- https://www.cloudskillsboost.google/paths/16/course_templates/1133

### Work with Gemini Models in BigQuery

#### CRM Use Case: Social Media Sentiment Analysis

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/video/567890

#### Work with AI/ML Models in BigQuery

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/video/567891

To get ready for creating an AI project, let's join Diane and Tate for an enlightening conceptual exploration. They first wonder what specific AI tasks can be solved with BigQuery. As mentioned in the previous lesson, BigQuery enables AI capabilities through a built-in feature called BigQuery ML. It allows you to create and run AI models by using either Structured Query Language (SQL) queries or Python code. It supports predictive AI tasks like sales forecasting and product classification, Gen AI tasks like feedback generation and marketing campaign automation, and a hybrid approach that combines both. Depending on the tasks, you can choose from various models. For prediction AI tasks: Regression model for housing price prediction Classification model for spam or non-spam email Clustering models for customer segmentation Time series model like ARIMA_PLUS for sales or stock market forecasting For generative AI tasks: Gemini for various content generation tasks, article summarization, text and image analysis, and translation. Note that Gemini is Google’s most advanced Gen AI model. It’s a family of models that can handle multimodal data including text, image, audio, video, and code. Cloud AI services like Natural Language API for specific language understanding and Translation API for machine translation. That sounds exciting! It appears the major issues encountered by Diane and Tate could be solved by combining forces with BigQuery ML and Gemini. The next significant question is how. The workflow to build an AI/ML model in BigQuery can be split into two primary stages: create and use. Model creation Construct an AI/ML model according to specific tasks. Model use Serve the model to solve tasks. To create a model, you use the SQL statement CREATE MODEL. This stage comprises three iterative steps: Data preparation: perform feature preprocessing, including both structured data like those in tables and unstructured data like texts and images. Model creation: either train a BigQuery built-in ML model or refer to remote Gen AI models. Model evaluation: assess how the model performs by using the function ML.EVALUATE. When the evaluation falls below expectation, you can retrain or tune the model with new training data, thus restarting the iterative process. In terms of deployment, BigQuery ML supports three main types of models based on their hosting location: Local models: reside within BigQuery and can be trained either internally in BigQuery or externally in Vertex AI. These built-in models mainly focus on predictive AI tasks. These include classification, like a logistic regression model, regression like a linear regression model, and time series forecasting, like an ARIMA_PLUS model. You are recommended to start with simple options such as logistic regression and linear regression, and use the results as a benchmark to compare against more complex models such as deep neural networks (DNN), which take more time and computing resources to train and deploy. Remote models: hosted in Vertex AI and referenced by BigQuery. These include pre-trained Gen AI models like Gemini and Cloud AI services like Natural Language APIs. Imported models: trained anywhere and imported into BigQuery from Cloud Storage. For example, Open Neural Network Exchange (ONNX), Tensorflow, and XGBoost. Depending on the model type, the CREATE MODEL statement differs slightly. Local models: specify the model type such as “LINEAR_REG” in the Options section. Remote models (pre-trained Gen AI foundation models): Set up the connection. Refer to the endpoint. An endpoint is a way to interact with a deployed ML model. It's essentially a URL or API that allows you to send input data (e.g., text, images, numerical features) to the model and receive predictions or results in return. Remote models (Cloud AI APIs): similarly, you need to Establish a connection to Vertex AI. Specify the service type, such as Cloud AI Vision. Imported models: in addition to describing the model type like TensorFlow, you must provide the path where the model is stored on Cloud Storage. You’ll explore the coding examples to create a remote model using both SQL and Python in the following labs. Once the model is created, it's time to unleash its potential. The use stage unfolds in three iterative steps: Model serving Model explanation (optional) And model monitoring Get ready for the thrilling climax—model serving! Simply run a BigQuery ML function against different models, and you're all set. For predictive AI tasks like prediction, classification, and clustering, use the ML.PREDICT function. For time series forecasting, use ML.FORECAST(). For generative AI tasks like content generation, summarization, and rephrasing, use ML.GENERATE_TEXT(). ML.understand_text() is available for language analysis, and ML.translate() handles machine translation. Next is model explanation, which aims to understand how each feature contributes to the predicted result. It’s optional and only applicable to predictive AI models. You can use: ML.explanation_predict for non-time-series models, particularly supervised models such as regression models and DNN. While ML.explanation_forecast is for time-series models. Finally is model monitoring. Model monitoring in BigQuery ML focuses on data to monitor model performance. This involves: comparing a model's serving data with its training data to prevent data skew, and comparing new serving data with previously used serving data to prevent data drift. Commonly used functions for data monitoring include ML.DESCRIBE_DATA, ML.VALIDATE_DATA_SKEW, and ML.VALIDATE_DATA_DRIFT. Link the dots. Now, you have a full workflow to create and use an AI model with BigQuery ML. Quiz time! Let's put your knowledge to the test before diving into the code. In terms of deployment, what are the major AI/ML model types in BigQuery (you can choose more than one)? Local models Remote models Imported models Exported models A, B and C are correct. Did you get it? Take a moment to consider the difference between the models, and how the SQL statement CREATE MODEL can be used to create them differently. All right, buckle up and get ready! Next, Diana and Tate will showcase their skills to build a customer relationship management application using BigQuery and Gemini.

#### Gemini in Action: Analyze Customer Reviews with SQL

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/video/567892

In the previous lesson, you explored the workflow for building an AI project in BigQuery, along with the key technologies and concepts involved in the process. Now Diane, the data analyst, is excited to show you how to analyze customer reviews with Gemini using pure SQL in BigQuery. Get ready for some SQL magic! Let's first take a quick tour of your workspace, BigQuery Studio! This nifty graphic user interface, GUI, will be your playground for discovering, exploring, analyzing, and predicting data in BigQuery like a pro. On the left, you'll find the menu, acting as your command center. In the middle, meet the explorer—your project folder and organizational guru for queries, notebooks, external connections, models, and tables. It's like a filing cabinet, but way cooler! And on the right, behold your real workspace—the place where the magic happens. Start a SQL query, whip up a Python notebook, or unleash your creativity with data canvas. The possibilities are endless. Remember Diane, Tate, and Bea, the fantastic trio who’re about to conquer the challenge of customer relationship management for Coffee on Wheels. Their quest began with data ingestion, model creation, data analysis, and finally actions. Ready to conquer the coffee world. From a technical perspective, the workflow can be broken down into a five-step pipeline: Establish a connection to the generative AI remote models hosted on Vertex AI. Construct a dataset for multimodal data using an object table that stores unstructured data such as text and images. Create a remote model to reference the endpoint of the Gemini model. Analyze customer reviews by extracting keywords, determining sentiment, and producing a report. Take action by formulating responses to customer feedback and planning marketing campaigns. Let’s delve into each of these steps in greater detail. In the previous lesson, you learned that you can create three main types of AI models in BigQuery: local, remote, and imported. The type of model you create depends on where the model is hosted. Which category do Gemini models fall into? Yes, Gemini models, like other large foundation models, are remote models trained and deployed in Vertex AI, Google's AI development platform. To use them in BigQuery, you must first establish the connection. This can be done either through the UI or code. Create the connection: In the Google Cloud console, click Add. Choose Connections to external data sources. Select Vertex AI remote models. No coding is required. 2. Grant permission: Specify IAM permissions to grant access. Choose a Vertex AI user to allow access to all resources in Vertex AI. For more information on roles, refer to "Vertex AI access control with IAM" in the reading list. Ready to build the multimodal dataset? If you have unstructured data like text and image, most likely your data is saved on Cloud Storage. Use the SQL statement LOAD DATA...FROM FILES. You need to specify the file name; each column; the data format, which in this case is the csv file that includes customer feedback in text; and the file location on Cloud Storage. You then upload the image data using an object table. Object tables provide a structured way to access and analyze unstructured data stored in Cloud Storage. Think of them as a magical bridge that connects BigQuery with your messy data, such as images, songs, and movies. It's like giving BigQuery superpowers to handle all kinds of unstructured multimodal data, not just structured data like the numbers in tables. What does an object table do? It stores the references to data objects in Cloud Storage. Each row represents an object. And columns contain metadata such as Uniform Resource Identifier (URI), content type, and size. One more thing to note: Object tables use access delegation, so users can access the object table without directly accessing Cloud Storage, which adds an additional layer of security. Let's examine the coding. Use the SQL statement CREATE EXTERNAL TABLE with the connection you created in the previous step. Set the following options: Object metadata: specify how BigQuery should handle metadata associated with the unstructured data objects you're accessing. URIs: specify the Cloud Storage paths (URIs) where your unstructured data files are located. What happens next to the unstructured data in an object table? Well, through a technology called embeddings, that data is converted to numeric vectors that represent semantic meanings. Combined with structured data, these vectors form the input for AI/ML models, enabling further tasks such as prediction or generation. Want to dive deeper into embeddings? Check out our Vector Search and Embeddings course in the reading list. Once the data is prepared, you proceed to create the Gemini models. Currently, the widely used gemini models include gemini-pro and gemini-flash. You get a chance to try both in the labs. As you have learned from the previous lesson, in addition to a regular CREATE MODEL statement, you must specify the remote connection and the endpoint that refers to the Gemini model you want to use. Diane also needs to create a Gemini vision model to process image data. Once the model is created, use it to analyze customer reviews. The ML.GENERATE_TEXT () function enables you to perform generative natural language tasks using data from BigQuery standard tables or object tables. This function works by sending requests to a remote BigQuery ML model like Gemini and then returning the model's response. You need to specify the model you want to use and the object table for unstructured data if any. Use STRUCT() to define the parameters for the generation tasks. For example, prompt the instruction to the model. Customize the model's response by adjusting parameters such as top-p and temperature. Top-p controls the range of words considered for output, while temperature influences the randomness within that range. For more information about parameter tuning, refer to the Introduction to Vertex AI Studio course in the reading list. Let’s explore the coding examples. ML.GENERATE_TEXT (), this is where the magic happens. Specify the model you want to use, which is the Gemini model you created earlier. Create a prompt to tell the model to extract the keywords, and set the parameters. Follow the same syntax for the sentiment analysis, only with a different prompt. This time, you tell the Gemini model to classify a review as either positive or negative. You can also prompt to analyze the images, for example, providing a caption and generating keywords. Note that, here, you refer to the gemini_pro_vision model and draft a prompt to explain the instructions as specifically as possible. After thoroughly analyzing customer reviews, the next step involves taking appropriate actions. Using the Gemini model, you can create responses to customer reviews automatically. To personalize the response according to your preferences, you can train the model by providing it with some examples. This process is known as prompt tuning. Three primary methods are available: Zero-shot prompting: give instructions to the model without any examples. For instance, you could simply prompt Gemini to "Generate a response to the customer review." One-shot prompting: provide a single example of the task. You can illustrate to Gemini if a customer says that the coffee tastes fresh, the response can be "Thank you for choosing us. We're delighted you enjoyed our service." Few-shot prompting: provide numerous examples. For instance, you can guide Gemini's responses to different customer reviews including both positive and negative comments. You’ll practice prompt tuning in the lab. Last but not least, prompts can be utilized to strategize marketing campaigns. Quiz time: unleash your knowledge! In BigQuery, which function can be used to perform language tasks by leveraging remote Gen AI models? ML.generate_text ML.create _model ML.generate_prompt ML.create_table Did you get it right? Time for a hands-on practice! Join a lab session to analyze customer reviews and uncover valuable insights with Gemini in BigQuery using SQL. Get ready for a journey of discovery!

#### Analyze Customer Reviews with Gemini Using SQL

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/labs/567893

#### Gemini in Action: Analyze Customer Reviews with Python Notebooks

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/video/567894

In the previous lesson, you learned how to use Gemini in BigQuery with SQL. In this lesson, Tate wants to demonstrate how to write Python to perform the similar tasks. BigQuery integrates Colab Enterprise notebooks, enabling developers to use Python directly within BigQuery, without switching to a separate development environment. How do you decide between using SQL queries or Python notebooks, besides personal preferences? Similarities: Both methods can accomplish similar tasks in BigQuery. They both have the features offered by Google Cloud and BigQuery, such as code assistance with Gemini, autocomplete, and code management. Despite their similarities, each tool has its own unique advantages. Skillset: SQL queries: SQL-only users. Python notebook: Python users can also code in SQL with the imported BigQuery library. Flexibility: Python offers greater flexibility with extensive libraries. Workflow automation: Python has more control for end-to-end workflow automation. Performance: SQL is highly optimized for large-scale data processing. Recommendation: SQL for straightforward data analysis. Python for complex tasks and pipeline automation. Combine both in a Python notebook for data analysis and pipeline automation. Let’s dive into the code. After installing the required packages, the first step is to import the Python libraries you use. from google.cloud import bigquery for querying data. from google.cloud import storage for uploading, downloading, and managing files in Google Cloud Storage. from vertexai.generative_models import GenerativeModel for interacting with Gen AI models, and import Part for accessing a section of the input or output for a Gen AI model. import other libraries if needed, like matplotlib.pyplot as plt for data visualization. You can use similar SQL code from the previous lab to build a dataset, create remote models, and analyze sentiment in the Python notebook. Tate wants to demonstrate a new task, audio analysis, by trying Gemini Flash, a new member of the Gemini family. First, create the model of Gemini Flash. Second, prompt to convert audio to scripts, summarize, decide sentiment, and generate a response. And yes, you can do all these in one prompt. Make sure to draft the prompt as specific, clear, and concise as possible. Finally, use the function mode.generate_content() to get the results and save them in a JSON file. The result contains the transcript, summary, sentiment, and response as key-value pairs in a JSON file. Recap time! In this course, you joined Diane, Tate, and Bea on their journey to solve a business challenge. They showed you how to manage customer relationships efficiently and at scale. They unlocked the secret workflow and chose Gemini models and BigQuery on Google Cloud as their development tools. They followed a two-stage journey, model creation, and model use when they developed the AI application in BigQuery. To create a model, they used the SQL statement CREATE MODEL. The specific syntax of this statement varies depending on the type of model being created, such as local, remote, or imported models. When it comes to using a generative AI model, they had several options, with ML.generate_text being the primary one. They specified the prompt and adjusted a few hyperparameters to tune the results as needed. It’s time for an exciting Python adventure in BigQuery! Let’s dive into a hands-on lab, using the Gemini models to analyze customer reviews and gain valuable insights. We hope you enjoyed this course! Be sure to check out other Google Cloud courses for continued learning.

#### Analyze Customer Reviews with Gemini Using Python Notebooks

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/labs/567895

#### Quiz

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/quizzes/567896

#### Reading

- https://www.cloudskillsboost.google/paths/16/course_templates/1133/documents/567897

### Your Next Steps


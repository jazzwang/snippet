# Cloud Engineer Learning Path

- https://www.cloudskillsboost.google/paths/11

[TOC]

## 02: Preparing for Your Associate Cloud Engineer Journey

- https://www.cloudskillsboost.google/paths/11/course_templates/77

### Introduction

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523295

>> Welcome to preparing for your associate cloud engineer journey. In this course, you'll learn more about the skills covered on the associate cloud engineer certification exam. It's important to clarify that this course by itself will not prepare you to take the A certification exam. This is not a cram session. The exam is purposely calibrated to test your ability to apply the knowledge required of an associate cloud engineer, not merely repeated. Cram sessions have minimal impact on your ability to pass the exam. Instead, the goal of this course is to help you structure your preparation time for the exam. You'll learn about the scope of each exam section, assess your current knowledge and skills through diagnostic questions, and review where to find additional tools and resources to include in your study plan. In this introductory module, you'll learn about the role of an associate cloud engineer, the types of resources available to support your study, and how you will use the workbook in this course to create your study plan.

#### Introduction to the Associate Cloud Engineer role

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523296

Person: So you're preparing for the Associate Cloud Engineer certification. But what exactly is an Associate Cloud Engineer? Let's review the job-role description. An Associate Cloud Engineer deploys and secures applications and infrastructure, monitors operations of multiple projects and maintains enterprise solutions to ensure that they meet target performance metrics. This individual has experience working with public clouds and on-premises solutions. They are able to use Google Cloud Console and the command line interface to perform common platform based tasks to maintain and scale one or more deployed solutions that leverage Google-managed or self-managed services on Google Cloud. In this course, you'll examine the role of an Associate Cloud Engineer by putting yourself in the shoes of an Associate Cloud Engineer at Cymbal Superstore, a fictional company that is preparing to migrate some of its on-premises applications to Google Cloud. Cymbal Superstore is experiencing several challenges common to legacy retailers. Customers increasingly expect a seamless online offline experience when they shop, making the Cymbal Superstore e-commerce application vital. Cymbal Superstore logistics wants to use data to more efficiently oversee transportation. Cymbal Superstore operations has been grappling with how to update their supply chain strategy to meet customer demand in a timely, efficient way. To meet these challenges, Cymbal Superstore plans to migrate their existing e-commerce, supply chain and transportation management applications to Google Cloud. As an Associate Cloud Engineer at Cymbal Superstore, your role involves helping set up the cloud environment. You're also involved in planning, configuring and managing the cloud solutions. This requires you to be familiar with variety of compute, storage and networking resources in Google Cloud. You also need a solid understanding of Google recommended practices for configuring access to cloud resources. As you continue through this course, you'll explore the role of an Associate Cloud Engineer as Cymbal Superstore migrates their applications to Google Cloud. We'll use this scenario to illustrate the types of configurations and tasks that correspond to each section of the exam guide. Cymbal Superstore's Cloud solutions will also provide context for many of the diagnostic questions you'll encounter along the way.

#### Certification value and benefits

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523297

>> Why become a Google Cloud Certified Associate Cloud Engineer? Becoming Google certified gives you industry recognition. It validates your technical expertise and can be the starting point to take your career to the next level. Certification value has skyrocketed. The associate cloud engineer certification is valuable on its own, and can also be used as a starting point on the path to professional certification. You may be curious about what separates an associate cloud certification from a professional cloud certification. There is some overlap between the two levels of certification. However, only the professional level certification expects the exam taker to know how to evaluate case studies and design solutions to meet business requirements. The associate level is mainly concerned with technical requirements and customer implementation. That is where you should concentrate your efforts.

#### Certification process

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523298

The Associate Cloud Engineer certification exam is based on the exam guide. In the following modules, you’ll take diagnostic questions to assess your knowledge of each section of the exam guide. The exam guide is divided into 5 sections. Each section has several objectives. We’ll focus on where you can find resources at the section objective level. You can find the exam guide on the certification page, at the URL shown on the screen. Throughout this course, you’ll be pointed to specific resources and documentation that can help you fill the gaps you identify through the diagnostic questions. Let’s go over the types of resources you may want to include in your study plan. Google provides resources to help you develop your skills and experience with Google Cloud solutions. The learning path for this certification includes in-person or online courses, online practice labs and skill badges, and practice questions. The courses recommended for the Associate Cloud Engineer certification include Google Cloud Fundamentals: Core Infrastructure, Architecting with Google Compute Engine, and Getting Started with Google Kubernetes Engine. You’ll learn more about how these courses relate to the sections of the exam guide as you complete the modules in this course. It’s important to note that these courses can be taken as instructor-led courses, or as on-demand courses on Coursera, Pluralsight, or Google’s Cloud Skills Boost. The on-demand content included in Architecting with Google Compute Engine is presented over three courses: Essential Google Cloud Infrastructure: Foundation, Essential Google Cloud Infrastructure: Core Services, and Elastic Google Cloud Infrastructure: Scaling and Automation. The Skill Badges provide hands-on experience working in Google Cloud. They consist of a series of related labs followed by a challenge lab, and are available on Cloud Skills Boost. As we review the diagnostic questions in this course, you’ll also get recommendations for Skill Badges to include in your study plan. Sample questions are another resource that you can use to prepare. The diagnostic questions in this course are designed to help you identify your knowledge gaps. On the certification page, Google provides a different set of sample questions that can help you familiarize yourself with the format of the exam questions. Once you complete the question set, you will receive feedback describing the rationale for the correct answers. The sample questions provide a good opportunity to practice taking the type of scenario-based, application-level questions on the exam. The exam questions present you with a scenario, explain the goal or what you are trying to achieve, and ask you what you would do in this situation. Remember these tips for multiple choice questions: Read the question stem carefully. Make sure you understand exactly what the question is asking. Try to anticipate the correct answer before looking at the options. You should be able to come up with the correct answer just from reading the question stem. You may find that more than one answer may be possible on multiple choice tests. Take questions at face value. If certain details are omitted, then they are unlikely to contribute to the selection of the best answer. Pay attention to the qualifiers, such as "usually", "all", "never", and "none". And the key words, such as "the best", "the least", "except". Google also supplies official public documentation for its products and services. This documentation can be found at the URL shown on the screen. In each of the following modules, you’ll learn about specific documentation resources to help you study that section in preparation for the exam.

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523299

One of the primary goals for this course is to help you devise a study strategy that focuses on the areas you need to work on. Let’s quickly explore how the course is set up. The course - and your course workbook - focuses on each section of the exam guide in turn. To help you craft a study strategy, you’ll take diagnostic questions as part of each module. Many of these questions relate to our Cymbal Superstore scenario and ask you to apply concepts you will need to be familiar with as an Associate Cloud Engineer. Keep in mind that these diagnostic questions are meant to help you identify gaps in your knowledge, but they don’t represent all possible topics on the exam. Remember, we don’t expect that you’ll answer all these questions correctly right now. This is meant to be a course that you take toward the beginning of your Associate Cloud Engineer journey, and many of you may not be Google Cloud experts yet. If there are any terms or concepts that are confusing, please ask. We’ll review the answers to the questions related to each section objective. As we cover each objective, you’ll learn more about where the key concepts appear in Google Cloud documentation, specific courses and modules, and/or specific Skill Badges. At the end of each section objective, you’ll find a list of related resources. Mark or highlight the specific resources you need to add to your study plan. In the final part of your workbook, you’ll find a template to help you identify weekly goals and study activities. We’ll talk more about putting together weekly goals at the end of this course. Now that you know about the overall setup of this course and how to use the workbook, let’s get started by exploring section 1 of the exam guide.

#### Workbook

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523300

### Setting Up a Cloud Solution Environment

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523301

In this module, you’ll explore the scope of tasks involved in setting up a cloud solution environment, which corresponds to the first section of the Associate Cloud Engineer Exam Guide. We’ll start by discussing some considerations for how you would set up Cymbal Superstore’s cloud environment. Next, you’ll assess your skills in this section through 8 diagnostic questions. Then, we’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

#### Setting up Cymbal Superstore's cloud environment

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523302

Let’s start with how to set up Cymbal Superstore’s cloud environment. Cymbal Superstore is ready to start implementing basic cloud infrastructure for their organization. As an Associate Cloud Engineer, you have been assigned to the team to help with this phase of the project. The team begins with a planning meeting to decide how Cymbal Superstore’s cloud structure will be organized. The outcomes of this phase of the migration journey include establishing a resource hierarchy, implementing organizational policies, managing projects and quotas, managing users and groups, and applying access management. Setting up billing and monitoring the use of your cloud resources are also things to consider. Let’s take a closer look. How to set up a resource hierarchy on Google Cloud depends on the needs and structure of the organization. Cymbal Superstore plans to migrate three applications to the cloud: its supply chain application, which will run on VMs; its containerized ecommerce application, which will use GKE; and its transportation management application, which will use Cloud Run functions to track truck location. These applications correspond to three different departments. Operations manages the B2B supply chain, Logistics oversees transportation, and Sales & Marketing manages the eCommerce application. The graphic shows a possible organizational hierarchy for Cymbal Superstore. The Cymbal Superstore organization is at the top, followed by optional folders underneath, one for each division and one for each application. There are multiple projects under the B2B supply chain app folder, one for each environment related to continuous integration/continuous delivery: a development, staging, and production environment. Other apps in the chart would have a similar project structure. As you set up the hierarchy, you will also need to grant organizational policies. For example, Marketing might need to access the data in the supply chain system to see if inventory levels affected recent marketing campaigns. Giving Marketing viewer permissions on the supply chain production environment might be a good choice to give them the access they need. An Associate Cloud Engineer could also be tasked with enabling Application Programming Interfaces (API’s) within projects during setup. Cymbal Superstore’s ecommerce project requires access to Google Kubernetes Engine and Cloud SQL as a database backend and you’ll need to enable the APIs for those services. Setting up the cloud environment also involves granting members IAM roles to ensure that they have the right access to projects depending on their needs of their job and role at Cymbal Superstore. For example, data analysts in the Marketing department will need to have access to historic sales data in the ecommerce system. This could be a great use case for BigQuery, Google Cloud’s data warehouse solution. How would we go about giving proper access? New data analysts would need to be added to the data_analyst group. Managing permissions and rules at a group level is easier than keeping track of permissions for individual users. Data analysts need to access the data in the dataset or table, so they require the bigquery.dataViewer role at the proper level. Queries in BigQuery are executed as an executable job. So to submit a query, a data analyst would also need the bigquery.jobs.create permission. This permission is included in the predefined bigquery.user role. You would give the data_analyst group access to this role in the Production project. As a cloud engineer, you’ll need to know how to manage Cymbal Superstore’s users and groups in Cloud Identity, a service for managing users and groups if you are not doing so via Google Workspace. Google Cloud Observability, which used to be called Google Cloud’s operations suite and Stackdriver before that, provides metrics and logging services for all of your services, resources, and projects in your cloud architecture. To monitor metrics from multiple projects, you set up project scoping. If Cymbal Superstore’s Operations department decides to monitor metrics across all three supply chain projects in the staging environment project, you will set staging as a scoping project and then add dev and production as monitored projects. While migrating to Google Cloud, Cymbal Superstore will be moving some of its IT expenditure to operational expenses, and different departments associated with each application will be responsible for compute and storage costs. You’ll need to create a different billing account for each group and link each project to the appropriate account. The department lead in Sales and Marketing expresses particular concern about the cost of housing its data warehouse in BigQuery and how to optimize queries and storage. You’ll need to set up custom billing budgets and alerts for this department. Each department will also need you to set up billing exports that can be used to track charges.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523303

It’s now your turn to assess your experience and skills in this section with some diagnostic questions. Remember, these questions are intended to help you understand, or diagnose, which areas you’ll want to focus on in your study plan, so we don’t expect you to know all the answers yet. Please take 15 minutes to complete the diagnostic questions for this section.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523304

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523305

You'll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under ‘Where to look’ and ‘Content mapping’ to build a study plan that meets your learning needs.

#### Study plan resources: Setting Up a Cloud Solution Environment

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523306

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523307

### Planning and Configuring a Cloud Solution

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523308

In this module, you'll explore the scope of tasks involved in planning and configuring Cymbal Superstore's cloud solutions, which corresponds to the second section in the Associate Cloud Engineer exam guide. We'll start by discussing some considerations for how you would decide the kinds of resources needed for Cymbal Superstore's application requirements. Next, you'll assess your skills in this section through 9 diagnostic questions. Then, we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Selecting resources for Cymbal Superstore’s cloud solutions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523309

Let’s discuss how to evaluate Cymbal Superstore’s existing applications so that we can plan and configure a cloud solution for them. The first planning meeting for Cymbal Superstore’s migration to Google Cloud was a success! Everyone is excited to get into the details of how Cymbal Superstore’s existing applications are going to be migrated to the cloud. As an Associate Cloud Engineer, you are tasked to help realize the cloud architect’s design during the next phase of Cymbal Superstore’s cloud migration - planning and configuring the solution. Compute options in Google Cloud include those based on virtual machines as well as those based on containers. There are also resource-based and serverless options available from a compute perspective, depending on your team’s expertise and focus. The level of control or flexibility needed by your solution can also be factors in deciding the correct compute product to use in a solution. Similarly, data solutions in Google Cloud are based on the needs of the application. Throughput and latency will determine how quickly your application responds. There are some common questions to ask while you are analyzing your data needs. Are your data processing needs transactional or analytical? Do you need to query your data in a relational way? Do you want large groups of related data to be returned through a non-relational get operation? Your application’s required network connectivity is another important facet of cloud solution planning. Does it require access to the internet, or do you just need to provide connectivity to internal components on your private network? Are you going to configure the connectivity of multiple application servers by load balancing incoming traffic across them? How are you going to protect your app against network or system outages? Let’s take a closer look at how these considerations affect Cymbal’s choices. Cymbal Superstore has three applications to migrate to Google Cloud. First, Cymbal Superstore’s Sales and Marketing has an existing web application that provides an interface for customers to look at and order products. This ecommerce app is currently based on containers and needs to have global availability and low latency. Second, delivery services is becoming an important aspect of Cymbal Superstore’s customer interactions. Cymbal Superstore would like to use Google Services to keep track of truck location. Logistics has a transportation management app implemented locally through a custom message broker. They would like to use Google Cloud to monitor the location of trucks for delivery status and the analysis of mileage for preventive maintenance. Third, Operations has decided to migrate their legacy supply chain application to the cloud. This is currently a virtual machine-based application based in a Linux operating system and is implemented with a LAMP stack. The application needs to be available local to Cymbal Superstore HQ. Cymbal Superstore’s ecommerce application is currently implemented with containers, so you decide to use Google Kubernetes Engine (GKE) for compute. For storage, you select Spanner to house Cymbal Superstore's ecommerce data because it allows for global availability and low latency. As an Associate Cloud Engineer, you assist Cymbal Superstore’s cloud architect in making these decisions and determining the appropriate configuration. Seeing as the ecommerce system is a web-based application, you will need an external http(s) load balancer. With GKE, you can do this by implementing an ingress object with a “gce” ingress class. Applying that manifest will create an external load balancer for you. As an Associate Cloud Engineer, you should be familiar with this type of load balancing in Google Cloud. Let’s move on to your role in planning and configuring Cymbal Superstore’s next application. Logistics’ transportation management system currently uses an on-premise message broker. Together with the cloud architect, you choose Pub/Sub as a solution to collect sensor data from the trucks for analysis in the cloud. You will use Cloud Run functions to pull the data from Pub/Sub and start a Dataflow pipeline. Dataflow will feed the data into Bigtable to store your sensor data. You can run federated queries in BigQuery to visualize your data in Looker. As an Associate Cloud Engineer, you’ll need to help configure all these resources. Monitoring and analysis of this data is done close to headquarters, so Cymbal opts for a regional VPC solution for this application. Finally, you play a role in helping configure Cymbal Superstore’s third application. Cymbal Superstore’s existing supply chain application is implemented using virtual machines with a Linux operating system and a common Linux, Apache, MySql and PHP development stack. After analyzing this application’s needs, the cloud architect determines that the recommended cloud solution is to migrate this application to Compute Engine virtual instances. The data solution uses a Cloud SQL instance configured with a MySQL database. The recommended network access would be external https access to the region for partners and internal connectivity between the application and the backing database service.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523310

It’s now your turn to assess your experience and skills in this section with some diagnostic questions. Remember, these questions are intended to help you understand, or diagnose, which areas you’ll want to focus on in your study plan, so we don’t expect you to know all the answers yet. Please take 15 minutes to complete the diagnostic questions for this section.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523311

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523312

You'll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under ‘Where to look’ and ‘Content mapping’ to build a study plan that meets your learning needs.

#### Study plan resources: Planning and Configuring a Cloud Solution

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523313

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523314

### Deploying and Implementing a Cloud Solution

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523315

In this module, you'll explore the scope of tasks involved in deploying and implementing Cymbal Superstore's recommended cloud solutions. These tasks correspond to the third section of the Associate Cloud Engineer exam guide. We'll start by discussing how to deploy and implement Cymbal Superstore's cloud solutions. Next, you'll assess your skills in this section through 10 diagnostic questions. Then we'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan.

#### Deploying and implementing Cymbal Superstore’s cloud recommended solutions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523316

Way to go! You've planned and configured cloud solutions for Cymbal Superstore's application requirements. It's time to think about how you can deploy and implement the resources needed to realize the company's goals. You've worked hard to make sure the resource entities and policies are set up correctly for Cymbal Superstore's cloud architecture. You've also selected cloud products for the applications Cymbal Superstore has decided to migrate to the cloud. Solution deployment is a critical part of your role. As an Associate Cloud Engineer, you're expected to have the knowledge to implement specific compute solutions, including Compute Engine, Kubernetes Engine, Cloud Run and Cloud Run functions. Understanding availability, concurrency, connectivity, and access options of these services are keys to success as you deploy them to support your needs. Solutions you implement in Google Cloud will also require data stores. Google Cloud's data solutions include products that utilize relational and no SQL data structures. There are different products that support transactional and analytical use cases. Some solutions are optimized for low latency and global availability. Properly implementing software-defined networking will ensure your application frontends are accessible, and your application backends are secured. A common devops practice is to deploy your infrastructure in a declarative way, and source control configuration files. Deploying resources through infrastructure as code reduces human error, and speeds up resource allocation. Knowing how to do this in the context of your role as an Associate Cloud Engineer is yet another tool you have at your disposal. As a review, here are Cymbal Superstore's proposed solutions. Their ecommerce solution, based on container management provided by Google Kubernetes Engine, data, provided by the globally available, horizontally scalable capabilities of Spanner, and external Application Load Balancer for user access. This use case also has a need for historical sales data to be analyzed by BigQuery, Google Cloud's modern data warehouse implementation. The transportation management cloud solution monitors Pub/Sub for incoming sensor data, triggers a Cloud Run function as new messages are posted to a specific topic, and starts a Dataflow job to transform data and save it into Bigtable. Finally, the supply chain application implements managed instance groups in Compute Engine. The backend store for this solution is Cloud SQL. Connectivity between the backend database and the Compute Engine instances is via TCP internal to the VPC. For the supply chain app, external access will be achieved via a regional HTTPS load balancer. Three ways you can interact with Google Cloud to work with and deploy services are via the Cloud console, the command line, and programmatically. Let's look at these in a little more detail. You want to implement a compute instance for the Cymbal Superstore development team to start developing code on. One of the ways you can do this is via the Google Cloud console. The screenshot shows some of the settings you’ll need to specify as you create this instance: The name of the instance, the region and zone where the instance resides, the machine configuration, the boot disk, and network settings and other persistent disks you're going to attach to them. Cymbal Superstore's supply chain app needs a Cloud SQL backend. Here’s an example of how you would do this via the CLI. Notice the parameters required include the name, resources, and region specified. Remember, you can access the CLI by loading the Google Cloud SDK on your local machine. You can also use Cloud Shell, a cloud-based terminal with the gcloud CLI already installed on it. The transportation management system is using Cloud Run functions. Cloud Run functions gives you the option of deploying your function code from the local directory where it resides. Here’s an example of the command to deploy a Cloud Run function with a Pub/Sub trigger from a directory on your local machine. trans_mg_function is going to be the name of the deployed function based on the logic in the directory. The runtime flag specifies the Python interpreter you want to use as you parse the function. The trigger-topic flag is the Pub/Sub topic you want to monitor. The data sent to your function includes the Pub/Sub event data and metadata.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523317

It’s now your turn to assess your experience and skills in this section with some diagnostic questions. Remember, these questions are intended to help you understand, or diagnose, which areas you’ll want to focus on in your study plan, so we don’t expect you to know all the answers yet. Please take 15 minutes to complete the diagnostic questions for this section.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523318

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523319

You'll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under ‘Where to look’ and ‘Content mapping’ to build a study plan that meets your learning needs.

#### Study plan resources: Deploying and Implementing a Cloud Solution

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523320

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523321

### Ensuring Successful Operation of a Cloud Solution

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523322

In this module, you’ll explore the scope of ensuring successful cloud operations. This involves managing the compute, storage, and networking resources as well as monitoring and logging tasks. These areas correspond to the fourth section of the Associate Cloud Engineer exam guide. We’ll start by discussing your role as an Associate Cloud Engineer in managing Cymbal Superstore’s cloud solutions. Next, you’ll assess your skills in this area through 9 diagnostic questions. When we review the questions, identify the resources you’ll want to include in your study plan.

#### Managing Cymbal Superstore’s cloud solutions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523323

Now that Cymbal Superstore’s cloud solutions have been deployed and implemented, your role as an Associate Cloud Engineer shifts focus to maintaining successful operations. Let’s explore what that means. In deploying and implementing Cymbal Superstore’s cloud architecture, you needed to know how to work with various compute, storage, and networking resources on Google Cloud. To ensure successful operations, an Associate Cloud Engineer needs the knowledge and skills to manage the resources used in an organization’s cloud solutions. You also need to be able to use Google Cloud Observability for monitoring and logging. Cymbal Superstore’s supply chain management app is made of up of resources implemented close to their headquarters in Minneapolis, Minnesota. It is architected using Compute Engine. Managed instance groups let the application scale automatically and remain available across zonal outages. Sometimes, the instance template that that group is based on might need to be changed. Some reasons why you might want to do this include the following: Updating the operating system of your instances. Conducting A/B or canary testing of capability upgrades. Changing the disk type, or attached disks, attached to your instances. Once you do update the template, you’ll need to ensure the change is propagated to all the VM instances in the group. Cymbal Superstore’s ecommerce app is architected using containers deployed to GKE pods. As an Associate Cloud Engineer on the ecommerce team, you might be asked to configure and monitor external connectivity. An external http(s) load balancer is a solution that advertises a single global IP, provides content close to your end user, and forwards content to backends that are available globally. Cymbal Superstore’s transportation management app uses Cloud Run functions to monitor incoming sensor data and implements a Dataflow pipeline that uses a sink to write data to Bigtable. As an Associate Cloud Engineer, it is common to provide information about the sources and sinks required by a pipeline to the data engineer responsible for developing it. You also need to know how to monitor your incoming data stream and manage your Cloud Run function instances. Let’s specifically discuss how you might set up the resources required to query that data on a regular basis. What would it involve? Let’s think about Cymbal Superstore’s transportation management app. You can use BigQuery sql to query your Bigtable data by defining a permanent external table using the Google Cloud console or the bq command line tool. You do this by creating a table definition file, which includes the uri for the table in Bigtable, and information about the column families and the columns defined in the table. The entries for the table definition are written in JSON. Next, you create the external table reference with a bq mk command. On screen, you’ll see an example of what it might look like for our Cymbal Superstore example. Then, you can submit the query using BigQuery sql.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523324

It’s now your turn to assess your experience and skills in this section with some diagnostic questions. Remember, these questions are intended to help you understand, or diagnose, which areas you’ll want to focus on in your study plan, so we don’t expect you to know all the answers yet. Please take 15 minutes to complete the diagnostic questions for this section.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523325

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523326

You'll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under ‘Where to look’ and ‘Content mapping’ to build a study plan that meets your learning needs.

#### Study plan resources: Ensuring Successful Operation of a Cloud Solution

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523327

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523328

### Configuring Access and Security

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523329

In this module, you'll explore the scope of configuring access and security. This involves managing IAM as well as service accounts for cloud solutions. These areas correspond to the fifth and last section of the Associate Cloud Engineer exam guide. We'll start by discussing your role as an Associate Cloud Engineer in managing access for Cymbal Superstore's cloud solutions. Next, you'll assess your skills in this section of the exam guide through five diagnostic questions. There are fewer questions for this module. While this is an important area for you to understand as an Associate Cloud Engineer, it is narrower in scope. When we review the questions, identify the resources you'll want to include in your study plan.

#### Managing access for Cymbal Superstore’s cloud solutions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523330

As Cymbal Superstore uses its application on Google Cloud, an Associate Cloud Engineer plays an ongoing role in configuring and managing IAM access and service accounts. Let's explore some examples of how you might do this at Cymbal Superstore. To successfully perform the Associate Cloud Engineer role at Cymbal Superstore, you need to be able to manage Identity and Access Management, or IAM, in Google Cloud. We talked about the basics of IAM in the first module from the perspective of setting up cloud projects and accounts. Here, you'll consider skills involved in managing access. You'll also need to be familiar with service accounts and recommended practices to manage them in Google Cloud. To give you a better idea of what configuring access and security involves in practice, let's explore an example of where you might use a service account at Cymbal Superstore. Cymbal Superstore's supply chain app is built on a lamp stack using Google Compute Engineer virtual machine instances. It uses Cloud SQL as a backing data store. The app needs to talk to Cloud SQL to update inventory levels. It does this through a service account attached to the virtual machine that it runs on. Service accounts are designed to enable machine-to-machine communication for just this purpose. The first step in setting up a service account for Cymbal Superstore's supply chain app is to create the service account. Next, you assign permissions to the service account you just created. Finally, you attach that service account to a Compute Engine virtual machine. Attaching a service account allows the virtual machine and all the apps running on it to use the permissions assigned to the service account. Let's look at these steps in more detail. Go to the project you want to add the service account to. Service accounts are both identities and managed resources in Google Cloud. Select the service account link in the IAM menu of your project, then select create service account. In the dialog that comes up, name your service account and note the email address associated with it. You can also provide a description of what this service account does. Once you select create, your new service account will be added to the list of all your service accounts. Select the three ellipsis under actions for a list of all the actions you can perform on your new service account. Next, we'll use one of these choices to manage permissions for the service account. Select manage permissions under the actions dialog in the service account list. A new menu let's you pick your service account and add permissions to it. Copy your service account email address identifier. Search or browse the permissions to find the ones you need to add. In our example, we'll give our service account permissions as a Cloud SQL instance user. Finally, when you add your virtual machine instance, you have a chance to add the service account to it under the identity and API access section. This covers authorization. Authentication is another important aspect of both user accounts and service accounts that you should be familiar with as an Associate Cloud Engineer.

#### Introduction: Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523331

It’s now your turn to assess your experience and skills in this section with some diagnostic questions. Remember, these questions are intended to help you understand, or diagnose, which areas you’ll want to focus on in your study plan, so we don’t expect you to know all the answers yet. Please take 15 minutes to complete the diagnostic questions for this section.

#### Diagnostic questions

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523332

#### Creating your study plan

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523333

You'll now review the diagnostic questions and your answers to help you identify what to include in your study plan. The diagnostic questions align with these objectives of this exam section. Use the PDF resource that follows to review the questions and how you answered them. Pay specific attention to the rationale for both the correct and incorrect answers. Use the resources detailed under ‘Where to look’ and ‘Content mapping’ to build a study plan that meets your learning needs.

#### Study plan resources: Configuring Access and Security

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523334

#### Knowledge Check

- https://www.cloudskillsboost.google/paths/11/course_templates/77/quizzes/523335

### Your next steps

#### Your next steps

- https://www.cloudskillsboost.google/paths/11/course_templates/77/video/523336

In this module, you'll focus on creating your individualized study plan. You'll use the notes you've been taking throughout this course to put together a study plan for each week in your Associate Cloud Engineer journey. Now that you've explored all five sections of the exam guide, consider what you've learned about your knowledge and skills through the diagnostic questions in this course. You should have a better understanding of what areas you need to focus on and what resources are available. Think about the answers to these questions: When will you take the exam? How many weeks does that give you to prepare? How many hours can you realistically spend preparing for the exam each week? How many total hours will you prepare? Be sure to leave enough time at the end of your plan to retake the diagnostic questions and the sample questions and fill in any gaps in your knowledge that may remain. Take a few minutes to think about how much time you will allocate to preparing for the exam and note your answers in the workbook. The number of weeks in your preparation journey will depend on a variety of factors, such as your prior experience with Google Cloud and how much time you have available to dedicate to studying each week. You might choose to focus on specific courses or skill badges each week, such as in this sample study plan or instead focus your weekly study on a specific topic, such as configuring VPCs. Once you have a high level idea of how many weeks you have to study and how you want to determine your weekly focus, you'll want to build out a plan with weekly goals and study activities. Use the template in your workbook to plan your study goals for each week. Consider: What exam guide sections or topic areas will you focus on? What courses or specific modules will help you learn more? What skill badges or labs will you work on for hands-on practice? What documentation links will you review? What additional resources will you use, such as sample questions? You may do some or all of these study activities each week. Let's review an example. If you've identified configuring access using IAM as a particular area you need to study, you might choose to structure your study for a week to include targeted modules from the on-demand training, a related skill badge for hands-on practice and documentation. Alternately, you might choose one week to complete an entire course and another week to focus on a skill badge. You can determine the approach that fits your existing skillset. Find the weekly study template at the end of your workbook. Duplicate the weekly template for the number of weeks in your individual preparation journey. Remember, you may need to adjust your plans based on the areas where you need to learn more. For more information about the resources we've discussed in this course, refer to your notes and the student copy of the slides. To register for the exam, follow the link on the ACE certification information page using the URL found on the slide. Thank you for attending the Preparing for your Associate Cloud Engineer Journey. Good luck as you begin your journey to study for the Associate Cloud Engineer certification.

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/77/documents/523337

### Your Next Steps

## 03: Google Cloud Fundamentals: Core Infrastructure

- https://www.cloudskillsboost.google/paths/11/course_templates/60

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531558

Hi, and welcome to the Google Cloud Fundamentals: Core Infrastructure training course. The goal of this course is to provide you with an overview of Google Cloud. Google Cloud offerings can be broadly categorised as compute, storage, big data, machine learning, and application services for web, mobile, analytics, and back-end solutions. Through a combination of videos, quizzes, and hands-on labs, you’ll learn the value of Google Cloud and how cloud solutions factor into business strategies. The intended target audience of today’s course consists of solutions developers, systems operations professionals, and solution architects planning to deploy applications and create application environments on Google Cloud. The course will also be useful for business decision makers evaluating Google Cloud. While you should be happy to hear that we’ll be finding out about services and concepts that are specific to Google Cloud in this course, do keep in mind that, as a ‘fundamentals’ level course, some content will be geared towards learners who are entirely new to cloud technologies. This course has no prerequisites, although it’s helpful be familiar with application development, Linux operating systems, systems operations, and data analytics/machine learning to best understand the technologies covered. There are seven key learning objectives that we’re hoping to achieve. By the end of this course, you should be able to: Identify the purpose and value of Google Cloud products and services. Define how infrastructure is organized and controlled in Google Cloud. Explain how to create a basic infrastructure in Google Cloud. Select and use Google Cloud storage options. Describe the purpose and value of Google Kubernetes Engine. Identify the use cases for serverless Google Cloud services. And combine Google Cloud knowledge with prompt engineering to improve Gemini responses. OK, all set? Let’s begin!

### Introducing Google Cloud

#### Cloud computing overview

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531559

Let’s start at the beginning with an overview of cloud computing. The cloud is a hot topic these days, but what exactly is it? The US National Institute of Standards and Technology created the term cloud computing, although there is nothing US-specific about it. Cloud computing is a way of using information technology (IT) that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they require without the need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they’re flexible, so customers can be. If customers need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. And that's it, that's the definition of cloud. But why is the cloud model so compelling nowadays? To understand why, we need to look at some history. The trend towards cloud computing started with a first wave known as colocation. Colocation gave users the financial efficiency of renting physical space, instead of investing in data center real estate. Virtualized data centers of today, which are the second wave, share similarities with the private data centers and colocation facilities of decades past. The components of virtualized data centers match the physical building blocks of hosted computing—servers, CPUs, disks, load balancers, and so on—but now they’re virtual devices. With virtualization, enterprises still maintain the infrastructure; but it also remains a user-controlled and user-configured environment. Several years ago, Google realized that its business couldn’t move fast enough within the confines of the virtualization model. So Google switched to a container-based architecture— a fully automated, elastic third-wave cloud that consists of a combination of automated services and scalable data. Services automatically provision and configure the infrastructure used to run applications. Today, Google Cloud makes this third-wave cloud available to Google customers. Google believes that, in the future, every company, regardless of size or industry, will differentiate itself from its competitors through technology. Increasingly, that technology will be in the form of software. Great software is based on high-quality data. This means that every company is, or will eventually become, a data company.

#### IaaS and PaaS

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531560

The move to virtualized data centers introduced customers to two new types of offerings: infrastructure as a service, commonly referred to as IaaS, and platform as a service, or PaaS. IaaS offerings provide raw compute, storage, and network capabilities, organized virtually into resources that are similar to physical data centers. Compute Engine is an example of a Google Cloud IaaS service. PaaS offerings, in contrast, bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. App Engine is an example of a Google Cloud PaaS service. In the IaaS model, customers pay for the resources they allocate ahead of time; in the PaaS model, customers pay for the resources they actually use. As cloud computing has evolved, the momentum has shifted toward managed infrastructure and managed services. Leveraging managed resources and services allows companies to concentrate more on their business goals and spend less time and money on creating and maintaining their technical infrastructure. It allows companies to deliver products and services to their customers more quickly and reliably. Serverless is yet another step in the evolution of cloud computing. It allows developers to concentrate on their code, rather than on server configuration, by eliminating the need for any infrastructure management. Serverless technologies offered by Google include Cloud Run, which allows customers to deploy their containerized microservices based application in a fully-managed environment. and Cloud Run functions, which manages event-driven code as a pay-as-you-go service. While it’s outside the scope of this course, you might have heard about software as a service, SaaS, and wondered what it is and how it fits into the Cloud ecosphere. SaaS provides the entire application stack, delivering an entire cloud-based application that customers can access and use. Software as a Service applications are not installed on your local computer. Instead, they run in the cloud as a service and are consumed directly over the internet by end users. Popular Google applications such as Gmail, Docs, and Drive, that are a part of Google Workspace, are all examples of SaaS.

#### The Google Cloud network

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531561

Google Cloud runs on Google’s own global network. It’s the largest network of its kind, and Google has invested billions of dollars over many years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by leveraging more than 100 content caching nodes worldwide. These are locations where high demand content is cached for quicker access, allowing applications to respond to user requests from the location that will provide the quickest response time. Google Cloud’s locations underpin all of the important work we do for our customers. From redundant cloud regions to high- bandwidth connectivity via subsea cables, every aspect of our infrastructure is designed to deliver your services to your users, no matter where they are around the world. Google Cloud’s infrastructure is based in seven major geographic locations: North America, South America, Europe, Africa, the Middle East, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency, the latter of which measures the time a packet of information takes to travel from its source to its destination. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy. You can run also resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, such as a natural disaster. Some of Google Cloud’s services support placing resources in what we call a multi-region. For example, Spanner multi-region configurations allow you to replicate the database's data not just in multiple zones, but in multiple zones across multiple regions, as defined by the instance configuration. These additional replicas enable you to read data with low latency from multiple locations close to or within the regions in the configuration, like The Netherlands, and Belgium. The number of zones and regions Google Cloud supports is increasing all the time. You can find the most up-to-date numbers at cloud.google.com/about/locations.

#### Environmental impact

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531562

The virtual world, which includes Google Cloud’s network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy. Altogether, existing data centers use roughly 2% of the world’s electricity. With this in mind, Google works to make their data centers run as efficiently as possible. Just like our customers, Google is trying to do the right things for the planet. We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals. Therefore, it’s useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a standard that maps out a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste. As an example of how this is being done, here’s Google’s data center in Hamina, Finland. This facility is one of the most advanced and efficient data centers in the Google fleet. Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world. In our founding decade, Google became the first major company to be carbon neutral. In our second decade, we were the first company to achieve 100% renewable energy. By 2030, we aim to be the first major company to operate completely carbon free.

#### Security

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531563

Nine of Google’s services have more than one billion users each, and so you can be assured that security is always on the minds of Google's employees. Design for security is prevalent throughout the infrastructure that Google Cloud and Google services run on. Let's talk about a few ways Google works to keep customers' data safe. The security infrastructure can be explained in progressive layers, starting from the physical security of our data centers, continuing on to how the hardware and software that underlie the infrastructure are secured, and finally, describing the technical constraints and processes in place to support operational security. We begin with the Hardware infrastructure layer which comprises three key security features: The first is hardware design and provenance. Both the server boards and the networking equipment in Google data centers are custom-designed by Google. Google also designs custom chips, including a hardware security chip that's currently being deployed on both servers and peripherals. The next feature is a secure boot stack. Google server machines use a variety of technologies to ensure that they are booting the correct software stack, such as cryptographic signatures over the BIOS, bootloader, kernel, and base operating system image. This layer's final feature is premises security. Google designs and builds its own data centers, which incorporate multiple layers of physical security protections. Access to these data centers is limited to only a very small number of Google employees. Google additionally hosts some servers in third-party data centers, where we ensure that there are Google-controlled physical security measures on top of the security layers provided by the data center operator. Next is the Service deployment layer, where the key feature is encryption of inter-service communication. Google’s infrastructure provides cryptographic privacy and integrity for remote procedure call (“RPC”) data on the network. Google’s services communicate with each other using RPC calls. The infrastructure automatically encrypts all infrastructure RPC traffic that goes between data centers. Google has started to deploy hardware cryptographic accelerators that will allow it to extend this default encryption to all infrastructure RPC traffic inside Google data centers. Then we have the User identity layer. Google’s central identity service, which usually manifests to end users as the Google login page, goes beyond asking for a simple username and password. The service also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also employ secondary factors when signing in, including devices based on the Universal 2nd Factor (U2F) open standard. On the Storage services layer we find the encryption at rest security feature. Most applications at Google access physical storage (in other words, “file storage”) indirectly via storage services, and encryption using centrally managed keys is applied at the layer of these storage services. Google also enables hardware encryption support in hard drives and SSDs. The next layer up is the Internet communication layer, and this comprises two key security features. Google services that are being made available on the internet, register themselves with an infrastructure service called the Google Front End, which ensures that all TLS connections are ended using a public-private key pair and an X.509 certificate from a Certified Authority (CA), as well as following best practices such as supporting perfect forward secrecy. The GFE additionally applies protections against Denial of Service attacks. Also provided is Denial of Service (“DoS”) protection. The sheer scale of its infrastructure enables Google to simply absorb many DoS attacks. Google also has multi-tier, multi-layer DoS protections that further reduce the risk of any DoS impact on a service running behind a GFE. The final layer is Google's Operational security layer which provides four key features. First is intrusion detection. Rules and machine intelligence give Google’s operational security teams warnings of possible incidents. Google conducts Red Team exercises to measure and improve the effectiveness of its detection and response mechanisms. Next is reducing insider risk. Google aggressively limits and actively monitors the activities of employees who have been granted administrative access to the infrastructure. Then there’s employee U2F use. To guard against phishing attacks against Google employees, employee accounts require use of U2F-compatible Security Keys. Finally, there are stringent software development practices. Google employs central source control and requires two-party review of new code. Google also provides its developers libraries that prevent them from introducing certain classes of security bugs. Additionally, Google runs a Vulnerability Rewards Program where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications. You can learn more about Google’s technical-infrastructure security at cloud.google.com/security/security-design.

#### Open source ecosystems

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531564

Some organizations are afraid to bring their workloads to the cloud because they're afraid they'll get locked into a particular vendor. However, if, for whatever reason, a customer decides that Google is no longer the best provider for their needs, we provide them with the ability to run their applications elsewhere. Google publishes key elements of technology using open source licenses to create ecosystems that provide customers with options other than Google. For example, TensorFlow, an open source software library for machine learning developed inside Google, is at the heart of a strong open source ecosystem. Google provides interoperability at multiple layers of the stack. Kubernetes and Google Kubernetes Engine give customers the ability to mix and match microservices running across different clouds, while Google Cloud Observability lets customers monitor workloads across multiple cloud providers.

#### Pricing and billing

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531565

To round off this section of the course, let’s take a brief look at Google Cloud’s pricing structure. Google was the first major cloud provider to deliver per-second billing for its infrastructure-as-a-service compute offering, Compute Engine. In addition, per-second billing is now also offered for users of Google Kubernetes Engine (our container infrastructure as a service), Dataproc (which is the equivalent of the big data system Hadoop, but operating as a service), and App Engine flexible environment VMs (a platform as a service). Compute Engine offers automatically applied sustained-use discounts, which are automatic discounts that you get for running a virtual machine instance for a significant portion of the billing month. Specifically, when you run an instance for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. Custom virtual machine types allow Compute Engine virtual machines to be fine-tuned with optimal amounts of vCPU and memory for their applications so that you can tailor your pricing for your workloads. Our online pricing calculator can help estimate your costs. Visit cloud.google.com/products/calculator to try it out. Now, you’re probably thinking, “How can I make sure I don’t accidentally run up a big Google Cloud bill?” You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month’s spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you’ll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud Console that allows you to monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack, protecting both account owners and the Google Cloud community as a whole. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources you can have in your projects. For example, by default, each Google Cloud project has a quota allowing it no more than 15 Virtual Private Cloud networks. Although projects all start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531566

### Resources and Access in the Cloud

#### Google Cloud resource hierarchy

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531567

In this section of the course we’ll look at the functional structure of Google Cloud. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. At the first level are resources. These represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are also inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let’s take a look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they’re billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can’t be changed after creation. They’re what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don’t have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It’s helpful to know that these Google-generated numbers exist, but we won’t explore them much in this course. They’re mainly used internally by Google Cloud to keep track of resources. Google Cloud’s Resource Manager tool is designed to programmatically help you manage projects. It’s an API that can gather a list of all the projects associated with an account, create new projects, update existing projects, and delete projects. It can even recover projects that were previously deleted,and can be accessed through the RPC API and the REST API. The third level of the Google Cloud resource hierarchy is folders. Folders let you assign policies to resources at a level of granularity you choose. The resources in a folder inherit policies and permissions assigned to that folder. A folder can contain projects, other folders, or a combination of both. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. As previously mentioned, the resources in a folder inherit policies and permissions from that folder. For example, if you have two different projects that are administered by the same team, you can put policies into a common folder so they have the same permissions. Doing it the other way--putting duplicate copies of those policies on both projects–could be tedious and error-prone. if you needed to change permissions on both resources, you would now have to do that in two places instead of just one. To use folders, you must have an organization node, which is the very topmost resource in the Google Cloud hierarchy. Everything else attached to that account goes under this node, which includes folders, projects, and other resources. There are some special roles associated with this top-level organization node. For example, you can designate an organization policy administrator so that only people with privilege can change policies. You can also assign a project creator role, which is a great way to control who can create projects and, therefore, who can spend money. How a new organization node is created depends on whether your company is also a Google Workspace customer. If you have a Workspace domain, Google Cloud projects will automatically belong to your organization node. Otherwise, you can use Cloud Identity, Google’s identity, access, application, and endpoint management platform, to generate one. Once created, a new organization node will let anyone in the domain create projects and billing accounts, just as they could before. folders underneath it and put projects into it. Both folders and projects are considered to be “children” of the organization node.

#### Identity and Access Management (IAM)

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531568

When an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The “who” part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A “who” is also called a “principal.” Each principal has its own identifier, usually an email address. The “can do what” part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. When a principal is given a role on a specific element of the resource hierarchy, the resulting policy applies to both the chosen element and all the elements below it in the hierarchy. You can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. Deny policies, like allow policies, are inherited through the resource hierarchy. There are three kinds of roles in IAM: basic, predefined, and custom. The first role type is basic. Basic roles are quite broad in scope. When applied to a Google Cloud project, they affect all resources in that project. Basic roles include owner, editor, viewer, and billing administrator. Let’s look at these basic roles in a bit more detail. Project viewers can access resources but can’t make changes. Project editors can access and make changes to a resource. And project owners can also access and make changes to a resource. In addition, project owners can manage the associated roles and permissions and set up billing. Often companies want someone to control the billing for a project but not be able to change the resources in the project. This is possible through a billing administrator role. A word of caution: If several people are working together on a project that contains sensitive data, basic roles are probably too broad. Fortunately, IAM provides other ways to assign permissions that are more specifically tailored to meet the needs of typical job roles. This brings us to the second type of role, predefined roles. Specific Google Cloud services offer sets of predefined roles, and they even define where those roles can be applied. Let’s look at Compute Engine, for example, a Google Cloud product that offers virtual machines as a service. With Compute Engine, you can apply specific predefined roles—such as “instanceAdmin”—to Compute Engine resources in a given project, a given folder, or an entire organization. This then allows whoever has these roles to perform a specific set of predefined actions. But what if you need to assign a role that has even more specific permissions? That’s when you’d use a custom role. Many companies use a “least-privilege” model in which each person in your organization is given the minimal amount of privilege needed to do their job. So, for example, maybe you want to define an “instanceOperator” role to allow some users to stop and start Compute Engine virtual machines, but not reconfigure them. Custom roles will allow you to define those exact permissions. Before you start creating custom roles, please note two important details. First, you’ll need to manage the permissions that define the custom role you’ve created. Because of this, some organizations decide they’d rather use the predefined roles. And second, custom roles can only be applied to either the project level or organization level. They can’t be applied to the folder level.

#### Service accounts

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531569

Imagine you have a Compute Engine virtual machine running a program that needs to access other cloud services regularly. Instead of requiring a person to manually grant access each time the program runs, you can give the virtual machine itself the necessary permissions. This is where service accounts come in. Service accounts allow you to assign specific permissions to a virtual machine, so it can interact with other cloud services without human intervention. Let’s say you have an application running in a virtual machine that needs to store data in Cloud Storage, but you don’t want anyone on the internet to have access to that data - just that particular virtual machine. You can create a service account to authenticate that VM to Cloud Storage. Service accounts are named with an email address, but instead of passwords they use cryptographic keys to access resources. So, if a service account has been granted Compute Engine’s Instance Admin role, this would allow an application running in a VM with that service account to create, modify, and delete other VMs. Service accounts do need to be managed. For example, maybe Alice needs to manage which Google accounts can act as service accounts, while Bob just needs to be able to view a list of service accounts. Fortunately, in addition to being an identity, a service account is also a resource, so it can have IAM policies of its own attached to it. This means that Alice can have the editor role on a service account, and Bob can have the viewer role. This is just like granting roles for any other Google Cloud resource.

#### Cloud Identity

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531570

When new Google Cloud customers start using the platform, it’s common to log in to the Google Cloud Console with a Gmail account and then use Google Groups to collaborate with teammates who are in similar roles. Although this approach is easy to start with, it can present challenges later because the team’s identities are not centrally managed. This can be problematic if, for example, someone leaves the organization. With this setup, there’s no easy way to immediately remove a user’s access to the team’s cloud resources. With a tool called Cloud Identity, organizations can define policies and manage their users and groups using the Google Admin Console. Admins can log in and manage Google Cloud resources using the same usernames and passwords they already use in existing Active Directory or LDAP systems. Using Cloud Identity also means that when someone leaves an organization, an administrator can use the Google Admin Console to disable their account and remove them from groups. Cloud Identity is available in a free edition and also in a premium edition that provides capabilities to manage mobile devices. If you’re a Google Cloud customer who is also a Google Workspace customer, this functionality is already available to you in the Google Admin Console.

#### Interacting with Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531571

There are four ways to access and interact with Google Cloud. The Google Cloud console, the Google Cloud SDK and Cloud Shell, the APIs, and the Google Cloud app. Let’s explore each of those now. First is the Google Cloud console, which is Google Cloud’s graphical user interface, or GUI, that helps you deploy, scale, and diagnose production issues in a simple web-based interface. With the Google Cloud console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Google Cloud console also provides a search facility to quickly find resources and connect to instances via SSH in the browser. Second is through the Google Cloud SDK and Cloud Shell. The Google Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These include the Google Cloud CLI, which provides the main command-line interface for Google Cloud products and services, and bq, a command-line tool for BigQuery. When installed, all of the tools within the Google Cloud SDK are located under the bin directory. Cloud Shell provides command-line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent 5 gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. With Cloud Shell, the Google Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The third way to access Google Cloud is through application programming interfaces, or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Google Cloud console includes a tool called the Google APIs Explorer that shows which APIs are available, and in which versions. You can try these APIs interactively, even those that require user authentication. So, suppose you’ve explored an API, and you’re ready to build an application that uses it. Do you have to start coding from scratch? No. Google provides Cloud Client libraries and Google API Client libraries in many popular languages to take a lot of the drudgery out of the task of calling Google Cloud from your code. Languages currently represented in these libraries are Java, Python, PHP, C#, Go, Node.js, Ruby, and C++. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app provides up-to-date billing information for your projects and billing alerts for projects that are going over budget. You can set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app also offers alerts and incident management. You can download the Google Cloud app at cloud.google.com/app.

#### Google Cloud Fundamentals: Getting Started with Cloud Marketplace

- https://www.cloudskillsboost.google/paths/11/course_templates/60/labs/531572

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531573

### Virtual Machines and Networks in the Cloud

#### Virtual Private Cloud networking

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531574

In this section of the course, we’re going to explore how Google Compute Engine works with a focus on virtual networking. Many users start with Google Cloud by defining their own virtual private cloud inside their first Google Cloud project or by starting with the default virtual private cloud. So, what is a virtual private cloud? A virtual private cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud – like Google Cloud! On a VPC, customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but this private cloud is hosted remotely by a public cloud provider. This means that VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing. VPC networks connect Google Cloud resources to each other and to the internet. This includes segmenting networks, using firewall rules to restrict access to instances, and creating static routes to forward traffic to specific destinations. Here's something that tends to surprise a lot of new Google Cloud users: Google VPC networks are global. They can also have subnets, which is a segmented piece of the larger network, in any Google Cloud region worldwide. Subnets can span the zones that make up a region. This architecture makes it easy to define network layouts with global scope. Resources can even be in different zones on the same subnet. The size of a subnet can be increased by expanding the range of IP addresses allocated to it, and doing so won’t affect virtual machines that are already configured. For example, let’s take a VPC network named vpc1 that has two subnets defined in the asia-east1 and us-east1 regions. If the VPC has three Compute Engine VMs attached to it, it means they’re neighbors on the same subnet even though they’re in different zones. This capability can be used to build solutions that are resilient to disruptions yet retain a simple network layout.

#### Compute Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531575

Earlier in the course, we explored infrastructure as a service, or IaaS. Now let’s explore Google Cloud’s IaaS solution: Compute Engine. With Compute Engine, users can create and run virtual machines on Google infrastructure. There are no upfront investments, and thousands of virtual CPUs can run on a system that’s designed to be fast and to offer consistent performance. Each virtual machine contains the power and functionality of a full-fledged operating system. This means a virtual machine can be configured much like a physical server: by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system. A virtual machine instance can be created via the Google Cloud console, which is a web-based tool to manage Google Cloud projects and resources, the Google Cloud CLI, or the Compute Engine API. The instance can run Linux and Windows Server images provided by Google or any customized versions of these images. You can also build and run images of other operating systems and flexibly reconfigure virtual machines. A quick way to get started with Google Cloud is through the Cloud Marketplace, which offers solutions from both Google and third-party vendors. With these solutions, there’s no need to manually configure the software, virtual machine instances, storage, or network settings, although many of them can be modified before launch if that’s required. Most software packages in Cloud Marketplace are available at no additional charge beyond the normal usage fees for Google Cloud resources. Some Cloud Marketplace images charge usage fees, particularly those published by third parties, with commercially licensed software, but they all show estimates of their monthly charges before they’re launched. At this point, you might be wondering about Compute Engine’s pricing and billing structure. For the use of virtual machines, Compute Engine bills by the second with a one-minute minimum, and sustained-use discounts start to apply automatically to virtual machines the longer they run. So, for each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every additional minute. Compute Engine also offers committed-use discounts. This means that for stable and predictable workloads, a specific amount of vCPUs and memory can be purchased for up to a 57% discount off of normal prices in return for committing to a usage term of one year or three years. And then there are Preemptible and Spot VMs. Let’s say you have a workload that doesn’t require a human to sit and wait for it to finish–such as a batch job analyzing a large dataset. You can save money, in some cases up to 90%, by choosing Preemptible or Spot VMs to run the job. A Preemptible or Spot VM is different from an ordinary Compute Engine VM in only one respect: Compute Engine has permission to terminate a job if its resources are needed elsewhere. Although savings are possible with preemptible or spot VMs, you'll need to ensure that your job can be stopped and restarted. Spot VMs differ from Preemptible VMs by offering more features. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. However, the pricing is, currently the same for both. In terms of storage, Compute Engine doesn’t require a particular option or machine type to get high throughput between processing and persistent disks. That’s the default, and it comes to you at no extra cost. And finally, you’ll only pay for what you need with custom machine types. Compute Engine lets you choose the machine properties of your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types or by creating your own custom machine types.

#### Scaling virtual machines

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531576

As we’ve just seen, with Compute Engine, you can choose the most appropriate machine properties for your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types, or by creating custom machine types. To do this, Compute Engine has a feature called Autoscaling, where VMs can be added to or subtracted from an application based on load metrics. The other part of making that work is balancing the incoming traffic among the VMs. Google’s Virtual Private Cloud (VPC) supports several different kinds of load balancing, which we’ll explore shortly. With Compute Engine, you can in fact configure very large VMs, which are great for workloads such as in-memory databases and CPU-intensive analytics, but most Google Cloud customers start off with scaling out, not up. The maximum number of CPUs per VM is tied to its “machine family” and is also constrained by the quota available to the user, which is zone-dependent. Specifications for currently available VM machine types can be found at cloud.google.com/compute/docs/machine-types

#### Important VPC compatibilities

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531577

Now let’s explore some of the most important Virtual Private Cloud compatibility features. Much like physical networks, VPCs have routing tables. VPC routing tables are built-in so you don’t have to provision or manage a router. They’re used to forward traffic from one instance to another within the same network, across subnetworks, or even between Google Cloud zones, without requiring an external IP address. Another thing you don’t have to provision or manage for Google Cloud is a firewall. VPCs provide a global distributed firewall, which can be controlled to restrict access to instances through both incoming and outgoing traffic. Firewall rules can be defined through network tags on Compute Engine instances, which is really convenient. For example, you can tag all your web servers with, say, “WEB,” and write a firewall rule saying that traffic on ports 80 or 443 is allowed into all VMs with the “WEB” tag, no matter what their IP address happens to be. You’ll remember that VPCs belong to Google Cloud projects, but what if your company has several Google Cloud projects, and the VPCs need to talk to each other? With VPC Peering, a relationship between two VPCs can be established to exchange traffic. Alternatively, to use the full power of Identity Access Management (IAM) to control who and what in one project can interact with a VPC in another, you can configu

#### Cloud Load Balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531578

Previously, we explored how virtual machines can autoscale to respond to changing loads. But how do your customers get to your application when it might be provided by four VMs one moment, and by 40 VMs at another? That’s done through Cloud Load Balancing. The job of a load balancer is to distribute user traffic across multiple instances of an application. By spreading the load, load balancing reduces the risk that applications experience performance issues. Cloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. And because the load balancers don’t run in VMs that you have to manage, you don’t have to worry about scaling or managing them. You can put Cloud Load Balancing in front of all of your traffic: HTTP or HTTPS, other TCP and SSL traffic, and UDP traffic too. Cloud Load Balancing provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if backends become unhealthy. Cloud Load Balancing reacts quickly to changes in users, traffic, network, backend health, and other related conditions. And what if you anticipate a huge spike in demand? Say, your online game is already a hit; do you need to file a support ticket to warn Google of the incoming load? No. No so-called “pre-warming” is required. Google Cloud offers a range of load balancing solutions that can be classified based on the OSI model layer they operate at and their specific functionalities. Application Load Balancers operate at the application layer and are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content-based routing and SSL/TLS termination. Application Load Balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules you define. They are highly flexible and can be configured for both internet-facing (external) and internal applications. Network Load Balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types: Proxy Network Load Balancers also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments. Unlike proxy Network Load Balancers, passthrough Network Load Balancers do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well-suited for applications that require direct server return or need to handle a wider range of IP protocols.

#### Cloud DNS and Cloud CDN

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531579

One of the most famous free Google services is 8.8.8.8, which provides a public Domain Name Service to the world. DNS is what translates internet hostnames to addresses, and as you might imagine, Google has a highly developed DNS infrastructure. It makes 8.8.8.8 available so that everyone can take advantage of it. But what about the internet hostnames and addresses of applications built in Google Cloud? Google Cloud offers Cloud DNS to help the world find them. It’s a managed DNS service that runs on the same infrastructure as Google. It has low latency and high availability, and it’s a cost-effective way to make your applications and services available to your users. The DNS information you publish is served from redundant locations around the world. Cloud DNS is also programmable. You can publish and manage millions of DNS zones and records using the Cloud console, the command-line interface, or the API. Google also has a global system of edge caches. Edge caching refers to the use of caching servers to store content closer to end users. You can use this system to accelerate content delivery in your application by using Cloud CDN - Content Delivery Network. This means your customers will experience lower network latency, the origins of your content will experience reduced load, and you can even save money. After an Application Load Balancer is set up, Cloud CDN can be enabled with a single checkbox. There are many other CDNs available out there, of course. If you are already using one, chances are, it’s a part of Google Cloud’s CDN Interconnect partner program, and you can continue to use it.

#### Connecting networks to Google VPC

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531580

Many Google Cloud customers want to connect their Google Virtual Private Cloud networks to other networks in their system, such as on-premises networks or networks in other clouds. There are several effective ways to accomplish this. One option is to start with a Virtual Private Network connection over the internet and use Cloud VPN to create a “tunnel” connection. To make the connection dynamic, a Google Cloud feature called Cloud Router can be used. Cloud Router lets other networks and Google VPC, exchange route information over the VPN using the Border Gateway Protocol. Using this method, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But using the internet to connect networks isn't always the best option for everyone, either because of security concerns or because of bandwidth reliability. So, a second option is to consider “peering” with Google using Direct Peering. Peering means putting a router in the same public data center as a Google point of presence and using it to exchange traffic between networks. Google has more than 100 points of presence around the world. Customers who aren’t already in a point of presence can work with a partner in the Carrier Peering program to get connected. Carrier peering gives you direct access from your on-premises network through a service provider's network to Google Workspace and to Google Cloud products that can be exposed through one or more public IP addresses. One downside of peering, though, is that it isn’t covered by a Google Service Level Agreement. If getting the highest uptimes for interconnection is important, using Dedicated Interconnect would be a good solution. This option allows for one or more direct, private connections to Google. If these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%. Also, these connections can be backed up by a VPN for even greater reliability. Another option we’ll explore is Partner Interconnect, which provides connectivity between an on-premises network and a VPC network through a supported service provider. A Partner Interconnect connection is useful if a data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if the data needs don’t warrant an entire 10 GigaBytes per second connection. Depending on availability needs, Partner Interconnect can be configured to support mission-critical services or applications that can tolerate some downtime. As with Dedicated Interconnect, if these connections have topologies that meet Google’s specifications, they can be covered by an SLA of up to 99.99%, but note that Google isn’t responsible for any aspects of Partner Interconnect provided by the third-party service provider, nor any issues outside of Google's network. And the final option is Cross-Cloud Interconnect. Cross-Cloud Interconnect helps you establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud network with your network that's hosted by a supported cloud service provider. Cross-Cloud Interconnect supports your adoption of an integrated multicloud strategy. In addition to supporting various cloud service providers, Cross-Cloud Interconnect offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps.

#### Getting Started with VPC Networking and Google Compute Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/60/labs/531581

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531582

### Storage in the Cloud

#### Google Cloud storage options

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531583

Every application needs to store data, like media to be streamed or perhaps even sensor data from devices, and different applications and workloads require different storage database solutions. Google Cloud has storage options for structured, unstructured, transactional, and relational data. In this section of the course, we’ll explore Google Cloud’s five core storage products: Cloud Storage, Cloud SQL, Spanner, Firestore, and Bigtable. Depending on your application, you might use one or several of these services to do the job.

#### Cloud Storage

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531584

Let’s begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as “objects” and not as a file and folder hierarchy (file storage), or as chunks of a disk (block storage). These objects are stored in a packaged format which contains the binary form of the actual data itself, as well as relevant associated meta-data (such as date created, author, resource type, and permissions), and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. Cloud Storage is Google’s object storage product. It allows customers to store any amount of data, and to retrieve it as often as needed. It’s a fully managed scalable service that has a wide variety of uses. A few examples include serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users via Direct Download. Cloud Storage’s primary use is whenever binary large-object storage (also known as a “BLOB”) is needed for online content such as videos and photos, for backup and archived data and for storage of intermediate results in processing workflows. Cloud Storage files are organized into buckets. A bucket needs a globally unique name and a specific geographic location for where it should be stored, and an ideal location for a bucket is where latency is minimized. For example, if most of your users are in Europe, you probably want to pick a European location, so either a specific Google Cloud region in Europe, or else the EU multi-region. The storage objects offered by Cloud Storage are immutable, which means that you do not edit them, but instead a new version is created with every change made. Administrators have the option to either allow each new version to completely overwrite the older one, or to keep track of each change made to a particular object by enabling “versioning” within a bucket. If you choose to use versioning, Cloud Storage will keep a detailed history of modifications -- that is, overwrites or deletes -- of all objects contained in that bucket. If you don’t turn on object versioning, by default new versions will always overwrite older versions. With object versioning enabled, you can list the archived versions of an object, restore an object to an older state, or permanently delete a version of an object, as needed. In many cases, personally identifiable information may be contained in data objects, so controlling access to stored data is essential to ensuring security and privacy are maintained. Using IAM roles and, where needed, access control lists (ACLs), organizations can conform to security best practices, which require each user to have access and permissions to only the resources they need to do their jobs, and no more than that. There are a couple of options to control user access to objects and buckets. For most purposes, IAM is sufficient. Roles are inherited from project to bucket to object. If you need finer control, you can create access control lists. Each access control list consists of two pieces of information. The first is a scope, which defines who can access and perform an action. This can be a specific user or group of users. The second is a permission, which defines what actions can be performed, like read or write. Because storing and retrieving large amounts of object data can quickly become expensive, Cloud Storage also offers lifecycle management policies. For example, you could tell Cloud Storage to delete objects older than 365 days; or to delete objects created before January 1, 2013; or to keep only the 3 most recent versions of each object in a bucket that has versioning enabled. Having this control ensures that you’re not paying for more than you actually need.

#### Cloud Storage: Storage classes and data transfer

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531585

There are four primary storage classes in Cloud Storage. The first is Standard Storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline Storage. This is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline Storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline Storage, Coldline Storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive Storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes has differences, it’s worth noting there are several characteristics that apply across all of these storage classes. These include: Unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience, which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data. Cloud Storage has no minimum fee because you pay only for what you use, and prior provisioning of capacity isn’t necessary. And from a security perspective, Cloud Storage always encrypts data on the server side, before it’s written to disk, at no additional charge. Data traveling between a customer’s device and Google is encrypted by default using HTTPS/TLS, which is Transport Layer Security. Regardless of which storage class you choose, there are several ways to bring data into Cloud Storage. Many customers simply carry out their own online transfer using gcloud storage, which is the Cloud Storage command from the Cloud SDK. Data can also be moved in by using a drag and drop option in the Cloud Console, if accessed through the Google Chrome web browser. But what if you have to upload terabytes or even petabytes of data? Storage Transfer Service enables you to import large amounts of online data into Cloud Storage quickly and cost-effectively. The Storage Transfer Service lets you schedule and manage batch transfers to Cloud Storage from another cloud provider, from a different Cloud Storage region, or from an HTTP(S) endpoint. And then there is the Transfer Appliance, which is a rackable, high-capacity storage server that you lease from Google Cloud. You connect it to your network, load it with data, and then ship it to an upload facility where the data is uploaded to Cloud Storage. You can transfer up to a petabyte of data on a single appliance. Cloud Storage’s tight integration with other Google Cloud products and services means that there are many additional ways to move data into the service. For example, you can import and export tables to and from both BigQuery and Cloud SQL. You can also store App Engine logs, Firestore backups, and objects used by App Engine applications, like images. Cloud Storage can also store instance startup scripts, Compute Engine images, and objects used by Compute Engine applications.

#### Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531586

Google Cloud’s second core storage option is Cloud SQL. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It’s designed to hand off mundane, but necessary and often time-consuming, tasks to Google—like applying patches and updates managing backups, and configuring replications—so your focus can be on building great applications. Cloud SQL doesn't require any software installation or maintenance. It can scale up to 128 processor cores, 864 GB of RAM, and 64 TB of storage. It supports automatic replication scenarios, such as from a Cloud SQL primary instance, an external primary instance, and external MySQL instances. Cloud SQL supports managed backups, so backed-up data is securely stored and accessible if a restore is required. The cost of an instance covers seven backups. Cloud SQL encrypts customer data when on Google’s internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. A benefit of Cloud SQL instances is that they are accessible by other Google Cloud services, and even external services. Cloud SQL can be used with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. Compute Engine instances can be authorized to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might use, like SQL Workbench, Toad, and other external applications using standard MySQL drivers.

#### Spanner

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531587

The third core storage option offered by Google Cloud is Spanner. Spanner is a fully managed relational database service that scales horizontally, is strongly consistent, and speaks SQL. Battle tested by Google’s own mission-critical applications and services, Spanner is the service that powers Google’s $80 billion business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, strong global consistency, and high numbers of input and output operations per second. We’re talking tens of thousands of reads and writes per second or more.

#### Firestore

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531588

Google Cloud’s fourth core storage option is Firestore. Firestore is a flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development. With Firestore, data is stored in documents and then organized into collections. Documents can contain complex nested objects in addition to subcollections. Each document contains a set of key-value pairs. For example, a document to represent a user has the keys for the firstname and lastname with the associated values. Firestore’s NoSQL queries can then be used to retrieve individual, specific documents or to retrieve all the documents in a collection that match your query parameters. Queries can include multiple, chained filters and combine filtering and sorting options. They're also indexed by default, so query performance is proportional to the size of the result set, not the dataset. Firestore uses data synchronization to update data on any connected device. However, it's also designed to make simple, one-time fetch queries efficiently. It caches data that an app is actively using, so the app can write, read, listen to, and query data even if the device is offline. When the device comes back online, Firestore synchronizes any local changes back to Firestore. Firestore leverages Google Cloud’s powerful infrastructure: automatic multi-region data replication, strong consistency guarantees, atomic batch operations, and real transaction support.

#### Bigtable

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531589

The last of Google Cloud’s core storage options we’re going to explore is Bigtable. Bigtable is Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding which storage option is best, customers often choose Bigtable if: They’re working with more than 1TB of semi-structured or structured data. Data is fast with high throughput, or it’s rapidly changing. They’re working with NoSQL data. This usually means transactions where strong relational semantics are not required. Data is a time-series or has natural semantic ordering. They’re working with big data, running asynchronous batch or synchronous real-time processing on the data. Or they’re running machine learning algorithms on the data. Bigtable can interact with other Google Cloud services and third-party clients. Using APIs, data can be read from and written to Bigtable through a data service layer like Managed VMs, the HBase REST Server, or a Java Server using the HBase client. Typically this is used to serve data to applications, dashboards, and data services. Data can also be streamed in through a variety of popular stream processing frameworks like Dataflow Streaming, Spark Streaming, and Storm. And if streaming is not an option, data can also be read from and written to Bigtable through batch processes like Hadoop MapReduce, Dataflow, or Spark. Often, summarized or newly calculated data is written back to Bigtable or to a downstream database.

#### Comparing storage options

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531590

Now that we’ve covered Google Cloud’s core storage options, let’s do a comparison to help highlight the most suitable service for a specific application or workflow. Consider using Cloud Storage if you need to store immutable blobs larger than 10 megabytes, such as large images or movies. This storage service provides petabytes of capacity with a maximum unit size of 5 terabytes per object. Consider using Cloud SQL or Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides up to 64 terabytes, depending on machine type, and Spanner provides petabytes. Cloud SQL is best for web frameworks and existing applications, like storing user credentials and customer orders. If Cloud SQL doesn’t fit your requirements because you need horizontal scalability, not just through read replicas, consider using Spanner. Consider Firestore if you need massive scaling and predictability together with real time query results and offline query support. This storage service provides terabytes of capacity with a maximum unit size of 1 megabyte per entity. Firestore is best for storing, syncing, and querying data for mobile and web apps. Finally, consider using Bigtable if you need to store a large number of structured objects. Bigtable doesn’t support SQL queries, nor does it support multi-row transactions. This storage service provides petabytes of capacity with a maximum unit size of 10 megabytes per cell and 100 megabytes per row. Bigtable is best for analytical data with heavy read and write events, like AdTech, financial, or IoT data. Depending on your application, it’s possible that you might use one, or several, of these services to do the job. You may have noticed that BigQuery hasn’t been mentioned in this section of the course. This is because it sits on the edge between data storage and data processing, and is covered in more depth in other courses. The usual reason to store data in BigQuery is so you can use its big data analysis and interactive querying capabilities, but it’s not purely a data storage product.

#### Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/60/labs/531591

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531592

### Containers in the Cloud

#### Introduction to containers

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531593

In this section of the course we’ll explore containers and help you understand how they are used. Infrastructure as a service, or IaaS, allows you to share compute resources with other developers by using virtual machines to virtualize the hardware. This lets each developer deploy their own operating system (OS), access the hardware, and build their applications in a self-contained environment with access to RAM, file systems, networking interfaces, etc. This is where containers come in. The idea of a container is to give the independent scalability of workloads in PaaS and an abstraction layer of the OS and hardware in IaaS. A configurable system lets you install your favorite runtime, web server, database, or middleware, configure the underlying system resources, such as disk space, disk I/O, or networking, and build as you like. But flexibility comes with a cost. The smallest unit of compute is an app with its VM. The guest OS might be large, even gigabytes in size, and take minutes to boot. As demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly. A container is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware. It only requires a few system calls to create and it starts as quickly as a process. All that’s needed on each host is an OS kernel that supports containers and a container runtime. In essence, the OS is being virtualized. It scales like PaaS but gives you nearly the same flexibility as IaaS. This makes code ultra portable, and the OS and hardware can be treated as a black box. So you can go from development, to staging, to production, or from your laptop to the cloud, without changing or rebuilding anything. As an example, let’s say you want to scale a web server. With a container, you can do this in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host. That's just a simple example of scaling one container running the whole application on a single host. However, you'll probably want to build your applications using lots of containers, each performing their own function like microservices. If you build them this way and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts. The hosts can scale up and down and start and stop containers as demand for your app changes or as hosts fail.

#### Kubernetes

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531594

A product that helps manage and scale containerized applications is Kubernetes. So to save time and effort when scaling applications and workloads, Kubernetes can be bootstrapped using Google Kubernetes Engine (GKE). So, what is Kubernetes? Kubernetes is an open-source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud which is a virtual machine running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Deploying containers on nodes by using a wrapper around one or more containers is what defines a Pod. A Pod is the smallest unit in Kubernetes that you can create or deploy. It represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per Pod, but if you have multiple containers with a hard dependency, you can package them into a single Pod and share networking and storage resources between them. The Pod provides a unique network IP and set of ports for your containers and configurable options that govern how your containers should run. One way to run a container in a Pod in Kubernetes is to use the kubectl run command, which starts a Deployment with a container running inside a Pod. A Deployment represents a group of replicas of the same Pod and keeps your Pods running even when the nodes they run on fail. A Deployment could represent a component of an application or even an entire app. To see a list of the running Pods in your project, run the command: $ kubectl get pods. Kubernetes creates a Service with a fixed IP address for your Pods, and a controller says "I need to attach an external load balancer with a public IP address to that Service so others outside the cluster can access it." In GKE, the load balancer is created as a network load balancer. Any client that reaches that IP address will be routed to a Pod behind the Service. A Service is an abstraction which defines a logical set of Pods and a policy by which to access them. As Deployments create and destroy Pods, Pods will be assigned their own IP addresses, but those addresses don't remain stable over time. A Service group is a set of Pods and provides a stable endpoint (or fixed IP address) for them. For example, if you create two sets of Pods called frontend and backend and put them behind their own Services, the backend Pods might change, but frontend Pods are not aware of this. They simply refer to the backend Service. To scale a Deployment, run the kubectl scale command. In this example, three Pods are created in your Deployment, and they're placed behind the Service and share one fixed IP address. You could also use autoscaling with other kinds of parameters. For example, you can specify that the number of Pods should increase when CPU utilization reaches a certain limit. So far, we’ve seen how to run imperative commands like expose and scale. This works well to learn and test Kubernetes step-by-step. But the real strength of Kubernetes comes when you work in a declarative way. Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like, and Kubernetes determines how to do it. You accomplish this by using a Deployment config file. You can check your Deployment to make sure the proper number of replicas is running by using either kubectl get deployments or kubectl describe deployments. To run five replicas instead of three, all you do is update the Deployment config file and run the kubectl apply command to use the updated config file. You can still reach your endpoint as before by using kubectl get services to get the external IP of the Service and reach the public IP address from a client. The last question is, what happens when you want to update a new version of your app? Well, you want to update your container to get new code in front of users, but rolling out all those changes at one time would be risky. So in this case, you would use kubectl rollout or change your deployment configuration file and then apply the change using kubectl apply. New Pods will then be created according to your new update strategy. Here’s an example configuration that will create new version Pods individually and wait for a new Pod to be available before destroying one of the old Pods.

#### Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531595

So now that we have a basic understanding of containers and Kubernetes, let’s talk about Google Kubernetes Engine, or GKE. GKE is a Google-hosted managed Kubernetes service in the cloud. The GKE environment consists of multiple machines, specifically Compute Engine instances, grouped together to form a cluster. You can create a Kubernetes cluster with Kubernetes Engine, but how is GKE different from Kubernetes? From the user’s perspective, it’s a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes responsibility for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need of a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes. Let’s examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. Autopilot also helps produce a strong security posture. And Autopilot also promotes operational efficiency. The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode. You can create a Kubernetes cluster with Kubernetes Engine by using the Google Cloud console or the gcloud command that's provided by the Cloud software development kit. GKE clusters can be customized, and they support different machine types, number of nodes, and network settings. Kubernetes provides the mechanisms through which you interact with your cluster. Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks, set policies, and monitor the health of deployed workloads. Running a GKE cluster comes with the benefit of advanced cluster management features that Google Cloud provides. These include: Google Cloud's load-balancing for Compute Engine instances, Node pools to designate subsets of nodes within a cluster for additional flexibility, Automatic scaling of your cluster's node instance count, Automatic upgrades for your cluster's node software, Node auto-repair to maintain node health and availability, And logging and monitoring with Google Cloud Observability for visibility into your cluster. To start up Kubernetes on a cluster in GKE, all you do is run this command: $> gcloud container clusters create k1

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531596

### Applications in the Cloud

#### Cloud Run

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531597

So far in this course, we’ve provided an introduction to Google Cloud and explored the options and benefits related to using virtual machines, networks, storage, and containers in the Cloud. In the final section of the course, we’ll turn our attention to developing applications in the Cloud. We’ll begin with Cloud Run, which is a managed compute platform that runs stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It’s built on Knative, an open API and runtime environment built on Kubernetes. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you‘ll never pay for over-provisioned resources. The Cloud Run developer workflow is a straightforward three-step process. First, you write your application using your favorite programming language. This application should start a server that listens for web requests. Second, you build and package your application into a container image. And third, the container image is pushed to Artifact Registry, where Cloud Run will deploy it. Once you’ve deployed your container image, you’ll get a unique HTTPS URL back. Cloud Run then starts your container on demand to handle requests, and ensures that all incoming requests are handled by dynamically adding and removing containers. Because Cloud Run is serverless, it means that you, as a developer, can focus on building your application and not on building and maintaining the infrastructure that powers it. For some use cases, a container-based workflow is great, because it gives you a great amount of transparency and flexibility. Sometimes, you’re just looking for a way to turn source code into an HTTPS endpoint, and you want your vendor to make sure your container image is secure, well-configured and built in a consistent way. With Cloud Run, you can do both. You can use a container-based workflow, as well as a source-based workflow. The source-based approach will deploy source code instead of a container image. Cloud Run then builds the source and packages the application into a container image. Cloud Run does this using Buildpacks - an open source project. Cloud Run handles HTTPS serving for you. That means you only have to worry about handling web requests, and you can let Cloud Run take care of adding the encryption. The pricing model on Cloud Run is unique; as you only pay for the system resources you use while a container is handling web requests, with a granularity of 100ms, and when it’s starting or shutting down. You don’t pay for anything if your container doesn’t handle requests. Additionally, there is a small fee for every one million requests you serve. The price of container time increases with CPU and memory. A container with more vCPU and memory is more expensive. You can use Cloud Run to run any binary, as long as it’s compiled for Linux sixty-four bit. Now, this means you can use Cloud Run to run web applications written using popular languages, such as: Java, Python, Node.js, PHP, Go, and C++. You can also run code written in less popular languages, such as: Cobol, Haskell, and Perl. As long as your app handles web requests, you’re good to go.

#### Development in the cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531598

Many applications contain event-driven parts. For example, an application that lets users upload images. When that event takes place, the image might need to be processed in a few different ways, like converting the image to a standard format, converting a thumbnail into different sizes, and storing each new file in a repository. This function could be integrated into the application, but then you’d have to provide compute resources for it–whether it happens once a millisecond or once a day. With Cloud Run functions, you write a single-purpose function that completes the necessary image manipulations and then arrange for it to automatically run whenever a new image is uploaded. Cloud Run functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events, without the need to manage a server or a runtime environment. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also connect and extend cloud services. You’re billed to the nearest 100 milliseconds, but only while your code is running. Cloud Run functions supports writing source code in a number of programming languages. These include Node.js, Python, Go, Java, . Net Core, Ruby, and PHP. For more information about the supported specific version, refer to the runtimes documentation. Events from Cloud Storage and Pub/Sub can trigger Cloud Run functions asynchronously, or you can use HTTP invocation for synchronous execution.

#### Hello Cloud Run

- https://www.cloudskillsboost.google/paths/11/course_templates/60/labs/531599

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531600

### Prompt Engineering

#### Prompt Engineering

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531601

Generative AI and large language models are proving to be powerful tools, but to leverage their capabilities, it's important to understand their architecture. It's also important to consider recommended practices when implementing these technologies. The goal of this module, titled Google Cloud: Prompt Engineer Guide, is to help with these important steps. In this guide to prompt engineering, you’ll get answers to the questions: What is generative AI? What is a large language model? What is prompt engineering? You’ll also explore prompt engineering best practices. Before we delve into this lesson, let's define the interchangeably used terms such as 'generative AI' and 'Large Language Model' (LLM). While both terms describe AI models capable of generating human-like responses based on input prompts in many references, it's important to note they're not identical. Generative AI encompasses a broader range of models capable of generating various types of content beyond just text, while LLM specifically refers to a subset of generative AI models focusing on language tasks. We'll thoroughly explore each term in this lesson. So let's begin with an important question: What is generative AI? Generative artificial intelligence, which is commonly referred to as gen AI, is a subset of artificial intelligence that is capable of creating text, images, or other data using generative models, often in response to prompts. It has grown in popularity hugely since 2021 but artificial intelligence has been around since the mid 1950s. By the way, a prompt is a specific instruction, question, or cue given to a computer program or user to initiate a specific action or response, but we examine this more later. In its current format, gen AI models are like conversational programs that can generate content based on the inputs supplied. Gen AI models learn the patterns and structure from input training data and then create new data with similar characteristics. Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, and sales. However, rather than exploring all generative AI applications, this training will specifically focus on articulating prompts to harness the power of gen AI effectively. Let me introduce a scenario that we’ll make reference to throughout this section of the training. We’ve included it to help put some of this theory into practice. Meet Sasha, a cloud architect, who needs to create a prototype design of Google Cloud VPC network architecture for Cymbal Bank. Sasha wants to save time by combining her existing knowledge of cloud architecture and generative AI tools to create a usable prototype design. Sasha was excited to learn about Gemini, since having the tool inside the Google Cloud Console means that she can access it without any additional installs. We’ll check back in with Sasha later. Let’s spend some time exploring large language models, which are a highly sophisticated computer programs trained on gigantic amounts of data that can be text or images. But how are they trained? And why do they need training at all? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. In this context, large refers to: The size of the training dataset, which can sometimes be at the petabyte scale. And the number of parameters. Parameters are the memories and knowledge that the machine has learned during model training. They determine the ability of a model to solve a problem, such as predicting text, and can reach billions or even trillions in size. General-purpose means that the models can sufficiently solve common problems. This is thanks to the commonality of a human language, regardless of the specific tasks. Saying LLMs are pre-trained and fine-tuned, means… …that they have been pre-trained for a general purpose with a large dataset… ...and then fine-tuned for specific goals with a much smaller dataset. But how are LLMs trained? When you submit a prompt to an LLM, it calculates the probability of the correct answer from its pre-trained model. The probability is determined through a task called pre-training. Pre-training an LLM involves feeding a massive dataset of text, images, and code to the model so that it can learn the underlying structure and patterns of the language. This process helps the model to understand and generate human language more effectively. In this way, the LLM works like a fancy autocomplete, suggesting the most common correct response to the prompt. But sometimes the LLM gives a completely wrong answer. This is called a hallucination. Hallucinations are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. This happens because LLMs can only understand the information they were trained on. This means that they might not be aware of your business's proprietary or domain-specific data. Also, they do not have access to real-time information. To make matters worse, LLMs only understand the information that is explicitly given to them in the prompt. In other words, they often assume that the prompt is true. They also do not have the ability to ask for more context information. Ultimately, an LLM does not know anything outside of what it was trained on, and it cannot truly know if that information is accurate. But what causes a hallucination. Hallucinations can be caused by a number of factors, including: The model is not trained on enough data. The model is trained on noisy or dirty data. The model is not given enough context. The model is not given enough constraints. Hallucinations can be a problem for LLMs because they can make the output text difficult to understand. They can also make the model more likely to generate incorrect or misleading information. But we will see in the Prompt Engineering section that there are things we can do to minimize this problem. OK, but knowing where the sun is will not help Sasha with her current task. Lucky for Sasha, Google Cloud offers a generative AI model called Gemini, [[Pause here for 5 seconds]] which can act as an always-on collaborator. This gen AI-powered assistant can help a wide range of Google Cloud users, including developers, data scientists, and operators. To provide an integrated assistance experience, Gemini is embedded in many Google Cloud products. Gemini has access to a massive range of data, including Google Cloud documentation, tutorials, and samples. With the right prompts, it can produce detailed suggestions and guides on what resources will best suit Sasha’s current challenge and their configuration. Gemini can even create detailed gcloud commands and insert them into Cloud Shell for her. She just needs to articulate her needs in a way that gets the best response from Gemini. For example, if she uses the prompt “How can I create a network that uses IPv4 and IPv6 addresses?”, she will get a response that details how to do just that. You’ve learned that a large language model is a huge object model containing a massive dataset of text. But how can you extract the information you need from this dataset? This is where prompt engineering comes in. A prompt is the text that you feed to the model, and prompt engineering is a way of articulating your prompts to get the best response from the model. The better structured a prompt is, the better the output from the model will be. Let’s explore what this means. Prompts can be in the form of a question, and are categorized into four categories: zero-shot, one-shot, few-shot, and role prompts. Zero-shot prompts do not contain any context or examples to assist the model. For example, the prompt “What’s the capital of France?” does not provide any examples of what a capital is. Clearly, that is not too important for this example. But for more specific and technical prompts, an example would help refine the scope of the response from Gemini. One-shot prompts, however, provide one example to the model for context. Here, we ask for the capital of France again, but we provide Italy and Rome as an example. And few-shot prompts provide at least two examples to the model for context. Here, our prompt is updated to also include Japan and Tokyo in our examples. And then, there are role prompts which require a frame of reference for the model to work from as it answers the questions. In our example, we state “I want you to act as a business professor. I’ll give you a term, and you will correctly explain its meaning. Make sure your answers are always right. What is ROI? “ For Sasha’s needs, using role prompts might be the best solution. She can define what is required and in what context. This means that the LLM will have a clear point of reference when supplying an answer. Now that you’ve seen the types of prompts you can create, let’s explore the two elements of a prompt: the preamble and the input. The preamble refers to the introductory text you provide to give the model context and instructions before your main question or request. Think of it as setting the stage for the LLM to better understand what you want. It can include the context for the task, the task itself, and some examples to guide the model. The input is the central request you're making to the LLM. It’s what the instruction or task will act upon, for example “Comment: I don’t know what to think about the video. The review is:” Based on the preamble, Gemini reviews the input and suggests if the review is positive, neutral, or negative. It is worth noting that not all the components are required for a prompt, and the format can change depending on the task at hand. The element order can also change. Let's amend Sasha’s original prompt “How can I create a network that uses IPv4 and IPv6 addresses?” and add a role context to the input fed into Gemini. She also adds the detail of needing a dual stack subnet. The new prompt is “I want you to act as a cloud architect in Google Cloud. How can I use gcloud to create a network that uses IPv4 and IPv6 subnets?” But since Gemini maintains its own interaction context, she could have just asked “I want you to act as a cloud architect in Google Cloud. How can I adjust the gcloud command provided to create a subnet to ensure the subnet is dual stack?” Now that you’ve had a chance to explore what Gen AI is, what large language models are and how they’re trained, and what prompt engineering is, it’s time to explore some prompt engineering best practices. The first best practice is to write detailed and explicit instructions. The more vague the prompt, the more chance that the model will produce a result that is not usable. Be clear and concise in the prompts that you feed into the model. Next, be sure to define boundaries for the prompt. It’s better to instruct the model on what to do rather than what not to do. If the model gets stuck, give it a few 'fallback' outputs that work in various situations. For example, something like "I'm still learning about that" to use when unsure. Another best practice is to adopt a persona for your input. Adding a persona for the model can provide meaningful context to help it focus on related questions, which can help improve accuracy. This prompt would help Sasha, the cloud architect, get started with prototyping a network architecture for Cymbal Bank. And finally, it’s a recommended practice to keep each sentence concise. Longer sentences can sometimes produce suboptimal results. It’s best to break long sentences in a prompt into a series of shorter sentences and simpler tasks. So, let’s return to Sasha, and use what we have learned so far. Sasha updates her prompt to: “You're a cloud architect. You want to build a Google Cloud VPC network that can be centrally managed. You also connect to other VPC networks in your company's other regions. You don't want to have many different sets of firewall policies to maintain. What sort of network architecture would you recommend?” With this new prompt, Gemini proposes a hub-and-spoke network architecture, which fits Sasha’s needs exactly. By refining and amending her prompts, Sasha has articulated her requirements in a way that Gemini can respond with the correct focus and level of detail.

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/60/quizzes/531602

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/11/course_templates/60/video/531603

Congratulations on completing the Google Cloud Fundamentals: Core Infrastructure training course. Before you go, let’s take a few minutes to review what we’ve covered. In module 1, you were introduced to Google Cloud and cloud computing. Specifically, you explored: The concept of managed infrastructure and managed services, through IaaS, infrastructure as a service, and PaaS, platform as a service. The Google Cloud network. Google Cloud’s focus on security throughout our infrastructure. How Google publishes key elements of technology using open source licenses. And Google Cloud’s pricing structure and billing tools. In module 2, you learned about the Google Cloud Resource Hierarchy, which is made up of four levels: resources, projects, folders, and an organization node. You also learned about: Defining policies and their downward inheritance. When to use Identity and Access Management, or IAM, And the four ways to access and interact with Google Cloud: through the Google Cloud console, the Cloud SDK and Cloud Shell, APIs, and the Google Cloud App. In module 3, you explored how Compute Engine works, with a focus on virtual machines and virtual networking. You were introduced to: The VPC, or virtual private cloud. Compute Engine’s Autoscaling feature. And important Google Virtual Private Cloud compatibility features, like routing tables, firewalls, VPC peering, and shared VPC, all of which result in the need for less network management. You also explored Cloud Load Balancing, a fully distributed, software-defined, managed service for all your traffic. Finally, you compared how on-premises or other-cloud networks can be interconnected with a Google VPC. In module 4, you explored Google Cloud's five core storage options: Cloud Storage, Bigtable, Cloud SQL, Spanner, and Firestore. You also examined the four storage classes that make up Cloud Storage: Standard Storage, which is used for frequently accessed hot data, Nearline Storage and Coldline Storage, which are used for less-frequently accessed cool data, and Archive Storage. In module 5, you learned about containers, which are invisible boxes around your code and its dependencies. You were introduced to containers, along with: Kubernetes, an open-source platform for managing containerized workloads and services. And Google Kubernetes Engine (GKE), a Google-hosted managed Kubernetes service in the cloud. In module 6, the focus was on developing applications in the cloud. You explored: Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. And Cloud Run functions, a lightweight, event-based, asynchronous compute solution to create single-purpose functions. Finally, in module 7, you explored how to combine Google Cloud knowledge with prompt engineering to improve Gemini responses. You discovered answers to the following questions: What is generative AI? What is a large language model? And what is prompt engineering? You ended the module by identifying prompt engineering best practices. We hope that this course is just the beginning of your Google Cloud journey. For more training and hands-on practice, explore the different learning paths available at cloud.google.com/training. And if you’re interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you might consider working toward a Google Cloud certification. You can learn more about Google Cloud’s certification offerings at cloud.google.com/certification. Thanks for completing this course. We’ll see you next time!

#### Course resources

- https://www.cloudskillsboost.google/paths/11/course_templates/60/documents/531604

### Your Next Steps

## 04: Essential Google Cloud Infrastructure: Foundation

- https://www.cloudskillsboost.google/paths/11/course_templates/50

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530340

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you‘ll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

#### Welcome to Essential Cloud Infrastructure: Foundation

- https://www.cloudskillsboost.google/paths/11/course_templates/50/documents/530341

### Interacting with Google Cloud

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530342

In this module, we will provide you with an introduction to GCP by building on what you learned about the GCP infrastructure from the course introduction. This module is focused on how to interact with GCP. In the labs of this module, you will explore both the GCP's graphical user interface and it's command-line interface. You will also deploy a solution from the GCP marketplace without having to manually configure the software, Virtual Machine instances, storage, or network settings. To complete your learning experience, I will provide a quick demo of Projects. Let's get started.

#### Using Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530343

There are four ways you can interact with Google Cloud, and we’ll talk about each in turn. There’s the Google Cloud console, Cloud Shell and the Google Cloud CLI, the APIs, and the Cloud Mobile App. The Google Cloud console provides a web-based, graphical user interface that you access through console.cloud.google.com. For example, you can view your virtual machines and their details, as shown on the top. If you prefer to work in a terminal window, the Google Cloud CLI provides the gcloud command-line tool. For example, you can list your virtual machines and their details as shown on the bottom with the “gcloud compute instances list” command. Google Cloud also provides Cloud Shell, which is a browser-based, interactive shell environment for Google Cloud that you can access from the Google Cloud console. Cloud Shell is a temporary virtual machine with 5 GB of persistent disk storage that has the Google Cloud CLI pre-installed. Throughout this course, you will apply what you learn in different labs. These labs will have instructions to use the Google Cloud console, such as, “On the Navigation menu, click Compute Engine > VM instances.” Let me dissect these instructions. First, within the console you will click on the icon with the three horizontal lines, which is the Navigation menu, as shown on the left. This opens a menu, as shown on the right. All of the major products and services are listed on this menu. Then, within the menu, hover over “Compute Engine” to open a submenu. Finally, click on “VM instances” on the submenu. You will get more comfortable with these instructions and the console as you work on labs. Labs will also use command-line instructions. You will enter these instructions either in Cloud Shell or an SSH terminal by simply copying and pasting them. In some cases, you will have to modify these commands, for example, when choosing a globally unique name for a Cloud Storage bucket. In addition to the Google Cloud CLI, you can also use client libraries that enable you to easily create and manage resources. Google Cloud client libraries expose APIs for two main purposes: App APIs provide access to services, and they are optimized for supported languages, such as Node.js or Python. Admin APIs offer functionality for resource management. For example, you can use admin APIs if you want to build your own automated tools. The Cloud Mobile App is another way to interact with Google Cloud. It allows you to manage Google Cloud services from your Android or iOS device. For example, you can start, stop, and SSH into Compute Engine instances and see logs from each instance. You can also set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app even offers alerts and incident management and allows you to get up-to-date billing information for your projects and get billing alerts for projects that are going over budget. You can download the Cloud Mobile App from Google Play or from the App Store.

#### Lab Intro: Working with the Google Cloud Console and Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530344

Slides are great for explaining concepts, but let's apply what we just talked about. In this first lab, you'll explore the Google Cloud interface and the entry point of the graphical user interface that's called the Google Cloud console. Within the Google Cloud console, you will create a storage bucket in Cloud Storage, which is Google's unified object storage. Then you will repeat the same task using Cloud Shell, which is the command-line interface in Google Cloud. I encourage you to develop familiarity with both the Google Cloud console and Cloud Shell and to become comfortable moving back and forth between them.

#### Working with the Google Cloud Console and Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530345

#### Lab Review: Working with the Google Cloud Console and Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530346

In this lab, you created a Cloud Storage bucket using both the Google Cloud console and Cloud Shell within Google Cloud. The Google Cloud Console can do things Cloud Shell can't do and vice versa. For example, the Google Cloud console can keep track of the context of your configuration activities. It can use the Cloud API to determine from current system state what options are valid, and it can perform repetitive and more leveraged activities on your behalf. Cloud Shell, in contrast, offers detailed and precise control, and through its commands, a way to script and automate activities. However, don't think of the console and Cloud Shell as alternatives. Think of them as one extremely flexible and powerful interface. You can stick around for a lab walkthrough, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. Hi, so here we are in the GCP Console and the first thing we're going to do is create a bucket using the GCP Console. So to do that, I'm going to use the navigation menu which is the icon up here in the top left corner, and I'm currently scroll down to Storage which is here, and click on Browser. What we want to do is create a bucket, so I'm going to click the Create bucket. The first thing we need to do is define a name, and now this name needs to be a globally unique name. So you could for example use your Qwiklab's project ID here, so that's what I'll do, copy that and paste it in there. The instructions just say to create, you could also choose a change the default storage class is currently set to multi-regional. We'll talk more about that in a later module. You can control the access to the objects and there are even some advanced settings around encryption. So I'm just going to go ahead and click Create. You can see that this now has created a bucket and here we see the bucket ID or the name. So now we're going to access Cloud Shell. Then what we're going to do this we're going to click this button up here on the right corner, says Activate Cloud Shell, and then it will prompt you to start clutches, so we'll click that as well. You can see that's coming up here. You could actually expand this and open this in a new tab, or you could realigned this to get a little bit more real estate in here. So we created a bucket using the GCP Console, now we're going to repeat the same using Cloud Shell. So I'm going to go ahead and copy the command from the lab instructions and paste it in here. Another command has the bucket name here in brackets and we want to change that. So this again has to be a globally unique name. So what we could do is we could again grab the ID of our project and maybe just add something to it. We could just add -shell to say that this is the one that we created from Cloud Shell. So the command is gsutil, these are the commands for Cloud Storage and mb is the make bucket command. You'll see that it has created that here, and we can see if we navigate in the GCP Console back to buckets, that we now have two buckets in here. So we're able to create both of those. So there are other Cloud Shell features that we can explore here. So while we're in Cloud Shell, we can click these three dots over here and get some more options. One of which is we can upload a file, and if I click that, I'm just present it with my browser, and I could for example, select this text file and click Open. We see that it's being uploaded and now that has finished, and then I can use the ls command to list that file. So here's that file. There's also a read me already in there. Then we could copy that now, that file to the bucket that we have. So there's a command for that also in the lab instructions. So again, we're working with Cloud Storage. So gsutil is going to be the command and CP to copy. We're going to give the name of the file, so MyFile.txt, and then we want to get to that Cloud Storage bucket. So we could choose either of the two buckets we've created. Why don't we choose the one we create a from Cloud Shell, paste it in there, and then it's telling us that it's copying over the files. If we now go into that, we can see that now that file is in there. The file doesn't contain anything, so that's why it is that size. Then we could also go ahead and close Cloud Shell, and do some other activities. Task 5 of the lab goes into creating a persistent state in Cloud Shell. So you could open Cloud Shell and we could list for example, all the variable regions with the G loud command that's listed in there, G Cloud compute regions list, and from these regions we can now select a region and store that in an environment variable. So let's take the command from the lab instructions and for class region equals, and let's say for example I pick the US Central 1 region, could paste that in there, store it, and then I could verify that with the echo command, just running that and it's not telling me that that is stored in there. The other thing we could do is we could expand this a little bit, we could also create a folder in here with the MK direct command, and now we could create a configuration file, and then we can append the environment variable that we just created to to that file. Then we could add another one for example, we could also store our project ID. So I can put that in there, grab my project ID, copy that, and store that in the environment variable, and then I run the command from the lab instructions to also append the value of the project ID to my environment variable and the configuration file. Then I can just verify all of that and make sure that that's been stored. So this gives us a method to create environment variables and easily recreate them as Cloud Shell is cycled. However, you will still need to remember to issue this source command each time Cloud Shell is opened. So let's modify the.profilefile so that the source command is issued automatically anytime a terminal Cloud Shell is opened. So we're going to close and reopen Cloud Shell. So let me do that, close it and then reopen it, and then I'm going to paste the echo command again. We see that it's not outputting anything, so that command is coming out down. So let's modify that.profilefile using nano, and at the end of that file, let's go all the way to the bottom. We'll go into paste in sourceinfraclassconfig, and then we're going to save that file to profile, and then exit. Then let's verify that we are able to get that environment variable, that is project ID. So that's currently not in there, that is because I haven't restarted it, propagates run when I restart, sorry for that. So let me close it, let me reopen it, and then let's verify. There we go. So now we can see that expected value and that's because we edit the d.profilefile. That's it. So we've leveraged in this lab, the GCP Console, we created a Storage bucket, we also created a Storage bucket using Cloud Shell, and then we looked into some features run Cloud Shell in terms of uploading files, than copying those files to the Storage bucket, and even at the end configuring the profile and setting some environment variables. That's the end of the lab.

#### Lab Intro: Infrastructure Preview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530347

In this lab, you're going to experience the power of Google Cloud automation by setting up a complete Jenkins continuous integration environment using the Google Cloud Marketplace. You will then verify that you can manage the service from the Jenkins UI and administer the service from the VM host through SSH. Now, you could accomplish a very similar result through manual configuration in a couple of hours or days. But in this lab, you will see it set up in only a few minutes.

#### Infrastructure Preview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530348

#### Lab Review: Infrastructure Preview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530349

In this lab, you were able to launch a complete continuous integration solution in a few minutes. You demonstrated that you had user access through the Jenkins UI and that you had administrative control over Jenkins by using SSH to connect to the VM where the service is hosted and by stopping and then restarting the services. Many of the activities that occurred in that lab were nearly transparent, and they used resources and methods that you’ll learn about in the rest of this course. Examples of this include the acquisition and configuration of a network IP address; the provisioning of a virtual machine instance along with the installation of software on that machine; and the passing of default state information from the environment during the setup process. You can stick around for a lab walk-through, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. So here I am in the GCP Console, and the first thing I want to do is navigate to the Marketplace. So up here, I've already clicked on the Navigation menu and Marketplace is pretty much on top. So I'm going click on that. Now, I want to search for Jenkins. Specifically, the one that's certified by Bitnami. So I can just directly paste that in the search address here. Here we go. This is the one I'm looking for. So I'm going to click on that. Now, I can read all about this. There's an overview. It doesn't mean that function Compute Engine, uses a single virtual machine, when it was last updated. It talks about all the packages, the operating system. If I scroll down, I can learn more about the pricing. There's obviously, pricing associate with the VM instance itself. It does not have a usage fee. If it did, that would be displayed here, and you'd be billed for all of that together. There's a standard discharge, and then there's the sustained use discount, which we'll learn more about in a later module. So once I'm happy with all that and I've read through, I can go ahead and click on "Launch on Compute Engine". Now, it's going to present me with an interface here, where I could change the name, the zone, the Machine tab, a lot of other settings that are very similar to configuring a virtual machine. I can again, see all the Software, Terms of Service, the cost one more time. Once I'm ready to go, I can click "I accept the Terms of Services", and click "Deploy". So now, I'm actually in a different interface. This is Deployment Manager, we'll learn about this later in the course series, but the interesting thing now, is I can see the setup process. So there is an actual file here that has all the configuration in a ginger file. There is a VM that's being created. There are two firewall rules that are created. TCP for port 80 and 443. So that's HTTP and HTTPS. I can wait for this machine to now come up. There's also some software configuration. I can again, learn about all the software that is installed here. I can click on the VM instance to get more information about it. We can see the VMs instance is up, the firewalls are up. So the last thing that's happening here is the software is being configured. I can even learn more about that software. Here, I already clicked on that. So these are again, all the different versions that we can get to and engage. Once this is running, this table up here will be populated, all currently pending because this is still being initialized. Here, we can see that the instance is now ready. So there are a couple different things we could do. We have an admin user, as well as a password. So we can copy that. We could click on "Visit The Site", and this is going to open that in a new tab, that's navigating us to the external IP address. It's going to load, let's see it's the starting. It's part of the service itself, it's still getting ready to work. So you can see that the software in the background on the instance is installed, but it also needs to launch. So that itself can take some time too, and now it's up and running. I can put my username in and I can put the password in. I can click "Sign In". Here, I should be asked to customize Jenkins. There'll be some suggested plug-ins that I can install. Once I've done that, I can restart the instance. Deployment Manager and the G Suite Marketplace, will also give you some time some suggests next steps. For example, this password up here, it's just temporary. So we could go change that. The other thing we could do is we could assign a static external IP address so that when you visit the site, it's always going to be the same IP address, and that really helps if you have a DNS setup for this instance. If I go back here, I can click that I want to install the suggested plug-ins. It's going to do that. It's going to tell me where that instance is. I can save and finish, and I can go start using it. They should again now restart service. So here we are. So I can explore this a little bit. I could manage Jenkins itself. There are lots of different actions that I could perform here. I could also now, further administer the service if I go back to the Console. I'm looking at this deployment here. I'm looking Jenkins-1, I could actually SSH now to this instance. So let me click that button. That's going to establish now, an SSH session to the service. I can then actually shut down all the services by copying the command that's in the lab instructions. So let me just paste that in here and run that. If we go back to the Jenkins UI and refresh that page, we'll see that it's gone. That is expected because I have gone ahead and I have restarted that service. So what I can do now, is I stop them, I can now restart it by running the Restart command in here. So let's grab that and paste that in here. Now, the service should come back up. We might have to refresh the page a couple times for that to happen. So let's just wait a couple seconds, refresh it and see if that's service comes back up. My tab name has changed to Start in Jenkins. So it looks like that service is already coming back up right now. We can see that the service is getting ready right now. So at this point, we've completed all the task. I could now go back to the SSH session and exit out of here. Here, we see that Jenkins is back up and running. That's the end of the lab.

#### Demo: Projects

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530350

Let's explore projects which are the key organizer of infrastructure resources and relate these resources to billing accounts. Resources can only be created and consumed within projects in a way that projects isolate related resources from one another. I will demonstrate how to create and delete projects, and switch contexts between projects. Some of these actions cannot be performed in the Qwiklabs environment due to security restrictions. Therefore, I'm going to demonstrate them in my environment. So here I am in the GCP console. You can actually see this is a trial account and you can also create a trial account yourself if you would like to follow along with this. Essentially, what I'm going to do first is go ahead and create a project. So I'm going to click up on my product name up here, and there's this icon up here to create a new project, so let me go click that. Now, the one thing I want to do is, I want to define a project name. So let me just say my new project. You can see that it automatically creates the project ID and project ID is going to be unique versus my name is really not so unique. So let me click "Create" on that. It is now telling me here that is going to create that project. I can follow along with that here in the notification pane. One thing to notice is, when you create a new project, that some of the services that you're going to use may not be initially available. So here, I now have my new project. I could now switch projects. So if I go to my home, for example, I see here the project itself, I could go to the project settings, I could shut that down, or I could switch to a different one. So let me actually change up here to this new project that I created. You go in there and let's follow the process for shutting that down. So I'm going to click on "Shutdown", I wants to make sure that I really want to do that. It's telling me a little bit about what's going to happen when I do this. Specifically, all building in traffic serving will stop, but the shutdown is actually scheduled. So it will take 30 days, and this is in case that you want up maybe undo this. So I need to just retype my project ID, and I can actually copy and paste it in here, and I can click ''Shutdown'', and it should now give me a notification. So here it's telling me when exactly it's going to shut this down, and I can click ''OK'' on that. So now, this is being scheduled for shutdown. So now, I can go back and obviously want to grab in project, it's automatically put me in the sight. Alternatively, if I go home, you'll see that I also have an option up here. It's telling me, hey, you really need to select a project. So lots of different ways to go about. So I could click on that and select a project. Now, I want to show in a second how we can also move switched projects on screen Cloud Shell. So let's actually go ahead and create another project. Let's just call this My Second Project. We can create that as well. They'll start in the background for us. So what I want to do now, as I said I want to go to Cloud Shell. So if I go up here on the right corner, it's Activate Cloud Shell. I'll just click on that. It doesn't ask me if you want to start Cloud Shell because I've already been using Cloud Shell with this user. So it's also telling me that I haven't used my Cloud Shell in awhile, so it has to unarchive my disk and that's going to take a little bit of time. But once that's up, we can actually go use gcloud config list command and we can paste it in, and it's going to give us more information about the configuration that we currently have. That will include the project that we currently have selected. We can actually see the project right here. This is the project I'm working on right now. So if I paste in, I automatically copy that when I clicked on it. So I want to instead type in here gcloud config list. So here, we get some more information. I can also use the grep command in here to directly got my project and there we see this is the project that we're currently using. I could actually now even changed the focus of my GCP console to this new project. You'll see if I run this command again, my focus of Cloud Shell is still focused on this other project that I had. So one thing we could do now, is we could store the project ID maybe in an environment variable and then we could maybe set it so we could swap back and forth. So let me get the project ID, it's right here. I'm going to maybe just store that in an environment variable. Let's just call that my project ID1. So let me grab the project ID, copy that, paste it in there. So now, I have that stored and now I could use the gcloud config set project to define an action to change the project ID. Now, you can see that I have that other project ID listed here. So I can actually see that, and I could also now use it the same gcloud config list command and grep the project, and you'll see that now I'm working with different project. That's how easy it is to create and delete projects, and switch contexts between projects.

#### Quiz: Interacting with Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/50/quizzes/530351

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530352

In this module, we looked at how to use Google Cloud, which you got to experience first-hand in two short labs. You also viewed a demonstration on how to use projects, which are the key organizer of infrastructure resources. Now that you can interact with Google Cloud, it's time to explore two of the foundational components of Google Cloud’s infrastructure: virtual networks and virtual machines. So, what are you waiting for? Move on to the next module to learn more.

### Virtual Networks

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530353

In this module, we will be covering virtual networks. Google Cloud uses a software-defined network, that is built on a global fiber infrastructure. This infrastructure makes Google Cloud one of the world's largest and fastest networks. Thinking about resources as services instead of as hardware, will help you understand the options that are available, and their behavior. In this module, we start by introducing Virtual Private Cloud, or VPC, which is Google's managed networking functionality for your Google Cloud resources. Then, we dissect networking into its fundamental components, which are projects, networks, subnetworks, IP addresses, routes and firewall rules, along with network pricing. Next, you will explore Google Cloud's network structure in a lab, by creating networks of many different varieties, and exploring the network relationships between them. After that, we will look at common network designs. This map represents Google Cloud. On a high level, Google Cloud consists of regions, which are the icons in blue, points of presence or PoPs, which are the dots in blue, a global private network, which is represented by the blue lines, and services. A region is a specific geographical location where you can run your resources. This map shows several regions that are currently operating, as well as future regions. Regions indicated with blue icons have three zones. Iowa is an exception, where the region called us-central1 has four zones: us-central1-a, us-central1-b, us-central1-c, and us-central1-f, For up-to-date information on regions and zones, please refer to the documentation in the student PDF for this module. The PoPs are where Google's network is connected to the rest of the internet. Google Cloud can bring its traffic closer to its peers because it operates an extensive global network of interconnection points. This reduces costs and provides users with a better experience. The network connects regions and PoPs, and is composed of a global network of fiber optic cables with several submarine cable investments.

#### Virtual Private Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530354

Let’s start by talking about Google Cloud’s network, and specifically Virtual Private Cloud, or VPC. With Google Cloud, you can provision your Google Cloud resources, connect them to each other, and isolate them from each other in a Virtual Private Cloud. You can also define fine-grained networking policies within Google Cloud, and between Google Cloud and on-premises or other public clouds. Essentially, VPC is a comprehensive set of Google-managed networking objects, which we will explore in detail throughout this module. Let me give you a high-level overview of these objects. Projects are going to encompass every single service that you use, including networks. Networks come in three different flavors; default, auto mode, and custom mode. Subnetworks allow you to divide or segregate your environment. Regions and zones represent Google's data centers, and they provide continuous data protection and high availability. VPC provides IP addresses for internal and external use, along with granular IP address range selections. As for virtual machines, in this module we will focus on configuring VM instances from a networking perspective. We'll also go over routes and firewall rules.

#### Projects, networks, and subnetworks

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530355

Let’s start exploring the VPC objects by looking at projects, networks, and subnetworks. Projects are the key organizer of infrastructure resources in Google Cloud. A project associates objects and services with billing. Now, it’s unique that projects actually contain entire networks. The default quota for each project is 15 networks, but you can simply request additional quota using the Google Cloud console. These networks can be shared with other projects, or they can be peered with networks in other projects, both of which we will cover later in the Architecting with Google Compute Engine course series. These networks do not have IP ranges but are simply a construct of all of the individual IP addresses and services within that network. Google Cloud’s networks are global, spanning all available regions across the world. So, you can have one network that literally exists anywhere in the world - Asia, Europe, Americas - all simultaneously. Inside a network, you can segregate your resources with regional subnetworks. I just mentioned that there are different types of networks: default, auto, and custom. Let’s explore these types of networks in more detail. Every project is provided with a default VPC network with preset subnets and firewall rules. Specifically, a subnet is allocated for each region with non-overlapping CIDR blocks and firewall rules that allow ingress traffic for ICMP, RDP, and SSH traffic from anywhere, as well as ingress traffic from within the default network for all protocols and ports. In an auto mode network, one subnet from each region is automatically created within it. The default network is actually an auto mode network. These automatically created subnets use a set of predefined IP ranges with a /20 mask that can be expanded to /16. All of these subnets fit within the 10.128.0.0/9 CIDR block. Therefore, as new Google Cloud regions become available, new subnets in those regions are automatically added to auto mode networks using an IP range from that block. A custom mode network does not automatically create subnets. This type of network provides you with complete control over its subnets and IP ranges. You decide which subnets to create, in regions you choose, and using IP ranges you specify. These IP ranges cannot overlap between subnets of the same network. Now, you can convert an auto mode network to a custom mode network to take advantage of the control that custom mode networks provide. However, this conversion is one way, meaning that custom mode networks cannot be changed to auto mode networks. So, carefully review the considerations for auto mode networks to help you decide which type of network meets your needs. Google Cloud now supports IPv6 in a custom VPC network mode, for example you can configure IPv6 addressing on ‘dual-stack’ VM instances running both IPv4 and IPv6. You can learn a lot more about IPv6 in the “Networking in Google Cloud” course. On this slide, we have an example of a project that contains 5 networks. All of these networks span multiple regions across the world, as you can see on the right. Each network contains separate virtual machines: A, B, C, and D. Because VMs A and B are in the same network, network 1, they can communicate using their internal IP addresses, even though they are in different regions. Essentially, your virtual machines, even if they exist in different locations across the world take advantage of Google's global fiber network. Those virtual machines appear as though they're sitting in the same rack when it comes to a network configuration protocol. VMs C and D, however, are not in the same network. Therefore, by default, these VMs must communicate using their external IP addresses, even though they are in the same region. The traffic between VMs C and D isn’t actually touching the public internet, but is going through the Google Edge routers. This has different billing and security ramifications that we will explore later. Because VM instances within a VPC network can communicate privately on a global scale, a single VPN can securely connect your on-premises network to your Google Cloud network, as shown in this diagram. Even though the two VM instances are in separate regions (us-west1 and us-east1), they leverage Google’s private network to communicate between each other and to an on-premises network through a VPN gateway. This reduces cost and network management complexity. I mentioned that subnetworks work on a regional scale. Because a region contains several zones, subnetworks can cross zones. This slide has a region, region 1, with two zones, zones A and B. Subnetworks can extend across these zones within the same region, such as, subnet-1. The subnet is simply an IP address range, and you can use IP addresses within that range. Notice that the first and second addresses in the range, .0 and .1, are reserved for the network and the subnet’s gateway, respectively. This makes the first and second available addresses .2 and .3, which are assigned to the VM instances. The other reserved addresses in every subnet are the second-to-last address in the range and the last address, which is reserved as the "broadcast" address. To summarize, every subnet has four reserved IP addresses in its primary IP range. Now, even though the two virtual machines in this example are in different zones, they still communicate with each other using the same subnet IP address. This means that a single firewall rule can be applied to both VMs, even though they are in different zones. Speaking of IP addresses of a subnet, Google Cloud VPCs let you increase the IP address space of any subnets without any workload shutdown or downtime. This diagram illustrates a network with subnets that have different subnet masks, allowing for more instances in some subnets than others. This gives you flexibility and growth options to meet your needs, but there are some things to remember: The new subnet must not overlap with other subnets in the same VPC network in any region. Each IP range for all subnets in a VPC network must be a unique valid CIDR block. Also, the new subnet IP address ranges are regional internal IP addresses and have to fall within valid IP ranges. Subnet ranges cannot match, be narrower, or be broader than a restricted range. Subnet ranges cannot span a valid RFC range and a privately used public IP address range. And subnet ranges cannot span multiple RFC ranges. The new network range must be larger than the original, which means the prefix length value must be a smaller number. In other words, you cannot undo an expansion. Now, auto mode subnets start with a /20 IP range. They can be expanded to a /16 IP range, but no larger. Alternatively, you can convert the auto mode subnetwork to a custom mode subnetwork to increase the IP range further. Also, avoid creating large subnets. Overly large subnets are more likely to cause CIDR range collisions when using Multiple Network Interfaces and VPC Network Peering, or when configuring a VPN or other connections to an on-premises network. Therefore, do not scale your subnet beyond what you actually need.

#### Demo: Expand a Subnet

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530356

Let me show you how to expand a custom subnet within GCP. I've already created a custom subnet with a slash 29 mask. A slash 29 mask provides you with eight addresses. But of those, four are reserved by GCP, which leaves you with another four for your VM instances. Let's try to create another VM instance in this subnet. So here we are on the GCP console, and I have my four instances, and if I go into the network interface details here, you can see that these are part of a network and I have a subnet here, and if I drilled further into that you can see that I currently have a slash 29. So let's go back and try to create that other instance. Just going to click on Create Instance. I don't need a very large machine, I'm okay with the micro, and let's hit Create. Ideally, we should be getting an error now about the fact that the IP space should have been exhausted, so we're just going to wait for that. You can also follow this along in the notification pane up here and see that it is trying to create that right now. So we're going to wait for that and see if we get an error here in a second. Once we have that, we're going to go ahead and expand the subnet. So here we can see that the instance creation has failed, I can hover over this and it's just telling me that the IP space of that subnet has been exhausted just as expected. We actually have a "Retry" button here as well as a notification pane. We're going to try to use that in a second once we expand the subnet to recreate that instance. Now, what's important is to note that all of these four instances are currently running. So we're not going to take any of these town during the subnet expansion. Now to expand the subnet, I could go to VPC networks through the navigation menu, or I can go back by clicking on nic0 here directly through the network interface details. So the subnet, this is what I want to change, so let me click the "Edit" button, and lets expand this all the way to a slash 23, and this is going to allow a lot of instances, actually over 500 instances. We're going to wait for this to update, and then we're going to head back and try to recreate that instance. So we can also follow this process along right here. It's still saving, so we're going to just hang on tight here. It should just take a couple seconds. All right. We see it's complete. Now, I still have that "Retry" button here to recreate that instance. So let me actually click that, and I can head back to Compute Engine to see if that is going to succeed. So here we are, instance five, it's being staged and will soon begin running. Let's see if this works out. We can see that already has now an internal IP address allocated now that we've expanded the subnet itself, and if I refresh this we can see that the instance is now created. That's how easy it is to expand a subnet in GCP without any workload shutdown or downtime

#### IP addresses

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530357

Now that we covered Google Cloud Networks at a high level, let's go deeper by exploring IP addresses. In Google Cloud, each virtual machine can have two IP addresses assigned. One of them is an internal IP address, which is going to be assigned via DHCP internally. Every VM that starts up and any service that depends on virtual machines gets an internal IP address. Examples of such services are App Engine and Google Kubernetes Engine, which are explored in other courses. When you create a VM in Google Cloud, its symbolic name is registered with an internal DNS service that translates the name to the internal IP address. DNS is scoped to the network, so it can translate web URLs and VM names of hosts in the same network, but it can't translate host names from VMs in a different network. The other IP address is the external IP address, but this one is optional. You can assign an external IP address if your device or your machine is externally facing. That external IP address can be assigned from a pool, making it ephemeral, or it can be assigned a reserved external IP address, making it static. If you reserve a static external IP address and do not assign it to a resource, such as a VM instance or a forwarding rule, you are charged at a higher rate than for static and ephemeral external IP addresses that are in use. You can use your own publicly routable IP address prefixes as Google Cloud external IP addresses and advertise them on the Internet. In order to be eligible, you must own and bring a /24 block or larger.

#### Demo: Internal and external IP

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530358

I just mentioned that VMs can have internal and external IP addresses. Let's explore this in the GCP Console. So here I am on the Compute Engine page. What I'm going to do is just create a VM and walk through the process of choosing your internal and external IP address. So let me click Create. I can leave the name. You have obviously a selection of regions and zones you can choose, but I want to focus on the IP addresses. So let me go down to this option, expand management security networking sole tenancy. Let's focus on networking. Here at the network interface, I'm going to click the pencil icon. I could choose between two different networks. So if I had different networks, I could choose between them. That's not the case here. Then I have the primary or internal IP and external IP. So if we look at those options, you can see that I can use an ephemeral address either the one that's created automatically or I could custom select one. So within the range that I have here I could just type IP address. I could also reserve a static internal IP address. This is great if you want to keep that IP address for a longer time and we have similar options with the external IP address. But one of the big differences is that you can also just select none. So as I mentioned your instances don't need to have an external IP address. So let's just leave this as ephemeral. By the way, with the slash 20 here we have a lot of space in this IP range over 4,000 addresses. So we could definitely have that many instances. There are also limits of how many instances you can have per network. As of this recording is actually 15,000. So do keep that in mind you might have a very large IP range but that doesn't mean that you actually can create that many instances. That's a quota. There may also be actual limitations on physical hardware that's even available within a specific region or zone. So let me go ahead and create this instance. We're going to keep an eye on the internal and as well as the external IP address. Once the instance is created. Then we're also going to stop and start the instance to see if any of the IP address has changed. So here we can see the internal IP address. So that is definitely within that space that we just looked at. The external IP address obviously is within Google strange here and we could have reserved that, but this is an fMRL one. So let's actually test this out. I'm going to select the instance. I'm going to stop it. So it's telling me that it doesn't move in 90 seconds that might be forced. So if you had any shutdown scripts in here you want to make sure that they can actually complete within 90 seconds. So let's run through that. Remember this external IP address that we currently have here as well as the internal IP address. So this is going to take it's time now. We can also click Refresh to keep an eye on this. But this will take about 90 seconds, and that's just to give your shutdown script enough time to perform any task to gracefully shut down this instance. So here we are, we can see the instance is stopped, the external IP address is gone. So now we're just going to startup that instance again. It's going to tell us it we're going to be build while it's running, that's fine. You can see that the internal IP address remained the same wildest instance stopped. So that has actually stayed for the time being. Now, while this instance spins up which we can by the way monitor the progress over here, we should see that we should be getting a new external IP address now because that was an ephemeral address. So here we can see the instance has started back up and we can see that the external IP address has changed. This demonstrates that every VM needs an internal IP address but external IP addresses are optional and by default, there are ephemeral.

#### Mapping IP addresses

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530359

Regardless of whether you use an ephemeral or static IP address, the external address is unknown to the OS of the VM. The external IP address is mapped to the VM's internal address transparently by VPC. I am illustrating this here by running ifconfig within a VM in Google Cloud, which only returns the internal IP address. Let’s explore this further by looking at DNS resolution for both internal and external addresses. Let’s start with internal addresses. Google Cloud has two types of internal DNS names, Zonal and Global (project wide) DNS. In general, Google strongly recommends using zonal DNS because it offers higher reliability guarantees by isolating failures in the DNS registration to individual zones. Each instance has a hostname that can be resolved to an internal IP address. This hostname is the same as the instance name. There is also an internal fully qualified domain name for an instance that uses the format shown on the slide. If you delete and recreate an instance, the internal IP address can change. This change can disrupt connections from other Compute Engine resources, which must obtain the new IP address before they can connect again. However, the DNS name always points to a specific instance, no matter what the internal IP address is. Each instance has a metadata server that also acts as a DNS resolver for that instance. The metadata server handles all DNS queries for local network resources and routes all other queries to Google's public DNS servers for public name resolution. I previously mentioned that an instance is not aware of any external IP address assigned to it. Instead, the network stores a lookup table that matches external IP addresses with the internal IP addresses of the relevant instances. Now, let’s look at external addresses. Instances with external IP addresses can allow connections from hosts outside of the project. Users can do so directly using the external IP address. Public DNS records pointing to instances are not published automatically; however, admins can publish these using existing DNS servers. Domain name servers can be hosted on Google Cloud, using Cloud DNS. This is a managed service that is definitely worth considering, so let’s explore it in more detail. Cloud DNS is a scalable, reliable, and managed authoritative Domain Name System, or DNS, service running on the same infrastructure as Google. Cloud DNS translates requests for domain names like google.com into IP addresses. Cloud DNS uses Google’s global network of Anycast name servers to serve your DNS zones from redundant locations around the world, providing lower latency and high availability for your users. High availability is very important because if you can't look up a domain name, the internet might as well be down. That’s why Google Cloud offers a 100% uptime Service Level Agreement, or SLA, for domains configured in Cloud DNS. Cloud DNS lets you create and update millions of DNS records without the burden of managing your own DNS servers and software. Instead, you use a simple user interface, command-line interface, or API. Another networking feature of Google Cloud is Alias IP Ranges. Alias IP Ranges let you assign a range of internal IP addresses as an alias to a virtual machine's network interface. This is useful if you have multiple services running on a VM, and you want to assign a different IP address to each service. In essence, you can configure multiple IP addresses, representing containers or applications hosted in a VM, without having to define a separate network interface. You just draw the alias IP range from the local subnet's primary or secondary CIDR ranges. This diagram provides a basic illustration of primary and secondary CIDR ranges and VM alias IP ranges.

#### IP addresses for default domains

- https://www.cloudskillsboost.google/paths/11/course_templates/50/documents/530360

#### Routes and firewall rules

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530361

So far you've learned about projects, networks, subnetworks, and IP addresses. Let's use what you learned to understand how GCP routes traffic. By default, every network has routes that let instances in a network send traffic directly to each other, even across subnets. In addition, every network has a default route that directs packets to destinations that are outside the network. Although these routes cover most of your normal routing needs, you can also create special routes that overwrite these routes. Just creating a route does not ensure that your packet will be received by the specified next top. Firewall rules must also allow the packet. The default network has pre-configured firewall rules that allow all instances in the network to talk with each other. Manually created networks do not have such rules, so you must create them, as you will experience in the first lab. Routes match packets by destination IP addresses. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, enabling traffic delivery from "anywhere". Also, a route is created when a subnet is created. This is what enables VMs on the same network to communicate. This slide shows a simplified routing table, but let's look at this in more detail. Each route in the Routes collection may apply to one or more instances. A route applies to an instance if the network and instance tags match. If the network matches and there are no instance tags specified, the route applies to all instances in that network. Compute Engine then uses the Routes collection to create individual read-only routing tables for each instance. This diagram shows a massively scalable virtual router at the core of each network. Every virtual machine instance in the network is directly connected to this router, and all packets leaving a virtual machine instance are first handled at this layer before they are forwarded to their next hop. The virtual network router selects the next hop for a packet by consulting the routing table for that instance. GCP firewall rules protect you virtual machine instances from unapproved connections, both inbound and outbound, known as ingress and egress, respectively. Essentially, every VPC network functions as a distributed firewall. Although firewall rules are applied to the network as a whole, connections are allowed or denied at the instance level. You can think of the firewall as existing not only between your instances and other networks, but between individual instances within the same network. GCP firewall rules are stateful. This means that if a connection is allowed between a source and a target or a target at a destination, all subsequent traffic in either direction will be allowed. In other words, firewall rules allow bidirectional communication once a session is established. Also, if for some reason, all firewall rules in a network are deleted, there is still an implied "Deny all" ingress rule and an implied "Allow all" egress rule for the network. You can express your desired firewall configuration as a set of firewall rules. Conceptually, a firewall rule is composed of the following parameters: The direction of the rule. Inbound connections are matched against ingress rules only, and outbound connections are matched against egress rules only. The source of the connection for ingress packets, or the destination of the connection for egress packets. The protocol and port of the connection, where any rule can be restricted to apply to specific protocols only or specific combinations of protocols and ports only. The action of the rule, which is to allow or deny packets that match the direction, protocol, port, and source or destination of the rule. The priority of the rule, which governs the order in which rules are evaluated. The first matching rule is applied. The rule assignment. By default, all rules are assigned to all instances, but you can assign certain rules to certain instances only. For more information on firewall rule components, please refer to the links section of this video. Let's look at some GCP firewall use cases for both egress and ingress. Egress firewall rules control outgoing connections originated inside your GCP network. Egress allow rules allow outbound connections that match specific protocol, ports, and IP addresses. Egress deny rules prevent instances from initiating connections that match non-permitted port, protocol, and IP range combinations. For egress firewall rules, destinations to which a rule applies may be specified using IP CIDR ranges. Specifically, you can use the destination ranges to protect from undesired connections initiated by a VM instance towards an external host, as shown on the left. You can also use destination ranges to prevent undesired connections from internal VM instances to specific GCP CIDR ranges. This is illustrated in the middle, where a VM in a specific subnet is shown attempting to connect inappropriately to another VM within the same network. Ingress firewall rules protect against incoming connections to the instance from any source. Ingress allow rules allow specific protocol, ports, and IP ranges to connect in. The firewall prevents instances from receiving connections on non-permitted ports and protocols. Rules can be restricted to only affect particular sources. Source CIDR ranges can be used to protect an instance from undesired connections coming either from external networks or from GCP IP ranges. This diagram illustrates a VM receiving a connection from an external address, and another VM receiving a connection from a VM within the same network. You can control ingress connections from a VM instance by constructing inbound connection conditions using source CIDR ranges, protocols, or ports.

#### Pricing

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530362

Before you apply what you just learned, let's talk about network pricing. It is important that you understand the circumstances in which you are built for GCP's network. This table, is from the Compute Engine documentation and it lists the price of each traffic type. First of all, egress or traffic coming into GCP's network is not charged, unless there is a resource, such as a load balancer that is processing egress traffic. Responses to request account as egress and are charged. The rest of this table, lists egress or traffic leaving a virtual machine. Egress traffic to the same zone, is not charged as long as that egress is through the internal IP address of an instance. Also egress traffic to Google products like YouTube, maps, drive, or traffic to a different GCP service within the same region, is not charged for. However, there is a charge for egress between zones in the same region, egress within a zone, if the traffic is through the external IP address of an instance, and egress between regions. As for the difference in egress traffic to the same zone, Compute Engine cannot determine the zone of a virtual machine through the external IP address. Therefore, this traffic is treated like egress between zones in the same region. Also there are some exceptions and pricing can always change. So refer to the documentation in the links section of these slides. Now, you are charged for static and ephemeral external IP addresses. This table, represents the external IP pricing for us-central1 as of this recording. You can see that if you reserve a static external IP address and do not assign it to a resource, such as a VM instance or a forwarding rule, you are charged at a higher rate and for static and ephemeral external IP addresses that are in use. Also external IP addresses on preemptible VMs, have a lower charge than for standard VM instances. Remember, pricing can always change, so please refer to the documentation link in the slides. Also I recommend using the GCP pricing calculator to estimate the cost of a collection of resources, because each GCP service has its own pricing model. The pricing calculator is a web-based tool, that you use to specify the expected consumption of certain services and resources, and it then provides you with an estimated cost. For example, you can specify a specific instance type, in a specific region along with 100 gigabytes of monthly egress traffic to Americas and EMEA. The pricing calculator then returns the total estimated cost. You can adjust the currency and time frame to meet your needs, and when you finish, you can e-mail the estimate or save it to a specific URL for future reference. To use the pricing calculator today, refer to the link in the slides.

#### Lab Intro: VPC Networking

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530363

Let's apply some of the network features we just discussed in a lab. In this lab, you create an auto mode VPC network with firewall rules and to VM instances. Then you convert the auto mode network to a custom mode network and create other custom mode networks as shown in this network diagram. You also explore the connectivity across networks.

#### VPC Networking

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530364

#### Lab Review: VPC Networking

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530365

In this lab, you explore the default network, and determine that you cannot create VM instances without a VPC network. So you created a new auto mode VPC network, with subnets, roots, firewall rules, and two VM instances, and tested connectivity for those VM instances. Because auto mode networks aren't recommended for production, you converted the auto mode network to a custom mode network. Next, you create two more custom mode VPC networks with firewall rules and VM instances using the GCP console, and the GCloud command line. Then you test the connectivity across VPC networks, which worked when you pinged external IP addresses, but not when you pinged internal IP addresses. VPC networks are, by default, isolated private networking domains. Therefore, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC peering, or a VPN connection. You can stay for a lab walk through. But remember, that GCP's user interface can change, so your environment might look slightly different. All right. So here I am in the GCP console. The first thing I'm going to do is I'm just going to explore the default network. So if I, on the left-hand, side click on the navigation menu, and scroll down to VPC network, we will see that this project has a default network. Every project has a default network. That is unless you have an organizational policy that prevents this default network from being created. But essentially, all the different projects that used through Qwiklabs will always have this. So in here, we can see we have a different subnet in each of the different regions. All of these are private IP addresses. I can also go to the routes, and these are established automatically with the networks. So we can see routes between subnets, as well as to the default route, to the Internet. We can even look at the firewall rules. The default network comes with some preset firewall rules to allow ICMP traffic from anywhere. RDP traffic, as well as SSH. Then also, all protocols imports within the network. So this is the range of the network. So we also allow all traffic from within the network itself. So let's go ahead and let's actually delete these firewall rules. I can just check them all right here and delete them. Let's just assume that we want to get rid of everything that's been created for us, and just create our own network instead. So I'm going to go ahead and delete these. I can look at the status up here. We can see that all four are being deleted. It'll update as each as being deleted. Once that is done which is now, I can head to the network, select the default network, and we're also just going to delete that entire network. Once we delete this network, we should see that there should be no routes without a network because there's no use case for them. So let's just wait for the network to be deleted and then we'll verify that. So we can, again, see the progress bar up here, that's deleting, you can also hit refresh, and this should just take a couple seconds. You can see that as I'm refreshing, some of the subnets are disappearing. It's actually just deleting them all the subnets first, and then it's getting rid of the network as a whole, because the network is really nothing else than just a combination of subnets. So all these subnets have to be deleted. There we go. They're all gone now. Now, it's just the network itself that is remaining. If I go to routes, we should see that all the routes already gone, because without the subnets, there's really no need for the routes. If I go back to the network, we should see that any moment now the network itself also disappears. There we go. All right. So without a VPC network now, we shouldn't be able to create any VM instances, containers, or app engine application. Let's actually verify that. I'm going to go to the navigation menu, go to compute engine, and let's just try to create an instance, just going to click create. I'm going to leave everything as its default. If I go actually under networking, we should see that it's going to complain here. If I click on networking, that actually doesn't have a local network available. But let's just click create and see what happens, and it does indeed give us an error, and point out the fact that this tab has an issue. So we clearly cannot create an instance, because again, these instances live in networks, and without a network, we can't create it. So let's hit cancel, and what we're going to do now is we're going to create our own auto mode network. So I'm going to head back to VPC networks. You can pin, by the way, the services. So I'm just going to pin VPC network, compute engine, because we're going to be going back and forth between these. Then within VPC network, we're just now going to create our own network. I can give it a name. I'm going to use the same name that I have in lab instructions, which is My Network. Now I have the option of creating a custom or an automatic. Let's start off by creating an automatic network. So that's going to preset all the distance subnets for us in all the different regions that are available. You can scroll through those and see those all in here. They have a preset to side arrange. You can expand that side arrange later. But again, as an auto network, you don't define the actual IP address range. There are also firewall rules that are available. What's interesting here is you see that there's a deny-all ingress and allow all ingress firewall rule. So these are here by default, and they're actually. You can't even uncheck them. So these are actually with all networks that you create, and you can see that this has the highest party integer, which really means it's a lowest priority. So by default, all ingress traffic is denied, and all ingress traffic is allowed. Unless we create other firewalls to see differently. So if I check all these boxes, we're now allowing ingress traffic for these IP ranges, and these protocols imports. So let's go ahead and click create, and we're going to wait for that network to be created. Then we're going to look at the IP addresses for two of the different regions, and we're going to create instances in those regions, and verify that it's taking those IP addresses. So here, you can see the subnets already all populated here. I can monitor the progress also up here, but this is really done any second now. I'm actually going to start heading over to compute engine, and to create our instances. So let's click create. I'm going to give it a name mynet-us-vm. This is going to be in your central one, specifically, the Zone C. I don't really need a big machine. We're just doing some testing here. So let me just create a micro that reduces the cost a little bit, and I'm going to now click create. Then we're going to repeat. I can close this panel over here. The same workflow and create an instance in Europe. So I'm going to grab the name from the lab instructions for that. I'm going to select the Europe West One region, specifically the Zone 1C. Again, a micro machine which is just a shared-core, and click create for that as well. We can see the US Central 1C machine is already up. We also see the internal IP address that has been provided. Again, there are some reserved IP addresses. The dot zero is reserved as well as the dot one. So in both of these ranges, the dot two is the first available address. Now, we can verify that these are part of the right subnet, if I click on nic0, I go to the network interface details. Here, we can see it's part of the sub-network. Now the sub-network, in this case, has the same name as the network because this is an auto network. Here, we can see that it's part of this range. So 1012800/20. Let's verify that, and that is correct. We are in there with a dot two, and let's verify that the other should be now a 10132.00/20. So again, click on nic0, go to the sub-network, and we can see that's true. You can also see here that this address is reserved for the gateway. All right. So that way, the dot two was really the first usable address within that range. So now, these are on the same network. So let's verify some connectivity between those. I'm going to grab the internal IP address of mynet-eu-vm, just copy that, and then we're going to SSH too this other instance. So again, these instances are in two separate regions but in the same network. So we should be able to ping these addresses now. So if I ping three times using the internal address, then we can see that this works. This works because we have that allow internal firewall rule that we selected earlier. I can actually repeat the same by using the name of the instance. You can see that it's taking that name. It's actually has, here, the fully qualified domain name, and it's just using the IP address for that. So VPC networks have an internal DNS service that allows you to address instances by that DNS names, instead of their internal IP addresses. That's very useful because, well, this internal IP address could change, right? But the name is not going to change. So it's always good to be aware of that, that you can use the fully qualified domain name to ping those. All right. Now we can try this whole thing the other way round. Let me exit this instance, grab the internal IP address of the instance in the US, and SSH to the instance in Europe. We're also going to ping the internal IP address here. We can see that works. We could even now try to ping the external IP address. So that's 34, in my case, 671818, and that works as well. The reason that I'm able to ping the external is because I have firewall rule that allows ICMP externally. I can verify those again. If I click on the network interface details, here I can see all of the firewall rules, and what filters they have, and what protocols, and ports. So this all works fine, and let's assume that this workflow has worked for us but now we have decided that we want to convert the auto mode network that we have to a custom mode network. So let's go ahead and do that. We're going to go to "VPC networks", and we're going to click on "my network", and then we're going to click on "edit", and we're going to change this subnet creation mode from auto to custom, and hit "save". So now we can navigate back. You can see that this is in progress up here. The mode still says "auto". We could have also flipped that here. Let's wait for that to be refreshed, and now we can see that this subnetwork is now a custom subnet. So let's say that this has worked so far, and now we've realized that we need a couple more networks. There's a network diagram in the lab that has two other networks, as well as some instances and everything. So let's go ahead and create those. So now we're going to go to "create a VPC network". We're going to create the management network, and rather than starting with automatic and converting, we're just going to start with the custom net. For that we have to define each of these subnets. The minimum information we need to provide is a name, the region, so let's select "us-central1", and then the IP address range, and then can click "done". Now I can add, if I wanted to, another subnet. But the other thing that's very interesting about this is, I'm creating this right now through the GCP console but you can also create networks, as well as subnets, from the command line using Cloud Shell. If I click down here on command line, I'm actually provided with the commands to do that. The first one just creates the network itself. You don't have to use the project flag in here. So we could just say G Cloud compute, networks create, the name of the network, and the fact that this subnet is a custom mode. Similarly then, we create these subnets which is "networks subnets create" the name of the subnets, add the subnet itself, the name of the network, the region, and the range. So again, that's the sort of minimal information. Let's just click close and "create". We'll create the other one from the command line. So it's creating that network, and in parallel I can go and now activate Cloud Shell by clicking up here in the right corner. Yes, I want to start using Cloud Shell. I'm just going to make this a little bit bigger, and once this is up, we're going to use those commands that we just saw to create first a network, and this is going to be the privatenet, which is also of the mode custom. Once we have that, we're going to create two subnets within that network. So you can see in the console that the other network was created. Privatenet, is being created right now here, and once that is ready we can add the two subnets to that. So there we go. There is the subnet. It's also telling us this is a new network. You don't have any firewall rules, here are some commands if you want to create some firewall rules. We'll do that in a second. Let's just create these subnets in here. So first we're going to create one in the US, and then we're also going to create one in Europe. If you wanted to speed this up you could actually launch another Cloud Shell session. Now that the network is up, you could create these subnets in parallel. But we're just going to wait for this to complete and then we'll paste that command in there. You can monitor all of this in a console. If we click "refresh," there we see it. It's also completed. It just returns, I've done exactly what you told me. Let's create the other one. It didn't copy the command correctly. There we go. This is now in Europe, specifically Europe-west1. Refresh. You see that's already being created there. So we can definitely display all of those in the GCP console. If you click the button over here in Cloud Shell you can actually open this in a new window. This actually opens it in a new tab, that way you preserve your real estate. You can keep focusing on the console, as well as focusing on Cloud Shell. So let me actually create some real estate by just clearing this, and then paste the command to list all the networks with just G Cloud compute networks list. So we can see them, three networks. They're all Custom. We can dig deeper into this by also listing the subnetworks, and using the "sort-by" command to sort these by network. So now we'll see my network has a lot of subnets because it used to be in auto mode. Then, I mentioned that we want subnet and for permanet. We've two subnets. So now we're going to create some firewall rules. So let's click on "Firewall rules" up here. You can see the ones that are already there. Click create "Firewall rule". We'll repeat the same process we did earlier. We'll first create this using the console, and then we'll repeat the firewalls for a different network using Cloud Shell. So let me give it a name. Let's make sure I select the right network that the firewall rule applies to. Let's just do all instances. For the IP ranges select all addresses. I'm allowing, in this case, ICMP, SSH, and RDP. So let me define ICMP, and then 22 for SSH, and 3389 for RDP, and now down here I can click on "command line", and we can see this as one long command. Again, you don't need to define the project flags as gcloud compute, firewalls create. The name of the rule, the fact it's an ingress party, that is also actually default. We could leave that out. Importantly is the name of the network. Action allows default too, and then the rules as well as source ranges. So let's create that in the console, and we'll grab the command from the lab instructions to do the same for either network. So here you can see. We paste that in, and that should now create the other firewall rule for us. We can monitor the firewall rules in the console, as well as in Cloud Shell. So we'll run a command to list all the firewall rules in a second. So they're all created. If we list them, we can see them all here. If I refresh this we can also see them right here. So now it's time to create some more Instances, and then explore the connectivity. So let's head back to compute engine. I'm going to create instances in these new networks I created. So let me click "create instance". I'm actually going to close Cloud Shell for now. Let me just make it smaller. We're going to provide a name, and "US- Central1-c". Small machine is very fine. Now importantly I need to expand this option down here to select the right network. We've three options right now, and it has actually pre-selected that network. That's because from an order, it's listed up top. That is correct. So let's click "done" and there's again a command. There's a lot of information here that we don't need. You'll see that in a second when we run our command, like the BootDisk. We're selecting a lot of standard options. So let's just hit "create", and let's pull the command from the lab that creates the same in a different network. That's "gcloud compute instance create", the name of the instance, the zone, the machine type, the subnet. That is the bare minimum that we need to provide. So let's run that. You can see the other instance is already created. I can refresh this. See that the other Instances are already coming up too, and once Cloud Shell is updated we can list all the instances. Let's do that here. Can sort them by zone, or we could sort them by network. So we can see in one zone here we have an instance, and then in another zone we have three instances. Keep in mind these Instances are in different networks, and we can display that if we go to columns and check "Network", you can see that these Instances, with exception of the "mynet" these are on the same IVPC network, the others are indifferent. That's going to now go into the connectivity that we're going to explore. We're going to try to again ping IP addresses, both external and internal, and see what works. So let me grab the Management USVM external IP address, and we're going to SSH to the "mynet-us-vm". They are in the same zone, but they're in different networks. So let's see if we can ping the external IP address, and then we'll try the internal. So external works. That's because we set up the firewall rules for that. I can also do the same for privatenet. I can plug that IP address in which is 35.188.20.220 That works as well. So you can ping those, even though they are in different networks. Now from an internal perspective I should only be able to ping mynet-uvm which we actually tried earlier already. So let me just hop on the other ones. I'm going to try 10.130.0.2, and we can see that's not leading to anything. We should be getting a 100 percent packet loss, and then we'll try the same from the other one. So 172.16.0.2 and we can see that again isn't working either. So even though this Instance is in the same zone as these other instances I'm trying to ping, the fact that they are in a different network does not allow me to ping on the internal IP, unless we set up other mechanisms such as VPC peering or a VPN. That's the end of the lab.

#### Common network designs

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530366

Let's use what we have learned so far and look at common network designs. Now, ‘common’ is a fairly relative term. While I could spend all day talking about network designs, I have picked a handful of designs that best relate to this module. Let's start by looking at availability. If your application needs increased availability, you can place two virtual machines into multiple zones, but within the same subnetwork as shown on this slide. Using a single subnetwork allows you to create a firewall rule against the subnetwork, 10.2.0.0/16. Therefore, by allocating VMs on a single subnet to separate zones, you get improved availability without additional security complexity. A regional managed instance group contains instances from multiple zones across the same region, which provides increased availability. Next, let's look at globalization. In the previous design we placed resources in different zones in a single region, which provides isolation for many types of infrastructure, hardware, and software failures. Putting resources in different regions as shown on this slide provides an even higher degree of failure independence. This allows you to design robust systems with resources spread across different failure domains. When using a global load balancer, like the external Application Load Balancer, you can route traffic to the region that is closest to the user. This can result in better latency for users and lower network traffic costs for your project. We'll explore both managed instance groups and load balancers later in the course series. Now, as a general security best practice, I recommend only assigning internal IP addresses to your VM instances whenever possible. Cloud NAT is Google's managed network address translation service. It lets you provision your application instances without public IP addresses, while also allowing them to access the internet in a controlled and efficient manner. This means your private instances can access the internet for updates, patching, configuration management, and more. In this diagram, Cloud NAT enables two private instances to access an update server on the Internet, which is referred to as outbound NAT. However, Cloud NAT does not Implement inbound NAT. In other words, hosts outside your VPC network cannot directly access any of the private instances behind the Cloud NAT gateway. This helps you keep your VPC networks isolated and secure. Similarly, you should enable Private Google Access to allow VM instances that only have internal IP addresses to reach the external IP addresses of Google APIs and services. For example, if your private VM instance needs to access a Cloud Storage bucket, you need to enable Private Google Access. You enable Private Google Access on a subnet-by-subnet basis. As you can see in this diagram, subnet-a has Private Google Access enabled and subnet-b has it disabled. This allows VM A1 to access Google APIs and services, even though it has no external IP address. Private Google Access has no effect on instances that have external IP addresses, That's why VMs A2 and B2 can access Google APIs and services. The only VM that can't access those APIs and services is VM B1. This VM has no public IP address and it is in a subnet where Google Private Access is disabled.

#### Lab Intro: Implement Private Google Access and Cloud NAT

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530367

Let's apply what we just covered. In this lab, you implement Private Google Access and Cloud NAT for a VM instance that doesn't have an external IP address. Then you verify access to public IP addresses of Google APIs and services and other connections to the Internet.

#### Implement Private Google Access and Cloud NAT

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530368

#### Lab Review: Implement Private Google Access and Cloud NAT

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530369

In this lab, you created an instance with no external IP address and access it using Cloud IAP. You then enable Private Google Access and configured a NAT gateway and verified that vm-internal can access Google APIs and services and other public IP addresses. VM instances without external IP addresses are isolated from external networks. Using Cloud NAT, these instances can access the Internet for updates and patches, and in some cases for bootstrapping. As a managed service, Cloud NAT provides high availability without user management and intervention. Let me walk you through the lab. Now, remember that the GCP user interface can change. So your environment might look slightly different. So the first thing I'm going to do is create the VM instance. After that, we are also going to have to create a VPC network and some firewall rules. So let me go to navigation menu, scroll down to VPC networks. We're going to create a network and call it privatenet. So I'm going to name it privatenet, keep this subnet creation mode as custom. We're just going to create one subnet in here. We're going to call it privatenet-us. Let's place this in the us-central1 region, as given to us in the instructions. Here we go, and we even have an IP address range for that. Now, we are going to enable Private Google Access later. So you want to keep that off for now. I turned it on by accident. So you can see the effect of it being off. So let me click "Done" and click "Create". Now, I'm going to wait for this network to be created and once it's up and running, we're going to add a firewall rule because we want to allow SSH to the instance that we're going to put on this network. So I can see the network here. A firewall rule is created for networks, so I had to wait for that to be ready. So let me go to firewall rules, create firewall rule, give it a name. Specify that the network is privatenet. Let's just do all instances and sort by IP ranges. Now, rather than just saying, "Hey, you can SSH this instance from anywhere," we are actually going to give it a very specific range. This is because we're using Cloud IAP. So we're going to use a Cloud IAP tunnel, and because of that, we can limit the site range. Now, this is for an SSH connection. So I want to enable TCP port 22, and then click "Create". While this is creating, I can go ahead and create my Compute Engine instance. So let's go to "Compute Engine", click "Create". We're going to give it the name vm-internal. Now, we need to make sure we choose a region for which we've created a subnet. So us-central1, so us-central1-c. I can keep them machine type as my standard, n1-standard-1, 1virtualCPU, and I'm going to scroll down. The important thing is I need to select the actual VPC networks. Let's go to networking. Networking again, we're going to edit the network interfaces. I want to select the privatenet network. It only has one subnet, and I'm going to set the external IP address to none. Click "Done" and click "Create". So this is a way to create a private instance. Let me close this. That has no external IP address. Now, when the instance comes up, you will see that we won't be able to directly SSH to it because it doesn't have an external IP address. So if we use this, this wouldn't work on us, so instead what we're going to use is, we're going to do an IAP tunnel. For that, we're going to open Cloud Shell. So let me go click "Activate Cloud Shell", and that popped up in a new window. That can certainly happen sometimes. Looks like there's some A, B testing going on here. So here I have Cloud Shell, doesn't look like it has the correct project set. So let's actually do that. I'm going to set the project and then just grab the project ID from here, and set this up for the correct project, and there we can see that now. So it's setup, and now what I'm going to do is, I'm going to run the command to SSH from here. I'm going to specify this is through IAP, and then I want to confirm. For passphrase, we're just going to hit "Enter" and then "Enter" again. Once this is complete, we should now see that the command prompt has changed to vm-internal. So we're now in vm-internal, it doesn't have an external IP address, but let's confirm that we can't just ping the World Wide Web. This ping command isn't working because vm-internal does not have an external IP address. So we can wait for this to complete and it's failing. Again, when instances don't have external IP addresses, they can only be reached by other instances on the network, either through a managed VPN gateway or Cloud IAP tunnel, and Cloud IAP enables contexts where access to VMs through SSH and RDP without a bastion host. That would be the other idea or option. We could create a bastion host, but that would still have an external IAP. Then we're just using the bastion host to then connect to this. Instead, we can just use Cloud Shell and IAP. So this isn't working. So what we're going to now is we're going to look into Private Google Access. So currently, VM instance with no external IP address can use Private Google Access to reach external IP addresses of Google APIs and services. But by default, this is disabled. We saw that earlier, we left it as disabled. So let's test the effect of this being disabled. I'm going to go to the navigation menu, and we're going to create a cloud storage bucket. So let's go to "Storage". I'm going to click "Create Bucket". Now, the most difficult piece is you need to have a unique bucket name. You could do that by grabbing the ID of a project. Click Continue, you can leave this as Multi-region, we can leave everything else by default, and just click Create. The important thing is you're going to have to remember that bucket names, so here's the bucket. So I'm going to do now, is I'm going to go back to Cloud Shell. Importantly, I'm still in my VM Instance here. So I want to change that, so let me exit out of here. So now I'm back in Cloud Shell and then I'm going to run a command to copy an image from a public bucket to my bucket, but I need to specify what my bucket is. So I can take the name of the bucket and add that here to copy this image, so that worked. We can go in here and refresh to verify that we now have an image in here. You can actually click on this image and this just shows you how Private Google Access is implemented pending if it's on or off for a network. We're going to explore that a little bit more. So now what we're going to do is, we're going to now try to copy this image, first from Cloud Shell. Well, Cloud Shell has an external IP address, so that is going to work, run that. I need to actually click Enter. Obviously, I didn't specify my bucket, that is on me. So I need to change my bucket, so typical error that you might see. Let me grab the name of the bucket, placed it in there. Let's try that again, okay, that works. We even use Cloud Shell to move this image anyway, so we're able to access Cloud Storage currently through Cloud Shell. Let's go back to our VM internal [inaudible]. So we use the same command use earlier to SSH through a IAP tunnel. Here, I can see the command prompt changed. Now, I'm just going to copy the same command here to copy this image, so I don't have to change the bucket name a couple times, and we're going to run that. We should see that this does not work, because currently VM internal can only send traffic within the VPC networks because again, Private Google Access is disabled. So with two options, we can wait for this to fail and give us an error or we can use Control C to just stop the request. So let's actually just stopped this. What we do now is I'm going to able Private Google Access. So let's go back to the Cloud Console, the Navigation menu and I'm going to navigate to my VPC network, specifically privatenet. Private Google Access is enabled at the subnet level. So I'm going to go directly to the subnet, click the Edit icon, scroll down and able Private Google Access or set it to on, click Save. I'm going to wait for this to update and then I'm going to come back to my instance, my SSH lessons through Cloud Shell and just try to run the command again. So it looks like it's all set, you can also see that here. Going back to my SSH window, run that command again, and now it works. So that's how easy it is to enable Private Google Access. So now in this last task of the lab, we're going to configure a Cloud NAT gateway. Now although our Instance here, VM internal can now access certain Google APIs and services without an external IP address, the instance cannot access the Internet for updates and patches. So for that, we're going to configure our Cloud NAT gateway, but again, we're going to try this behavior first without the NAT gateway and then we're going to enable it. So what we're going to do is I'm going to exit here to just get to my Cloud Shell Instance. There we go, you can see the command prompt changed to Cloud Shell. I'm just going to run sudo apt-get update, and that should obviously work for my Cloud Shell instance because it has an external IP address. So we can see it's getting all these packages and that is working just fine. So now that's complete, we're going to use the SSH command again using the IP tunnel to get to VM internal, there we can see this change. Now we're going to run the same command here. You might say, "Well, hold on." It's actually able to get some of these packages. Yes, that's because we've enabled Private Google Access, so it's able to get those within Google. Once it's trying to get something else here, it's failing. So we can just stop that, this is not going to happen. Now we're going to go ahead and configure Cloud NAT gateway and then try to run that command again. So let's go to the Cloud Console and under the Navigation menu, we're going to go to Network services and Cloud NAT. We're going to go click Get started, just give this a name called nat-config. It's just a name that we have in the lab instructions. You really want to follow these lab instructions because any of our labs that are scored, we'll use names that we're defining in the lab instructions. So important distance to be on privatenet, Region is us-central1. For Cloud Router, we currently don't have one, so we're going to go create one. This is super simple, you just give it a name and click Create. Now, there's also a NAT mapping section and this allows you to choose the subnets to map to the NAT gateway, so you could manually assign static IP addresses that should be used when performing that. But in this case, we're not going to go that and get that fancy, we're just going to click Create. We're going to wait now for the gateway status to change to running. So we can see that the status changed to running, it actually only took a couple seconds. Now, even though this is running, it may actually take up to three minutes for the NAT configuration to propagate all the way to the VM. So you want to wait at least a minute before trying to access the Internet again. What I mean by that is in our SSH session that we currently still have to VM internal, we're going to run the command again. I want to make sure it works this time. So I could actually just try it right now and see if it's ready or not. If I do, you see it's still failing at the step. So let me hit Control C and let's get a couple more minutes and then try to run the command again. So we've waited a couple minutes, let's try to run the command one more time and now we can see that's working. It's getting all the packages and with that we can confirm that Cloud NAT decline a gateway is not working. Now, couple of things to remember, the Cloud NAT gateway implements outbound net, but not inbound net. In other words, what that means is that hosts outside of your VPC network, can only respond to connections initiated by your instances. They cannot initiate their own. So new connections to your instances via the net, so keep that in mind. The other thing is in this lab we used IAP, and IAP uses your existing project roles and permissions when you connect to VM instances. So by default, instance owners, which your instance owner since you created this instance. They're the only ones that have the IAP secure tunnel user role. If you want to allow other users to connect to access using VM, using IP tunneling, you need to grant them those roles. You can actually do that directly through the Navigation menu and go to Cloud IP, and just give people those roles. That's the end of the lab.

#### Quiz: Virtual Networks

- https://www.cloudskillsboost.google/paths/11/course_templates/50/quizzes/530370

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530371

In this module, I gave you an overview of Google's Virtual Private Cloud. We looked at the different objects within VPC like projects, networks, IP addresses, routes, and firewall rules. I also provided a brief overview of how you're network design choices can affect billing. Then you apply the different concepts that we covered in a thorough lab. Next, we looked at common network designs and you got to implement private Google axis and Cloudnet in a lab. Now, that you have a solid understanding of how GCP has implemented networking, let's move on to learn more about other services. Next up, is Compute Engine which offers scalable, high-performance Virtual Machines.

### Virtual Machines

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530372

In this module, we cover virtual machine instances, or VMs. VMs are the most common infrastructure component and in Google Cloud they're provided by Compute Engine. A VM is similar, but not identical, to a hardware computer. VMs consists of a virtual CPU, some amount of memory, disk storage, and an IP address. Compute Engine is Google Cloud’s service to create VMs. It is very flexible and offers many options, including some that cannot exist in physical hardware. For example, a micro VM shares a CPU with other virtual machines, so you can get a VM with less capacity at a lower cost. Another example of a function that cannot exist in hardware is that some VMs offer burst capability, meaning that the virtual CPU will run above its rated capacity for a brief period, using the available shared physical CPU. The main VM options are CPU, memory, discs, and networking. Now, this is going to be a very robust module. There's a lot of detail to cover here with how virtual machines work on Google Cloud. First, we'll start with the basics of Compute Engine, followed by a quick little lab to get you more familiar with creating virtual machines. Then, we'll look at the different CPU and memory options that enable you to create different configurations. Next, we will look at images and the different disk options available with Compute Engine. After that, we will discuss very common Compute Engine actions that you might encounter in your day-to-day job. This will be followed by an in-depth lab that explores many of the features and services covered in this module. Let's get started with an overview of Compute Engine.

#### Compute Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530373

As mentioned in the introduction to the course, there is a spectrum of different options in Google Cloud for compute and processing. We will focus on the traditional virtual machine instances. Now the difference is, Compute Engine gives you the utmost flexibility: run whatever language you want - it's your virtual machine. This is purely an infrastructure as a service, or IaaS model. You have a VM and an operating system, and you can choose how to manage it and how to handle aspects, such as autoscaling, where you’ll configure the rules about adding more virtual machines in specific situations. Autoscaling will be covered later in the course. The primary work case of Compute Engine is any generic workload, especially an enterprise application that was designed to run on a server infrastructure. This makes Compute Engine very portable and easy to run in the cloud. Other services, like Google Kubernetes Engine, which consists of containerized workloads, may not be as easily transferable as what you’re used to from on-premises. So what is Compute Engine? At its heart, it's physical servers that you're used to, running inside the Google Cloud environment, with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You choose the type of disk you want, whether you want to use persistent disks backed up by standard hard drives or solid-state drives, local SSDs, Cloud Storage, or a mix. You can even configure the networking interfaces and run a combination of Linux and Windows machines. We will discuss these options in more detail later in the module. Several different features will be covered throughout this module, such as machine rightsizing, startup and shutdown scripts, metadata, availability policies, OS patch management, and pricing and usage discounts. It is important to mention that hardware manufacturers have run up against limitations, and CPUs, which are central processing units, and GPUs, which are graphics processing units, can no longer scale to adequately reach the rapid demand for ML. To help overcome this challenge, in 2016 Google introduced the Tensor Processing Unit, or TPU. TPUs are Google’s custom-developed application-specific integrated circuits used to accelerate machine learning workloads. TPUs act as domain-specific hardware, as opposed to general-purpose hardware with CPUs and GPUs. This allows for higher efficiency by tailoring architecture to meet the computation needs in a domain, such as the matrix multiplication in machine learning. TPUs are generally faster than current GPUs and CPUs for AI applications and machine learning. They are also significantly more energy-efficient. Cloud TPUs have been integrated across Google products, making this state-of-the-art hardware and supercomputing technology available to Google Cloud customers. TPUs are mostly recommended for models that train for long durations and for large models with large effective batch sizes. Refer to the available online documentation for more details. Let’s start by looking at the compute options. Compute Engine provides several different machine types that we’ll discuss later in this module. If those machines don’t meet your needs, you can also customize your own machine. Your choice of CPU will affect your network throughput. Specifically, your network will scale at 2 gigabits per second for each CPU core, except for instances with 2 or 4 CPUs, which receive up to 10 gigabits per second of bandwidth. There is a theoretical maximum throughput of 200 gigabits per second for an instance with 176 vCPU, when you choose an C3 machine series. When you're migrating from an on-premises setup, you're used to physical cores, which have hyperthreading. On Compute Engine, each virtual CPU (or vCPU) is implemented as a single hardware hyper-thread on one of the available CPU platforms. For an up-to-date list of all the available CPU platforms, refer to the CPU platforms documentation link in the course resources. After you pick your compute options, you want to choose your disk. You have three options: Standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives, or flash memory solid-state drives? Both of these options provide the same amount of capacity in terms of disk size when choosing a persistent disk. Therefore, the question really is about performance versus cost, because there's a different pricing structure. Basically, SSDs are designed to give you a higher number of IOPS per dollar versus standard disks, which will give you a higher amount of capacity for your dollar. Local SSDs have higher throughput and lower latency than SSD persistent disks, because they are attached to the physical hardware. However, the data that you store on local SSDs persists only until you stop or delete the instance. Typically, a local SSD is used as a swap disk, just like you would do if you want to create a ramdisk, but if you need more capacity, you can store those on a local SSD. Standard and non-local SSD disks can be sized up to 257 TB for each instance. The performance of these disks scales with each GB of space allocated. As for networking, we have already seen networking features applied to Compute Engine in the previous module’s lab. We looked at the different types of networks and created firewall rules using IP addresses and network tags. You’ll also notice that you can do Application Load Balancing and Network Load Balancing. This doesn’t require any pre-warming because a load balancer isn't a hardware device that needs to analyze your traffic. A load balancer is essentially a set of traffic engineering rules that are coming into the Google network, and VPC is applying your rules destined to your IP address subnet range.

#### Demo: Create a VM

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530374

Let me give you a quick walk through of the VM instance creation process, and point out CPU, storage, and network options in the GCP Console. So here I am already on the Compute Engine instance page. You can get to here by going to the navigation menu and then clicking on "Compute Engine". As we use this in the course a lot, you might actually want to pin this sometimes so that you can get to it more easily. Then within there I have gone into VM instances. So I just want to again show you some of the options that are available when creating instance. To get started I'm going to click on "Create". The first thing I want to choose is a name, so you have that right up here. Then maybe more importantly is actually where you want the instance to be located. So you have an option of all the different available regions. It has the name of the regions as well as the closest city as to where that region is located. Then within the regions you have different zones that you can choose from. You also see on the right-hand side that there is a cost associated with the current configuration, net cost is going to change as we change the configuration. So for example, if I instead of creating an instance in US Central one, I create one maybe in Europe West One. You will see that the cost is slightly adjusted. So I can try that a couple different ways by choosing different locations. You should see that the cost changes depending on the region that we choose. Now it goes further if I then choose the machine type. There are different types, we'll go into all of those. But if I go in here, the different types explain to me what they provide. This standard n1 standard one provides one virtual CPU with 3.75 gigabytes of memory. If I change to a machine with more CPU and more memory, we'll see that the cost is adjusted. You can also go into details here. It actually spells it out for you that there's a cost for the CPU and memory, but there's also costs for the persistent disk. We haven't configured that yet, but this is the default value and if we configure that, it's going to ingest a cost. There's also sustained use discount would go into that as well, but essentially all of that is what ultimately gets you to this total monthly cost. It's also broken down in an hourly cost here, and we'll talk more about pricing later within the module. So again I can choose different machine types, maybe we want a larger machine type that's going to be more expensive. Maybe I just need a shared core. So something really, a micro machine or a small machine and that can really drive the cost down a lot. So let me go back to the default. The other thing to think about in terms of your region and zone is not just the cost, but really you want to create your instances that are close to your users. Maybe you want to have them spread out across different regions for habilability. You might be having restrictions for data locality meaning that your data has to be in a specific region. So these are all the different things that you want to consider when choosing the region and zone. Now if I scroll further down, one of the next pix options is the boot disk. So we can see here that currently it has a 10 gigabytes standard persistent disk. I can change that. I can change the image itself, but I can also change the boot disk type. Now the boot disk needs to be a persistent disk. We have the standard persistent disk. Think of an HDD. And we have the SSD. You can see that we can define the size here, and you can see that both of them have the same exact maximum size. So if I make this larger, let's say 1,000, then we're going to see that the cost now is adjusted to that disk size. So it can go back, that's very large. Maybe I'm just okay with 10 gigabytes as the boot disk. You can also add more disks. So if I scroll down and go to management security disk networking, I can go to disks here. So here I can choose the type of encryption for the disk. I have Google managed key, Customer managed key, Customer supplied key. Then I can add more disks. So if I add a new disk here then under type I could also choose a local SSD. Disks come in predefined sizes, depending on how many you choose your performance as you can see down here, is going to get adjusted. There is a limit. So at some point the more disk you choose, you're going to hit a limit into your performance, and same if I choose an SSD disk and change the size here, you'll see that also there's a limit but it's also adjusted if I scale as you are changing the IOPS as well as the sustained throughput limit. Now another important thing is obviously networking. So if I click on here, you want to choose the network interface. We already went into this a little bit in the previous module in terms of near choosing your primary internal IP, choosing if you want an external IP or not. So those are all of the different options that you can get there. Now what's really cool is this is all using the GCP Console, but down the road you might say well, I want to create these instances quickly and I want to use a command line. Well, this user interface gives you the command line options. So it's spelling out exactly all the different options you have chosen, how you would recreate that using GCloud. So this can help you get started using the command line and make you more comfortable using that command line. So let me just go ahead and create this instance. Once we create it we have these different columns that are listed here, there are more columns that we can choose from. For example, when you created it, what the machine type is, what network this is a part off, if you had labels or other things, so lots of different options you can list here. So for example, I can just hear when the machine was created, the type as well as the network it is a part off. That's how easy it is to configure the location, CPU, memory, storage, and network interface for a VM instance using the GCP Console. Let's get back to the slides to go over VM axis and lifecycle.

#### VM access and lifecycle

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530375

For accessing a VM, the creator of an instance has full root privileges on that instance. On a Linux instance, the creator has SSH capability and can use the Google Cloud console to grant SSH capability to other users. On a Windows instance, the creator can use the console to generate a username and password. After that, anyone who knows the username and password can connect to the instance using a Remote Desktop Protocol, or RDP, client. We listed the required firewall rules for both SSH and RDP here, but you don’t need to define these if you are using the default network we covered in the previous module. The lifecycle of a VM is represented by different statuses. We will cover this lifecycle on a high level, but we recommend returning to this diagram as a reference. When you define all the properties of an instance and click Create, the instance enters the provisioning state. Here the resources such as CPU, memory, and disks are being reserved for the instance, but the instance itself isn’t running yet. Next, the instance moves to the staging state where resources have been acquired and the instance is prepared for launch. Specifically, in this state, Compute Engine is adding IP addresses, booting up the system image, and booting up the system. After the instance starts running, it will go through pre-configured startup scripts and enable SSH or RDP access. Now, you can do several things while your instance is running. For example, you can live migrate your virtual machine to another host in the same zone instead of requiring your instance to be rebooted. This allows Google Cloud to perform maintenance that is integral to keeping the infrastructure protected and reliable, without interrupting any of your VMs. While your instance is running, you can also move your VM to a different zone, take a snapshot of the VM’s persistent disk, export the system image, or reconfigure metadata. We will explore some of these tasks in later labs. Some actions require you to stop your virtual machine; for example, if you want to upgrade your machine by adding more CPU. When the instance enters this state, it will go through pre-configured shutdown scripts and end in the terminated state. From this state, you can choose to either restart the instance, which would bring it back to its provisioning state, or delete it. You also have the option to reset a VM, which is similar to pressing the reset button on your computer. This action wipes the memory contents of the machine and resets the virtual machine to its initial state. The instance remains in the running state through the reset. The VM may also enter a repairing state. Repairing occurs when the VM encounters an internal error or the underlying machine is unavailable due to maintenance. During this time, the VM is unusable. You are not billed when a VM is in repair. VMs are not covered by the Service Level Agreement while they are in repair. If repair succeeds, the VM returns to one of the above states. Finally, when you suspend the VM, it enters in the suspending state, before being suspended. You can then resume the VM or delete it. There are different ways you can change a VM state from running. Some methods involve the Google Cloud console and the gcloud command, while others are performed from the OS, such as for reboot and shutdown. It’s important to know that if you are rebooting, stopping, or even deleting an instance, the shutdown process will take about 90 seconds. For a preemptible VM, if the instance does not stop after 30 seconds, Compute Engine sends an ACPI G3 Mechanical Off signal to the operating system. Remember that when writing shutdown scripts for preemptible VMs. As mentioned previously, Compute Engine can live migrate your virtual machine to another host due to a maintenance event to prevent your applications from experiencing disruptions. A VM’s availability policy determines how the instance behaves in such an event. The default maintenance behavior for instances is to live migrate, but you can change the behavior to terminate your instance during maintenance events instead. If your VM is terminated due to a crash or other maintenance event, your instance automatically restarts by default, but this can also be changed. These availability policies can be configured both during the instance creation and while an instance is running by configuring the Automatic restart and On host maintenance options, For more information on live migration, refer to the documentation. OS updates are a part of managing an infrastructure. Let’s see how we can manage the updates to a fleet of Windows VMs. When you provision a premium image, there is a cost associated with the image. This cost includes both the usage of the OS but also the patch management of the OS. Using Google Cloud, we can easily manage the patching of your operating systems. Managing patches effectively is a great way to keep your infrastructure up-to-date and reduce the risk of security vulnerabilities. But without the right tools, patching can be daunting and labor intensive. Use OS patch management to apply operating system patches across a set of Compute Engine VM instances. Long-running VMs require periodic system updates to protect against defects and vulnerabilities. The OS patch management service has two main components: Patch compliance reporting, which provides insights on the patch status of your VM instances across Windows and Linux distributions. Along with the insights, you can also view recommendations for your VM instances. And patch deployment, which automates the operating system and software patch update process. A patch deployment schedules patch jobs. A patch job runs across VM instances and applies patches. There are several tasks that can be performed with patch management. You can: Create patch approvals. You can select what patches to apply to your system from the full set of updates available for the specific operating system. Set up flexible scheduling. You can choose when to run patch updates (one-time and recurring schedules). Apply advanced patch configuration settings. You can customize your patches by adding configurations such as pre and post patching scripts. And you can manage these patch jobs or updates from a centralized location. When a VM is terminated, you do not pay for memory and CPU resources. However, you are charged for any attached disks and reserved IP addresses. In the terminated state, you can perform any of the actions listed here, such as changing the machine type, but you cannot change the image of a stopped VM. Also, not all of the actions listed here require you to stop a virtual machine. For example, VM availability policies can be changed while the VM is running, as discussed previously.

#### Lab Intro: Creating Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530376

Let's take some of the Compute Engine constants we just discussed and apply them in a lab. In this lab, you explore virtual machine instance options by creating several standard VMs and a custom VM. You also connect to those VMs using both SSH for Linux machines and RDP for Windows machines.

#### Creating Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530377

#### Lab Review: Creating Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530378

In this lab, you created several Virtual Machine instances of different types with different characteristics. Specifically, you created a small utility VM for administration purposes, a windows VM, and accustomed Linux VM. You also acts as both the Windows and Linux VM and deleted all your creative VMs. In general, start with a smaller VM when you're prototyping solutions to keep the cost down. When you're ready for production, trade up to larger VMs based on capacity. If you building and redundancy for availability, remember to allocate excess capacity to meet performance requirements. Finally, consider using custom VMs when your applications requirements fit between the features of the standard types. You can stay for a lab walk through but remember that GCP user interface can change, so your environment might look slightly different. So in the GCP console, I'm going to navigate to Compute engine and then VM instances, and in here we're going to click "Create". Now, we can define a name there's a small question mark here and if you hover over it can tell you a little bit more about some of the restrictions you have in regards to creating a name, choosing a name that is, and I'm just going to call this my utilityVM. We're going to go over some of the options that actually went over a little bit in the demo, but we obviously can choose region and zones. So let's change the zone to what the lab is instructing, which is 1-C, and then for the machine type we have a lot of different options to choose from. We can see that the cost changes if I scale up to a machine with four virtual CPUs versus a machine that's just maybe a micro, which is a shared core machine. So the cost can change quite drastically. So let's just leave all the remaining settings and click "Create", and once the machine is up and running, we're going to explore the different VM details that we have. So we're going to go into the VM Instances page, and look at things like the CPU platform, the availability policies and so on. So let me do that, let me click on "Utility VM" because it's now in a running state. I'm going to look for a CPU platform, you can see that right here and if I click "Edit", you'll see that I actually am unable to modify that. So that's because I can't do that while the instances is running. There are other things I could do, I could change the firewall rules, I can add network tags. So certain things are available to change while and instances is running. In some cases, you have to stop the instance to change some of the properties. In other cases, you cannot actually even change it unless you delete it. One of those is for example the network interfaces, if you had multiple network interfaces, you'd have to recreate your instance. The good thing is you could keep your boot disk and just reattach that boot disk later on. Now, I can also go and look at the availability policies, just scroll down to some what the enhanced maintenance is. By default, it's set to migrate the VM instance, and that's recommended but you could set this to terminate the instance. It's also going to automatically restart that instance so you could configure that as well. All right, so this is just a little bit exploring the different options, I'm going to go click "Cancel". What we're going to now is explore some of the VM logs. So I'm looking at the detail page here. We want to get a little bit more information about the monitoring options that are available. We can click "Monitoring" here, and we'll get more information about the CPU. This instances barely runs, we don't have much data yet. We get information about the network bytes and packets, disk I/O. We can also, if we go back to details, look at stackdriver logging. So this is now a different user interface and here we now have individual logs that we can explore. We can view options here, we could expand all of these and dig into all of these different logs that are in here and even within there, expand each of the logs to get more information. So this uses stackdriver logging, we'll cover this feature a little bit but more in a later course in the course series if you're interested to learn more about both the logging piece that we just looked at as well as the monitoring. So let's go to Test 2, we're now going to create a windows virtual machine. So I'm going to go back through the navigation menu Compute engine to VM instances, and I'm not going to create a another instance. So I'm going to define a name, and this is just going to call it Windows VM, and we're going to choose a different region and zone this time. Why don't we put this into Europe-West2, and specifically to zone 2A. Let's pick a larger machine. Let's pick one that has two virtual CPUs and 7.5 gigabytes of memory. We can even go ahead now and changed the boot disk because by default, this would be a Linux machine, so if we want to change this because we want to create a windows machine. Specifically, the lab is instructing me to look for the Windows Server 2016 Datacenter Core image. It's first scroll down, I can see that image right here, can change the boot disk. Maybe I want some higher IOPS, I can choose an SSD, and I can even make this larger and click "Select". All of that again is going to affect obviously the cost. I have the cost of the machine, I have the cost of the disc, but the new thing I have now also is the image, I've chosen zupimages which means there is a cost associated with using that image, but it's built all together for you. So you can see that cost broken up right here. Now, the other thing we're going to do is we're going to allow a specific traffic, HTTP and HTTPS traffic. This just creates a network tag for us and then creates filer roles on the network tag so that we can enable traffic on those ports for the TCP protocol. So let's hit "Create" and create this instance. One thing we'll notice when the instance comes up is that under the connect column [inaudible] now seeing an SSH button which is we would have for a Linux machine. We should now see a RDP, which is for the Remote Desktop Protocol. So that's how you would access a Windows machine. Now, the important thing is there you obviously want to configure your username and password so that only authorized users access that machine. So here you can see the RDP button now. What we're going to do now is we're going to click onto the machine and set the Windows password. You can actually also do this by clicking "Down here" set windows password there as well. So actually, let's just do it that way. So you have a username here. It's taking the username that I have for my lab account. So this is the username right now. So I can set that and then it's going to provide me with a password. So there we go. So I can now copy that password and if I use an RDP connection, I can then get into that. This is a little bit outside of the scope for this lab, but if you want to and have an RDP client, you can actually install one through Chrome, through an extension. You could access that instance that way and then configure it and do anything else you wanted to in this Windows Virtual Machine. So let me go ahead and close that, and I'm going to move on to a Task 3, now which is to create a custom Virtual Machine. So I'm going to go back to Create Instance, and to find a name, let's just call it my custom-VM. I'll follow the lab instructions here for setting the region and zone which is US-West1-B. Now, rather than choosing a specific machine type, I can go in here and just select Custom as the machine type and then define the exact numbers of cores memory. So let's say, my specification I want six virtual CPU, and you can see how the scales by the way, there are only certain options. You can choose it goes all the way to 96. So let me choose six here. It's going to scale that memory automatically for us. It gives us a range now depending on that CPU, there's an option to extend the memory so you could get more than 39 and see all the way to 624. This is a separate option, we'll talk more about this in the slides. So let me choose 32, and rather than scrolling here I could also just type the value in and that's also going to adjust the cost now. Sometimes, it's important to note that your custom machine may be between two machine types are actually already provided. A custom machine is generally going to be slightly more expensive. So if you have a standard machine that's very close to the custom machine, it's definitely something you would want to consider. Once the machine runs more than 24 hours, you'll get right sides recommendations. So It'll tell you if the machine is too small or too large and make recommendations based on that. So let's go ahead and create that. Once it's up and running, we're going to SSH to the machine. We're going to run some commands on that machine, and that's actually going to wrap up the Lab for us. Now, with any new project, you get this column here on the right-hand side to help you get started because we're using Qwiklabs generated projects, they're always going to be new products. So you'll see this throughout the training. You can certainly leveraged this if you want but I'm going to collapse that. So VM is up and running, let me SSH to it. Then we're going to run the free command to see information about any unused and used memory and swap space. So let me type free. So we can see that here and that lines up with the memory selections that we made in the machine. I can also see I get more information or details about the RAM installed. So here we get more information about that as well. I can verify the number of processors. So that should have been six, and yep, and prox is sixth, great. We can see details about the CPU itself. So here we get information about the architecture, the byte order, which model exactly, so you can get all this information about any VM that you create. You can also get more information about this in the documentation depending on which region and zone you choose. You'll have different architectures and different models available to choose from. So that's all we wanted to show you here with this lab. We went ahead and created that Virtual Machine, the Utility VM, we created a Windows VM, and then we created a custom Virtual Machine and verified that whatever custom settings we applied were actually used to create the machine by running commands within that machine.

#### Compute options

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530379

Now that you have completed the lab, let’s dive deeper into the compute options that are available to you in Google Cloud, by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the Google Cloud console as you did in the previous lab, the Cloud Shell command line, or the RESTful API. If you’d like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the Google Cloud console and then ask Compute Engine for the equivalent REST request or command line, as shown in the demo earlier. This way you avoid any typos and get dropdown lists of all the available CPU and memory options. Speaking of CPU and memory options, let’s look at the different machine types that are currently available. When you create a VM, you select a machine type from a machine family that determines the resources available to that VM. There are several machine families you can choose from and each machine family is further organized into machine series and predefined machine types within each series. A machine family is a curated set of processor and hardware configurations optimized for specific workloads. When you create a VM instance, you choose a predefined or custom machine type from your preferred machine family. Alternatively, you can create custom machine types. These let you specify the number of vCPUs and the amount of memory for your instance. There are four Compute Engine machine families. General-purpose, Compute-optimized, Memory-optimized, and Accelerator-optimized. Let's look at each in more detail. The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads. The E2 machine series is suited for day-to-day computing at a lower cost, especially where there are no application dependencies on a specific CPU architecture. E2 VMs provide a variety of compute resources for the lowest price on Compute Engine, especially when paired with committed-use discounts. You simply pick the amount of vCPU and memory you want, and Google provisions it for you. The Standard E2 VMs have between 2 to 32 vCPUs with a ratio of 0.5 GB to 8 GB of memory per vCPU. They are a great choice for web servers, small to medium databases, development and test environments, and many applications that don't have strict performance requirements. They offer a compatible performance baseline with the current N1 VMs for those of you who have been using them. The E2 machine series also contains shared-core machine types that use context-switching to share a physical core between vCPUs for multitasking. Different shared-core machine types sustain different amounts of time on a physical core. In general, shared-core machine types can be more cost-effective for running small, non-resource intensive applications than standard, high-memory, or high-CPU machine types. Shared-core E2 machine types have 0.25 to 1 vCPUs with 0.5 GB to 8 GB of memory. The N2 and N2D are the next generation following the N1 VMs, offering a significant performance jump. N2 and N2D are the most flexible VM types and provide a balance between price and performance across a wide range of VM shapes, including enterprise applications, medium-to-large databases, and many web and app-serving workloads. Committed use and sustained use discounts are supported. N2 VMs support the latest second generation scalable processor from Intel with up to 128 vCPUs and 0.5 to 8 GB of memory per vCPU. Cascade Lake is the default processor for machine types up to 80 vCPUs. For larger machine types, Ice Lake is the default processor for specific regions and zones. N2D are AMD-based general purpose VMs. They leverage the latest EPYC Milan and EPYC Rome processors, and provide up to 224 vCPUs per node. Tau T2A and Tau T2D VMs are optimized for cost-effective performance of demanding scale-out workloads. T2D VMs are built on the latest 3rd Gen AMD EPYC processors and offer full x86 compatibility. They are suited to scale-out workloads including web servers, containerized microservices, media transcoding, and large-scale Java applications. T2D VMs come in predefined VM shapes, with up to 60 vCPUs per VM and 4 GB of memory per vCPU. Tau T2A machine series is the first machine series in Google Cloud to run on Arm processors. The Tau T2A machine series runs on a 64 core Ampere Altra processor with an Arm instruction set and an all-core frequency of 3 GHz. If you have containerized workloads, Tau VMs are supported by Google Kubernetes Engine to help optimize price-performance. You can add T2D nodes to your GKE clusters by specifying the T2D machine type in your GKE node pools. The compute-optimized machine family has the highest performance per core on Compute Engine and is optimized for compute-intensive workloads. C2 VMs are the best fit VM type for compute-intensive workloads, including AAA gaming, electronic design automation, and high-performance computing across simulations, genomic analysis, or media transcoding. They might also be applications that have very expensive per core licensing and thus would benefit from higher per core performance. Powered by high-frequency Intel- scalable processors, Cascade Lake, C2 machine types offer up to 3.8 Ghz sustained all-core turbo and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. The C2 series comes in different machine types ranging from 4 to 60 vCPUs, and offers up to 240 GB of memory. You can also attach up to 3 TB of local storage to these VMs for applications that require higher storage performance. The C2D machine series provides the largest VM sizes and are best-suited for high-performance computing. The C2D series also has the largest available last-level cache per core. The C2D machine series come in different machine types ranging from 2 to 112 vCPUs, and offer 4 GB of memory per vCPU . You can also attach up to 3 TB of local storage to these machine types for applications that require higher storage performance. C2D VMs are available on the third generation AMD EPYC Milan platform. The H3 series offer 88 cores and 352 GB of DDR5 memory and are available on the Intel Sapphire Rapids CPU platform and Google's custom Intel Infrastructure Processing Unit The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose machine family. The M1 machine series has up to 4 TB of memory, while the M2 machine series has up to 12 TB of memory. These machine series are well-suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads. Both the M1 and M2 machine series offer the lowest cost per GB of memory on Compute Engine, making them a great choice for workloads that utilize higher memory configurations with low compute resources requirements. Additionally, they offer up to 30% sustained use discounts and are also eligible for committed use discounts, bringing additional savings of greater than 60% for three-year commitments. M3 VMs offer up to 128 vCPUs, with up to 30.5 GB of memory per vCPU, and are available on the Intel Ice Lake CPU platform. These machines are well-suited for memory intensive applications, such as genomic modeling and electronic design automation and high performance computing. The accelerator-optimized machine family is ideal for massively parallelized Compute Unified Device Architecture compute workloads, such as machine learning and high-performance computing. This family is the optimal choice for workloads that require GPUs. The A2 series has 12 to 96 vCPUs, and up to 1,360 GB of memory. Each A2 machine type has a fixed number (up to 16) of NVIDIA’s Ampere A100 GPUs attached. An A100 GPU provides 40 GB of GPU memory - ideal for large language models, databases, and high-performance computing. G2 VMs offer 4 to 96 vCPUs, up to 432 GB of memory, and are available on the Intel Cascade Lake CPU platform. These machines are well-suited for CUDA-enabled ML training and inference, video transcoding, and remote visualization workstations. Additional information, including the latest specs for currently available VM machine types, can be found in the machine types documentation. If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios: When you have workloads that are not a good fit for the predefined machine types that are available to you. Or when you have workloads that require more processing power or more memory, but don't need all of the upgrades that are provided by the next larger predefined machine type. It costs slightly more to use a custom machine type than an equivalent predefined machine type, and there are still some limitations in the amount of memory and vCPUs you can select: Only machine types with 1 vCPU or an even number of vCPUs can be created. Memory must be between 1 GB and 8 GB per vCPU. The total memory of the instance must be a multiple of 256 MB. Selected custom machine types can allow up to 8 GB of memory per vCPU. However, this might not be enough memory for your workload. At an additional cost, you can get more memory per vCPU beyond the 8 GB limit. This is referred to as extended memory, and you can learn more about this in the link provided in the module PDF located in Course Resources.

#### Compute pricing

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530380

Now that you have completed the lab, let’s dive deeper into the compute options that are available to you in Google Cloud, by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the Cloud Console as you did in the previous lab, the Cloud Shell command line, or the RESTful API. If you’d like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the Cloud Console and then ask Compute Engine for the equivalent REST request or command line, as shown in the demo earlier. This way you avoid any typos and get dropdown lists of all the available CPU and memory options. Speaking of CPU and memory options, let’s look at the different machine types that are currently available. When you create a VM, you select a machine type from a machine family that determines the resources available to that VM. There are several machine families you can choose from and each machine family is further organized into machine series and predefined machine types within each series. A machine family is a curated set of processor and hardware configurations optimized for specific workloads. When you create a VM instance, you choose a predefined or custom machine type from your preferred machine family. Alternatively, you can create custom machine types. These let you specify the number of vCPUs and the amount of memory for your instance. There are four Compute Engine machine families. General-purpose Compute-optimized Memory-optimized, and Accelerator-optimized The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads. The E2 machine series is suited for day-to-day computing at a lower cost, especially where there are also no application dependencies on a specific CPU architecture. E2 VMs provide a variety of compute resources for the lowest price on Compute Engine, especially when paired with committed-use discounts. You simply pick the amount of vCPU and memory that you want, and Google provisions it for you. Standard E2 VMs have between 2 to 32 vCPUs with a ratio of 0.5 GB to 8 GB of memory per vCPU. They are a great choice for web servers, small to medium databases, development and test environments, and many applications that don't have strict performance requirements. They offer a compatible performance baseline with the current N1 VMs for those of you who have been using them. The E2 machine series also contains shared-core machine types that use context-switching to share a physical core between vCPUs for multitasking. Different shared-core machine types sustain different amounts of time on a physical core. In general, shared-core machine types can be more cost-effective for running small, non-resource intensive applications than standard, high-memory, or high-CPU machine types. Shared-core E2 machine types have 0.25 to 1 vCPUs with 0.5 GB to 8 GB of memory. N2 and N2D are the next generation following N1 VMs, offering a significant performance jump. N2 and N2D are the most flexible VM types and provide a balance between price and performance across a wide range of VM shapes, including enterprise applications, medium-to-large databases, and many web and app-serving workloads. Committed use and sustained use discounts are supported. N2 VMs support the latest second generation scalable processor from Intel with up to 128 vCPUs and 0.5 to 8 GB of memory per vCPU. Cascade Lake is the default processor for machine types up to 80 vCPUs. For larger machine types Ice Lake is the default processor for specific regions and zones. N2D are AMD-based general purpose VMs. They leverage the latest EPYC Milan and EPYC Rome processors, and provide up to 224 vCPUs per node. Tau T2D VMs are optimized for cost-effective performance of demanding scale-out workloads. T2D VMs are built on the latest 3rd Gen AMD EPYCTM processors and offer full x86 compatibility. They are suited to scale-out workloads including web servers, containerized microservices, media transcoding, and large-scale Java applications. T2D VMs come in predefined VM shapes, with up to 60 vCPUs per VM and 4 GB of memory per vCPU. If you have containerized workloads, Tau VMs are supported by Google Kubernetes Engine to help optimize price-performance. You can add T2D nodes to your GKE clusters by specifying the T2D machine type in your GKE node pools. The compute-optimized machine family has the highest performance per core on Compute Engine and is optimized for compute-intensive workloads. C2 VMs are the best fit VM type for compute-intensive workloads, including AAA gaming, electronic design automation, and high-performance computing across simulations, genomic analysis, or media transcoding. They might also be applications that have very expensive per core licensing and thus would benefit from higher per core performance. Powered by high-frequency Intel-scalable processors, Cascade Lake, C2 machine types offer up to 3.8 Ghz sustained all-core turbo and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. The C2 series comes in different machine types ranging from 4 to 60 vCPUs, and offers up to 240 GB of memory. You can also attach up to 3 TB of local storage to these VMs for applications that require higher storage performance. The C2D machine series provides the largest VM sizes and are best-suited for high-performance computing (HPC). The C2D series also has the largest available last-level cache (LLC) per core. The C2D machine series come in different machine types ranging from 2 to 112 vCPUs, and offer 4 GB of memory per vCPU. You can also attach up to 3TB of local storage to these machine types for applications that require higher storage performance. C2D VMs are available on the third generation AMD EPYC Milan platform. The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose machine family. The M1 machine series has up to 4 TB of memory, while the M2 machine series has up to 12 TB of memory. These machine series are well-suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads. Both the M1 and M2 machine series offer the lowest cost per GB of memory on Compute Engine, making them a great choice for workloads that utilize higher memory configurations with low compute resources requirements. Additionally, they offer up to 30% sustained use discounts and are also eligible for committed use discounts, bringing additional savings of greater than 60% for three-year commitments. The accelerator-optimized machine family is ideal for massively parallelized Compute Unified Device Architecture (CUDA) compute workloads, such as machine learning (ML) and high-performance computing (HPC). This family is the optimal choice for workloads that require GPUs. The A2 series has 12 to 96 vCPUs, and up to 1360 GB of memory. Each A2 machine type has a fixed number (up to 16) of NVIDIA’s Ampere A100 GPUs attached. An A100 GPU provides 40 GB of GPU memory—ideal for large language models, databases, and HPC. Additional information, including the latest specs for currently available VM machine types, can be found in the machine types documentation. If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios: * When you have workloads that are not a good fit for the predefined machine types that are available to you. * Or when you have workloads that require more processing power or more memory, but don't need all of the upgrades that are provided by the next larger predefined machine type. It costs slightly more to use a custom machine type than an equivalent predefined machine type, and there are still some limitations in the amount of memory and vCPUs you can select: * Only machine types with 1 vCPU or an even number of vCPUs can be created. * Memory must be between 0.9 GB and 6.5 GB per vCPU (by default). * The total memory of the instance must be a multiple of 256 MB. By default, a custom machine can have up to 6.5 GB of memory per vCPU. However, this might not be enough memory for your workload. At an additional cost, you can get more memory per vCPU beyond the 6.5 GB limit. This is referred to as extended memory, and you can learn more about this in the link provided in the module PDF located in Course Resources. The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. This map shows the current and planned Google Cloud regions and number of zones. For up-to-date information on the available regions and zones, see the documentation linked in the module PDF located in Course Resources. Each zone supports a combination of Ivy Bridge, Sandy Bridge, Haswell, Broadwell, and Skylake platforms. When you create an instance in the zone, your instance will use the default processor supported in that zone. For example, if you create an instance in the us-central1-a zone, your instance will use a Sandy Bridge processor.

#### Special compute configurations

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530381

As mentioned earlier, a preemptible VM is an instance that you can create and run at a much lower cost than normal instances. See whether you can make your application function completely on preemptible VMs, because a 60 to 91% discount  is a significant investment in your application. Now, just to reiterate, these VMs might be preempted at any time, and there is no charge if that happens within the first minute. Also, preemptible VMs are only going to live for up to 24 hours, and you only get a 30-second notification before the machine is preempted. It's also worth noting that there are no live migrations nor automatic restarts in preemptible VMs, but something that we will highlight is that you can actually create monitoring and load balancers that can start up new preemptible VMs in case of a failure. In other words, there are external ways to keep restarting preemptible VMs if you need to. One major use case for preemptible VMs is running a batch processing job. If some of those instances terminate during processing, the job slows but does not completely stop. Therefore, preemptible instances complete your batch processing tasks without placing additional workload on your existing instances, and without requiring you to pay full price for additional normal instances. Spot VMs are the latest version of preemptible VMs. Spot VMs are virtual machine instances with the spot provisioning model. New and existing preemptible VMs continue to be supported, and preemptible VMs use the same pricing model as Spot VMs. However, Spot VMs provide new features that preemptible VMs do not support. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. Like preemptible VMs, Compute Engine might preempt Spot VMs if it needs to reclaim those resources for other tasks. The probability that Compute Engine stops Spot VMs for a system event is generally low, but might vary from day-to-day and from zone-to-zone depending on current conditions. Spot VMs are finite Compute Engine resources, so they might not always be available. Like preemptible VMs, it's worth noting that Spot VMs cannot live-migrate to become standard VMs while they are running or be set to automatically restart when there is a maintenance event. There are many best practices which can help you get  the most of using Spot VMs. For example, resources for Spot VMs come out of excess and backup Google Cloud capacity. Capacity for Spot VMs is often easier to get for smaller machine types, meaning machine types with less resources like vCPU and memory. If you have workloads that require physical isolation from other workloads or virtual machines in order to meet compliance requirements, you want to consider sole-tenant nodes. A sole-tenant node is a physical Compute Engine server that is dedicated to hosting VM instances only for your specific project. Use sole-tenant nodes to keep your instances physically separated from instances in other projects, or to group your instances together on the same host hardware. For example, if you have a payment processing workload that needs to be isolated to meet compliance requirements. The diagram on the left shows a normal host with multiple VM instances from multiple customers. A sole-tenant node as shown on the right also has multiple VM instances, but they all belong to the same project. You can also fill the node with multiple smaller VM instances of varying sizes, including custom machine types and instances with extended memory. Also, if you have existing operating system licenses, you can bring them to Compute Engine using sole-tenant nodes while minimizing the physical core usage with the in-place restart feature. For more information on sole tenancy and allowed node types, please refer to the sole-tenancy overview in the documentation. Another compute option is to create shielded VMs. Shielded VMs offer verifiable integrity to your VM instances, so you can be confident that your instances haven't been compromised by boot or kernel-level malware or rootkits. Shielded VM's verifiable integrity is achieved through the use of Secure Boot, virtual trusted platform module or vTPM-enabled Measured Boot, and integrity monitoring. Shielded VMS is the first offering in the Shielded Cloud Initiative. The Shielded Cloud Initiative is meant to provide an even more secure foundation for all of Google Cloud by providing verifiable integrity and offering features, like vTPM shielding or sealing, that help prevent data exfiltration. To use these shielded VM features, you need to select a shielded image. We'll learn about images in the next section. Confidential VMs are a breakthrough technology that allows you to encrypt data in use - while it's been processed. Google Cloud's approach to encrypt data in use is simple, easy-to-use deployment without making any code changes to their applications or having to compromise on performance. You can collaborate with anyone, all while preserving the confidentiality of your data. A Confidential Virtual Machine is a type of N2D Compute Engine VM instance running on hosts based on the second generation of AMD EPYC processors, code-named "Rome". Using AMD Secure Encrypted Virtualization, Confidential VM features built-in optimization of both performance and security for enterprise-class high memory workloads, as well as inline memory encryption that doesn't introduce significant performance penalty to those workloads. The AMD Rome processor family is specifically optimized for compute-heavy workloads, with high memory capacity, high throughput, and support for parallel workloads. In addition, AMD SEV provides for Confidential Computing support. With the confidential execution environments provided by Confidential VM and AMD SEV, Google Cloud keeps customers' sensitive code and other data encrypted in memory during processing. Google does not have access to the encryption keys. You can select the Confidential VM service when creating a new VM using the Google Cloud Console, the Compute Engine API, or the gcloud command-line tool.

#### Images

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530382

Next, let’s focus on images. When creating a virtual machine, you can choose the boot disk image. This image includes the boot loader, the operating system, the file system structure, any pre-configured software, and any other customizations. You can select either a public or custom image. As you saw in the previous lab, you can choose from both Linux and Windows images. Some of these images are premium images, as indicated in parentheses with a p. These images will have per-second charges after a 1-minute minimum, with the exception of SQL Server images, which are charged per minute after a 10-minute minimum. Premium image prices vary with the machine type. However, these prices are global and do not vary by region or zone. You can also use custom images. For example, you can create and use a custom image by pre-installing software that's been authorized for your particular organization. You also have the option of importing images from your own premises or workstation, or from another cloud provider. This is a no-cost service that is as simple as installing an agent, and I highly recommend that you look at it. You can also share custom images with anybody in your project or among other projects, too. A machine image is a Compute Engine resource that stores all the configuration, metadata, permissions, and data from one or more disks required to create a virtual machine (VM) instance. You can use a machine image in many system maintenance scenarios, such as creation, backup and recovery, and instance cloning. Machine images are the most ideal resources for disk backups as well as instance cloning and replication.

#### Disk options

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530383

At this point you've chosen an operating system, but that operating system is going to be included as part of some kind of disk. So let’s look at the disk options. Every single VM comes with a single root persistent disk, because you're choosing a base image to have that loaded on. This image is bootable in that you can attach it to a VM and boot from it, and it is durable in that it can survive if the VM terminates. To have a boot disk survive a VM deletion, you need to disable the “Delete boot disk when instance is deleted” option in the instance’s properties. As discussed earlier, there are different types of disks. Let’s explore these in more detail. The first disk that we create is what we call a persistent disk. That means it's going to be attached to the VM through the network interface. Even though it's persistent, it's not physically attached to the machine. This separation of disk and compute allows the disk to survive if the VM terminates. You can also perform snapshots of these disks, which are incremental backups that we’ll discuss later. The choice between HDD and SSD disks comes down to cost and performance. To learn more about disk performance and how it scales with disk size, refer to the link in the course resources for this module. Another cool feature of persistent disks is that you can dynamically resize them, even while they are running and attached to a VM. You can also attach a disk in read-only mode to multiple VMs. This allows you to share static data between multiple instances, which is cheaper than replicating your data to unique disks for individual instances. Zonal persistent disks offer efficient, reliable block storage. Regional persistent disks provide active-active disk replication across two zones in the same region. Regional persistent disks deliver durable storage that is synchronously replicated across zones and are a great option for high-performance databases and enterprise applications that also require high availability. When you configure a zonal or regional persistent disk, you can select one of the following disk types: Standard persistent disks are backed by standard hard disk drives and are suitable for large data processing workloads that primarily use sequential I/Os. Performance SSD persistent disks are backed by solid-state drives and are suitable for enterprise applications and high-performance databases that require lower latency and more IOPS than standard persistent disks provide. Balanced persistent disks are also backed by solid-state drives. They are an alternative to SSD persistent disks that balance performance and cost. These disks have the same maximum IOPS as SSD persistent disks and lower IOPS per gigabyte. For most VM shapes, except very large ones, this disk type offers performance levels suitable for most general-purpose applications at a price point between that of standard and performance persistent disks. Extreme persistent disks are zonal persistent disks also backed by solid-state drives. Extreme persistent disks are designed for high-end database workloads, providing consistently high performance for both random access workloads and bulk throughput. Unlike other disk types, you can provision your desired IOPS. By default, Compute Engine encrypts all data at rest. Google Cloud handles and manages this encryption for you without any additional actions on your part. However, if you wanted to control and manage this encryption yourself, you can either use Cloud Key Management Service to create and manage key encryption keys (which is known as customer-managed encryption keys) or create and manage your own key encryption keys (known as customer-supplied encryption keys). Now, local SSDs are different from persistent disks in that they are physically attached to the virtual machine. Therefore, these disks are ephemeral but provide very high IOPS. For up-to-date numbers, I recommend referring to the documentation in the course resources for this module. Currently, local SSDs are 375 GB in size and you can attach up to to 24 local SSD partitions for a total of 9 TB per instance. Data on these disks will survive a reset but not a VM stop or terminate, because these disks cannot be reattached to a different VM. You also have the option of using a RAM disk. You can simply use tmpfs if you want to store data in memory. This will be the fastest type of performance available if you need small data structures. I recommend a high-memory virtual machine if you need to take advantage of such features, along with a persistent disk to back up the RAM disk data. In summary, you have several different disk options. Persistent disks can be rebooted and snapshotted, but local SSDs and RAM disks are ephemeral. I recommend choosing a persistent HDD disk when you don't need performance but just need capacity. If you have high performance needs, start looking at the SSD options. The persistent disks offer data redundancy because the data on each persistent disk is distributed across several physical disks. Local SSDs provide even higher performance, but without the data redundancy. Finally, RAM disks are very volatile but they provide the highest performance. Now, just as there is a limit on how many Local SSDs you can attach to a VM, there is also a limit on how many persistent disks you can attach to a VM. As illustrated in this table, this limit depends on the machine type. For the Shared-core machine type, you can attach up to 16 disks. For the Standard, High Memory, High-CPU, Memory-optimized, and Compute-optimized machine types, you can attach up to 128 disks. So you can create massive amounts of capacity for a single host. Now remember that little nuance when I told you about how throughput is limited by the number of cores that you have? That throughput also shares the same bandwidth with Disk I/O. So if you plan on having a large amount of Disk I/O throughput, it will also compete with any network egress or ingress throughput. So remember that, especially if you will be increasing the number of drives attached to a virtual machine. There are many differences between a physical hard disk in a computer and a persistent disk, which is essentially a virtual networked device. First of all, if you remember with normal computer hardware disks, you have to partition them. Essentially, you have a drive and you’re carving up a section for the operating system to get its own capacity. If you want to grow it, you have to repartition, and if you want to make changes you might even have to reformat. If you want redundancy, you might create a redundant disk array, and if you want encryption, you need to encrypt files before writing them to the disk. With cloud persistent disks, things are very different because all that management is handled for you on the backend. You can simply grow disks and resize the file system because disks are virtual networked devices. Redundancy and snapshot services are built in and disks are automatically encrypted. You can even use your own keys, and that will ensure that no party can get to the data except you.

#### Common Compute Engine actions

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530384

Now that we have covered all the different compute, image, and disk options, let’s look at some common actions that you can perform with Compute Engine. Every VM instance stores its metadata on a metadata server. The metadata server is particularly useful in combination with startup and shutdown scripts, because you can use the metadata server to programmatically get unique information about an instance, without additional authorization. For example, you can write a startup script that gets the metadata key/value pair for an instance's external IP address and use that IP address in your script to set up a database. Because the default metadata keys are the same on every instance, you can reuse your script without having to update it for each instance. This helps you create less brittle code for your application. Storing and retrieving instance metadata is a very common Compute Engine action. We recommend storing the startup and shutdown scripts in Cloud Storage, as you will explore in the upcoming lab of this module. Another common action is to move an instance to a new zone or region. For example, you might do so for geographical reasons or because a zone is being deprecated. To move your VM, you must shut down the VM, move it to the destination zone or region, and then restart it. After you move your VM, update any  references that you have to the original resource, such as any target VMs or target pools that point to the earlier VM. During the move, some server-generated properties of your VM and disks change. At a high level, to move a VM across zones or regions, you can do the following: Create a machine image of your source VM. And create a VM from the machine image in a different zone or region. In this example, a VM named myinstance is to be moved from europe-west1-c to us-west1-b. The VM has two persistent disks named mybootdisk and mydatadisk. For more information in moving a VM instance between zones or regions, refer to the documentation. Snapshots have many use cases. For example, they can be used to back up critical data into a durable storage solution to meet application, availability, and recovery requirements. These snapshots are stored in Cloud Storage, which is covered later. Snapshots can also be used to migrate data between zones. We just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard HDD persistent disk to a SSD persistent disk. Now that we’ve covered some of the snapshot use cases, let’s explore the concept of a disk snapshot. First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images, which are used primarily to create instances or configure instance templates, in that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. You can create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. As we saw with the previous examples, snapshots can be restored to a new persistent disk, allowing for a move to a new zone. To create a persistent disk snapshot, refer to the module PDF under Course Resources. Another common Compute Engine action is to resize your persistent disk. The added benefit of increasing storage capacity is to improve I/O performance. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disks in size, you can never shrink them, so keep this in mind.

#### Lab Intro: Working with Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530385

Let's get started with the second lab of this module. In this lab, you will be setting up an application server. Now, this example happens to be a gaming application, but it applies to many other use cases. You will configure the VM and also add capacity for a production gaming system, and you will build the infrastructure that you need for production activities. These include backups and graceful shutdown and restart services.

#### Working with Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/labs/530386

#### Lab Review: Working with Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530387

In this lab, you created a customized Virtual Machine instance by installing base software which was a headless Java runtime environment and application software specifically a Minecraft Game Server. You customize the VM by preparing and attaching a high-speed SSD and you've reserved a static external IP address so that the address will remain consistent. Using that IP address, you then verify the availability of the gaming server online. Next, you set up a backup system to backup the service data to a Cloud Storage bucket, and then you tested that backup system. You then automated backups using cron. Finally, you set up maintenance scripts using metadata for graceful startup and shutdown off the server. Many of these techniques including these script automation can be adapted to administration of production servers in any application. You can stay for a lab walk-through. But remember that GCPs user interface can change. So your environment might look slightly different. So here I am in the VM Instances page. Let's go ahead and create our instance. We're going to use the same properties that are provided to us, properties and values in the lab. So I'm going to call this the mc server for our Minecraft Server. We're going to place this in the US Central one, a zone. We're going to modify the access scopes for this. So I'm going to set axis for each API. I'm going to modify for storage that besides just read only, I want read write. This is going to allow the VM instance to write to the Cloud Storage bucket that we're going to create later on. Now we're also going to modify the disk of this instance. So let's expand the option down here, and under disks we're going to add a new disk. We're going to call this the Minecraft disk, and we're going to make that an SSD persistent disk. It's going to be blank so no source. Fifty gigabytes is more than enough for what we're trying to do. I'm going to leave the encryption as Google managed key. So let me click done and this is going to create that disk and automatic attach it to the VM. Now under networking, we're also going to add a network tag. This is going to then allow us to locate specific firewall rules, we call that Minecraft Server. On the network interface I'm going to click on the pencil icon here to edit. We are leaving the internal IP as is but for the external IP, we're actually going to create an IP address which means that we are reserving a static IP address. This is going to make sure that these IP address is not ephemeral and doesn't change. So I'll just give it a name and I click reserve and then we're going to click done once that is reserved and from there we're going to create this instance. It's done and then create. Now once the instance is up and running we're going to have to prepare the data disks. So we're not going to create a directory, format and mount the disk. I don't need this tab over here so I can close that. We're going to wait for the instance there it is. So the SSH to the instance. I'm going to start by creating a director that serves as the amount point for the data off the disk. For that I'm just going to use the command that's provided in the lab and then we're going to format the disk itself. So we're just going to wait for that SSH connection to be established. This is allowed because the default network has a default firewall rule for SSH. So let me go ahead and run that, and then we're going to format the disk. Great. Now we're going to mount it, and this is not going to display any outputs, so don't be surprised about that. There's a checkpoint in the lab. So you can check your progress, worked for me. So I'm going to move on to task three and now install and run the application and the micro server itself runs on top of the Java virtual machine. So we do require the Java Runtime Environment, or JRE, to run. But we don't need the user interface. So we're just going to install actually headless version and that's going to reduce a lot of the resource usage on that machine which will ensure that the Minecraft Server has enough room to expand its own resource usage if needed. So let me go ahead and start by updating the repository. Then I'm going to install that headless JRE, and after that I'm going to navigate to the directory where we mounted that persistent disk. Into that we're then going to download the Minecraft jar file. So we navigate into that under command. You can see it's downloading and the lab manual also provides information on the download page itself. So you can read more about where this comes from. There are also lots of instructions actually in there on how to set this up on a Linux machine. So if you wanted to customize this, I definitely recommend referring to that link. So let's go ahead and initialize the Minecraft Server. Run that command and it's telling us that this is not going to run unless we agree to the end-user licensing agreement. So we need to do that now. Let me just check my progress. Make sure that the JRE installation and Minecraft server installation worked out and I got a green check in my lab. So let's look at the files that were created to identify where this license agreement is, and there it is. We can see it right there. So let me use nano to edit that now. All we really have to do is we have to change this last line, instead of saying false, we just have to agree to it by setting this to true. So let me change that and then we're going to click Control O to write that to that filename hit Enter and then Control X to come back out. So we're not going to try to restart the Minecraft Server yet. We're going to use a different technique in a second. What we're going to do next is we're going to create a virtual terminal screen to start that server, and to do that we're going to install screen. So let's grab that command from the lab instructions. It seems like it was already actually installed. Then we're going to go ahead and start that now using the screen command. So let's run that and this might take a while now but it's going to establish the whole environment for us. So we can see here it's preparing the level world. It's loading some recipe. So these are all now very specific commands in regards to the gaming application that we're installing here and we're going to wait for this to complete before detaching from this and moving on. So we can see that the spawn area here has been completed. We could not detached from this, but one thing I want to point out that we're going to have to do next is when this whole thing started it told us which port it is going to do that for. So the port is right here and we're going to have to create a firewall rule in a second to actually allow client traffic to that port. So we can now detach from this. So we're going to just use Control A and Control D. To get out of here. There is a command if you wanted to reattach to the terminal, we're not going to do that. So I'm just going to exit out of here and we're now going to allow Cloud traffic. So for that, we need to create a firewall rule and we're going to use the network tag that we created which we can display by going to Columns and then Network tags. We can see that Minecraft Server was the network tag. So let's do that. I'm going to navigate to VPC network and specifically Firewall rules. I'm going to give a new firewall rule the name of Minecraft rule. It's going to be on the default network. Could this be the only network we have right now? For specified target tags, we're now going to define Minecraft Server so only apply to the instances that have that tag. So let me define the IP ranges as from anywhere. Now specifically for the protocol that's TCP, and then that port was 25565. Then I'm going to go ahead and click Create. Once it's up and running we're going to verify availability of the server. So I can already start navigating back and I'll monitor the process up here in the notification pane. I'm going back to Compute Engine and we have the external IP address here. We're now going to use a couple of different ways to verify that this is running. Note that we can't click on it because we didn't enable HTTP, that would have been TCP for port 80. In the lab instructions, we have listed a website and we also currently have a Chrome extension there, have that Chrome extension actually right up here. So let's try that. I'm going to go to Options, change the IP address that is in here. Save that, and then we're going to try to verify. I can change this through my Minecraft Server, save those changes and then we're going to keep an eye on here to see if this is coming up. Alternative also we could use any of the websites that are listed in here. Since these are third-party tools sometimes they don't work. So that's definitely something to keep in mind. I think that's actually what's going on right now. With this extension it doesn't seem to want to display this to us right now. But if I check the box in the lab instructions itself, it's telling me that everything is tracked correctly. So we've done all the work. It's just that sometimes, again, these third-party tools that we're using to test the status may not always work. There is another one that I can try really fast. We could grab the external IP address and copy it in there. Get the service status that way. It is telling us that it does have it, so it is up and running currently has no players in it, and it tells us the exact version that we're running. So clearly it is working for this page, just not for the Chrome extension right now. All right. So then let's move on. What we're going to do now is, these services up and running but now we want to actually scheduled some regular backups, have some maintenance around the server so that we plan for the long term. So what I can do now is I can SSH back into the server. Since I allowed for read write access to Cloud Storage, I can actually directly create a bucket now through my server here similarly as you would from Cloud Shell. So the first thing I'm going to do is I'm just going to define my own bucket name, and store it in an environment variable. So here we go, export your bucket name. You want to use something that's globally unique. So one thing we could do is we could take our project ID. You take that right here, and you go back to that server and paste it in there. Whenever you create a an environment variable, you want to run the echo command to make sure that you created it correctly. Here we can see that worked. Now I can use the gsutil command specifically MB for make bucket for Google Cloud Storage, and then use that unique part that I just entered and just append Minecraft backup so that I also know what this is. So this becomes a little bit more readable. Great. So there it is. I could also know verify by the way that it is created in my project. I could go to the navigation menu, and if we go to storage we'll be able to see our bucket right here. We could have also just created this way. But this way we now have everything stored that is the variable in here, and then going forward we can do all of the backup right through the VM. So let's go ahead, and create a backup script. I'm just going to navigate to the home directory that we have within Minecraft, and we're going to just create a new script using nano. I'm going to paste the script that we already have in here which has the screen command, and then talks about the backups. So let me paste this in there, and then we're going to press Control O, and then enter to save and control X to come back. So this script saves the current state of the server's world and pauses a service odyssey functionality. Then it's going to backup the service world data directory and place its content in a Timestamp directory in the Cloud Storage bucket. After the script, I've finished his backup the data it resumes odyssey saving on the Minecraft Server. Now we got to make sure that this is actually executable. So let's run the following command, and now we can go and test this. So let's actually run the backup script so there we can see that we are copying some files. Let's verify that. So I'm going to now navigate into my Cloud Storage bucket that I already have here. If I open that, we can now see a folder in there, and I could dig further into there to get more information about the world. So clearly we can see that the backup is working for us. We can also now schedule the backup to run in and more automated fashion. So I'm going to go back to my SSH session, run the pseudo crontab command. Now we want to choose nano in this case, it does tell us it's easiest but you do have other options available if those are more comfortable. At the bottom, we're now going to define how often this runs. This is going to tell it to run the backup every four hours. There's documentation that you can look into and how to define this, but in this case, that's more than enough for what we're trying to achieve. So let's save that file and get back out. This is going to create a lot of backups. I mean about 300 a month. So maybe you want to look into regular deleting those Cloud Storage does offer Object Lifecycle Management features that let you set the time to live for objects and even archive older objects to a different Storage class. You'll learn more about that in the next course of this series when we talk about Cloud Storage. I'm just going to go ahead and check my progress. In my lab looks like everything worked. The last thing we're going to do is now perform some maintenance. So specifically when we shut down and restart that certain actions happen. So let me run the pseudo screen command, and then I'm going to go and actually stop this instance. So I'm going to go navigate to Compute Engine, click on the server so select it and click stop. It's going to ask us if we sure we want to do that, and yes we're going to stop. Then later if we want to start it back up we can do that. This is also going to log us out of our SSH session obviously. So let's wait for this to stop, and then we're going to automate the server maintenance with some startup and shutdown scripts. So the instance has stopped, am going to click on it now to edit some of the custom metadata. So let me click Edit, and we're going to scroll down to the metadata. Here we go. What we're going to define now is a startup script as well as the shutdown script. We're going to point those to files that we have in Cloud Storage that are publicly available. So the key is going to be startup script URL, and then the value is going to be the location of the file. I can make them bigger to make sure that formatting that correctly. I'll add another item, and we'll do the same for the shutdown script. You can actually navigate yourself to these files if you want to, and you could read more about what exactly happens in these startup and shutdown script. So now I can click Save, and I could restart the service. I did in the meantime while the service was shutting down. I went back to the status page, and you can see that the status as currently says could not resolve so clearly the server is shut down. Now when we restart this, once all the startup script is done running, we can go back and we can verify that this service is indeed now accessible again. Just keep in mind that that might take a while for the actual instance to startup which it is now, and then for the startup script to actually finish.

#### Quiz: Virtual Machines

- https://www.cloudskillsboost.google/paths/11/course_templates/50/quizzes/530388

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530389

In this module, we cover the different Compute image and disk options within Compute Engine, along with some common actions. The two labs provided you with real-world applications of most of the topics covered in this course. Remember there are many Compute options to choose from. If a predefined machine type does not meet your needs, you can also customize your own VM and you can even create a sole-tenant node. You can also install different public and custom images on the boot disk of you instances, and you can attach more disks if needed.

#### Course Review

- https://www.cloudskillsboost.google/paths/11/course_templates/50/video/530390

Thank you for taking the Essential Cloud Infrastructure: Foundation course. I hope you have a better understanding of how to architect with Compute Engine, and I also hope that the demos and labs made you feel more comfortable with using the different Google Cloud services that we covered. Next, I recommend enrolling in the Essential Cloud Infrastructure: Core Services course of the Architecting with Google Compute Engine series. In that course, we start by talking about IAM, and you will administer identity and access management for resources. Next, we'll cover the different data storage services in Google Cloud, and you will implement some of those services. Then, we’ll go over resource management, where you will manage and examine billing data of Google Cloud resources. Lastly, we'll talk about resource monitoring, and you will monitor Google Cloud resources using Cloud Monitoring services. Enjoy that course!

#### Next Course: Essential Cloud Infrastructure: Core Services

- https://www.cloudskillsboost.google/paths/11/course_templates/50/documents/530391

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/50/documents/530392

### Your Next Steps

## 05: Essential Google Cloud Infrastructure: Core Services

- https://www.cloudskillsboost.google/paths/11/course_templates/49

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548385

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you‘ll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

#### Welcome to Essential Cloud Infrastructure: Core Services

- https://www.cloudskillsboost.google/paths/11/course_templates/49/documents/548386

### Identity and Access Management (IAM)

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548387

In this module, we cover Identity and Access Management, or IAM. IAM is a sophisticated system built on top of email-like address names, job-type roles, and granular permissions. If you're familiar with IAM from other implementations, look for the differences that Google has implemented to make IAM easier to administer and more secure. We start by introducing IAM from a high-level perspective. We will then dive into each of the components within IAM, which are organizations, roles, members, and service accounts. We talk about the Organization Restrictions feature and also introduce some best practices to help you apply these concepts in your day-to-day work. Finally, you will gain first-hand experience with IAM through a lab. Let's get started with an overview of Identity and Access Management.

#### Identity and Access Management

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548388

So, what is Identity and Access Management? It’s a way of identifying who can do what on which resource. The who can be a person, group, or application. The what refers to specific privileges or actions, and the resource could be any Google Cloud service. For example, I could give you the privilege or role of Compute Viewer. This provides you with read-only access to get and list Compute Engine resources, without being able to read the data stored on them. IAM is composed of different objects as shown on this slide. We are going to cover each of these in this module. To get a better understanding of where these fit in, let's look at IAM policies and the IAM resource hierarchy. Google Cloud resources are organized hierarchically, as shown in this tree structure. The organization node is the root node in this hierarchy, folders are the children of the organization. projects are the children of the folders, and the  individual resources are the children of projects. Each resource has exactly one parent. The organization resource represents your company. IAM roles granted at this level are inherited by all resources under the organization. The folder resource could represent your department. IAM roles granted at this level are inherited by all resources that the folder contains. Projects represent a trust boundary within your company. Services within the same project have the same default level of trust.

#### Organization

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548389

Let's learn more about the organization node. As I mentioned earlier, the organization resource is the root node in the Google Cloud resource hierarchy. This node has many roles, like the Organization Admin. The Organization Admin provides a user like Bob, with access to administer all resources belonging to his organization, which is useful for auditing. There is also a Project Creator role, which allows a user like Alice, to create projects within her organization. I am showing the Project Creator role here because it can also be applied at the organization level, which would then be inherited by all the projects within the organization. The organization resource is closely associated with a Google Workspace or Cloud Identity Account. When a user with a Google Workspace or Cloud Identity account creates a Google Cloud project, an organization resource is automatically provisioned for them. Then, Google Cloud communicates its availability to the Google Workspace or Cloud Identity super admins. These super admin accounts should be used carefully because they have a lot of control over your organization and all the resources underneath it. The Google Workspace or Cloud Identity super administrators, and the Google Cloud Organization Admin, are key roles during the setup process and for lifecycle control for the organization resource. The two roles are generally assigned to different users or groups, although this depends on the organization structure and needs. In the context of Google Cloud organization setup, the Google Workspace or Cloud Identity super administrator responsibilities are: Assign the Organization Admin role to some users, be a point of contact in case of recovery issues, and control the lifecycle of the Google Workspace or Cloud Identity account and organization resource. The responsibilities of the Organization Admin role are: Define IAM policies, determine the structure of the resource hierarchy, and delegate responsibility over critical components, such as networking, billing, and resource hierarchy, through IAM roles. Following the principle of least privilege, this role does not include the permission to perform other actions, such as creating folders. To get these permissions, an Organization Admin must assign additional roles to their account. Let's talk more about folders, because they can be viewed as sub organizations within the organization. Folders provide an additional grouping mechanism and isolation boundary between projects. Folders can be used to model different legal entities, departments, and teams within a company. For example, a first-level of folders could be used to represent the main departments in your organization, like departments X and Y. Because folders can contain projects and other folders, each folder could then include other subfolders to represent different teams, like teams A and B. Each team folder could contain additional subfolders, to represent different applications, like products 1 and 2. Folders allow delegation of administration rights, so for example, each head of a department can be granted full ownership of all Google Cloud resources that belong to their department. Similarly, access to resources can be limited by folder, so users in one department can only access and create Google Cloud resources within that folder. Let's look at some other resource manager roles, while remembering that policies are inherited from top to bottom. The organization node also has a Viewer role that grants view access to all resources within an organization. The folder node has multiple roles that mimic the organizational roles, but are applied to resources within a folder. There is an Admin role that provides full control over folders; a Creator role to browse the hierarchy and create folders; and a Viewer role to view folders and projects below a resource. Similarly, for projects, there is a Creator role that allows a user to create new projects, making that user automatically the owner. There is also a project deleter role that grants deletion privileges for projects.

#### Roles

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548390

Let's talk more about roles, which define the “can do what on which resource” part of IAM. There are three types of roles in IAM, basic roles, predefined roles, and custom roles. Basic roles are the original roles that were available in the Google Cloud console, but they are broad. You apply them to a Google Cloud project, and they affect all resources in that project. In other words, IAM basic roles offer fixed, coarse-grained levels of access. The basic roles are the Owner, Editor, and Viewer roles. The Owner has full administrative access. This includes the ability to add and remove members and delete projects. The Editor role has modify and delete access. This allows the developer to deploy applications and modify or configure its resources. The Viewer role has read-only access. All of these roles are concentric. That is, the Owner role includes the permissions of the Editor role, and the Editor role includes the permissions of the Viewer role. There is also a Billing Administrator role to manage billing and add or remove administrators without the right to change the resources in the project. Each project can have multiple Owners, Editors, Viewers, and Billing Administrators. Google Cloud services offers their own sets of predefined roles, and they define where those roles can be applied. This provides members with granular access to specific Google Cloud resources and prevents unwanted access to other resources. These roles are collections of permissions because, to do any meaningful operations, you usually need more than one permission. For example, as shown here, a group of users is granted the InstanceAdmin role on project_a. This provides the users of that group with all the Compute Engine permissions listed on the right and more. Grouping these permissions into a role makes them easier to manage. The permissions themselves are classes and methods in the APIs. For example, compute.instances.start can be broken down into the service, resource, and verb that mean that this permission is used to start a stopped Compute Engine instance. These permissions usually align with the action’s corresponding REST API. Compute Engine has several predefined IAM roles. Let's look at three of those. The Compute Admin role provides full control of all Compute Engine resources. This includes all permissions that start with compute, which means that every action for any type of Compute Engine resource is permitted. The Network Admin role contains permissions to create, modify, and delete networking resources, except for firewall rules and SSL certificates. In other words, the Network Admin role allows read-only access to firewall rules, SSL certificates, and instances, to view their ephemeral IP addresses. The Storage Admin role contains permissions to create, modify, and delete disks, images, and snapshots. For example, if your company has someone who manages project images, and you don't want them to have the Editor role on the project, grant their account the Storage Admin role on the project. For the full list of predefined roles for Compute Engine, refer to the link in the student PDF. Now, roles are meant to represent abstract functions and are customized to align with real jobs. But what if one of these roles does not have enough permissions, or you need something even finer-grained? That's what custom roles permit. A lot of companies use the ”least-privilege” model, in which each person in your organization is given the minimal amount of privilege needed to do their job. Let's say you want to define an “Instance Operator” role to allow some users to start and stop Compute Engine virtual machines, but not reconfigure them. Custom roles allow you to do that.

#### Demo: Custom roles

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548391

This demo shows you how to create a custom role in Google Cloud. The goal is to create an Instance Operator role that allows some users to start and stop Compute Engine virtual machines but not reconfigure them. So here I am in the GCP console, and I'm going to click the Navigation menu to go to IAM & Admin, and specifically actually want to go to Roles. And here you can see all the different roles are available. Now, I could select one of these roles and create a role from that selection and then either remove or assign more permissions. You can see over here the permissions that are assigned to a role. Or I can just create a role from scratch. So let me go do that, I'm going to click "Create role" and I'm going to give it a name. I'm going to call this the Instance Operator. There's also an ID to that. And that must be unique and cannot be changed. There is a launch stage selection, Alpha, Beta, General Availability, and Disabled. This is essentially just a launch stage, so you want to make sure that you start small, tested Alpha, and then roll it out at some point so that other users know that they can leverage that availability. So what I'm going to do now is click "Add permissions" because currently there are no assigned permissions given that I started from scratch. So let's go in here. And now we have over 2,000 different permissions, so we obviously want to filter for that just a little bit and specifically I'm interested in the permissions for compute instances. So let me type compute dot instances dot and hit "Enter". And now I'm down to 44. So I want to select a couple different ones from here, I'm interested in "Get", I want to be able to get the different instances. I want to be able to list all of the instances as well as reset them and resume. Resume is if an instance was suspended, which is equivalent to if it's in sleep or in standby mode. I also want to start and stop, and suspend. So I can go click "Add" now and I can see the permissions that I just assigned. So I can get, list, reset, resume, start, stop, and suspend. And from here, I can now click "Create". It's created that and I can click on it here, I can review that, I have an ID, and I have a launch stage, and these are my permissions. That's how easy it is to create a custom role in Google Cloud. Alternatively, you could have started with the Instance Admin role as a base and removed the permissions that you don't want the role to have. Now remember that custom roles are not maintained by Google. That means that when new permissions, features, or services are added to Google Cloud, your custom roles will not be updated automatically.

#### Members

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548392

Let’s talk more about members, which define the “who” part of “who can do what on which resource.” There are five different types of members: Google Accounts, service accounts, Google groups, Google Workspace domains, and Cloud Identity domains. A Google Account represents a developer, an administrator, or any other person who interacts with Google Cloud. Any email address that is associated with a Google Account can be an identity, including gmail.com or other domains. New users can sign up for a Google Account by going to the Google account signup page, without receiving mail through Gmail. A service account is an account that belongs to your application instead of to an individual end user. When you run code that is hosted on Google Cloud, you specify the account that the code should run as. You can create as many service accounts as needed to represent the different logical components of your application. A Google group is a named collection of Google Accounts and service accounts. Every group has a unique email address that is associated with the group. Google groups are a convenient way to apply an access policy to a collection of users. You can grant and change access controls for a whole group at once instead of granting or changing access controls one-at-a-time for individual users or service accounts. A Google Workspace domain represents a virtual group of all the Google Accounts that have been created in an organization's Google Workspace account. Google Workspace domains represent your organization's internet domain name, such as example.com, and when you add a user to your Google Workspace domain, a new Google Account is created for the user inside this virtual group, such as username@example.com. Google Cloud customers who are not Google Workspace customers can get these same capabilities through Cloud Identity. Cloud Identity lets you manage users and groups using the Google Admin console, but you do not pay for or receive Google Workspace collaboration products, such as Gmail, Docs, Drive, and Calendar. Now it’s important to note that you cannot use IAM to create or manage your users or groups. Instead, you can use Cloud Identity or Google Workspace to create and manage users. A policy consists of a list of bindings. A binding binds a list of members to a role, where the members can be user accounts, Google groups, Google domains, and service accounts. A role is a named list of permissions defined by IAM. Let’s revisit the IAM resource hierarchy. A policy is a collection of access statements attached to a resource. Each policy contains a set of roles and role members, with resources inheriting policies from their parent. Think of it like this: resource policies are a union of parent and resource, where a less restrictive parent policy will always override a more restrictive resource policy. The IAM policy hierarchy always follows the same path as the Google Cloud resource hierarchy, which means that if you change the resource hierarchy, the policy hierarchy also changes. For example, moving a project into a different organization will update the project's IAM policy to inherit from the new organization's IAM policy. Also, child policies cannot restrict access granted at the parent level. For example, if we grant you the Editor role for Department X, and we grant you the Viewer role at the bookshelf project level, you still have the Editor role for that project. Therefore, it is a best practice to follow the principle of least privilege. The principle applies to identities, roles, and resources. Always select the smallest scope that’s necessary for the task in order to reduce your exposure to risk. You can also use a recommender for role recommendations to identify and remove excess permissions from your principals, improving your resources’ security configurations. Each role recommendation suggests that you remove or replace a role that gives your principals excess permissions. At scale, these recommendations help you enforce the principle of least privilege by ensuring that principals have only the permissions that they actually need. Recommender identifies excess permissions using policy insights. Policy insights are ML-based findings about permission usage in your project, folder, or organization. You can grant access to Google Cloud resources by using allow policies, also known as IAM policies, which are attached to resources. The allow policy controls access to the resource itself and any descendants of that resource that inherit the allow policy. An allow policy associates, or binds, one or more principals, also known as a member or identity, with a single IAM role and any context-specific conditions that change how and when the role is granted. In the example on this slide, Jie (jie@example.com) is granted the Organization Admin predefined role, roles/resourcemanager.organizationAdmin, in the first role binding. This role contains permissions for organizations, folders, and limited projects operations. In the second role binding, both Jie and Raha (raha@example.com) are granted the ability to create projects via the Project Creator role, roles/resourcemanager.projectCreator. Together, these role bindings grant fine- grained access to both Jie and Raha, and Jie is granted more access than Raha. IAM deny policies let you set guardrails on access to Google Cloud resources. With deny policies, you can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. Deny policies are made up of deny rules. Each deny rule specifies a set of principals that are denied permissions, and the permissions that the principals are denied, or unable to use. Optionally, you can define the condition that must be true for the permission to be denied. When a principal is denied a permission, they can't do anything that requires that permission, regardless of the IAM roles they've been granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. IAM Conditions allow you to define and enforce conditional, attribute-based access control for Google Cloud resources. With IAM Conditions, you can choose to grant resource access to identities (members) only if configured conditions are met. For example, this could be done to configure temporary access for users in the event of a production issue or to limit access to resources only for employees making requests from your corporate office. Conditions are specified in the role bindings of a resource's IAM policy. When a condition exists, the access request is only granted if the condition expression evaluates to true. Each condition expression is defined as a set of logic statements allowing you to specify one or more attributes to check. An organization policy is a configuration of restrictions, defined by configuring a constraint with the desired restrictions for that organization. An organization policy can be applied to the organization node, and all of its folders or projects within that node. Descendants of the targeted resource hierarchy inherit the organization policy that has been applied to their parents. Exceptions to these policies can be made, but only by a user who has the organization policy admin role. What if you already have a different corporate directory? How can you get your users and groups into Google Cloud? Using Google Cloud Directory Sync, your administrators can log in and manage Google Cloud resources using the same usernames and passwords they already use. This tool synchronizes users and groups from your existing Active Directory or LDAP system with the users and groups in your Cloud Identity domain. The synchronization is one-way only; which means that no information in your Active Directory or LDAP map is modified. Google Cloud Directory Sync is designed to run scheduled synchronizations without supervision, after its synchronization rules are set up. Google Cloud also provides single sign-on authentication. If you have your identity system, you can continue using your own system and processes with SSO configured. When user authentication is required, Google will redirect to your system. If the user is authenticated in your system, access to Google Cloud is given; otherwise, the user is prompted to sign in. This allows you to also revoke access to Google Cloud. If your existing authentication system supports SAML2, SSO configuration is as simple as 3 links and a certificate, as shown on this slide. Otherwise, you can use a third-party solution, like ADFS, Ping, or Okta. Also, if you want to use a Google account, but are not interested in receiving mail through Gmail, you can still create an account without Gmail.

#### Service Accounts

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548393

As mentioned earlier, another type of member is a service account. A service account is an account that belongs to your application instead of to an individual end user. This provides an identity for carrying out service-to-service interactions in a project without supplying user credentials. For example, if you write an application that interacts with Cloud Storage, it must first authenticate to either the Cloud Storage XML API or JSON API. You can enable service accounts and grant read-write access to the account on the instance where you plan to run your application. Then, program the application to obtain credentials from the service account. Your application authenticates seamlessly to the API without embedding any secret keys or credentials in your instance, image, or application code. Service accounts are identified by an email address, like the example shown here. There are three types of service accounts: user-created or custom, built-in, and Google APIs service accounts. By default, all projects come with the built-in Compute Engine default service account. Apart from the default service account, all projects come with a Google Cloud APIs service account, identifiable by the email: project-number@cloudservices.gserviceaccount.com. This is a service account designed specifically to run internal Google processes on your behalf, and it is automatically granted the Editor role on the project. Alternatively, you can also start an instance with a custom service account. Custom service accounts provide more flexibility than the default service account, but they require more management from you. You can create as many custom service accounts as you need, assign any arbitrary access scopes or IAM roles to them, and assign the service accounts to any virtual machine instance. Let’s talk more about the default Compute Engine service account. As I mentioned, this account is automatically created per project. This account is identifiable by the email: project-number-compute@developer.gserviceaccount.com, and it is automatically granted the Editor role on the project. When you start a new instance using gcloud, the default service account is enabled on that instance. To override this behavior, you can specify another service account, or disable service accounts for the instance. Now, authorization is the process of determining what permissions an authenticated identity has on a set of specified resources. Scopes are used to determine whether an authenticated identity is authorized. In the example shown here, Applications A and B contain Authenticated Identities (or service accounts). Let’s assume that both applications want to use a Cloud Storage bucket. They each request access from the Google Authorization server, and in return they receive an access token. Application A receives an access token with read-only scope, so it can only read from the Cloud Storage bucket. Application B, in contrast, receives an access token with read-write scope, so it can read and modify data in the Cloud Storage bucket. Scopes can be customized when you create an instance using the default service account, as shown in this screenshot. These scopes can be changed after an instance is created by stopping it. Access scopes are actually the legacy method of specifying permissions for your VM. Before the existence of IAM roles, access scopes were the only mechanism for granting permissions to service accounts. For user-created service accounts, use IAM roles instead to specify permissions. Roles for service accounts can also be assigned to groups or users. Let’s look at the example shown on this slide. First, you create a service account that has the InstanceAdmin role, which has permissions to create, modify, and delete virtual machine instances and disks. Then you treat this service account as the resource, and decide who can use it by providing users or a group with the Service Account User role. This allows those users to act as that service account to create, modify, and delete virtual machine instances and disks. Users who are Service Account Users for a service account can access all the resources that the service account has access to. Therefore, be cautious when granting the Service Account User role to a user or group. Here is another example. The VMs running component_1 are granted Editor access to project_b using Service Account 1. VMs running component_2 are granted objectViewer access to bucket_1 using an isolated Service Account 2. This way you can scope permissions for VMs without re-creating VMs. Essentially, IAM lets you slice a project into different microservices, each with access to different resources, by creating service accounts to represent each one. You assign the service accounts to the VMs when they are created, and you don’t have to ensure that credentials are being managed correctly because Google Cloud manages security for you. Now, you might ask, how are service accounts authenticated? There are two types of service account keys. By default, when using service accounts within Google Cloud, for example, from Compute Engine or App Engine, Google automatically manages the keys for service accounts. However, if you want to be able to use service accounts outside of Google Cloud, or want a different rotation period, it is possible to also manually create and manage your own service account keys. All service accounts have Google-managed key-pairs. With Google-managed service account keys, Google stores both the public and private portion of the key, and rotates them regularly. Each public key can be used for signing for a maximum of two weeks. Your private key is always held securely in escrow and is never directly accessible. You may optionally create one or more user-managed key pairs, also known as "external" keys, that can be used from outside of Google Cloud. Google only stores the public portion of a user-managed key. The user is responsible for security of the private key and performing other management operations such as key rotation, whether manually or programmatically. Users can create up to 10 service account keys per service account to facilitate key rotation. User-managed keys can be managed by using the IAM API, the gcloud command-line tool, or the Service Accounts page in the Google Cloud console. Google does not save your user-managed private keys, so if you lose them, Google cannot help you recover them. You are responsible for keeping these keys safe and also responsible for performing key rotation. User-managed keys should be used as a last resort. Consider the other alternatives, such as short-lived service account credentials (tokens), or service account impersonation. The gcloud command-line shown on this slide is a fast and easy way to list all of the keys associated with a particular service account.

#### Organization Restrictions

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548394

Let’s talk now about Organization Restrictions. The Organization Restrictions feature lets you prevent data exfiltration through phishing or insider attacks. For managed devices in an organization, the Organization Restrictions feature restricts access only to resources in authorized Google Cloud organizations. There is a need in organizations to restrict access of their employees only to resources in authorized Google Cloud organizations. Google Cloud administrators who administer Google Cloud, and egress proxy administrators, who configure the egress proxy, engage together to set up organization restrictions. The managed device is governed by the organizational policies of a company. Employees of an organization use a managed device to access the organization resources. An egress proxy administrator configures the proxy to add organization restrictions headers to any requests originating from a managed device. This proxy configuration prevents users from accessing any Google Cloud resources in a non-authorized Google Cloud organization. The Organization Restrictions feature in Google Cloud inspects all requests for organization restrictions headers, and allows or denies the requests based on the organization being accessed. Organization Restrictions can be used to restrict access to employees in your organization so that employees can access resources only in your Google Cloud organization and not other organizations. They can also be used to allow your employees to read from Cloud Storage resources but restrict employee access only to resources in your Google Cloud organization. Or, allow your employees to access a vendor Google Cloud organization in addition to your Google Cloud organization.

#### IAM best practices

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548395

Let's talk about some IAM best practices to help you apply the concepts you just learned in your day-to-day work. First, leverage and understand the resource hierarchy. Specifically, use projects to group resources that share the same trust boundary. Check the policy granted on each resource and make sure you recognize the inheritance. Because of inheritance, use the principle of least privilege when granting roles. Finally, audit policies using Cloud audit logs and audit memberships of groups used in policies. Next, I recommend granting roles to groups instead of individuals. This allows you to update group membership instead of changing an IAM policy. If you do this, make sure to audit membership of groups used in policies and control the ownership of the Google group used in IAM policies. You can also use multiple groups to get better control. In the example on this slide, there is a Network Admin group. Some of those members also need a read_write role to a Cloud Storage bucket, but others need the read_only role. Adding and removing individuals from all three groups controls their total access. Therefore, groups are not only associated with job roles but can exist for the purpose of role assignment. Here are some best practices for using service accounts. As mentioned before, be very careful when granting the Service Account User’s role because it provides access to all the resources that the service account has access to. Also, when you create a service account, give it a display name that clearly identifies its purpose, ideally using an established naming convention. As for keys, establish key rotation policies and methods and audit keys with the serviceAccount.keys.list method. Finally, I recommend using Identity-Aware Proxy, or IAP. IAP lets you establish a central authorization layer for applications accessed by HTTPS, so you can use an application-level access control model instead of relying on network-level firewalls. Applications and resources protected by IAP can only be accessed through the proxy by users and groups with the correct IAM role. When you grant a user access to an application or resource by IAP, they're subject to the fine-grained access controls implemented by the product in use without requiring a VPN. IAP performs authentication and authorization checks when a user tries to access an IAP-secured resource as shown on the right.

#### Lab Intro: Exploring IAM

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548396

It's time to apply what you learned. In this lab, you'll grant and revoke roles to change access. Specifically, you will use IAM to implement access control, restrict access to specific features and resources, and use the Service Account User role. Now, anytime you make changes to IAM roles, the Google Cloud console refreshes faster than the actual system. Therefore, you should expect some short delays when making changes to a member's role.

#### Exploring IAM

- https://www.cloudskillsboost.google/paths/11/course_templates/49/labs/548397

#### Lab Review: Exploring IAM

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548398

In this lab, you granted and revoked IAM roles, first to a user, Username 2, and then to a Service Account User. Having access to both users allowed you to see the results of the changes you made. You can stay for a lab walk-through, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. Welcome to the walk-through of the Cloud IAM lab. In this lab, we have set up two users for you, and at this point I have logged into the console as username 1. So Qwiklabs will provide you with two usernames to log into and we'll do some operations with both, but right now I have already logged in as username 1. So the first instruction tells you to log into the console in another tab as username 2. So console, I'm going to grab that username, certainly going here, I'm going to say add account, username 2, and luckily we've been given the same password for both usernames and login here, and with Qwiklabs you have new accounts. So it's always going to ask you for all of this new user acceptance, and I'll accept this terms, I'm good to go. So task two is to explore the IAM console. So I'm going to go to the username 1 tab. I'm going to go to IAM, and I'm going to click on there. If I hit Add, I can look around at the different roles I can provide. Let me go ahead and click Cancel, feel free to explore as much as you want, you can see here there are roles based on different products and services that we have. I'll hit Cancel. Let me go to username 2, and I'm going to do the same thing. Let me go to IAM. So I'm going to browse this list now and I am going to look for the names associated with username 1, which in my case ends in 82462, I see it here, and here is username 2. You can see there are different roles associated with each one of them. Username 1 has App Engine admin, BigQuery admin, editor, owner, and viewer. Whereas, username 2, which is the one that I'm logged in in this tab only has Viewer Access. So now I'm going to move on to task three. So I'm going to go back to username 1, there's going to go to be a lot of switching back and forth in this lab. So make sure you keep track of which tab is username 1 and which is username 2. I am going to go to Google Cloud Storage here, and I'm going to create a bucket in here. So buckets need to be globally unique. So I am going to use my Cloud Project ID because it is pretty unique, and I'm going to click Create and keep all the other defaults, and make sure to note the name of your bucket because we'll use it as your bucket name across the lab. So here I'm going to go to upload files, and let me find just any sample file here just a screenshot, and I've uploaded it there. Once it's uploaded, I'm going to rename it here, and I'm going to call it sample.txt. The reason I'm doing this is because it's going to be much easier to run any of the commands I'm going to do with sample.txt as a name as opposed to that long name I already had. So at this point in the lab, you can hit the Check my progress button inside the lab and it'll give you a green check and five points. If you've correctly created a bucket and uploaded a sample file. So now I'm going to switch to username 2 and I'm going to go to storage browser. I'm going to verify that username 2 has view access to that bucket, and here it is. Because it's inherited that, I can view the sample file. Task four is I'm going to remove project viewer role for username 2. So in order to do that I have to go back to username 1. I'm going to go back to IAM, and then I'm going to find username 2 which is this one right here, 73. I'm going to hit Edit, and then I am going to hit the garbage can so that I can remove it. I hit Save, and then I at this point I can also check my progress and I'll get five points and a green check mark. I should have 10 points out of 20 in the lab. If I have properly done that. Throughout these labs if you ever get to a point where you realize that you didn't get the points necessary, it's probably because you missed a step or two, granted sometimes the lab is actually broken because they're based on technology that changes a lot. But if a lab isn't broken, chances are you just missed a step. So I usually recommend go back three steps. Check your work, make sure you did everything. Usually, that's what happened. So now we're going to verify that the username 2 has lost access. So I'm going to go back to the username 2 bucket tab, and then I'm going to click Home, and then I'm going to go back to storage to verify. I could've just refresh the screen as well, Refresh. List of buckets could not be loaded. So as you can see I do not have access anymore. So now the next task, task five, is to add Storage Access. So I'm going to copy the value of username 2 from the Qwiklabs lab name, from their connection details on the left of your lab instructions. So I'll copy that, I'm going to go back to username 1 tab, and I'm already in I am. So I'm going to hit Add, and then for new members I'm going to paste the value here. That is it, and I am going to select Storage, scroll down. Luckily it's alphabetical, and I am giving it Storage Object Viewer. Then I'm going to hit Save. This is another checkpoint in the lab where you can go back and hit Check my progress, and you should get another five points that you have checked, that you have actually provided the right permissions. Now we had one in the modules that sometimes the permissions upload faster than will be displayed in the GCP Console. Sometimes you just have to be patient. Maybe click Check my progress, wait a couple seconds if you didn't get it, and then you'll get the five points and the green check. So the next piece of task 5 is to verify that username 2 now has storage access. So if I go back here, and I'm going to start Cloud Shell. Because username 2 doesn't have project viewer roles, so it won't be able to see anything in the console, but we can see things in Cloud Storage. So we're going to use Cloud Shell for that. So let's make sure I know my bucket name. I copied it earlier, but I have definitely forgotten it by now. So let me go back here to Storage easily, and I can easily copy paste the bucket name here. Copy that, and back in Cloud Shell, I am going to do a gsutil ls to list for that bucket, gs://my bucket name, and username 2 should be able to see that there's sample.txt in the bucket and there it is. So now I can close username 2 tab because the rest of the lab is done in the username 1 console. So task 6 is to set up the service account user. So in IAM, I'm going to go to Service accounts. I'm going to create a service account, and the service account name is going to be read-bucket-objects, and I'm going to hit Create. It's going to ask me which role to provide, and I am going to be giving it Storage, Storage Object Viewer. Hit Continue, and then I'm going to hit Done. So now we've created our service account, so we're going to go back to the main IAM page, and we are going to select the service account we just created, and we're going to hit Add. In order to add members, normally you could perform this activity for a specified user group or a domain. But for training purposes and for this video, we're just going to grant the service account user role to everyone at a company called autostrat.com, which is a fake company used for demonstrating and training. So the new member is going to be autostrat.com, and I am going to give it Service Accounts, Service Account User, and then I'm going to hit Save. So now I'm going to go back to IAM, and I am going to add, and I am going to provide compute engine access. So the new member is autostrat.com. Make sure you're typing it correctly. I am giving it Compute Engine, and Compute Instance Admin V1 and save. So essentially, that step is a rehearsal of activity that you would probably perform for a specific user. It gives the user limited abilities with a VM Instance. It would be able to connect via SSH to a VM and perform possibly some administration tasks. So now, I am going to create a VM with the service account I created. Create, I am going to use the same name provided in a lab, demoIAM. I'm using us-central1. The zone is us-central1-c, and the machine type is an F1-micro. It is just for demonstration purposes. So let's not waste resources, and the service account is the read bucket objects account, and I'm going to hit Create. So this is another checkpoint in the lab, and you should be able to hit Check my progress, and verify that you have gotten the last five points in the lab. Again, this is another one that might take a couple seconds to propagate. So just give it a second, and make sure that you get the green check in the final five points. Task 7, you explore the service account user role, and now you've already completed all of the tasks in the labs. So this is just for learning purposes. So I'm going to go in here, and I'm going to SSH into this account into the VM that I just created. Then I am going to run a gcloud compute instances list. I am expecting to see an error because I do not have the correct permissions to list those for my project. Just wait for that to show up, and there you can see error. Some request did not succeed because I don't have permission to do that. So now I'm going to try to copy the file from the bucket that I created earlier. So my bucket name is the Project ID which I've forgotten already. Here gets/sample.txt, and you can see it successfully copied it. Now I'm going to copy it into another file, and then I'm going to try to upload into my bucket. Bucket name is here, and you'll see I can download, but I cannot add. In review in this lab, you granted and revoked Cloud IAM roles, first for user, and then to service account user. I hope you enjoyed the walkthrough. Thank you.

#### Quiz: Identity and Access Management

- https://www.cloudskillsboost.google/paths/11/course_templates/49/quizzes/548399

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548400

In this module, we covered Identity and Access Management along with its components and best practices. IAM builds on top of other Google Cloud and entity services. The creation and administration of corporate identities occurs through the Workspace Admin or Cloud Identity interface, and is commonly handled by a person separate from the Google Cloud administrator. Google Groups are a great way for these two business functions to collaborate. You establish the roles and assign them to the group, and then the Workspace Admin administers membership in the group. Finally, remember that service accounts are very flexible, and they can enable you to build an infrastructure- based level of control in your application.

### Storage and Database Services

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548401

In this module, we cover storage and database services in Google Cloud. Every application needs to store data, whether it's business data, media to be streamed, or sensor data from devices. From an application-centered perspective, the technology stores and retrieves the data. Whether it's a database or an object store is less important than whether that service supports the application’s requirements for efficiently storing and retrieving the data, given its characteristics. Google offers several data storage services to choose from. In this module, we will cover Cloud Storage, Filestore, Cloud SQL, Spanner, AlloyDB, Firestore, Bigtable, and Memorystore. Let me start by giving you a high-level overview of these different services. This table shows the storage and database services and highlights the storage service type, what each service is good for, and intended use. BigQuery is also listed on the right. I’m mentioning this service because it sits on the edge between data storage and data processing. You can store data in BigQuery, but the intended use for BigQuery is big data analysis and interactive querying. For this reason, BigQuery is covered later in the course. If tables aren’t your preference, here’s a decision tree to help you identify the solution that best fits your application. Let’s walk through this together. First, ask yourself: Is your data structured? If it’s not, then ask yourself if you need a shared file system. If you do, then choose Filestore. If you don't, then choose Cloud Storage. If your data is structured, does your workload focus on analytics? If it does, you will want to choose Bigtable or BigQuery, depending on your latency and update needs. BigQuery is recommended as a data warehouse, is the default storage for tabular data, and is optimized for large-scale, ad-hoc SQL-based analysis and reporting. While BigQuery data manipulation language enables you to update, insert, and delete data from your BigQuery tables, because it has a built-in cache, BigQuery works really well in cases where the data does not often change. Bigtable is a NoSQL wide-column database. It's optimized for low latency, large numbers of reads and writes, and maintaining performance at scale. In addition to analytics, Bigtable is also suited as a ‘fast lookup’ non-relational database for datasets too large to store in memory, with use cases in areas such as IoT, AdTech and FinTec. If your workload doesn’t involve analytics, check whether your data is relational. If it’s not relational, do you need application caching? If caching is a requirement, choose Memorystore, an in-memory database. Otherwise choose Firestore, a document database. If your data is relational and you need hybrid transactional and analytical processing, also known as HTAP, choose AlloyDB. If you don’t need HTAP and don’t need global scalability, choose Cloud SQL. If you don’t need HTAP and need global scalability, choose Spanner. Depending on your application, you might use one or several of these services to get the job done. For more information on how to choose between these different services, please refer to the links provided in the course resources for this module. Before we dive into each of the data storage services, let’s define the scope of this module. The purpose of this module is to explain which services are available and when to consider using them from an infrastructure perspective. I want you to be able to set up and connect to a service without detailed knowledge of how to use a database system. If you want a deeper dive into the design, organizations, structures, schemas and details on how data can be optimized, served and stored properly within those different services, I recommend Google Cloud’s Data Engineering courses. Let’s look at the agenda. This module covers all of the services we’ve mentioned so far. To become more comfortable with these services, you will apply them in two labs. We’ll also provide a quick overview of Memorystore, which is Google Cloud’s fully managed Redis service. Let’s get started by diving into Cloud Storage and Filestore.

#### Cloud Storage

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548402

Cloud Storage is Google Cloud’s object storage service, and it allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download. Cloud Storage has a couple of key features: It’s scalable to exabytes of data; The time to first byte is in milliseconds; It has very high availability across all storage classes; And It has a single API across those storage classes. Some like to think of Cloud Storage as files in a file system but it’s not really a file system. Instead, Cloud Storage is a collection of buckets that you place objects into. You can create directories, so to speak, but really a directory is just another object that points to different objects in the bucket. You’re not going to easily be able to index all of these files like you would in a file system. You just have a specific URL to access objects. Cloud Storage has four storage classes: Standard, Nearline, Coldline and Archive, and each of those storage classes provide 3 location types: There’s a multi-region is a large geographic area, such as the United States, that contains two or more geographic places. Dual-region is a specific pair of regions, such as Finland and the Netherlands. A region is a specific geographic place, such as London. Objects stored in a multi-region or dual-region are geo-redundant. Now, let’s go over each of the storage classes: Standard Storage is best for data that is frequently accessed (think of "hot" data) and/or stored for only brief periods of time. This is the most expensive storage class but it has no minimum storage duration and no retrieval cost. When used in a region, Standard Storage is appropriate for storing data in the same location as Google Kubernetes Engine clusters or Compute Engine instances that use the data. Co-locating your resources maximizes the performance for data-intensive computations and can reduce network charges. When used in a dual-region, you still get optimized performance when accessing Google Cloud products that are located in one of the associated regions, but you also get improved availability that comes from storing data in geographically separate locations. When used in multi-region, Standard Storage is appropriate for storing data that is accessed around the world, such as serving website content, streaming videos, executing interactive workloads, or serving data supporting mobile and gaming applications. Nearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data like data backup, long-tail multimedia content, and data archiving. Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs. Coldline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs. Archive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the so-to-speak "coldest" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days. Archive Storage also has higher costs for data access and operations, as well as a 365-day minimum storage duration. Archive Storage is the best choice for data that you plan to access less than once a year. Let’s focus on durability and availability. All of these storage classes have 11 nines of durability, but what does that mean? Does that mean you have access to your files at all times? No, what that means is you won't lose data. You may not be able to access the data, which is like going to your bank and saying well my money is in there, it's 11 nines durable. But when the bank is closed we don't have access to it, which is the availability that differs between storage classes and the location type. Cloud Storage is broken down into a couple of different items here. First of all, there are buckets which are required to have a globally unique name and cannot be nested. The data that you put into those buckets are objects that inherit the storage class of the bucket and those objects could be text files, doc files, video files, etc. There is no minimum size to those objects and you can scale this as much as you want as long as your quota allows it. To access the data, you can use the gcloud storage command, or either the JSON or XML APIs. When you upload an object to a bucket, the object is assigned the bucket's storage class, unless you specify a storage class for the object. You can change the default storage class of a bucket but you can't change the location type from regional to multi-region/dual-region or vice versa. You can also change the storage class of an object that already exists in your bucket without moving the object to a different bucket or changing the URL to the object. Setting a per-object storage class is useful, for example, if you have objects in your bucket that you want to keep, but that you don't expect to access frequently. In this case, you can minimize costs by changing the storage class of those specific objects to Nearline, Coldline or Archive Storage. In order to help manage the classes of objects in your bucket, Cloud Storage offers Object Lifecycle Management. More on that later. Let’s look at access control for your objects and buckets that are part of a project. We can use IAM for the project to control which individual user or service account can see the bucket, list the objects in the bucket, view the names of the objects in the bucket, or create new buckets. For most purposes, IAM is sufficient, and roles are inherited from project to bucket to object. Access control lists or ACLs offer finer control. For even more detailed control, signed URLs provide a cryptographic key that gives time-limited access to a bucket or object. Finally, a signed policy document further refines the control by determining what kind of file can be uploaded by someone with a signed URL. Let’s take a closer look at ACLs and signed URLs. An ACL is a mechanism you use to define who has access to your buckets and objects, as well as what the level of access is they have. The maximum number of ACL entries you can create for a bucket or object is 100. Each ACL consists of one or more entries, and these entries consist of two pieces of information: A scope, which defines who can perform the specified actions (for example, a specific user or group of users). And a permission, which defines what actions can be performed (for example, read or write). The allUsers identifier listed on this slide represents anyone who is on the internet, with or without a Google account. The allAuthenticatedUsers identifier, in contrast, represents anyone who is authenticated with a Google account. For more information on ACLs, refer to the links of this video. For some applications, it is easier and more efficient to grant limited-time access tokens that can be used by any user, instead of using account-based authentication for controlling resource access. (For example, when you don’t want to require users to have a Google account). Signed URLs allow you to do this for Cloud Storage. You create a URL that grants read or write access to a specific Cloud Storage resource and specifies when the access expires. That URL is signed using a private key associated with a service account. When the request is received, Cloud Storage can verify that the access-granting URL was issued on behalf of a trusted security principal, in this case the service account, and delegates its trust of that account to the holder of the URL. After you give out the signed URL, it is out of your control. So you want the signed URL to expire after some reasonable amount of time.

#### Cloud Storage Features

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548403

There are also several features that come with Cloud Storage. We will cover these at a high-level for now because we will soon dive deeper into some of them. Earlier in the course series, we already talked a little about Customer-supplied encryption keys when attaching persistent disks to virtual machines. This allows you to supply your own encryption keys instead of the Google-managed keys, which is also available for Cloud Storage. Cloud Storage also provides Object Lifecycle Management which lets you automatically delete or archive objects. Another feature is object versioning which allows you to maintain multiple versions of objects in your bucket. You are charged for the versions as if they were multiple files, which is something to keep in mind. Cloud Storage also offers directory synchronization so that you can sync a VM directory with a bucket. Object change notifications can be configured for Cloud Storage using Pub/Sub. When enabled, Autoclass manages all aspects of storage classes for a bucket. We will discuss this later. In Cloud Storage, objects are immutable, which means that an uploaded object cannot change throughout its storage lifetime. To support the retrieval of objects that are deleted or overwritten, Cloud Storage offers the Object Versioning feature. Object Versioning can be enabled for a bucket. Once enabled, Cloud Storage creates an archived version of an object each time the live version of the object is overwritten or deleted. The archived version retains the name of the object but is uniquely identified by a generation number as illustrated on this slide by g1. When Object Versioning is enabled, you can list archived versions of an object, restore the live version of an object to an older state, or permanently delete an archived version, as needed. You can turn versioning on or off for a bucket at any time. Turning versioning off leaves existing object versions in place and causes the bucket to stop accumulating new archived object versions. Google recommends that you use Soft Delete instead of Object Versioning to protect against permanent data loss from accidental or malicious deletions. A link to the Object Versioning documentation can be found in the Course Resources for this module. Soft Delete provides default bucket-level protection for your data from accidental or malicious deletion by preserving all recently deleted objects for a specified period of time. The objects stored in Cloud Storage buckets are immutable. If you overwrite or change the data of an object, Cloud Storage deletes its earlier version and replaces it with a new one. Soft Delete retains all these deleted objects, whether from a delete command or because of an overwrite, essentially capturing all changes made to bucket data for the configured retention duration. When you create a Cloud Storage bucket, the Soft Delete feature is enabled by default with a retention duration of seven days. During the retention duration, you can restore deleted objects, but after the duration ends, Cloud Storage permanently deletes the objects. By updating the bucket's configuration, you can increase the retention duration to 90 days or disable it by setting the retention duration to 0. A link to the Soft Delete documentation can be found in the Course Resources for this module. To support common use cases like setting a Time to Live for objects, archiving older versions of objects, or "downgrading" storage classes of objects to help manage costs, Cloud Storage offers Object Lifecycle Management. You can assign a lifecycle management configuration to a bucket. The configuration is a set of rules that apply to all the objects in the bucket. So when an object meets the criteria of one of the rules, Cloud Storage automatically performs a specified action on the object. Here are some example use cases: First, downgrade the storage class of objects older than a year to Coldline Storage. Second, delete objects created before a specific date. For example, January 1, 2017. Or third, keep only the 3 most recent versions of each object in a bucket with versioning enabled. Object inspection occurs in asynchronous batches, so rules may not be applied immediately. Also, updates to your lifecycle configuration may take up to 24 hours to go into effect. This means that when you change your lifecycle configuration, Object Lifecycle Management may still perform actions based on the old configuration for up to 24 hours. So keep that in mind. A link to the Object Lifecycle Management documentation can be found in the Course Resources for this module. The Object Retention Lock feature lets you set retention configuration on objects within Cloud Storage buckets that have enabled the feature. A retention configuration governs how long the object must be retained and has the option to permanently prevent the retention time from being reduced or removed. This helps you meet data retention regulatory and compliance requirements, such as those associated with FINRA, SEC, and CFTC. This also helps provide Google Cloud immutable storage solutions with leading enterprise backup software vendor partners. A link to the Object Retention Lock documentation can be found in the Course Resources for this module. The Cloud Console allows you to upload individual files to your bucket. But what if you have to upload terabytes or even petabytes of data? There are three services that address this: Transfer Appliance, Storage Transfer Service, and Offline Media Import. Transfer Appliance is a hardware appliance you can use to securely migrate large volumes of data (from hundreds of terabytes up to 1 petabyte) to Google Cloud without disrupting business operations. The images on this slide are transfer appliances. The Storage Transfer Service enables high-performance imports of online data. That data source can be another Cloud Storage bucket, an Amazon S3 bucket, or an HTTP/HTTPS location. Finally, Offline Media Import is a third party service where physical media (such as storage arrays, hard disk drives, tapes, and USB flash drives) is sent to a provider who uploads the data. For more information on these three services, refer to the Course Resources. When you upload an object to Cloud Storage and you receive a success response, the object is immediately available for download and metadata operations from any location where Google offers service. This is true whether you create a new object or overwrite an existing object. Because uploads are strongly consistent, you will never receive a 404 Not Found response or stale data for a read-after-write or read-after-metadata-update operation. Strong global consistency also extends to deletion operations on objects. If a deletion request succeeds, an immediate attempt to download the object or its metadata will result in a 404 Not Found status code. You get the 404 error because the object no longer exists after the delete operation succeeds. Bucket listing is strongly consistent. For example, if you create a bucket, then immediately perform a list buckets operation, the new bucket appears in the returned list of buckets. Finally, object listing is also strongly consistent. For example, if you upload an object to a bucket and then immediately perform a list objects operation, the new object appears in the returned list of objects.

#### Choosing a storage class

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548404

Let’s explore the decision tree to help you find the appropriate storage class in Cloud Storage. If you will read your data less than once a year, you should consider using Archive storage. If you will read your data less than once per 90 days, you should consider using Coldline storage. If you will read your data less than once per 30 days, you should consider using Nearline storage. And if you will be doing reads and writes more often than that, you should consider using Standard storage. You also want to take into account the location type: Use a region to help optimize latency and network bandwidth for data consumers, such as analytics pipelines, that are grouped in the same region. Use a dual-region when you want similar performance advantages as regions, but also want the higher availability that comes with being geo-redundant. Use a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed across large geographic areas, or when you want the higher data availability that comes with being geo-redundant. If your data has a variety of access frequencies, or the access patterns for your data are unknown or unpredictable, you should consider Autoclass. The Autoclass feature automatically transitions objects in your bucket to appropriate storage classes based on the access pattern of each object. Even if a different storage class is specified in the request, all objects added to the bucket begin in Standard storage. The feature moves data that is not accessed to colder storage classes to reduce storage cost. Data that is accessed is also moved to Standard storage to optimize future accesses. When object data is read, the object transitions to Standard storage if it's not already stored in Standard storage. Autoclass simplifies and automates cost saving for your Cloud Storage data. When enabled on a bucket, there are no early deletion charges, no retrieval charges, and no charges for storage class transitions. For more information, view the storage classes documentation. So far we have only considered unstructured data. Before we look at unstructured data, let's explore a high-performance, fully managed file storage offering; Filestore.

#### Filestore

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548405

Filestore is a managed file storage service for applications that require a file system interface and a shared file system for data. Filestore gives users a simple native experience for standing up managed network attached storage with either Compute Engine or Google Kubernetes Engine instances. The ability to fine tune Filestore's performance and capacity independently, leads to predictably fast performance for your file-based workloads. Filestore offers native compatibility with existing enterprise applications and supports any NFSV3 compatible clients. Applications gain the benefit of features such as scale-out performance, hundreds of terabytes of capacity, and file locking without the need to install or maintain any specialized plug-ins or client-side software. Filestore has many use cases. Using Filestore, you can expedite migration of enterprise applications. Many on-premises applications require a file system interface to data. As these applications continue to migrate to the Cloud, Filestore can support a broad range of enterprise applications that need a shared file system. For media rendering, you can easily meant filestore file shares on Compute Engine instances, enabling visual effects artists to collaborate on the same file share. As rendering workflows typically run across fleets of Compute Machines, all of which meant a shared file system. Filestore and Compute Engine can scale to meet your jobs rendering needs. Electronic Design Automation, or EDA, is all about data management. It requires the ability to batch workloads across thousands of cores and has a large memory needs. Filestore offers the necessary capacity and scale to meet the needs of manufacturing customers doing intensive EDA, and also make sure that files are universally accessible. Data analytics workloads include Compute complex financial models or analysis of environmental data. These workloads are latency sensitive. Filestore offers low latency for file operations, and as capacity or performance needs change, you can easily grow or shrink your instances as needed. As a persistent and shareable storage layer, Filestore enables immediate access to data for high-performance, smart analytics without the need to lose valuable time on loading an offloading data to clients drives. Genome sequencing requires an incredible amount of raw data in the order of billions of data points per person. This type of analysis requires speed, scalability, and security. Filestore meets the needs of companies and research institutions performing scientific research while also offering predictable prices for the performance. Web developers and large hosting providers also rely on Filestore to manage and serve web content, including needs such as WordPress hosting.

#### Lab Intro: Cloud Storage

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548406

Let's take some of the Cloud Storage concepts that we just discussed and apply them in a lab. In this lab, you'll create buckets and perform many of the advanced options available in Cloud Storage. You'll set access control lists to limit who can have access to your data and what they're allowed to do with it. You'll use the ability to supply and manage your own encryption keys for additional security. You'll enable object versioning to track changes in the data and you'll configure lifecycle management, so that objects are automatically archived or deleted after a specified period. Finally, you'll use the directory synchronization feature.

#### Cloud Storage

- https://www.cloudskillsboost.google/paths/11/course_templates/49/labs/548407

#### Lab Review: Cloud Storage

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548408

In this lab, you learn to create and work with buckets and objects, and apply the following Cloud Storage features: Customer-supplied encryption keys, Access control lists, Lifecycle management, Object versioning, Directory synchronization, and cross-project resource sharing using IAM. Now that you're familiar with many of the advanced features of Cloud Storage, you might consider using them in a variety of applications that you might not have previously considered. A common, quick, and easy way to start using Google Cloud is to use Cloud Storage as a backup service. You can stay for a lab walkthrough, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. Welcome to the walk-through of the Cloud Storage Lab. At this point, I've already started the lab in Qwiklabs and I am logged into the GCP console using the username and password that was provided by Qwiklabs for me to log in to the GCP console. So the first task is preparation. I'm going to create a bucket in here. When I go to create a bucket, it specifically tells me that I should be using a globally unique ID. So I'm going to use my project ID, which is pretty unique. I'm going to call it myproj- and then my project ID, and it's telling us multi-regional. So storage class is multi-regional, and then it's telling me access control is set object-level and bucket-level permissions, and I'm going to hit create. So at this point, you can now go back to the lab page, and you can hit check my progress, and you should get a check mark in five points that you created the Cloud Storage Bucket. Next step is downloading a file. So I'm going to start Cloud Shell so I can do the curl command, and the first thing I'm going to do is I'm going to set an environment variable to the bucket name of the bucket I just created, just for ease of copy paste of commands. Export bucket name one equals and the bucket name. If I want to verify that that worked, I'm going to do an echo dollar sign, and the variable name to make sure that it got set correctly, and there it is. So now I'm going to download a file, which is just a publicly available Hadoop documentation, HTML file, and if I do an ls, I can see there's my setup.html and I am now going to copy it a couple times to make a setup two and a setup three. If I do an ls, I should see three files. There they are. So the second task is ACLs. We're going to copy this file into the bucket and then configure the access control list for it. So the first one is gsutil command, where I am copying setup.html into my bucket. Once it's copied, I then want to get the default access list that has been assigned to setup.html, which is based on the bucket because that's how we set it. Then right here, I piped it into acl.txt, and now I'm going to cut that, and we can see all of the permissions that had been assigned. So now I want to set the permissions to private. So I'm going to set it to private, and then in order to see it, I'm going to pipe it into acltwo.txt, and then cut that file, and you can see it's now set to private. Update the access list to make the file publicly readable by running the following command, and then I'm going to pipe it into aclthree, so that I can verify what that looks like. You can see it is readable by all users. This is another check point in a lab where you can hit check my progress and in this case is checking if you properly made that file publicly readable. So now I'm going to verify in my bucket using the console that my file is there and that is publicly viewable, and you can tell that based on this little icon and the public link that says that it's accessible to the public. So now in Cloud Shell, I'm going to remove the setup.html in my local Cloud Shell Instance. There it is. Let me remove it from the search here. If I do an ls, I'll see setup two and setup three, but not setup. You can see it got deleted. Let's say I accidentally deleted that from my Cloud Shell Instance, but now I want the copy that was in the bucket back on my local Cloud Shell. So I could just copy from the bucket to my local Cloud Shell, and if I do an ls again, I'll see all three setup files. There they are. The third task is to generate a customer supplied encryption key. To create the key, I'm going to run this command, and that's going to give me some output, and then I can copy this. But first, I'm going to see if I have a boto file. I'm going to do ls-al, and I do not see a boto file. So what I'm going to do is I'm going to run gsutilconfig-n, and then I'm going to do ls-al, and I should now see a boto file. There it is. So I'm going to do a nano.boto, and then I'm going to find the encryption key field, which I'm going to exit back out because I didn't not copy the key that I created, which I need. That is right here. Let me copy that, and let me go back to nano, and let me find the line with encryption underscore key. Could need to expand this because it's very hard to see. See decryption key here is encryption key. I'm going to uncomment this, and then I'm going to paste in my key here. I'm going to press control l, write that file, and then control x to exit nano. So now that I've set that up, I am going to upload the remaining setup two and setup three into the bucket. There's one, and there's the other. Now back in the console, let's go down, I'm going to refresh the bucket. I can see both of these files, and it shows that they are encrypted by a customer supply key. So this is another opportunity to check my progress and make sure I got the points for doing that step. Now what I'm going to do is I am deleting my local files by running remove setup star. So it's going to delete setup, setup two, and setup three. Now I am going to copy the files down from the bucket again, and if I want to cut the encrypted files to see whether I need them back, you can see there they are, and I successfully was able to bring them back even though they're encrypted. So now I'm going to move the current customer supplied encryption key to the decrypt key. So let's go to nano.boto. I'm going to find the comment out the line that I added earlier. I should've noted the line number, so that I wouldn't have to find it again. Decrypt keys in the GSUtil section. Let's see. I think I'm close. I'm looking for that line. So I'm going to comment out encryption key line and uncomment decryption key one right there. Then, I'm going to copy this into decryption key one, and then we save x. So a best practices you would actually delete the old customer key from the encryption line. But in this case, we just copy pasted it. So it's not a big deal. So I'm going generate a new key and then I'm going back to boto files. So I am going to add a new encryption key line, make sure that I copied the new key I made, and then do the same thing again. Sparsed it so I am adding a new encryption key equals, and I'll paste in the new key. Then control O to save, control X to exit. Now, I'm going to rewrite the key for file 1, and comment out the old decrypt key. Again, to bottom. Then I am going to comment out the decryption key 1. Now, while the instructions have you using nano, you definitely could use the Cloud Shell editor as well. That might be a little more pleasant than using this tool, but I'll leave it to you. You would just access that by hitting this little pencil here. It's fine. Decryption key 1 real quick. So we're commenting that out. Then, we're going to save it, and click. Now, we're going to download setup 2, and download setup 3. What happened, no decryption key matches because we commented it out, which makes sense. So the last task in this lab is, we are going to run the following command to view the current life-cycle policy. So we're going to do this. It says it has no life-cycle configuration. So I'm going to create a JSON lifecycle policy file. I'm going to paste the following rule in here. So it's saying if it's over 31 days I'm going to delete it. Writing exit. Then, to set the policy I'm going to run the command provided in the box, and to verify that the policy worked, I'm going to press that. This is another opportunity for you to check your progress and get more points in the lab. This point you should have about 20 out of 35 points. The task 6 is enabling versioning and you can do that by using the following command. Says it suspended, which means it's not enabled. So if we want to enable versioning, we're going to run this command. Then if we were to run the Get-Command again, we would not see that it was suspended we would say that it was enabled. There it is. So check your progress again you'll get more points. The next step, we're going to create several versions of the sample file in the bucket. So I'm going do an ls here. Going to open the setup HTML file. Delete any five lines to change the size. So I'm going to comment out this link and then I'm going to delete all of these links. Probably a faster way to do this thing just holding down delete. That's what I'm doing here. I'm going to delete it all the way to the banner. So I have now effectively changed the size of the file. So I'm going to control O, enter, control X. I'm going to copy the file to the bucket. I'm going to go back to setup.html, delete another five lines. Let's delete some more links. I'm just going to delete up to here. I'm going to save it. Then I'm going to copy it again. So if I wanted to list all versions of the file, which each subsequent one I was deleting different lines and making the size smaller, I was creating a new version. You can see there are three versions: the original one, the one where I deleted the first five lines, and then the one where I deleted the next set of lines. So I am now going to store the version value in the environment variables. So I'm going to say, export version name equals, the oldest version is this one. I'm going to copy that. I'm going to set this variable here make sure it got set correctly, and it is set correctly. Now, I'm going to download the oldest version, call it recovery.text. Then I'm going to verify recovery with a couple of commands. It is saying, c ls setup.html. Looks like that piece didn't work. I think what I did was I set the version name to the wrong thing, it should have been here. So now I can do the Gsutil again, and it still didn't match. So you do this, it's because you didn't follow instructions like me. You should have copied the entire URL for that object. Usually, what happens with the lab is if you have an issue is usually not that the lab is broken. It's usually that you missed a step. So go back three steps and repeat, and that usually works, because you can see here that just worked. Ls.al Setup.html, there's the file. I want to see the recovered text. You can see that the size is different here. So task 7, we're going to synchronize a directory to a bucket and just copy these in. Then I'm going to sync the first-level directory on the VM with my bucket. I'm going to verify that versioning was enabled. How I can check in the browser, I'm going to refresh the bucket, and we go back here, first level. You can see there's a second level. We can see the same thing in the console as we do in the command line. So I can exit Cloud Shell. So now we're going to do some cross-project sharing, this is the last little piece of this lab. So I'm going to open another tab. I'm also going to go to console.cloud.google.com, and I am now signed in, I'm going to select the other project. This one I have 26. I am going to copy the project from the Qwiklabs site, I'm here at lab guide, and I'm going to select that project. This is my other project. Then I am going to now create a bucket for this project. There shouldn't be one in here because it's a new project. I'm going to also call it myproj in project ID, and Create. This will now be bucket name two. So I'm going to upload a file, any file. I've uploaded a screenshot, and this will be my file name. So I'm actually going to rename it. I can't. Now I'm going to go to IAM, service accounts, and then create a service account. I'm going to call it cross-project-storage, and click create. Then I'm going to give it the Storage Object Viewer. Click continue. I'm going to create a key. I'm going to select JSON, create, and then it's going to download that file for me. It's there. Hit close, and I can hit done. So I am now going to rename this credentials.json. Here it is. I'm going to switch back to the other project, check my progress, and I should get five more points. So now we are just five points away from finishing the lab. Now we are in project ID one, and we're going to create a VM, create. Calling it crossproject. I'm going to make it in Europe in D, and I'm making it a micro, and create. VM is ready, I'm going to SSH into it. There it is, click SSH, then move my window back here to get the bucket name of the project I created here. I'm going to verify that it worked, and then I'm going to export the file name of the file that I uploaded. Grab that, put quotes around there because that space is in it. Verify that worked, and there it is. LS what's in that bucket. Form a VM on this side, it tells me that I don't have access to do that, so now I'm going to verify that. I am going to upload here, upload file. I'm going to select the credentials.json that I downloaded. Close, then I'm going to authorize that file to verify access. I'm going to do this again, and now I can see my file in there. I can do it with the file as well. Let me try to copy these credentials so they don't have access to that project. So if I wanted to do that, I would go back to this project, and modify the role in IAM. It Should be my last step here, going back to IAM, cross-project-storage, pencil. I'm also going to give Storage Object Admin, save. Once I hit save, I can check my progress, and then you will have all of the points in the lab. The last step is optional, you're just going to return to your SSH terminal, and verify that everything is good to go but that is the entire walkthrough for this lab. I hope you enjoyed it.

#### Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548409

Let’s dive into the structured or relational database services. First up is Cloud SQL. Why would you use a Google Cloud service for SQL, when you can install a SQL Server application image on a VM using Compute Engine? The question really is, should you build your own database solution or use a managed service? There are benefits to using a managed service, so let’s learn about why you’d use Cloud SQL as a managed service inside of Google Cloud. Cloud SQL is a fully managed service of either MySQL, PostgreSQL, or Microsoft SQL Server databases. This means that patches and updates are automatically applied, but you still have to administer MySQL users with the native authentication tools that come with these databases. Cloud SQL supports many clients, such as Cloud Shell, App Engine, and Google Workspace scripts. It also supports other applications and tools that you might be used to like SQL Workbench, Toad, and other external applications using standard MySQL drivers. Cloud SQL delivers high performance and scalability with up to 64 TB of storage capacity, 60,000 IOPS, and 624 GB of RAM per instance. You can easily scale up to 96 processor cores and scale out with read replicas. Currently, you can use Cloud SQL with either MySQL 5.6, 5.7, or 8.0, PostgreSQL 9.6, 10, 11, 12, 13, 14, or 15, or either of the Web, Express, Standard, or Enterprise SQL Server 2017 or 2019 editions. Let’s focus on some other services provided by Cloud SQL. In High Availability configuration, within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the persistent disk is attached to the standby instance, and it becomes the new primary instance. Users are then rerouted to the new primary. This process is called a failover. Cloud SQL also provides automated and on-demand backups with point-in-time recovery. You can import and export databases using mysqldump, or import and export CSV files. Cloud SQL can also scale up, which does require a machine restart or scale out using read replicas. That being said, if you are concerned about horizontal scalability, you’ll want to consider Spanner, which we’ll cover later in this module. Choosing a connection type to your Cloud SQL instance will affect how secure, performant, and automated it will be. If you’re connecting an application that is hosted within the same Google Cloud project as your Cloud SQL instance, and it’s collocated in the same region, choosing the Private IP connection will provide you with the most performant and secure connection using private connectivity. In other words, traffic is never exposed to the public internet. Note that connecting to the Cloud SQL Private IP address from VMs in the same region is only a performance-based recommendation and not a requirement. If the application is hosted in another region or project, or if you’re trying to connect to your Cloud SQL instance from outside of Google Cloud, you have 3 options. In this case, I recommend using the Cloud SQL Auth Proxy, which handles authentication, encryption, and key rotation for you. If you need manual control over the SSL connection, you can generate and periodically rotate the certificates yourself. Otherwise, you can use an unencrypted connection by authorizing a specific IP address to connect to your SQL server over its external IP address. You will explore these options in an upcoming lab. To summarize, let’s explore this decision tree to help you find the right data storage service with full relational capability. Memorystore provides a fully-managed in-memory data store service for workloads requiring microsecond response times, or that have large spikes in traffic, as seen in gaming environments and real-time analytics. If you don’t need an in-memory data store, but your use case is relational data used primarily for analytics, these workloads are best supported by BigQuery. However, if your relational data workload isn’t analytics, the choice lies between Spanner and Cloud SQL. If you don’t need horizontal scaling or a globally available system, Cloud SQL is a cost-effective solution.

#### Lab Intro: Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548410

Let's take some of the Cloud SQL concepts that we just discussed, and apply them in a lab. In this lab, you configure a Cloud SQL Server and learn how to connect an application to it via a proxy over an external connection. You also configure a connection over a private IP link that offers performance and security benefits. The app we chose to demonstrate in this lab is WordPress, but the information and best practices are applicable to any application that needs a SQL Server. By the end of this lab, you will have two working instances of a WordPress front end connected over two different connection types to its SQL instance back end, as shown in this diagram.

#### Implementing Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/49/labs/548411

#### Lab Review: Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548412

In this lab, you created a Cloud SQL database and configured it to use both an external connection over a secure proxy and a private IP address, which is more secure and performant. If your application is hosted in another region, VPC, or even project, use a proxy to secure its connection over the external connection. You can stay for a lab walkthrough, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. Welcome to the lab walkthrough for implementing Cloud SQL. At this point in the lab, I have logged in with the username and password that QwikLabs has provided me. My first task is to create a Cloud SQL database. And go here to SQL, And hit Create Instance. I'm going to choose my SQL, And I'm going to call this wordpress-db. For password I'm just going to use the word password, so that I make sure I'm not forgetting it. I recommend you use something very simple. I'm going to use US Central 1. I am going to expand the configuration options, and then in Connectivity I'm going to select the private IP. Hit Enable API, and once that's been enabled, which could take a couple of seconds, I'm going to hit Allocate and Connect. And just so you know, this could take three to five minutes, so just be patient. Once that's done, it is going to make this Create button enabled. Feel free to look through some of the other things that it's calling out in the lab, like configuring the machine type and changing the storage capacity. If you add a couple of zeros, you can see the throughput increases. Set it back to 10, hit Close here. Again, this could take three to five minutes, so be patient. Once it's done, it'll say Create here. All right, so my IP has been allocated, and now I can hit Create. Here we go. And now, that took a while, but creating your Cloud SQL instance or Cloud SQL instance, might take even longer. So be patient, but while this is creating I can do other steps in the lab. The verification in step 15 is going to require that your Cloud SQL instance is running, and that there is a green check here. So you won't be able to get that step and those five points until this is done, but you can do some of the other steps while we wait. So while this is going, I'm going to open another tab so that this is still running and I can check on it. And I am now going to go to Compute Engine, which I could either go here, or this tile right here has my Compute Engine instances. As you can see, two have been created for me. wordpress-europe-proxy, which is the proxy for my Cloud SQL instance and for the private IP instance. So for this one, I'm going to click SSH. And when that is ready, to SSH into I am going to download the Cloud SQL proxy and then I'm going to make it executable. So I'm going to do a wget, Enter. It's copied it and made it executable. So in order to start the proxy, you need the connection name of the Cloud SQL instance, which requires that it's actually running. So we'll go ahead, and go back here, and see what the status is. I'm going to refresh and see if anything is there. Now it's not there, but it does let me click on it. And let's see if the connection name is there, and it is here. Instance connection name, so I'm going to copy that, and I'm going to go back to my SSH window. And I am going to create an environment variable for that connection instance. So I'm going to do an export SQL_CONNECTION =, paste that in there. And if I want to verify that that environment variable is set, I'm going to do an echo SQL, oops, spell it right. And it should output that, and there it is. So I'm not going to get too far ahead, because the rest of the steps do require that my Cloud SQL instance is running. You can see it's still creating, so it is not going to let me create a database yet. So I'm just going to leave it on this screen, so that when it is completely done it'll let me create a database, which I need for the next step. All right, at this point my instance is running. So if I go to Cloud SQL, it'll have a green checkmark next to wordpress-db. Right here, it says it's runnable. So the step I was missing is I need to create a database, and I am going to create a database called wordpress, because that's what the application expects, and I'm going to hit Create. And now I am going to return to, My SSH window, and I'm just going to make sure that it still has my environment variable. Then I am going to activate the proxy connection to my SQL database. By running this, I'll run it in the background. And then I'm going to expect that it says Ready for new connections, which it has output. I'm going to press Enter. And this is a point in the lab where you can also hit Check my Progress. And at this point you should have all ten points in the lab, but we're still going to do one more step, which is task 3. Actually, task 3 and task 4, where we're going to connect the application to the Cloud SQL instance. So I'm going to configure the wordpress application. So I'm going to copy the curl command. Go ahead and run that, and it is going to output the external IP address for my virtual machine. I'm going to copy that, and open it here, And then I'm going to hit Let's Go. I am going to leave everything with default, except I'm going to change the username to root and I'm going to put in the password that I defined, which is password. And then for database host I am going to use a localhost IP, which is 127.0.0.1, and then I'm going to hit Submit. Now, when the connection has been made, it is going to let me install WordPress, and I'm going to click Run the Installation. This could take a few moments to complete. Once this is done I should get a success window, and this can take up to three minutes depending on where you are running your lab from. So here it goes, apparently it's very fast for me. And so once the connection has been made, I am going to go here, and I am going to remove everything passed to the external IP. Delete, hit Enter. And when this loads, I should be able to see my blog. So it looks like it's still installing, so I will go ahead and be patient. Add some Information that I don't need to remember. So this is My Fake Site title. Any username, leave that password it gave me, m@b.com, and Install. And this was actually a step in the lab that I ignored, which was step 7, so don't do what I did and skip a step. So I had a successful installation. So what I'm going to do here is I'm going to remove all of the information that's after the external IP. When I hit Enter, it should take me to my blog. And Hello world! This is my blog, success. So the last task is to connect to Cloud SQL via the internal IP. So I'm going to go back here, and go to SQL and gcp. I'm going to click on wordpress-db, and then I am going to note the private IP address here. And I'm actually going to note it on a note, because pretty sure that it's going to have me copy something else. So make sure you copy it down somewhere in a clipboard, and then I'm going to go to Compute Engine. And it is going to want me to copy the external IP address for WordPress private IP. I'm going to copy that, and paste it in a new tab, press Enter. I'm going to hit Let's Go, and then the database name I'm going to leave alone, and I'm going to change this to root, and leave password because that's what I put before. Then I'm going to put the SQL private IP that I copied earlier to my Notepad, just need to find it. It's here, I'm going to copy that in here and hit Submit. And then I'm going to hit Run the Installation, and I should get Already Installed. So I created a direct connection to a private IP instead of configuring a proxy, and that connection is private. If I remove here, the same private IP should get my blog, and there it is. So in review, we created a Cloud SQL database and we configured it to use an external connection over a secure proxy as well as a private IP address. Hope you enjoyed the lab, thanks for watching.

#### Spanner

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548413

If Cloud SQL does not fit your requirements because you need horizontal scalability, consider using Spanner. Spanner is a service built for the cloud, specifically to combine the benefits of relational database structure with non-relational horizontal scale. This service can provide petabytes of capacity and offers transactional consistency at global scale, schemas, SQL, and automatic synchronous replication for high availability. Use cases include financial applications and inventory applications, traditionally served by relational database technology. Depending on whether you create a multi-regional or regional instance, you'll have different monthly uptime SLAs. For up-to-date numbers, you should always refer to the documentation. Let's compare Spanner with both relational and non-relational databases. Like a relational database, Spanner has schema, SQL, and strong consistency. Also, like a non-relational database, Spanner offers high availability, horizontal scalability, and configurable replication. As mentioned, Spanner offers the best of the relational and non-relational worlds. These features allow for mission-critical use cases, such as building consistent systems for transactions and inventory management in the financial services and retail industries. To better understand how all of this works, let's look at the architecture of Spanner. A Spanner instance replicates data in N cloud zones, which can be within one region or across several regions. The database placement is configurable, meaning you can choose which region to put your database in. This architecture allows for high availability and global placement. The replication of data will be synchronized across zones using Google's global fiber network. Using atomic clocks ensures atomicity whenever you are updating your data. That's as far as we're going to go with Spanner. Because the focus of this module is to understand the circumstances when you would use Spanner, let's look at a decision tree. If you’ve outgrown any relational database, are sharding your databases for throughput high performance, need transactional consistency, global data and strong consistency, or just want to consolidate your database, consider using Spanner. If you don't need any of these, nor full relational capabilities, consider a NoSQL service, such as Firestore, which we will cover next. If you're now convinced that using Spanner as a managed service is better than using or re-implementing your existing MySQL solution, refer to the documentation for a solution on how to migrate from MySQL to Spanner .

#### AlloyDB

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548414

Let’s now talk about AlloyDB. AlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service that’s designed for demanding workloads such as hybrid transactional and analytical processing. AlloyDB pairs a Google-built database engine with a cloud-based, multi-node architecture to deliver enterprise-grade performance, reliability, and availability. AlloyDB automates administrative tasks, such as backups, replication, patching, and capacity management. AlloyDB also uses adaptive algorithms and machine learning for PostgreSQL vacuum management, storage and memory management, data tiering, and analytics acceleration. AlloyDB provides fast transactional processing, more than 4 times faster than standard PostgreSQL for transactional workloads. It's suitable for demanding enterprise workloads, including workloads that require high transaction throughput, large data sizes, or multiple read replicas. AlloyDB provides high-availability and a 99.99% uptime SLA, inclusive of maintenance. AlloyDB also provides real-time business insights and is up to 100 times faster than standard PostgreSQL for analytical queries. Built-in integration with Vertex AI, Google's artificial intelligence platform, lets you call machine learning models.

#### Firestore

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548415

If you’re looking for a highly-scalable, NoSQL database for your applications, consider using Firestore. Firestore is a fast, fully managed, serverless, cloud-native, NoSQL, document database that simplifies storing, syncing and querying data for your mobile, web, and IoT apps at global scale. Its client libraries provide live synchronization and offline support, and it's security features and integrations with Firebase and Google Cloud accelerate building truly serverless apps. Firestore also supports ACID transactions, so if any of the operations in the transaction fail and cannot be retried, the whole transaction will fail. Also, with automatic multi-region replication and strong consistency, your data is safe and available even when disasters strike. Firestore even allows you to run sophisticated queries against your NoSQL data without any degradation in performance. This gives you more flexibility in the way you structure your data. Firestore is actually the next generation of Datastore. Firestore can operate in Datastore mode, making it backwards-compatible with Datastore. By creating a Firestore database in Datastore mode, you can access Firestore's improved storage layer while keeping Datastore system behavior. This removes the following Datastore limitations: Queries are no longer eventually consistent; instead, they are all strongly consistent. Transactions are no longer limited to 25 entity groups. And writes to an entity group are no longer limited to 1 per second. Firestore in Native mode introduces new features such as: A new, strongly consistent storage layer. A collection and document data model. Real-time updates. And mobile and web client libraries. Firestore is backward-compatible with Datastore, but the new data model, real-time updates, and mobile and web client library features are not. To access all of the new Firestore features, you must use Firestore in Native mode. A general guideline is to use Firestore in Datastore mode for new server projects, and Native mode for new mobile and web apps. As the next generation of Datastore, Firestore is compatible with all Datastore APIs and client libraries. For more information, refer to the documentation. To summarize, let's explore this decision tree to help you determine whether Firestore is the right storage service for your data. If your schema might change and you need an adaptable database, you need to scale to zero, or you want low maintenance overhead scaling up to terabytes, consider using Firestore. Also, if you don't require transactional consistency, you might want to consider Bigtable, depending on the cost or size. Bigtable is covered next.

#### Bigtable

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548416

If you don’t require transactional consistency, you might want to consider Bigtable. Bigtable is a fully managed NoSQL database with petabyte-scale and very low latency. It seamlessly scales for throughput and it learns to adjust to specific access patterns. Bigtable is actually the same database that powers many of Google’s core services, including Search, Analytics, Maps, and Gmail. Bigtable is a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis, because it supports high read and write throughput at low latency. It’s also a great storage engine for machine learning applications. Bigtable integrates easily with popular big data tools like Hadoop, Dataflow, and Dataproc. Plus, Bigtable supports the open source industry standard HBase API, which makes it easy for your development teams to get started. Dataflow and Dataproc are covered later in the course series. For more information on the HBase API, refer to the documentation Bigtable stores data in massively scalable tables, each of which is a sorted key/value map. The table is composed of rows, each of which typically describes a single entity, and columns, which contain individual values for each row. Each row is indexed by a single row key, and columns that are related to one another are typically grouped together into a column family. Each column is identified by a combination of the column family and a column qualifier, which is a unique name within the column family. Each row/column intersection can contain multiple cells, or versions, at different timestamps, providing a record of how the stored data has been altered over time. Bigtable tables are sparse. If a cell does not contain any data, it does not take up any space. The example shown here is for a hypothetical social network for United States presidents, where each president can follow posts from other presidents. Let me highlight some things: The table contains one column family, the “follows” family. This family contains multiple column qualifiers. Column qualifiers are used as data. This design choice takes advantage of the sparseness of Bigtable tables, and the fact that new column qualifiers can be added as your data changes. The username is used as the row key. Assuming usernames are evenly spread across the alphabet, data access will be reasonably uniform across the entire table. This diagram shows a simplified version of Bigtable’s overall architecture. It illustrates that processing, which is done through a frontend server pool and nodes, is handled separately from the storage. A Bigtable table is sharded into blocks of contiguous rows, called tablets, to help balance the workload of queries. Tablets are similar to HBase regions, for those of you who have used the HBase API. Tablets are stored on Colossus, which is Google's file system, in SSTable format. An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. As mentioned earlier, Bigtable learns to adjust to specific access patterns. If a certain Bigtable node is frequently accessing a certain subset of data, Bigtable will update the indexes so that other nodes can distribute that workload evenly, as shown here. That throughput scales linearly, so for every single node that you do add, you're going to see a linear scale of throughput performance, up to hundreds of nodes. In summary, if you need to store more than 1 TB of structured data, have very high volume of writes, need read/write latency of less than 10 milliseconds along with strong consistency, or need a storage service that is compatible with the HBase API, consider using Bigtable. If you don’t need any of these and are looking for a storage service that scales down well, consider using Firestore. Speaking of scaling, the smallest Bigtable cluster you can create has three nodes and can handle 30,000 operations per second. Remember that you pay for those nodes while they are operational, whether your application is using them or not.

#### Memorystore

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548417

Let me give you a quick overview of Memorystore. Memorystore for Redis provides a fully managed in-memory datastore service built on scalable, secure, and highly available infrastructure managed by Google. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments. This allows you to spend more time writing code so that you can focus on building great apps. Memorystore also automates complex tasks like enabling high availability, failover, patching, and monitoring. High availability instances are replicated across two zones and provide a 99.9% availability SLA. You can easily achieve the sub-millisecond latency and throughput your applications need. Start with the lowest tier and smallest size, and then grow your instance effortlessly with minimal impact to application availability. Memorystore can support instances up to 300 GB and network throughput of 12 gigabits per second. Because Memorystore for Redis is fully compatible with the Redis protocol, you can lift-and-shift your applications from open source Redis to Memorystore without any code changes by using the import/export feature. There is no need to learn new tools because all existing tools in client libraries just work.

#### Quiz: Storage and Database Services

- https://www.cloudskillsboost.google/paths/11/course_templates/49/quizzes/548418

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548419

In this module, we covered the different storage and database services that Google Cloud offers. Specifically, you learned about Cloud Storage, a fully managed object store; Filestore, a fully managed file storage service; Cloud SQL, a fully managed MySQL and PostgreSQL database service; Spanner, a relational database service with transactional consistency, global scale, and high availability; AlloyDB, a fully managed, PostgreSQL- compatible database service; Firestore, a fully managed NoSQL document database; Bigtable, a fully managed NoSQL wide-column database; and Memorystore, a fully managed in-memory datastore service for Redis. From an infrastructure perspective, the goal was to understand what services are available and how they're used in different circumstances. Defining a complete data strategy is beyond the scope of this course; however, Google offers courses on data engineering and machine learning on Google Cloud that cover data strategy.

### Resource Management

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548420

In this module, we will cover Resource Management. Resources in Google Cloud are billable, so managing them means controlling cost. There are several methods in place for controlling access to the resources, and there are quotas that limit consumption. In most cases, the default quotas can be raised on request, but having them in place provides a checkpoint or a chance to make sure that this really is a resource you intend to consume in greater quantity. In this module, we will build on what we learned in the IAM module. First, I’ll provide an overview of the Resource Manager. Then, we’ll go into quotas, labels, and names. Next, we’ll cover billing to help you set budgets and alerts. To complete your learning experience, you’ll get to examine billing data with BigQuery in a lab.

#### Resource Manager

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548421

Let’s get started with an overview of Resource Manager. The Resource Manager lets you hierarchically manage resources by project, folder, and organization. This should sound familiar because we covered it in the IAM module. Let me refresh your memory. Policies contain a set of roles and members, and policies are set on resources. These resources inherit policies from their parent, as we can see on the left. Therefore, resource policies are a union of parent and resource if an IAM allow policy is associated. However, if an IAM deny policy is associated with the resource, then the policy can prevent certain principals from using certain permissions, regardless of the roles they’re granted. Although IAM policies are inherited top-to-bottom, billing is accumulated from the bottom up, as we can see on the right. Resource consumption is measured in quantities, like rate of use or time, number of items, or feature use. Because a resource belongs to only one project, a project accumulates the consumption of all its resources. Each project is associated with one billing account, which means that an organization contains all billing accounts. Let’s explore organizations, projects, and resources more. Just to reiterate, an organization node is the root node for all Google Cloud resources. This diagram shows an example where we have an individual, Bob, who is in control of the organizational domain through the Organization Admin role. Bob has delegated privileges and access to the individual projects to Alice by making her a Project Creator. Because a project accumulates the consumption of all its resources, it can be used to track resources and quota usage. Specifically, projects let you enable billing, manage permissions and credentials, and enable services and APIs. To interact with Google Cloud resources, you must provide the identifying project information for every request. A project can be identified by: The Project Name, which is a human- readable way to identify your projects, but it isn't used by any Google APIs. There is also the Project Number, which is automatically generated by the server and assigned to your project. And there is the Project ID, which is a unique ID that is generated from your project name. You can find these three identifying attributes on the dashboard of your Google Cloud console, or by querying the Resource Manager API. Finally, let’s talk about the resource hierarchy. From a physical organization standpoint, resources are categorized as global, regional, or zonal. Let’s look at some examples: Images, snapshots, and networks are global resources; External IP addresses are regional resources; and instances and disks are zonal resources. However, regardless of the type, each resource is organized into a project. This enables each project to have its own billing and reporting.

#### Quotas

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548422

Now that we know that a project accumulates the consumption of all its resources, let's talk about quotas. All resources in Google Cloud are subject to project quotas or limits. These typically fall into one of the three categories shown here. How many resources you can create per project. For example, you can only have fifteen VPC networks per project. How quickly you can make API requests in a project or rate limits. For example, by default, you can only make five administrative actions per second per project when using the Spanner API. There are also regional quotas. For example, by default, you can only have 24 CPUs per region. Given these quotas, you may be wondering, how do I spin up one of those 96-core VMs. As your use of Google Cloud expands over time, your quotas may increase accordingly. If you expect a notable upcoming increase in usage, you can proactively request quota adjustments from the quotas page in the Google Cloud console. This page will also display your current quotas. If quotas can be changed, why do they exist? Project quotas prevent runaway consumption in case of error or malicious attack. For example, imagine you accidentally create 100 instead of 10 Compute Engine instances using the g-cloud command line. Quotas also prevent billing spikes or surprises. Quotas are related to billing, but we will go through how to set up budgets and alerts later, which will really help you manage billing. Finally, quotas force sizing consideration and periodic review. For example, do you really need a 96-core instance, or can you go with a smaller and cheaper alternative? It is also important to mention that quotas are the maximum amount of resources you can create for that resource type as long as those resources are available. Quotas do not guarantee that resources will be available at all times. For example, if a region is out of local SSDs, you cannot create local SSDs in that region, even if you still have quota for local SSDs.

#### Labels

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548423

Projects and folders provide levels of segregation for resources, but what if you want more granularity? That’s where labels come in. Labels are a utility for organizing Google Cloud resources. Labels are key-value pairs that you can attach to your resources, like VMs, disks, snapshots and images. You can create and manage labels using the Google Cloud console, gcloud, or the Resource Manager API, and each resource can have up to 64 labels. For example, you could create a label to define the environment of your virtual machines. Then you define the label for each of your instances as either production or test. Using this label, you could search and list all your production resources for inventory purposes. Labels can also be used in scripts to help analyze costs or to run bulk operations on multiple resources. The screenshot shows an example of 4 labels that are created on an instance. Let’s go over some examples of what to use labels for: I recommend adding labels based on team or cost center to distinguish instances owned by different teams. You can use this type of label for cost accounting or budgeting. For example, team:marketing and team:research. You can also use labels to distinguish components. For example, component:redis, and component:frontend. Again, you can label based on environment or stage. You should also consider using labels to define an owner or a primary contact for a resource. For example, owner:gaurav, or contact:opm. Or add labels to your resources to define their state. For example, state:inuse, or state:readyfordeletion. It’s important to not confuse labels with network tags. Labels, we just learned, are user-defined strings in key-value format that are used to organize resources, and they can propagate through billing. Network tags, on the other hand, are user-defined strings that are applied to instances only and are mainly used for networking, such as applying firewall rules and custom static routes. For more information about using labels, see the link in the Course Resources.

#### Billing

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548424

Because the consumption of all resources under a project accumulates into one billing account, let’s talk billing. To help with project planning and controlling costs, you can set a budget. Setting a budget lets you track how your spend is growing toward that amount. This screenshot shows the budget creation interface. First, you set a budget name and specify which project this budget applies to. Then, you can set the budget at a specific amount or match it to the previous month's spend. After you determine your budget amount, you can set the budget alerts. These alerts send emails to Billing Admins after spend exceeds a percent of the budget or a specified amount. In our case, it would send an email when spending reaches 50%, 90%, and 100% of the budget amount. You can even choose to send an alert when the spend is forecasted to exceed the percent of the budget amount by the end of the budget period. In addition to receiving an email, you can use Pub/Sub notifications to programmatically receive spend updates about this budget. You could even create a Cloud Run function that listens to the Pub/Sub topic to automate cost management. Here’s an example of an email notification. The email contains the project name, the percent of the budget that was exceeded, and the budget amount. Another way to help optimize your Google Cloud spend is to use labels. For example, you could label VM instances that are spread across different regions. Maybe these instances are sending most of their traffic to a different continent, which could incur higher costs. In that case, you might consider relocating some of those instances or using a caching service like Cloud CDN to cache content closer to your users, which reduces your networking spend. I recommend labeling all your resources and exporting your billing data to BigQuery to analyze your spend. BigQuery is Google’s scalable, fully managed enterprise data warehouse with SQL and fast response times. Creating a query is as simple as shown in this screenshot, which you’ll explore in the upcoming lab. You can even visualize spend over time with Looker Studio. Looker Studio turns your data into informative dashboards and reports that are easy to read, easy to share, and fully customizable. For example, you can slice and dice your billing reports using your labels.

#### Demo: Billing Administration

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548425

In the upcoming lab, you will examine billing data that we exported for you. Let me show you how to export billing data and demonstrate other common activities that a billing administrator performs. These actions cannot be performed in the Qwiklabs environment because of security restrictions. Therefore, I'm going to walk you through them as a demo. So here I am in the GCP console. And what I want to do is navigate to billing. So I'm going to click on the navigation menu and click on Billing. So here I'm provided with an overview. I'm actually using a trial account, as you can see, but the same concepts apply to any account. We can see the consumption for the current month. In my case, I have some promotional credits here. If I had multiple billing accounts, then you'd be able to choose from them right up here. Again, credits, billing account also has a name. If you go to Payment Overview, you'll can see the payment method that is currently selected. Now, the other really big thing we can do is here is set up budgets and alerts. So if I click on that and then click on Create Budget, I first just give it a name. So you My-Budget-Alert. I can select the specific projects that I want these alerts on. It could just be one, it could be several, click Next. Then the type so either specify an exact dollar amount or I start with my last month spend. So if last month I spent would say a certain dollar amount, it would pull that dollar amount directly in there. In our case, let's say we want to target an amount of $500. It's also including credits in this cost. You could disable that. And then we define the thresholds for the alerts. So by default, it sets up this 50, 90 at a 100%, and the dollar amounts are being pulled from the 500 that I just plugged in there earlier. You can also choose between actual and forecasted. And you can read a little bit more what this forecast it is about here. We could remove these. So let's say maybe we wanted to add an earliest threshold already at 25%, then we could do that. We could even go further and actually connect to a cloud pub/sub topic and then do all sorts of automation as we discussed in the slides. So then from here I can just click Finish. And it's then going to be sending me emails around this. I can also see sort of a menu here that shows how far I've gone in the spend. And so I can come in here at any moment and also see, am I close to that 25% mark already? Then the other thing that's pretty interesting on this page is the Transaction page. So this will show all of the different charges. Again, I have credit in here. So every charge is being offset by credit, but you can see all the different usages that I've had across Compute Engine instances, disk space that I've been using. So you could always go in here. Now more interestingly is to probably export all this information. So if I click on Billing Export, I'm presented with two options. I could export to BigQuery or export it as a file, and to enable that I just choose the one I'm interested in. Let's say BigQuery. And then go to edit settings and then I would define where this is going to go to. In this case, I would have to define a BigQuery data set, so you can navigate there and set it up and click Save. And similarly, if I wanted to export to a file, I could edit those settings as well. In this case, could be exported as a CSV or JSON file and stored in a cloud storage bucket. So I can define the name in here. So I would have to create a bucket first and then give it a prefix. And then it's going to export that to there. The other big thing, I can reviews if I click on payment method. I can review the different payment accounts. Payment profiles, payment method if it's a credit card or bank account. You can review all of that information in there. That's how easy it is to administer billing in GCP. A billing administrator can set up accounts and run reports which are ordinary tasks. But becoming familiar with the available options and seeing how these tasks are performed reduces the chances of confusion. For example, you know that reports can be generated in JSON or CSV format. Now more sophisticated processing or filtering of data occurs after the billing is exported, as you will explore in the next lab.

#### Lab Intro: Examining Billing Data with BigQuery

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548426

Let's examine billing data with BigQuery. In this lab, you’ll sign into BigQuery and create a dataset. In this dataset, you’ll create a table by importing billing data that is stored in a Cloud Storage bucket. Next, you’ll run simple queries on the imported data, and then you’ll run more complex queries on a larger dataset.

#### Examining Billing data with BigQuery

- https://www.cloudskillsboost.google/paths/11/course_templates/49/labs/548427

#### Lab Review: Examining Billing Data with BigQuery

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548428

In this lab, you imported billing data into BigQuery that had been exported as a CSV file. You first ran a simple query on that data. Next, you accessed a shared dataset containing more than 22,000 records of billing information, You then ran a variety of queries on that data to explore how you can use BigQuery to gain insight into your resources’ billing consumption. If you use BigQuery on a regular basis, you'll start to develop your own queries for searching out where resources are being consumed in your application. You can also monitor changes in resource consumption over time. This kind of analysis is an input to capacity planning and can help you determine how to scale up your application to meet growth or scale down your application for efficiency. Welcome to the walk-through of the lab Examining Billing Data with BigQuery. At this point in the lab, I have logged in with the username and password that Qwiklabs has provided me from the lab. So the first task is to use BigQuery to import data. So what I did is as the Billing Administrator, I exported my billing data and put it in a bucket. So I am going to go into BigQuery and I'm going to import some stuff. So BigQuery? Yes. The Cloud Council. Thank you. Make sure that you are logged in to BigQuery and have the correct Qwiklabs project ID selected at the top. So I'm going to go here and I'm going to click Create dataset and I am going to call it imported billing data. My data location is in the US and I want it to expire one day afterwards. I'm going to hit Create dataset. You can see my dataset is created and I should see it right here. There it is. So now I'm going to create a table in that dataset. Table. For the source, I'm going to use Cloud Storage, I'm going to copy and paste the bucket location from the lab and it is in CSV format. For destination, I'm using that and native table and I am going to call it sampleinfotable. Under schema, I I'm going to hit auto detect so it detects a schema and input parameters from the dataset. I'm going to open up the advance and I am going to specify that I want to skip one row because that's the headers, and then I'm going to hit Create table. So this is a point where you can check the progress in your lab. If you click check my progress, it's going to check that you have dataset at a table and that you have imported that data into that table, and you should get five points for that. So Task 2, you are going to examine the data that you've just input. So I'm going to click on my table and it's going to, by default, show the schema. I can click Details and it's going to tell me a little bit more information about number of rows. You can see it has 44 rows, it's a pretty small table. I can hit Preview and it's going to show me a couple of the first rows of the table. So it now wants me in the lab. There are some formative questions that are going to ask you just to make sure that you're understanding the learning. So I'm not going to go over that in the walk-through because that is more and to make sure that you're understanding what you're doing. So I'm going to go to Task 3 where we're composing a simple query. A couple of cool things about BigQuery, if you are in a table by default, if you click Query table, it's going to auto-populate the query editor with the project dataset table for you and then you just specify what you want to look at. So I do select star and I'm doing this. Oops. Where cost is greater than zero. So I just want to see in this table how much of it. I only want to see the rows where the cost is more than zero. You can see right here it's validating that my SQL or my SQL is right. I'm going to hit Run. Here are my query results and you can see out of a table that has 44, there are 20 rows in this table that actually have costs that are more than zero. So again, there are a couple more questions that you can answer and you can also check your progress that you've run this query. So if you run the query, then it's going to give you another five points. At this point, you're actually done with the points that are awarded in the lab, but you still have another task to go into a little more complex query. So I'm going to go ahead and copy the query from Task 4, and I'm going to erase this and paste it in. It's a valid query here. I'm going to hit Run, then I'm going to verify that the result has what my lab is telling me is, supposed to return 22,537 lines of billing data. I can see right here, that is correct. Let's say I wanted to find the latest 100 records where the charges were greater than zero. So I'm going to copy paste the query that's provided to me, and make sure it's valid. Always good to check that your SQL is valid. Hit Run, and it is going to show me the last 100 records where charges were greater than zero. Let's say I wanted to find all of the charges that were more than $3, the next query shows you that. You can feel free to click through each one of these more complex queries and feel free to try out some queries of your own. If you wanted to just peruse the data and figure out maybe the last two days of billing anything that was over $10. Any kind of question that you might need to provide data for to your senior leadership about your billing of resource usage in GCP. After all of these complex queries, in review, you imported billing data that was technically exported for you from a billing admin into BigQuery, and then you run a simple query and then you ran some more complex queries. I hope you enjoyed the lab. Thank you.

#### Quiz: Resource Management

- https://www.cloudskillsboost.google/paths/11/course_templates/49/quizzes/548429

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548430

In this module, we covered the Cloud Resource Manager and went into quotas, labels, and billing. Then we analyzed billing data with BigQuery in a lab. Reporting is an important part of resource management. You can generate reports to track consumption and to establish accountability. A key principle in Google Cloud is transparency, and that means it's straightforward to access and process consumption data, as you observed in this module.

### Resource Monitoring

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548431

In this module, I’ll give you an overview of the resource monitoring options in Google Cloud. The features covered in this module rely on Google Cloud Observability, a service that provides monitoring, logging, and diagnostics for your applications. In this module, we are going to explore the Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace, and Cloud Profiler services.

#### Google Cloud Observability Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548432

Let me start by giving you a high-level overview of Google Cloud Observability and its features. Google Cloud Observability dynamically discovers cloud resources and application services based on deep integration with Google Cloud and Amazon Web Services. Because of its smart defaults, you can have core visibility into your cloud platform in minutes. This provides you with access to powerful data and analytics tools plus collaboration with many different third-party software providers. As mentioned earlier, Google Cloud Observability has services for monitoring logging, error reporting, and fault tracing. You only pay for what you use, and there are free usage allotments so that you can get started with no upfront fees or commitments. For more information about pricing, refer to the link in the Course Resources. Now, in most other environments, these services are handled by completely different packages, or by a loosely integrated collection of software. When you see these functions working together in a single, comprehensive, and integrated service, you'll realize how important that is to creating reliable, stable, and maintainable applications.

#### Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548433

Now that you understand Google Cloud Observability from a high-level perspective, let’s look at Cloud Monitoring. Monitoring is important to Google because it is at the base of site reliability engineering, a discipline that applies aspects of software engineering to operations whose goals are to create ultra-scalable and highly reliable software systems. This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world. Cloud Monitoring dynamically configures monitoring after resources are deployed and has intelligent defaults that allow you to easily create charts for basic monitoring activities. This allows you to monitor your platform, system, and application metrics by ingesting data, such as metrics, events, and metadata. You can then generate insights from this data through dashboards, charts, and alerts. For example, you can configure and measure uptime and health checks that send alerts via email. A metrics scope is the root entity that holds monitoring and configuration information in Cloud Monitoring. Each metrics scope can have between 1 and 375 monitored projects. Monitoring data for all projects in that scope will be visible. A metrics scope contains the custom dashboards, alerting policies, uptime checks, notification channels, and group definitions that you use with your monitored projects. A metrics scope can access metric data from its monitored projects, but the metrics data and log entries remain in the individual projects. The first monitored Google Cloud project in a metrics scope is called the scoping project, and it must be specified when you create the metrics scope. The name of that project becomes the name of your metrics scope. To access an AWS account, you must configure a project in Google Cloud to hold the AWS Connector. Because metrics scopes can monitor all your Google Cloud projects in a single place, a metrics scope is a “single pane of glass” through which you can view resources from multiple Google Cloud projects and AWS accounts. All users of Google Cloud Observability with access to that metrics scope have access to all data by default. This means that a role assigned to one person on one project applies equally to all projects monitored by that metrics scope. In order to give people different roles per project and to control visibility to data, consider placing the monitoring of those projects in separate metrics scopes. Cloud Monitoring allows you to create custom dashboards that contain charts of the metrics that you want to monitor. For example, you can create charts that display your instances’ CPU utilization, the packets or bytes sent and received by those instances, and the packets or bytes dropped by the firewall of those instances. In other words, charts provide visibility into the utilization and network traffic of your VM instances, as shown on this slide. These charts can be customized with filters to remove noise, groups to reduce the number of time series, and aggregates to group multiple time series together. For a full list of supported metrics, please refer to the documentation. Although charts are extremely useful, they can only provide insight while someone is looking at them. But what if your server goes down in the middle of the night or over the weekend? Do you expect someone to always look at dashboards to determine whether your servers are available or have enough capacity or bandwidth? If not, you want to create alerting policies that notify you when specific conditions are met. For example, as shown on this slide, you can create an alerting policy when the network egress of your VM instance goes above a certain threshold for a specific timeframe. When this condition is met, you or someone else can be automatically notified through email, SMS, or other channels in order to troubleshoot this issue. You can also create an alerting policy that monitors your usage of Google Cloud Observability and alerts you when you approach the threshold for billing. For more information about this, please refer to the documentation. Here is an example of what creating an alerting policy looks like. On the left, you can see an HTTP check condition on the summer01 instance. This will send an email that is customized with the content of the documentation section on the right. Let’s discuss some best practices when creating alerts: We recommend alerting on symptoms, and not necessarily causes. For example, you want to monitor failing queries of a database and then identify whether the database is down. Next, make sure that you are using multiple notification channels, like email and SMS. This helps avoid a single point of failure in your alerting strategy. We also recommend customizing your alerts to the audience’s needs by describing what actions need to be taken or what resources need to be examined. And finally, avoid noise, because this will cause alerts to be dismissed over time. Specifically, adjust monitoring alerts so that they are actionable and don’t just set up alerts on everything possible. Uptime checks can be configured to test the availability of your public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource to be checked can be an App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. For each uptime check, you can create an alerting policy and view the latency of each global location. Here is an example of an HTTP uptime check. The resource is checked every minute with a 10-second timeout. Uptime checks that do not get a response within this timeout period are considered failures. So far there is a 100% uptime with no outages. Monitoring data can originate at a number of different sources. With Google Compute Engine instances, because the VMs are running on Google hardware, the hypervisor cannot access some of the internal metrics inside a VM, for example, memory usage. The Ops Agent collects metrics inside the VM, not at the hypervisor level. The Ops Agent is the primary agent for collecting telemetry data from your Compute Engine instances. This diagram shows how data is collected to monitor workloads running on a Compute Engine instance. Ops Agent installed on Compute Engine collects data beyond the system metrics. The collected metric is then used by Cloud Monitoring to create dashboards, alerts, uptime checks, and notifications to drive observability for workloads running in your application. You can configure the Ops Agent to monitor many third-party applications. For a detailed list, refer to the documentation. The Ops Agent supports most major operating systems, such as CentOS, Ubuntu, and Windows. If the standard metrics provided by Cloud Monitoring do not fit your needs, you can create custom metrics. For example, imagine a game server that has a capacity of 50 users. What metric indicator might you use to trigger scaling events? From an infrastructure perspective, you might consider using CPU load or perhaps network traffic load as values that are somewhat correlated with the number of users. But with a custom metric, you could actually pass the current number of users directly from your application into Cloud Monitoring. To get started with creating custom metrics, please refer to the documentation. When you want to maintain a metric at a target value, specify a utilization target. The autoscaler creates VMs when the metric value is above the target and deletes VMs when the metric value is below the target. If the metric comes from each VM in your managed instance group, then the autoscaler takes the average metric value across all VMs in the managed instance group and compares it with the utilization target. If the metric applies to the whole managed instance group and does not come from the VMs in your managed instance group, then the autoscaler compares the metric value with the utilization target. When your metric has multiple values, apply a filter to autoscale using an individual value from the metric.

#### Lab Intro: Resource Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548434

Let's take some of the monitoring concepts that we just discussed and apply them in a lab. In this lab, you learn how to use Cloud Monitoring to gain insight into applications that run on Google Cloud. Specifically, you will enable Cloud Monitoring, add charts to dashboards, and create alerts, resource groups, and up-time checks.

#### Resource Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/49/labs/548435

#### Lab Review: Resource Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548436

In this lab you got an overview of Cloud Monitoring. You learned how to monitor your project, create alerts with multiple conditions, add charts to dashboards, create resource groups, and create uptime checks for your services. your services. Monitoring is critical to your application’s health, and Cloud Monitoring provides a rich set of features for monitoring your infrastructure, visualizing the monitoring data, and triggering alerts and events for you. You can stay for a lab walkthrough, but remember that Google Cloud’s interface can change, so your environment might look slightly different. Welcome to the lab walkthrough for resource monitoring with Stackdriver. At this point I have logged into the GCP console with the credentials that the Qwiklabs lab has provided me. And in the first task I am going to verify that the proper Vms had been set up for me using deployment manager in this lab. And as you can see, there are three Vms. 3 engine X stacks right here. So now that I verified that my instances are running and were created for me, I am going to go to stackdriver monitoring which will open in a new tab. And then it is going to set up the workspace for my project. And this can take a few minutes. So just be patient or go get a cup of coffee and come back. Once your workspace has been set up for you, you are going to be redirected to the monitoring overview page. There are some questions in the labs that are going to ask you some questions and those are just making sure that your understanding and actually reading. But you don't have to fill those out in order to get the full score for the labs. So in task two, we're going to create a dashboard, so I'm going to go here. To monitoring overview when I click create dashboard. I am going to name it my dashboard instead of untitled hit enter. And then I'm going to add a chart. For the title, I am going to say this is my chart. And I am going to find GCE. VM instance. For metrics, I am going to select CPU utilization. CPU utilization and for filter. Where is filter here? I'm going to add a filter. There are various options you can filter by resource label by metadata label. I'm not going to add any filters, I want to see everything. And then we click here on view options. There's a couple of chart modes. There's color mode X Ray mode. You can preview it on the right. Stats mode and like that. So I actually like the X Ray mode, so I'm going to go ahead and click that. And then you're going to hit save to add the chart to your dashboard. There it is, looking nice. So then we also have a metrics explorer which allows you to examine resources and metrics without having to create a chart on the dashboard. So if I go to resources metrics explorer. Find resource type in metric, I can type any metric or resource name. So let's say I do CPU utilization. And as you can see I didn't have to add this, but I could still explore it. Again, you'll have another question in the lab, but that is to prompt your understanding. So now, I'm going to create an alert and add the first condition. So I'm going to go here and create a policy. And I'm going to click add condition. Here I'm going to do GCE VM instance. And for metrics, I'm going to use CPU usage. For condition, I'm going to say is above threshold. Of one minute, the threshold is 20. And then I'm going to hit save. And I'm going to add another condition. And then what said do I do it for another VM? So if I do this one. Maybe I do it for another metric going to, do you? Stu reserved course. And then above 15. I'm going to head, save. So now in policy triggers I'm going to trigger when all conditions are met. Then I'm going to configure the notifications so that I can be actually told that this has triggered. And I'm going to click here. Email, and I'm going to add some email going to do fake email. I want you guys spamming me. And then add. That has been added. And then I'm going to stick, skip the documentation step, but in reality this is a pretty important part of your notification. You want to say. What happened, why whoever it is that's getting notified is being alerted and a best practices to actually tell them how to maybe fix it. because otherwise that's not a very useful notification if they don't know how they can fix it. And then you're going to name it, and this is my. I first alerting policy. And then I'm going to hit save. This is a checkpoint in the lab where you can check your progress that you have created an alerting policy. The next task you are going to create some groups here. Create group, I'm going to give it a name. VM instances. Name, I am going to select. Contains and I'm going to type engine X. And I'm going to save group. And you can see it's showing me instances. All three of my instances because the name matches Anjanette stack. And again, you're going to have another. Question to make sure that your understanding and reading all of the extra tidbits that are available in the lab. So now, we're going to go back to the Dashboard. You' re going to go to uptime checks. Overview and then we're going to add an uptight and check. So in here we're going to add a title, so my first uptime check were using HTTP. It is to check an instance in, applies to a group and I'm going to select group I created which is VM instances and I'm going to check every one minute. I'm going to hit save. You could also hit test for it and make sure that it works. I'm going to say no thanks. I don't want to create the alert policy right now. So this is the last piece where you can hit check my progress. It's going to make sure that you created that uptime check, and if so, you're going to get the full points for the lab. So in this lab, we got to walk through monitoring your projects, creating a stackdriver workspace which gets created for you. Creating some alerts with multiple condition, adding some charts to a dashboard, and creating resource groups, and finally we created an uptime check for your services. Hope you enjoyed it.

#### Logging

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548437

Monitoring is the basis of Google Cloud Observability, but the service also provides logging, error reporting, and tracing. Let’s learn about logging. Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud and AWS. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. Logging includes storage for logs, a user interface called Logs Explorer, and an API to manage logs programmatically. The service lets you read and write log entries, search and filter your logs, and create log-based metrics. Logs are only retained for 30 days, but you can export your logs to Cloud Storage buckets, BigQuery datasets, and Pub/Sub topics. Exporting logs to Cloud Storage makes sense for storing logs for more than 30 days, but why should you export to BigQuery or Pub/Sub? Exporting logs to BigQuery allows you to analyze logs and even visualize them in Looker Studio. BigQuery runs extremely fast SQL queries on gigabytes to petabytes of data. This allows you to analyze logs, such as your network traffic, so that you can better understand traffic growth to forecast capacity, network usage to optimize network traffic expenses, or network forensics to analyze incidents. For example, in this screenshot I queried my logs to identify the top IP addresses that have exchanged traffic with my web server. Depending on where these IP addresses are, and who they belong to, I could relocate part of my infrastructure to save on networking costs or deny some of these IP addresses if I don’t want them to access my web server. If you want to visualize your logs, I recommend connecting your BigQuery tables to Looker Studio. Looker Studio transforms your raw data into the metrics and dimensions that you can use to create easy-to-understand reports and dashboards. I mentioned that you can also export logs to Pub/Sub. This enables you to stream logs to applications or endpoints.

#### Error Reporting

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548438

Let’s learn about another feature of Google Cloud Observability: Error Reporting. Error Reporting counts, analyzes, and aggregates the errors in your running cloud services. A centralized error management interface displays the results with sorting and filtering capabilities, and you can even set up real-time notifications when new errors are detected. Currently, Error Reporting is generally available for App Engine on both standard and flexible environments, Apps Script, Compute Engine, Cloud Run, Cloud Run functions, Google Kubernetes Engine, and Amazon EC2. In terms of programming languages, the exception stack trace parser is able to process Go, Java, . NET, Node.js, PHP, Python, and Ruby.

#### Tracing

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548439

Tracing is another Google Cloud Observability feature integrated into Google Cloud. Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud console. You can track how requests propagate through your application and receive detailed, near real-time performance insights. Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports that surface performance degradations and can capture traces from App Engine, global external Application Load Balancers, and applications instrumented with the Cloud Trace API. Managing the amount of time it takes for your application to handle incoming requests and perform operations is an important part of managing overall application performance. Cloud Trace is actually based on the tools used at Google to keep our services running at extreme scale.

#### Profiling

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548440

Finally, let’s cover the last feature of Google Cloud Observability in this module, which is the profiler. Poorly performing code increases the latency and cost of applications and web services every day. Cloud Profiler continuously analyzes the performance of CPU or memory-intensive functions executed across an application. While it’s possible to measure code performance in development environments, the results generally don’t map well to what’s happening in production. Many production profiling techniques either slow down code execution or can only inspect a small subset of a codebase. Profiler uses statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete picture of an application’s performance without slowing it down. Profiler allows developers to analyze applications running anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Node.js, and Python.

#### Partner Integrations

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548441

Google Cloud Observability also supports a rich and growing ecosystem of technology partners, as shown on this slide. This helps expand the IT ops, security, and compliance capabilities available to Google Cloud customers. The diagram shows the architecture of how BindPlane ingests logs and then how those logs are ingested into Cloud Logging plus collects metrics and then how these metrics are ingested into Cloud Monitoring. Adding Blue Medora’s software, BindPlane, helps collect the metrics and logs and push them into the open APIs that form our core observability platform. Once a log source is ingested into Cloud Logging, you can view and search through the raw log data and create metrics off of those log files just like logs collected from Google Cloud. You can use all the features of Google Cloud Observability, including viewing logs in real time in the Google Cloud console or through log-based metrics to view logs and metrics side-by-side and alert on logs. This diagram depicts the architecture involved in filtering and exporting log data from Cloud Logging to Splunk Enterprise. Pub/Sub is used to temporarily store logged messages as they are published from Cloud Logging, before delivering them to Splunk. Cloud Logging begins by gathering logs into a centralized location, then forwards them to Google Cloud's Pub/Sub messaging service. Pub/Sub manages the log data using a dedicated channel. A primary Dataflow pipeline extracts logs from Pub/Sub and delivers them to Splunk for analysis. To safeguard against errors, a secondary Dataflow pipeline runs in parallel, capable of resending logs if delivery fails. Finally, Splunk (deployed on-premises, in Google Cloud as SaaS, or via a hybrid approach) receives the logs and enables in-depth analysis. The workflow uses a streaming pipeline to natively push your Google Cloud data to your Splunk Cloud or Splunk Enterprise instance using the Pub/Sub to Splunk Dataflow template. Using this Dataflow template, you can export data from Pub/Sub to Splunk. So, any message that can be delivered to a Pub/Sub topic can now be forwarded to Splunk. For more information about integrations, refer to the link in the Course Resources.

#### Quiz: Resource Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/49/quizzes/548442

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548443

In this module, I gave you an overview of Google Cloud Observability and its monitoring, logging, error reporting, fault tracing, and profiling features. Having all of these integrated into Google Cloud allows you to operate and maintain your applications, which is known as site reliability engineering, or SRE. If you’re interested in learning more about SRE, you can explore the SRE book or some of our SRE courses.

#### Course Review

- https://www.cloudskillsboost.google/paths/11/course_templates/49/video/548444

Thank you for taking the “Essential Cloud Infrastructure: Core Services” course. I hope you have a better understanding of how to administer IAM, choose between the different data storage services in Google Cloud, examine billing of Google Cloud resources, and monitor those resources. Hopefully the demos and labs made you feel more comfortable using the different Google Cloud services that we covered. Next, I recommend enrolling in the “Elastic Cloud Infrastructure: Scaling and Automation” course, of the “Architecting with Google Compute Engine” series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to Google Cloud. Next, we’ll go over Google Cloud’s load balancing and autoscaling services, which you’ll get to explore directly. Then, we’ll cover infrastructure automation services like Terraform, so that you can automate the deployment of Google Cloud infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in Google Cloud. Enjoy that course!

#### Next Course: Elastic Cloud Infrastructure: Scaling and Automation

- https://www.cloudskillsboost.google/paths/11/course_templates/49/documents/548445

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/49/documents/548446

### Your Next Steps

## 06: Elastic Google Cloud Infrastructure: Scaling and Automation

- https://www.cloudskillsboost.google/paths/11/course_templates/178

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567821

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you‘ll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

#### Welcome to Elastic Cloud Infrastructure: Scaling and Automation

- https://www.cloudskillsboost.google/paths/11/course_templates/178/documents/567822

### Interconnecting Networks

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567823

In this module, we focus on interconnecting networks. Different applications and workloads require different network connectivity solutions. That’s why Google supports multiple ways to connect your infrastructure to Google Cloud. In this module, we'll focus on Google Cloud’s hybrid connectivity products, which are Cloud VPN, Cloud Interconnect, and Peering. We'll also look at options for sharing VCP networks within Google Cloud. Let's start by talking about Cloud VPN so that you can explore it in a lab.

#### Cloud VPN

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567824

Google Cloud offers two types of Cloud VPN gateways: HA VPN and Classic VPN. All Cloud VPN gateways created before the introduction of HA VPN are considered Classic VPN gateways. Classic VPN securely connects your on-premises network to your Google Cloud VPC network through an IPsec VPN tunnel. Traffic traveling between the two networks is encrypted by one VPN gateway, then decrypted by the other VPN gateway. This protects your data as it travels over the public internet, and that’s why Classic VPN is useful for low-volume data connections. As a managed service, Classic VPN provides an SLA of 99.9% service availability and supports site-to-site VPN, static and dynamic routes, and IKEv1 and IKEv2 ciphers. Classic VPN doesn't support use cases where client computers need to “dial in” to a VPN using client VPN software. Also, dynamic routes are configured with Cloud Router, which we will cover briefly. For more information about the SLA and these features, please refer to the documentation link in the course resources for this module. Let me walk through an example of Cloud VPN. This diagram shows a Classic VPN connection between your VPC and on-premises network. Your VPC network has subnets in us-east1 and us-west1, with Google Cloud resources in each of those regions. These resources are able to communicate using their internal IP addresses because routing within a network is automatically configured (assuming that firewall rules allow the communication). Now, in order to connect to your on-premises network and its resources, you need to configure your Cloud VPN gateway, on-premises VPN gateway, and two VPN tunnels. The Cloud VPN gateway is a regional resource that uses a regional external IP address. Your on-premises VPN gateway can be a physical device in your data center or a physical or software-based VPN offering in another cloud provider's network. This VPN gateway also has an external IP address. A VPN tunnel then connects your VPN gateways and serves as the virtual medium through which encrypted traffic is passed. In order to create a connection between two VPN gateways, you must establish two VPN tunnels. Each tunnel defines the connection from the perspective of its gateway, and traffic can only pass when the pair of tunnels is established. Now, one thing to remember when using Cloud VPN is that the maximum transmission unit, or MTU, for your on-premises VPN gateway cannot be greater than 1460 bytes. This is because of the encryption and encapsulation of packets. For more information about this MTU consideration, please refer to the documentation link in the course resources. In addition to Classic VPN, Google Cloud also offers a second type of Cloud VPN gateway, HA VPN. HA VPN is a high availability Cloud VPN solution that lets you securely connect your on-premises network to your Virtual Private Cloud (VPC) network through an IPsec VPN connection in a single region. HA VPN provides an SLA of 99.99% service availability. To guarantee a 99.99% availability SLA for HA VPN connections, you must properly configure two or four tunnels from your HA VPN gateway to your peer VPN gateway or to another HA VPN gateway. When you create an HA VPN gateway, Google Cloud automatically chooses two external IP addresses, one for each of its fixed number of two interfaces. Each IP address is automatically chosen from a unique address pool to support high availability. Each of the HA VPN gateway interfaces supports multiple tunnels. You can also create multiple HA VPN gateways. When you delete the HA VPN gateway, Google Cloud releases the IP addresses for reuse. You can configure an HA VPN gateway with only one active interface and one external IP address; however, this configuration does not provide a 99.99% service availability SLA. VPN tunnels connected to HA VPN gateways must use dynamic (BGP) routing. Depending on the way that you configure route priorities for HA VPN tunnels, you can create an active/active or active/passive routing configuration. HA VPN supports site-to-site VPN in one of the following recommended topologies or configuration scenarios: An HA VPN gateway to peer VPN devices An HA VPN gateway to an Amazon Web Services (AWS) virtual private gateway Two HA VPN gateways connected to each other Let's explore these configurations in a bit more detail. There are three typical peer gateway configurations for HA VPN. An HA VPN gateway to two separate peer VPN devices, each with its own IP address, an HA VPN gateway to one peer VPN device that uses two separate IP addresses and an HA VPN gateway to one peer VPN device that uses one IP address. Let's walk through an example. In this topology, one HA VPN gateway connects to two peer devices. Each peer device has one interface and one external IP address. The HA VPN gateway uses two tunnels, one tunnel to each peer device. If your peer-side gateway is hardware-based, having a second peer-side gateway provides redundancy and failover on that side of the connection. A second physical gateway lets you take one of the gateways offline for software upgrades or other scheduled maintenance. It also protects you if there is a failure in one of the devices. In Google Cloud, the REDUNDANCY_TYPE for this configuration takes the value TWO_IPS_REDUNDANCY. The example shown here provides 99.99% availability. When configuring an HA VPN external VPN gateway to Amazon Web Services (AWS), you can use either a transit gateway or a virtual private gateway. Only the transit gateway supports equal-cost multipath (ECMP) routing. When enabled, ECMP equally distributes traffic across active tunnels. Let’s walk through an example. In this topology, there are three major gateway components to set up for this configuration. An HA VPN gateway in Google Cloud with two interfaces, two AWS virtual private gateways, which connect to your HA VPN gateway, and an external VPN gateway resource in Google Cloud that represents your AWS virtual private gateway. This resource provides information to Google Cloud about your AWS gateway. The supported AWS configuration uses a total of four tunnels. Two tunnels from one AWS virtual private gateway to one interface of the HA VPN gateway, and two tunnels from the other AWS virtual private gateway to the other interface of the HA VPN gateway. You can connect two Google Cloud VPC networks together by using an HA VPN gateway in each network. The configuration shown provides 99.99% availability. From the perspective of each HA VPN gateway you create two tunnels. You connect interface 0 on one HA VPN gateway to interface 0 on the other HA VPN, and interface 1 on one HA VPN gateway to interface 1 on the other HA VPN. For more information on HA VPN and moving to HA VPN, refer to the documentation links in the course resources. We mentioned earlier that Cloud VPN supports both static and dynamic routes. In order to use dynamic routes, you need to configure Cloud Routers. Cloud Router can manage routes for a Cloud VPN tunnel using Border Gateway Protocol, or BGP. This routing method allows for routes to be updated and exchanged without changing the tunnel configuration. For example, this diagram shows two different regional subnets in a VPC network, namely Test and Prod. The on-premises network has 29 subnets, and the two networks are connected through Cloud VPN tunnels. Now, how would you handle adding new subnets? For example, how would you add a new “Staging” subnet in the Google Cloud network and a new on-premises 10.0.30.0/24 subnet to handle growing traffic in your data center? To automatically propagate network configuration changes, the VPN tunnel uses Cloud Router to establish a BGP session between the VPC and the on-premises VPN gateway, which must support BGP. The new subnets are then seamlessly advertised between networks. This means that instances in the new subnets can start sending and receiving traffic immediately, as you will explore in the upcoming lab. To set up BGP, an additional IP address has to be assigned to each end of the VPN tunnel. These two IP addresses must be link-local IP addresses, belonging to the IP address range 169.254.0.0/16. These addresses are not part of IP address space of either network and are used exclusively for establishing a BGP session.

#### HA VPN

- https://www.cloudskillsboost.google/paths/11/course_templates/178/documents/567825

#### Lab Intro: Configuring Google Cloud HA VPN

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567826

Let’s apply what we just covered. In this lab you create a global VPC called vpc-demo, with two custom subnets in us-east1 and us-central1. In this VPC, you add a Compute Engine instance in each region. You then create a second VPC called on-prem to simulate a customer's on-premises data center. In this second VPC, you add a subnet in region us-central1 and a Compute Engine instance running in this region. Finally, you add an HA VPN and a cloud router in each VPC and run two tunnels from each HA VPN gateway before testing the configuration to verify the 99.99% SLA.

#### Configuring Google Cloud HA VPN

- https://www.cloudskillsboost.google/paths/11/course_templates/178/labs/567827

#### Cloud Interconnect and Peering

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567828

Next, let’s talk about the Cloud Interconnect and Peering services. There are different Cloud Interconnect and Peering services available to connect your infrastructure to Google’s network. These services can be split into Dedicated versus Shared connections and Layer 2 versus Layer 3 connections. The services are Direct Peering, Carrier Peering, Dedicated Interconnect, and Partner Interconnect, which supports both layer 2 and layer 3. Dedicated connections provide a direct connection to Google’s network, but shared connections provide a connection to Google’s network through a partner. Layer 2 connections use a VLAN that pipes directly into your GCP environment, providing connectivity to internal IP addresses in the RFC 1918 address space. Layer 3 connections provide access to Google Workspace services, YouTube, and Google Cloud APIs using public IP addresses. Now, as explained earlier, Google also offers its own Virtual Private Network service, called Cloud VPN. This service uses the public internet, but traffic is encrypted and provides access to internal IP addresses. That’s why Cloud VPN is a useful addition to Direct Peering and Carrier Peering. Let’s explain the Cloud Interconnect and Peering services separately first before discussing guidance on choosing the right connection.

#### Cloud Interconnect

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567829

Dedicated Interconnect provides direct physical connections between your on-premises network and Google’s network. This enables you to transfer large amounts of data between networks, which can be more cost-effective than purchasing additional bandwidth over the public internet. In order to use Dedicated Interconnect, you need to provision a cross connect between the Google network and your own router in a common colocation facility, as shown in this diagram. To exchange routes between the networks, you configure a BGP session over the interconnect between the Cloud Router and the on-premises router. This will allow user traffic from the on-premises network to reach GCP resources on the VPC network, and vice versa. Dedicated Interconnect can be configured to offer a 99.9% or a 99.99% uptime SLA. See the Dedicated Interconnect documentation for details on how to achieve these SLAs [https://cloud.google.com/interconnect/docs/concepts/dedicated-overview#redundancy]. In order to use Dedicated Interconnect, your network must physically meet Google’s network in a supported colocation facility. This map shows the locations where you can create dedicated connections. For a full list of these locations, see the links section of this video: [https://cloud.google.com/interconnect/docs/concepts/colocation-facilities]. Now, you might look at this map and say, “well I am nowhere near one of those locations.” That’s when you want to consider Partner Interconnect. Partner Interconnect provides connectivity between your on-premises network and your VPC network through a supported service provider. This is useful if your data center is in a physical location that cannot reach a Dedicated Interconnect colocation facility or if your data needs don't warrant a Dedicated Interconnect. In order to use Partner Interconnect, you work with a supported service provider to connect your VPC and on-premises networks. For a full list of providers, see the links section of this video: [https://cloud.google.com/interconnect/docs/concepts/service-providers] These service providers have existing physical connections to Google's network that they make available for their customers to use. After you establish connectivity with a service provider, you can request a Partner Interconnect connection from your service provider. Then, you establish a BGP session between your Cloud Router and on-premises router to start passing traffic between your networks via the service provider's network. Partner Interconnect can be configured to offer a 99.9% or a 99.99% uptime SLA between Google and the service provider. See the Partner Interconnect documentation for details on how to achieve these SLAs [https://cloud.google.com/interconnect/docs/concepts/partner-overview#redundancy]. Cross-Cloud Interconnect helps you to establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. When you buy Cross-Cloud Interconnect, Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud (VPC) network with your network that's hosted by a supported cloud service provider. Google currently supports Amazon Web Services, Microsoft Azure, Oracle Cloud Infrastructure, and Alibaba Cloud for use with Cross-Cloud Interconnect. Cross-Cloud Interconnect supports the adoption of an integrated multi cloud strategy and offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps. First, you identify supported locations where you want Google to place your connections. Then you purchase primary and redundant Cross-Cloud Interconnect ports. You also buy primary and redundant ports from your cloud service provider. After provisioning the connection, Google supports the connection up to the point where it reaches the network of your other cloud service provider. Google does not guarantee uptime from the other cloud service provider and cannot create a support ticket on your behalf. Let me compare the interconnect options that we just discussed. All of these options provide internal IP address access between resources in your on-premises network and in your VPC network. The main differences are the connection capacity and the requirements for using a service. The IPsec VPN tunnels that Cloud VPN offers have a capacity of 1.5 to 3 Gbps per tunnel and require a VPN device on your on-premises network. The 1.5-Gbps capacity applies to traffic that traverses the public internet, and the 3-Gbps capacity applies to traffic that is traversing a direct peering link. You can configure multiple tunnels if you want to scale this capacity. Dedicated Interconnect has a capacity of 10 Gbps or 100 Gbps per link and requires you to have a connection in a Google-supported colocation facility. You can have up to 8 links to achieve multiples of 10 Gbps, or up to 2 links to achieve multiples of 100 Gbps, but 10 Gbps is the minimum capacity. Partner Interconnect has a capacity of 50 Mbps to 50 Gbps per connection, and requirements depend on the service provider. Cross-Cloud Interconnect enables you to establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps. If you need a lower cost solution, have lower bandwidth needs, or you are experimenting with migrating your workloads to Google Cloud, you can choose Cloud VPN. If you need an enterprise-grade connection to Google Cloud that has higher throughput, you can choose Dedicated Interconnect or Partner Interconnect. If you need to connect to another cloud service provider, choose Cross-Cloud Interconnect. Google recommends using Cloud Interconnect instead of Direct Peering and Carrier Peering, which you would only use in certain circumstances.

#### Peering

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567830

Let's talk about the Cloud Peering services, which are Direct Peering and Carrier Peering. These services are useful when you require access to Google and Google Cloud properties. Google allows you to establish a direct peering connection between your business network and Google's. With this connection, you will be able to exchange Internet traffic between your network and Google's at one of the Google's broad-reaching edge network locations. Direct Peering with Google is done by exchanging BGP routes between Google and the peering entity. After a Direct Peering connection is in place, you can use it to reach all of Google services, including the full suite of Google Cloud products. Unlike Dedicated Interconnect, Direct Peering does not have an SLA. In order to use Direct Peering, you need to satisfy the peering requirements. Refer to the link in the Course Resources for details. Google Cloud’s Edge Points of Presence, or PoPs, are where Google's network connects to the rest of the Internet via peering. PoPs are present on over 90 Internet exchanges and at over 100 interconnection facilities around the world. For more information about these exchange points and facilities, I recommend looking at Google's PeeringDB entries links provided in the Course Resources. If you look at this map and say, "Hey, I am nowhere near one of those locations," you will want to consider Carrier Peering. If you require access to Google public infrastructure and cannot satisfy Goggle's peering requirements, you can connect via a Carrier Peering partner. Work directly with your service provider to get the connection you need and to understand the partner's requirements. For a full list of available service providers, refer to the documentation link in the Course Resources. Now, just like Direct Peering, Carrier Peering does not have an SLA. Let me compare the peering options that we just discussed. All of these options provide public IP address access to all of Google's services. The main differences are capacity and the requirements for using a service. Direct Peering has a capacity of 10 Gbps per link and requires you to have a connection in a Google Cloud Edge Point of Presence. And Carrier Peering's capacity and requirements vary depending on the service provider that you work with.

#### Choosing a connection

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567831

Now that we’ve discussed all the different connection services, let me help you determine which service best meets your hybrid connectivity needs. We started this lesson by introducing the 5 different ways to connect your infrastructure to Google Cloud. We split these services into Dedicated versus Shared connections and Layer 2 versus Layer 3 connections. Another way to organize these services is by Interconnect services and by Peering services. Interconnect services provide direct access to RFC1918 IP addresses in your VPC, with an SLA. Peering services, in contrast, offer access to Google public IP addresses only, without an SLA. Another way to choose the right service that meets your needs is with a flow diagram. Let me walk you through three diagrams, using the assumption that you want to extend your infrastructure to the cloud. Ask yourself whether you need to extend your network for Workspace services, YouTube, or Google Cloud APIs. If you do, choose one of the Peering services. If you can meet Google’s Direct Peering requirements, choose Direct Peering; otherwise, choose Carrier Peering. If you don’t need to extend your network for Workspace services or Google Cloud APIs but want to connect your Google Cloud network with other cloud services, Cross Cloud Interconnect is the option for you. Cross Cloud Connect is a Google-managed routing solution that is an ideal choice for workloads that need high bandwidth and need a Google-managed routing solution. If you’re looking to simply connect Google Cloud VPC with another cloud service and have no high bandwidth requirements and want to keep your encryption managed by Google, choose Cloud VPN. If you want to extend the reach of your network to Google Cloud, you want to pick one of the Interconnect services. Start with colocation facilities. If you cannot meet Google at one of its colocation facilities, choose Cloud VPN or Partner Interconnect. This choice will depend on your bandwidth and encryption requirements, along with the purpose of the connection. Specifically, if you have modest bandwidth needs, will use the connection for short durations and trials, and require an encrypted channel, choose Cloud VPN. Otherwise, choose Partner Interconnect. Within Partner Interconnect, you can choose between L2 Partner Interconnect and L3 Partner Interconnect. Choose L2 Partner Interconnect, if you need BGP peering and L3 Partner Interconnect if you don’t. If you can meet Google at one of its colocation facilities, you might jump to Dedicated Interconnect. However, if you cannot provide your own encryption mechanisms for sensitive traffic, feel that a 10 Gbps connection is too large, or want access to multiple cloud services, you want to consider Cloud VPN or Partner Interconnect instead. Google now supports VPN over Interconnect options. After you have made your decision on the Interconnect options, you can choose whether or not you want Google-managed encryption. If you want Google-managed encryption, choose the Cloud VPN over Interconnect option.

#### Shared VPC and VPC Peering

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567832

Let’s move our attention from hybrid connectivity to sharing VPC networks. In the simplest cloud environment, a single project might have one VPC network, spanning many regions, with VM instances hosting very large and complicated applications. However, many organizations commonly deploy multiple, isolated projects with multiple VPC networks and subnets. In this lesson, we are going to cover two configurations for sharing VPC networks across GCP projects. First, we will go over shared VPC, which allows you to share a network across several projects in your GCP organization. Then, we will go over VPC Network Peering, which allows you to configure private communication across projects in the same or different organizations. Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network so that they can communicate with each other securely and efficiently by using internal IP addresses from that network. When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the Shared VPC network. Shared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to Service Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls. A project that participates in Shared VPC is either a host project or a service project. A project that does not participate in Shared VPC is called a standalone project. This emphasizes that it is neither a host project nor a service project. A standalone VPC network is an unshared VPC network that exists in either a standalone project or a service project. VPC Network Peering, in contrast, allows private RFC 1918 connectivity across two VPC networks, regardless of whether they belong to the same project or the same organization. Now, remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the networks. For example, in this diagram there are two organizations that represent a consumer and a producer, respectively. Each organization has its own organization node, VPC network, VM instances, Network Admin, and Instance Admin. In order for VPC Network Peering to be established successfully, the Producer Network Admin needs to peer the Producer Network with the Consumer Network, and the Consumer Network Admin needs to peer the Consumer Network with the Producer Network. When both peering connections are created, the VPC Network Peering session becomes Active and routes are exchanged. This allows the virtual machine instances to communicate privately using their internal IP addresses. VPC Network Peering is a decentralized or distributed approach to multi-project networking, because each VPC network may remain under the control of separate administrator groups and maintains its own global firewall and routing tables. Historically, such projects would consider external IP addresses or VPNs to facilitate private communication between VPC networks. However, VPC Network Peering does not incur the network latency, security, and cost drawbacks that are present when using external IP addresses or VPNs. Now that we’ve talked about Shared VPC and VPC Network Peering, let me compare both of these configurations to help you decide which is appropriate for a given situation. If you want to configure private communication between VPC networks in different organizations, you have to use VPC Network Peering. Shared VPC only works within the same organization. Somewhat similarly, if you want to configure private communication between VPC networks in the same project, you have to use VPC Network Peering. This doesn’t mean that the networks need to be in the same project, but they can be. Shared VPC only works across projects. In my opinion, the biggest difference between the two configurations is the network administration models. Shared VPC is a centralized approach to multi-project networking, because security and network policy occurs in a single designated VPC network. In contrast, VPC Network Peering is a decentralized approach, because each VPC network can remain under the control of separate administrator groups and maintains its own global firewall and routing tables.

#### Quiz: Interconnecting Networks

- https://www.cloudskillsboost.google/paths/11/course_templates/178/quizzes/567833

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567834

person: In this module, we looked at five different ways of connecting your infrastructure to GCP, which are dedicated interconnect, partner interconnect, Cloud VPN, direct peering, and carrier peering. I also gave you some guidance on how to choose between different services. Remember, you might start out using one service, and as your requirements change, or new core location facilities open, you switch to a different service. I also gave you a brief overview of shared VPC and VPC network peering, which are two configurations for sharing VPC networks across GCP projects.

### Load Balancing and Autoscaling

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567835

In this module, we focus on load balancing and autoscaling. Cloud Load Balancing gives you the ability to distribute load-balanced compute resources in single or multiple regions to meet your high availability requirements, to put your resources behind a single anycast IP address, and to scale your resources up or down with intelligent autoscaling. Using Cloud Load Balancing, you can serve content as close as possible to your users on a system that can respond to over 1 million queries per second. Cloud Load Balancing is a fully distributed, software-defined, managed service. It isn’t instance- or device based, so you don't need to manage a physical load balancing infrastructure. Application Load Balancers and Network Load Balancers are two primary types of load balancers offered by Google Cloud, each designed for specific use cases. Application Load Balancers operate at the application layer, Layer 7, of the OSI model. They are ideal for applications that require load balancing based on HTTP(S) headers, cookies, or URL paths. Application Load Balancers provide features like SSL/TLS termination session affinity, and content-based routing. Network Load Balancers operate at the network layer (Layer 4) of the OSI model. They are suitable for load balancing based on IP addresses and ports. Network Load Balancers are often used for TCP and UDP traffic, as well as for scenarios where low latency and high throughput are critical. They support features like TCP/UDP load balancing and health checks. For more information on load balancers, please refer to the documentation link in the Course Resources. In this module, we cover the different types of load balancers that are available in Google Cloud. We also go over managed instance groups and their autoscaling configurations, which can be used by these load balancing configurations. You explore many of the covered features and services throughout the two labs of this module. The module wraps things up by helping you determine which Google Cloud load balancer best meets your needs.

#### Managed instance groups

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567836

Person: A managed instance group is a collection of identical VM instances that you control as a single entity using an instance template. You can easily update all the instances in a group by specifying a new template in a rolling update. Also when your applications require additional compute resources, managed instance groups can scale automatically to the number of instances in the group. Managed instance groups can work with load balancing services to distributor network traffic to all of the instances in the group. If an instance in the group stops, crashes or is deleted by an action other than the instance group commands, the managed instance group automatically recreates the instance so it can resume its processing tasks. The recreated instance uses the same name and the same instance template as the previous instance. Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all instances are running optimally. Regional managed instance groups are generally recommended over zonal managed instance groups because they allow you to spread the application load across multiple zones instead of confining your application to a single zone or having you manage multiple instance groups across different zones. This replication protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions. If that happens, your application can continue serving traffic from instances running in another zone in the same region. In order to create a managed instance group, you first need to create a instance template. Next, you're going to create a managed instance group of N specified instances. The instance group manager then automatically populates the instance group based on the instance template. You can easily create instance templates using the cloud console. The instance template dialogue looks and works exactly like creating an instance, except that the choices are recorded so they can be repeated. When you create an instance group, you define the specific rules for that instance group. First, you decide what type of managed instance group you want to create. You can use managed instance groups for stateless serving or batch workloads. such as website front end or image processing from a queue, or for stateful applications. such as databases or legacy applications. Second, provide a name for the instance group. Third, decide whether the instance group is going to be single or multizoned and where those locations will be. You can optionally provide port name mapping details. Fourth, select the instance template that you want to use. Fifth, decide whether you want to autoscale and under what circumstances. Finally, consider creating a health check to determine which instances are healthy and should receive traffic. Essentially, you're creating virtual machines, but you're applying more rules to that instance group.

#### Autoscaling and health checks

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567837

Let me provide more details on the autoscaling and health checks of a managed instance group. As I mentioned earlier, managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. Applicable autoscaling policies include scaling based on CPU utilization, load balancing capacity, or monitoring metrics, or by a queue-based workload like Pub/Sub or schedule such as start-time, duration and recurrence. For example, let’s assume you have 2 instances that are at 100% and 85% CPU utilization as shown on this slide. If your target CPU utilization is 75%, the autoscaler will add another instance to spread out the CPU load and stay below the 75% target CPU utilization. Similarly, if the overall load is much lower than the target, the autoscaler will remove instances as long as that keeps the overall utilization below the target. Now, you might ask yourself how do I monitor the utilization of my instance group. When you click on an instance group (or even an individual VM), you can choose to view different metrics. By default you’ll see the CPU utilization over the past hour, but you can change the time frame and visualize other metrics like disk and network usage. These graphs are very useful for monitoring your instances’ utilization and for determining how best to configure your Autoscaling policy to meet changing demand. If you monitor the utilization of your VM instances in Cloud Monitoring, you can even set up alerts through several notification channels. A link to more information on autoscaling can be found in the Course Resources for this module. Another important configuration for a managed instance group and load balancer is a health check. A health check is very similar to an uptime check in Cloud Monitoring. You just define a protocol, port, and health criteria, as shown in this screenshot. Based on this configuration, Google Cloud computes a health state for each instance. The health criteria define how often to check whether an instance is healthy (that’s the check interval); how long to wait for a response (that’s the timeout); how many successful attempts are decisive (that’s the healthy threshold); and how many failed attempts are decisive (that’s the unhealthy threshold). In the example on this slide, the health check would have to fail twice over a total of 15 seconds before an instance is considered unhealthy. Configuring stateful IP addresses in a managed instance group ensures that applications continue to function seamlessly during autohealing, update, and recreation events. Both internal and external IPv4 addresses can be preserved. You can configure IP addresses to be assigned automatically or assign specific IP addresses to each VM instance in a managed instance group. Preserving an instance’s IP addresses is useful in many different scenarios. Your application requires an IP address to remain static after it has been assigned. Your application’s configuration depends on specific IP addresses. Users, including other applications, access a server through a dedicated static IP address. You need to migrate existing workloads without changing network configuration. You can do the following operations by configuring stateful policy on an existing managed instance group: Configure IP addresses as stateful for all existing and future instances in the group. This will promote the corresponding ephemeral IP addresses of all existing instances to static IP addresses. And update the existing stateful configuration for IP addresses.

#### Overview of Application Load Balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567838

Now let's talk about Application Load Balancing, which acts at Layer 7 of the OSI model. This is the application layer, which deals with the actual content of each message, allowing for routing decisions based on the URL. The Application Load Balancer distributes HTTP and HTTPS traffic to backends hosted on a variety of Google Cloud platforms, such as Compute Engine, Google Kubernetes Engine, Cloud Storage, and Cloud Run, as well as external backends connected over the internet or by using hybrid connectivity. Application Load Balancers are available in the following deployment modes: external and internal. You will learn about internal Application Load Balancers later in this module. External Application Load Balancers are implemented using Google Front Ends (GFEs) or managed proxies. Global external Application Load Balancers and classic Application Load Balancers use GFEs that are distributed globally, operating together by using Google's global network and control plane. GFEs offer multi-region load balancing in the Premium tier, directing traffic to the closest healthy backend that has capacity and terminating HTTP(S) traffic as close as possible to your users. Global external Application Load Balancers and regional external Application Load Balancers use the open source Envoy proxy software to enable advanced traffic management capabilities. These load balancers can be deployed in one of the following modes: global, regional, or classic. Let me walk through the architecture of an Application Load Balancer, by using this diagram. An external forwarding rule specifies an external IP address, port, and target HTTP(S) proxy. Clients use the IP address and port to connect to the load balancer. A target HTTP(S) proxy receives a request from the client. The HTTP(S) proxy evaluates the request by using the URL map to make traffic routing decisions. The proxy can also authenticate communications by using SSL certificates. A backend service distributes requests to healthy backends. The global external Application Load Balancers also support backend buckets. One or more backends must be connected to the backend service or backend bucket. The backend services contain a health check, session affinity, a timeout setting, and one or more backends. A health check polls instances attached to the backend service at configured intervals. Instances that pass the health check are allowed to receive new requests. Unhealthy instances are not sent requests until they are healthy again. Normally, Application Load Balancing uses a round-robin algorithm to distribute requests among available instances. This can be overridden with session affinity. Session affinity attempts to send all requests from the same client to the same virtual machine instance. Backend services also have a timeout setting, which is set to 30 seconds by default. This is the amount of time the backend service will wait on the backend before considering the request a failure. This is a fixed timeout, not an idle timeout. If you require longer-lived connections, set this value appropriately. The backends themselves contain an instance group, a balancing mode, and a capacity scaler. An instance group contains virtual machine instances. The instance group may be a managed instance group with or without autoscaling or an unmanaged instance group. A balancing mode tells the load balancing system how to determine when the backend is at full usage. If all the backends for the backend service in a region are at full usage, new requests are automatically routed to the nearest region that can still handle requests. The balancing mode can be based on CPU utilization or requests per second (RPS). A capacity setting is an additional control that interacts with the balancing mode setting. For example, if you normally want your instances to operate at a maximum of 80% CPU utilization, you would set your balancing mode to 80% CPU utilization, and your capacity to 100%. If you want to cut instance utilization in half, you could leave the balancing mode at 80% CPU utilization and set capacity to 50%. Now, any changes to your backend services are not instantaneous, so don't be surprised if it takes several minutes for your changes to propagate throughout the network.

#### Example: Application Load Balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567839

Let me walk through an Application Load Balancer in action. The project on this slide has a single global IP address, but users enter the Google Cloud network from two different locations: one in North America and one in EMEA. First, the global forwarding rule directs incoming requests to the target HTTP proxy. The proxy checks the URL map to determine the appropriate backend service for the request. In this case, we are serving a guestbook application with only one backend service. The backend service has two backends: one in us-central1-a and one in europe-west1-d. Each of those backends consists of a managed instance group. Now, when a user request comes in, the load balancing service determines the approximate origin of the request from the source IP address. The load balancing service also knows the locations of the instances owned by the backend service, their overall capacity, and their overall current usage. Therefore, if the instances closest to the user have available capacity, the request is forwarded to that closest set of instances. In our example, traffic from the user in North America would be forwarded to the managed instance group in us-central1-a, and traffic from the user in EMEA would be forwarded to the managed instance group in europe-west1-d. If there are several users in each region, the incoming requests to the given region are distributed evenly across all available backend services and instances in that region. If there are no healthy instances with available capacity in a given region, the load balancer instead sends the request to the next closest region with available capacity. Therefore, traffic from the EMEA user could be forwarded to the us-central1-a backend if the europe-west1-d backend does not have capacity or has no healthy instances as determined by the health checker. This is referred to as cross-region load balancing. Another example of an Application Load Balancer is a content-based load balancer. In this case, there are two separate backend services that handle either web or video traffic. The traffic is split by the load balancer based on the URL header as specified in the URL map. If the user is navigating to /video, the traffic is sent to the backend video service, and if the user is navigating anywhere else, the traffic is sent to the web-service backend. All of that is achieved with a single global IP address.

#### Application Load Balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567840

An Application Load Balancer that is configured to use HTTP(S) has the same basic structure when configured to balance using HTTP, but differs in the following ways: An Application Load Balancer uses a target HTTPS proxy instead of a target HTTP proxy. An Application Load Balancer requires at least one signed SSL certificate installed on the target HTTPS proxy for the load balancer. The client SSL sessions terminate at the load balancer. Application Load Balancer supports the QUIC transport layer protocol. QUIC is a transport layer protocol that allows faster client connection initiation, eliminates head-of-line blocking in multiplexed streams, and supports connection migration when a client's IP address changes. For more information on the QUIC protocol, refer to the documentation link in the Course Resources. To use HTTPS, you must create at least one SSL certificate that can be used by the target proxy for the load balancer. You can configure the target proxy with up to 15 SSL certificates. For each SSL certificate, you first create an SSL certificate resource, which contains the SSL certificate information. SSL certificate resources are used only with load balancing proxies, such as a target HTTPS proxy or target SSL proxy, which we will discuss later in this module. Backend buckets allow you to use Cloud Storage buckets with Application Load Balancing. An external Application Load Balancer uses a URL map to direct traffic from specified URLs to either a backend service or a backend bucket. One common use case is: Send requests for dynamic content, such as data, to a backend service; Send requests for static content, such as images, to a backend bucket. In this diagram, the load balancer sends traffic with a path of /love-to-fetch/ to a Cloud Storage bucket in the europe-north region. All the other requests go to a Cloud Storage bucket in the us-east region. After you configure a load balancer with the backend buckets, requests to URL paths that begin with /love-to-fetch/ are sent to the europe-north Cloud Storage bucket, and all other requests are sent to the us-east Cloud Storage bucket. A network endpoint group, or NEG, is a configuration object that specifies a group of backend endpoints or services. A common use case for this configuration is deploying services in containers. You can also distribute traffic in a granular fashion to applications running on your backend instances. You can use NEGs as backends for some load balancers and with Traffic Director. Zonal and internet NEGs define how endpoints should be reached, whether they are reachable, and where they are located. Unlike these NEG types, serverless NEGs don't contain endpoints. A zonal NEG contains one or more endpoints that can be Compute Engine VMs or services running on the VMs. Each endpoint is specified either by an IP address or an IP:port combination. An Internet NEG contains a single endpoint that is hosted outside of Google Cloud. This endpoint is specified by hostname FQDN:port or IP:port. A hybrid connectivity NEG points to Traffic Director services running outside of Google Cloud. A serverless NEG points to Cloud Run, App Engine, Cloud Run functions services residing in the same region as the NEG. For more information on using NEGs, please refer to the Network endpoint groups overview page.

#### Lab Intro: Configure an Application Load Balancer with Autoscaling

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567841

Let's apply what we just covered. In this lab, you will configure an Application Load Balancer with autoscaling. Specifically, you create two managed instance groups that serve as backends in two different regions. Then, you create and stress-test a load balancer to demonstrate global load balancing and autoscaling.

#### Configure an Application Load Balancer with Autoscaling

- https://www.cloudskillsboost.google/paths/11/course_templates/178/labs/567842

#### Lab Review: Configure an Application Load Balancer with Autoscaling

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567843

In this lab, you configured an Application Load Balancer with backends in us-central1 and europe-west1. Then you stress-tested the load balancer with a VM to demonstrate global load balancing and autoscaling. You can stay for a lab walk-through, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. Right, so here we are in the GCP Console and the first thing I'm going to do is configure the HTTP and health check firewall rules. So let me go ahead and do that by navigating to VPC network and specifically firewall rules. So you'll notice that there are already some firewall rules here for ICMP internal RDP and SSH traffic. These are the ones that always come with the default network, and we're now going to create a firewall rule to allow HTTP. So let me click, create firewall rule, and I'll provide it a name. It's going to be for the default network. I'm going to specify the target tags by using HTTP server, and then we'll have to define that target tag on our instances later. The source, I'm going to set to IP ranges and just set from anywhere, and then I can specify the TCP port to 80. That's for HTTP, and then we can click create, and I'm going to create a similar firewall rule for our health checkers. So I can do that while this rule is being created, and again, on the same network, I'm going to use the same target tags. So it just applies to instances that have that tag, and now for the IP ranges, I'm going to be a lot more specific, and these are provided in the lab instructions for you, but these are the IP ranges of the health checker. Now when you enter those, make sure you enter one first. You can click space and you can see that, sort of, has acknowledged that. Then you can copy and paste the other IP range and then click away and you can see that it's got that as well. Now for the protocol supports, in this case, we're just going to specify all of TCP, but you could narrow that down a little bit depending on what kind of health check you're doing. So let me click create on that, and while these are being created, I can now create my custom image. So I'm going to go to Compute Engine, and we're going to create a VM. We're going to call that Web Server. So let me go ahead and do that. I can leave the region as us-central1, zone us-central1-a, and I'm going to now expand this option down here, management security disk network and sole tenancy. A couple things I want to do, first, under Disks, I want to make sure that this disk is not deleted when the instance is deleted, and that works because these are just persistent disks, they're just network attached, and on our networking, I'm going to define the network tag, HTTP sever, and this is going to be for our default network. So that way the firewalls that we just created are going to be plied to this instance. So let me go ahead and click Create, and once this instance is up and running, we're going to customize it by installing some software. So I'm going to just wait for the instance to be created. There it is. I can click on SSH, and I'm going to just run the commands that are in the lab instructions. So first, I'm just going to install Apache 2, and then I'm going to start the Apache server after that. We're going to double check that server by navigating to the external IP address that we have here, and that is why we attached that firewall rule for the external IP. We don't really need the firewall rule for the health checker yet, that is going to be later for our backend instances, and we haven't really configured that health check yet anyway. So here we are. It's still connecting, so let's just give it a couple of seconds. There we are. I'm going to paste those two commands in there, and let that run, and then I'm going to start the service. So let me now go back to the console and click on external IP. Here, we can see the Apache 2 default page. So we see that this has worked. Now I want to set that service to start on boot. So there's a command for that. So let me go back to my SSH terminal and paste in that command, and now I'm going to go back to Compute Engine, and for the web sever, I'm going to select Reset, and, yep, I want to make sure I do that. So I'm going to click Reset on that confirmation. So it's now going to stop and reboot the machine. I'll keep the same IP addresses and the same persistent boot disk, but the memory is essentially wiped, so therefore, the Apache service should be available after the reset and the update RC command should have been successful. So we can wait for that. We have two options of checking that status. We could navigate to the external IP address, or once it's back up, we could SSH back to the instance and just run a command to check the status, and it's telling me that the Apache service is actually running. So let's now prepare the disk and we'll create a custom image from that disk. So first, let's get out of the SSH session, and let's verify one more time that the instance that we have here has a disk associated, that that disk is not deleted when I delete the instance. I can verify that by just clicking on the name of the instance, and then I'm going to scroll down to where it talks about my boot disk. Here it is. And under 'When deleting instance', it says, keep disk, and if that was not the case, I could click edit and I could change that behavior. In our case, it's all good, so I'm going to go ahead and delete the instance. Here, it's asking me would you also want to delete that disk? Which, in our case, we're not going to do. So we are going to delete the instance, and if I go over here to disks, we can see that here, we have the disk itself. Now, we can go back to instances. We could wait for this to be deleted but the disk will remain. So really, what we can do now is get on and create an image. So I'm going to click on the Images section, and here we have the images that are available. I'm going to create my custom image, give it a name, mywebserver we're going to use as a source disk, but you can see there are lots of other options, like a snapshot, you could even do it from another disk or a Cloud Storage file. So we're going to do that from the disk. We only have one disk available, so let's choose that. We can keep all the other settings, like the encryption, the location, and I can click Create. So this is now going to create an image from that disk. And at this point, we could even delete the disk itself once that image has been created, because we're actually being charged for the disk while it's there, but for the purposes of the lab, we can leave that, as all your resources are being cleaned up in every quick labs project that you're using. So let's go ahead and configure the instance tablet and create the instance groups. So I'm going to go to Compute Engine, and I'm going to go to Instance templates, and we're going to create new instance template, and I'm going to give it a name, mywebserver-template. We're going to change the machine type to a micro. We're just doing some very small prototyping here, and now the important thing is I need to change the boot disk to select my custom image. So I'm going to change that, go to Custom images, and here, I have my web server image from this project. If you have access to other projects, you could also grab an image from there. It's all set. I can choose a size as well as the type of disk. We're just going to leave those and click Select. Now, I also need to make sure that I have the right network tags. So let me go and expand the management security and disk options. By the way, you can see that this whole instance template UI is very similar to the VM instance template, because all you're doing is you're just defining rules for the VM instances, and once you can group some of those, it will just use all those settings. So under here, I'm going to go to networking, and pick the network, and I can then make sure that I have the default network and then I want to make sure that I have my network tags so that the firewall rules that we created at the beginning are going to be applied to all the instances created from this template. So let's go click Create. That really shouldn't take long. It's just going to create a template, not create any instances yet, and sometimes if I'm a little impatient I'll just click Refresh, and we see we have everything here. So now I can click on Instance group, and create my instance group. So I'm going to start by creating an instance group in us-central1, and this is going to be a multi-zone or a regional, across the region us-central1. I could look into the zones and, maybe, un-select certain zones or select more zones if I wanted to. This is going to be based on the template that we just created, and now the important piece is we're going to have some autoscaling. So we're going to have autoscaling on, and we're going to do that on the HTTP load balance usage. This is going to be in port 80. We want a minimum of one instance and maximum of five, and we can leave the cool-down period, and you can hover over here to see that it just waits that much time before collecting information. So we have some initialization in this instance, so you want to make sure that it at least waits those 60 seconds before it starts looking into that, and then we can also go to Health check. We don't have one yet, so we can go Create a health check, and we can just call it the http-health-check, protocol, we could use HTTP or leave it as TCP 80, and this is now -- What it's going to do is it's going check every 10 seconds. It's going to wait 5 seconds in between and if there are two consecutive successes, it's successful. Three consecutive failures means it's a failure and it means it's unhealthy instance. So let me click Save and continue on that. There's this initial delay here. This is for the boot, so we're going to set that to 60 seconds. That's for the health check, and them I'm going to click Create. Now, it's telling me that, well, the autoscaling isn't really complete yet, because we haven't set up the HTTP load balancing. That's okay. We're about to do that. So let's just click okay, and we're going to repeat the same now for our instance group in europe-west1. So let me grab that name. It's also going to be a multi-zone, obviously, in this case, the region is europe-west1, same instance template, autoscaling, also based on HTTP, port 80, minimum one, maximum five, cool down, and now we can just select the health check. It seems like it doesn't have that health check yet. That could actually happen if you just go into those too fast. So let's actually click Cancel. Let's go back. Let's see if this instance group has been created. Let's try that one more time and see if we can get that health check, and there it is, okay? So we're just a little bit too fast, so that could certainly happen. So let me back track, put my information back in here, multiple zones, europe-west1, my template. I don't need to create one, I want to just select that HTTP, maximum of five, and set that initial delay, again, to 60. We don't want to wait this long for the lab, and then we're going to click Create, and it's again giving us the same warning that we just saw. So we can just click Okay. So here, we can see the creation of this instance group. We can also go to VM instances, and we'll see that one of the instance groups has already created an instance, so you can see it starts off with that name of the instance group, and MIG, by the way, it's what I put in here. That's short for Maddish Instance Group. We can see the scaling happening here. This one already has one instance. This is scaling from zero to one, and you can actually click in here and get a ton more information. If I go to monitoring, you'll see CPU usage, details, members, it will show us that it's scaling and how many it has. So you can get a lot of information by either going into the instance group page or the VM instances. So either way, we have at least once instance in each of the groups. So we are ready to now configure the backend. So let's just verify these actually. We can go to the navigation menu in VM instance. We're already here, and we could look into these IP addresses. I can click on both of these and we'll see that both of them have the default page up, so that proves that the custom image that we created earlier is actually being leveraged here. So we installed all that custom software and our backend now has that. So let's configure the HTTP load balancer. We're going to go to the navigation menu, network services, load balancing, create load balancer. This is going to be an HTTP load balancer, so we'll start that. I can choose if it's Internet facing or internal only, so from Internet to my VMs, yep. Click Continue. I can give it a name, HTTP load balancer, and I'll start by configuring the backend. I want to create a backend service. I'm going to give it a name, and I'm going to select the instance groups. So let's start first with us-central1, port number 80. The balancing mode is going to be rate, maximum of 50 requests per second, capacity 100, so just following the lab instructions here. So it just means that that load balancer tends to keep each of the instances that will happen there at or below 50 requests per second. So I can click Done and add another backend, which is just the only other one left, and let's here, for example, utilization at a CPU utilization rate of 80 and a capacity of 100. So that's just going to mean that this configuration means that a load balance attempts to keep each instance of europe-west1, at or below 80 percent CPU utilization, and I can also attach the same health check here, and then click Create. I could configure host and path rules that could define that certain traffic is being sent to other backends depending on the URL of the traffic, so video service could be sent to, maybe, a video backend versus static content to a static backend. We're not leveraging that here, so let's move on to the frontend configuration. I could give it a name, but really, I just need to specify the protocol, the IP version, let's just keep it Ephemeral, port 80, click Done, and we can review and finalize. So here we have our backend, our instances, I should say our instance groups, as well as our frontend. I could also add, if I go back here, another frontend. I have HTTP. We could also also add IPv6. So let's do that, and then we finalize, so now we have two frontends, and we'll get two IP address. So let's go ahead and create that, and once that is up and running, we should be seeing two addresses, and the one in hexadecimal format is going to be our IPv6 address and you're only going to be able to navigate to that if your connection actually allows it from where you are. Cell phones, for example, very often use IPv6. So you could maybe try to plug in the address on your cell phone and see if your able to access those backends. So let's wait for that to load up. So I click on my load balancer. It's just a frontend -- It just isn't ready yet. I went into here a bit fast, so refresh and just wait for that service to be ready, and then we can go in and get some more information about it. So here I am. The load balancer is now set up. It only took, actually, a couple more seconds. So here, we can see the IP addresses, again, this is the IPv4. This is the IPv6. So the first thing I could do is I could actually just navigate to those using my browser because I did allow HTTP traffic from anywhere. So let me just plug that into my browser, and first navigate to the IPv4, and I'm actually getting a 404 error, and the manual does talk about that. So let me also open another tab and type in the IPv6 address, and run that, and it says it hasn't found the service yet, and so the LAN manual does talk about the fact that you could be getting a 404 or a 502 for a while. So what you want to do here is just refresh for a while, and what you're really just doing is you're waiting for this configuration to be applied to all of the Google frontends, and so this is, again, a global load balance. This has to be applied everywhere, so the actual implementation, even though the console looks like everything is ready, the service can sometimes take some time to actually be reachable, and this can take a couple minutes to be set up. So just refresh a couple times, and let's wait for that to come up. All right. So here we are. I'm looking at the IPv4 address, so I just refreshed that a couple times and I can see the backend, which, as we know, should be the Apache 2 Debian default page, and I'm also navigating to the IPv6 address. I actually have access to that here, so that is working as well as expected. So now that we know the backend is working, it's time to stress test it. So what we're going to do is we're going to create another instance now, and just generate a ton of traffic to the load balancer, and then we're going to monitor that traffic. So let me open up another tab here because I want to be able to come back to the load balancer, and I'm going to create another instance now, by going back to Compute Engine, and Create instance. I'm going to define a name, just stress-test. I'm going to put this in a whole different region now. I'm going to select us-west1. Now, in terms of my backends, I have a backend us-central1 and a backend in europe-west1. The closest backend from this new instance I'm creating is going to be us-central1. So we would imagine that the traffic should be forwarded from us-west to us-central. That's going to be unless the load is too high, and let's see if we can actually break that and create a really high load so that we also have traffic that, sort of, spills over into the Europe region that we created. So I want to change the boot disk here. Let's actually select the custom image that we already have, that way we get a bunch of software preinstalled, and then I'm just going to go create that, and once that is up, I'm going to take the IP address of our load balancer, I'm going to store that in an environment variable. We'll verify to make sure that we have that, and then we're going to place a load on it. So let's wait for that instance to come up. With any new project, you always have a lot of information here on the right-hand side. It's useful to check out if you're new to GCP. The instance is up. Let me go SSH, and let's store the IP address. Now I need to grab that. So let me go back here. We're going to use the IPv4 address, and let me get my stress test back up and store that. Let's also verify, make sure this is stored, and here, we can see it returning, and it matches that IP address. That's great. And then let's run a command to place a load on our load balancer, okay? So this uses ApacheBench, and it's now going to benchmark this and this is now going to run the background. So now, what I can do is I can go back to my load balancer, which I'm looking at right now, and if I'm looking at it this way, I can actually directly look at the backend and click on HTTP backend, and we don't really have any traffic yet. This takes a little while to update here. We can see the two backends we have. We can see that one is scaling on rate. One is scaling on CPU utilization, and once we have a lot of traffic, we'll start showing here where that traffic is coming from and which instance it is going to. So what we want to do is just hang on here and refresh this page for a couple minutes until we can actually see some traffic being generated. So I'm actually just going to go back and go back in here, and no traffic yet, so let's just wait a minute or two and see what we can visualize here. All right. So this only took a couple seconds. So here we are. We can see that there's a lot of traffic coming from North America, that's from our stress test, and we can see it's going both to us-central1, which is the closest and that's where most of our traffic is going, most of requests, but we also have some traffic that is actually spilling over to our europe-west1 instance. So we can see that we have global load balancing here, and what we could do now is also we can monitor the backends to see if they're actually scaling. So if I go to Compute Engine, and refresh, we can already see that we have a bunch more backends now that are trying to handle all of this sudden increase in traffic and I really am stress testing this quite a lot. If I go to instance groups, we can get more information here. It's saying that it's already having issue with the amount of instances I've selected, the maximum that is five. If we go in here, into the europe-west1, we can get more details and monitoring. So it's showing us how it scaled up, how it's managing the load that's being placed on it, and we could also look in us-central1 and see that we now have up to five instances already across different zones, and I can also go into the monitoring here, and get more information. See that, you know, when we scaled up, and as we refresh this a little bit, we'll see more instances in here and I'll talk more about the capacity it has. I can come back here. Now we can see that we have -- because I really provided very minimal traffic to us-central1, just 50 requests per second, but I'm making, you know, almost 281 here. So now we have a lot of the traffic is spilling over. So this is a really good view to come back to, to always monitor your load balancer. Visually you can also use Stackdrive or locking and monitoring. You set up alerts, you could set up rules, so maybe you need to, you know, increase that max limit of five now. That's really a cost limit. You set that so that you don't exceed your cost too much, but if you're saying, "Oh my God, I need to work on this traffic," you could have more instances. Maybe you're getting an attack, actually, that at that point you could use a product called Cloud Armor to, maybe, allow and deny certain IP addresses, but this is really all we wanted to achieve for the lab.

#### Cloud CDN

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567844

Cloud CDN, or Content Delivery Network, uses Google's globally distributed edge points of presence to cache HTTP(S) load-balanced content close to your users. Specifically, content can be cached at CDN nodes as shown on this map. There are over 90 of these cache sites spread across metropolitan areas in Asia Pacific, Americas, and EMEA. For an up-to-date list, please refer to the Cloud CDN documentation link in the Course Resources. Now, why should you consider using Cloud CDN? Well, Cloud CDN caches content at the edges of Google's network providing faster delivery of content to your users while reducing serving costs. You can enable Cloud CDN with a simple checkbox when setting up the backend service of your Application Load Balancer. So it’s easy to enable and benefits you and your users, but how does Cloud CDN do all of this? Let’s walk through the Cloud CDN response flow with this diagram. In this example, the Application Load Balancer has two types of backends. There are managed VM instance groups in the us-central1 and asia-east1 regions, and there is a Cloud Storage bucket in us-east1. A URL map will decide which backend to send the content to: the Cloud Storage bucket could be used to serve static content and the instance groups could handle PHP traffic. Now, when a user in San Francisco is the first to access a piece of content, the cache site in San Francisco sees that it can't fulfill the request. This is called a cache miss. The cache might attempt to get the content from a nearby cache, for example if a user in Los Angeles has already accessed the content. Otherwise, the request is forwarded to the Application Load Balancer, which in turn forwards the request to one of your backends. Depending on what content is being served, the request will be forwarded to the us-central1 instance group or the us-east1 storage bucket. If the content from the backend is cacheable, the cache site in San Francisco can store it for future requests. In other words, if another user requests the same content in San Francisco, the cache site might now be able to serve that content. This shortens the round trip time and saves the origin server from having to process the request. This is called a cache hit. For more information on what content can be cached, please refer to the documentation link in the Course Resources. Now, each Cloud CDN request is automatically logged within Google Cloud. These logs will indicate a “Cache Hit” or “Cache Miss” status for each HTTP request of the load balancer. You will explore such logs in the next lab. But how do you know how Cloud CDN will cache your content? How do you control this? This is where cache modes are useful. Using cache modes, you can control the factors that determine whether or not Cloud CDN caches your content by using cache modes. Cloud CDN offers three cache modes, which define how responses are cached, whether or not Cloud CDN respects cache directives sent by the origin, and how cache TTLs are applied. The available cache modes are USE_ORIGIN_HEADERS, CACHE_ALL_STATIC, and FORCE_CACHE_ALL. USE_ORIGIN_HEADERS mode requires origin responses to set valid cache directives and valid caching headers. CACHE_ALL_STATIC mode automatically caches static content that doesn't have the no-store, private, or no-cache directive. Origin responses that set valid caching directives are also cached. And FORCE_CACHE_ALL mode unconditionally caches responses, overriding any cache directives set by the origin. You should make sure not to cache private, per-user content (such as dynamic HTML or API responses) if using a shared backend with this mode configured.

#### Network load balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567845

Network Load Balancers are Layer 4 load balancers that can distribute traffic to backends located either in a single region or across multiple regions. Let’s discuss these next. Network Load Balancers are Layer 4 load balancers that can handle TCP, UDP, or other IP protocol traffic. These load balancers are available as either proxy load balancers or passthrough load balancers. You can pick a load balancer depending on the needs of your application and the type of traffic that it needs to handle. Choose a proxy Network Load Balancer if you want to configure a reverse proxy load balancer with support for advanced traffic controls and backends on-premises and in other cloud environments. Choose a passthrough Network Load Balancer if you want to preserve the source IP address of the client packets, you prefer direct server return for responses, or you want to handle a variety of IP protocols, such as TCP, UDP, ESP, GRE, ICMP, and ICMPv6. Let's explore these in more detail. Proxy Network Load Balancers are Layer 4 reverse proxy load balancers that distribute TCP traffic to virtual machine instances in your Google Cloud VPC network. Traffic is terminated at the load balancing layer and then forwarded to the closest available backend by using TCP. Proxy Network Load Balancers can be deployed externally or internally depending on whether your application is internet-facing or internal. We will discuss internal proxy Network Load Balancers later in this module. External proxy Network Load Balancers are Layer 4 load balancers that distribute traffic that comes from the internet to backends in your Google Cloud VPC network, on-premises, or in other cloud environments. These load balancers are built on either Google Front Ends (GFEs) or Envoy proxies. These load balancers can be deployed in the following modes: global, regional, or classic. Proxy Network Load Balancers are intended for TCP traffic only, with or without SSL. For HTTP(S) traffic, we recommend that you use an Application Load Balancer instead. Depending on the type of traffic your application needs to handle, you can configure an external proxy Network Load Balancer with either a target TCP proxy or a target SSL proxy. This network diagram illustrates an external proxy Network Load Balancer configured with a target TCP proxy. In this example, traffic from users in Iowa and Boston is terminated at the global external proxy Network Load Balancer layer. From there, a separate connection is established to the closest backend instance. As in the target SSL proxy example in the next slide, the user in Boston would reach the us-east region, and the user in Iowa would reach the us-central region, if there is enough capacity. Now, the traffic between the proxy and the backends can use SSL or TCP, and we also recommend using SSL here. This network diagram illustrates an external proxy Network configured with a target SSL proxy. In this example, traffic from users in Iowa and Boston is terminated at the global external proxy Network Load Balancer layer. From there, a separate connection is established to the closest backend instance. In other words, the user in Boston would reach the us-east region, and the user in Iowa would reach the us-central region, if there is enough capacity. Now, the traffic between the proxy and the backends can use SSL or TCP. We recommend using SSL. For HTTP(S) traffic, we recommend that you use an external Application Load Balancer. Passthrough Network Load Balancers are Layer 4 regional, passthrough load balancers. These load balancers distribute traffic among backends in the same region as the load balancer. They are implemented by using Andromeda virtual networking and Google Maglev. These load balancers are not proxies. Load-balanced packets are received by backend VMs with the packet's source and destination IP addresses, protocol, and, if the protocol is port-based, the source and destination ports unchanged. Load-balanced connections are terminated at the backends. Responses from the backend VMs go directly to the clients, not back through the load balancer. The industry term for this is direct server return (DSR). These load balancers are deployed in two modes, depending on whether the load balancer is internet-facing or internal. We will discuss internal passthrough Network Load Balancers later in this module. External passthrough Network Load Balancers are built on Maglev. Clients can connect to these load balancers from anywhere on the internet regardless of their Network Service Tiers. The load balancer can also receive traffic from Google Cloud VMs with external IP addresses or from Google Cloud VMs that have internet access through Cloud NAT or instance-based NAT. Backends for external passthrough Network Load Balancers can be deployed using either a backend service or a target pool. For new deployments, we recommend using backend services. The architecture of an external passthrough Network Load Balancer depends on whether you use a backend service or a target pool to set up the backend. New network load balancers can be created with a regional backend service that defines the behavior of the load balancer and how it distributes traffic to its backend instance groups. Backend service-based external passthrough Network Load Balancers support IPv4 and IPv6 traffic, multiple protocols (TCP, UDP, ESP, GRE, ICMP, and ICMPv6 ), managed and unmanaged instance group backends, zonal network endpoint group backends with GCE_VM_IP endpoints, fine-grained traffic distribution controls, failover policies, and let you use non-legacy health checks that match the type of traffic (TCP, SSL, HTTP, HTTPS, or HTTP/2) that you are distributing. You can also transition an existing target pool-based network load balancer to use a backend service instead. But what is a target pool resource? A target pool is the legacy backend supported with external passthrough Network Load Balancers. A target pool resource defines a group of instances that receive incoming traffic from forwarding rules. When a forwarding rule directs traffic to a target pool, the load balancer picks an instance from these target pools based on a hash of the source IP and port and the destination IP and port. These target pools can only be used with forwarding rules that handle TCP and UDP traffic. Now, each project can have up to 50 target pools, and each target pool can have only one health check. Also, all the instances of a target pool must be in the same region, which is the same limitation as for the Network Load Balancer.

#### Internal load balancing

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567846

Next, let's talk about internal load balancing. Internal Application Load Balancers are Envoy proxy-based regional Layer 7 load balancers that enable you to run and scale your HTTP application traffic behind an internal IP address. Internal Application Load Balancers support backends in one region, but can be configured to be globally accessible by clients from any Google Cloud region. You can configure an internal Application Load Balancer in either regional, or cross region, internal application load balancer mode. Internal Application Load Balancers optimize traffic distribution within your VPC network or networks connected to your VPC network. You can configure an internal Application Load Balancer in the following modes: regional internal, or cross-region. A regional internal Application Load Balancer is implemented as a managed service based on the open-source Envoy proxy. Regional mode ensures that all clients and backends are from a specified region, which helps when you need regional compliance. This load balancer is enabled with rich traffic control capabilities based on HTTP(S) parameters. After the load balancer is configured, it automatically allocates Envoy proxies to meet your traffic needs. A cross-region internal Application Load Balancer is a multi-region load balancer that is implemented as a managed service based on the open-source Envoy proxy. The cross-region mode enables you to load balance traffic to backend services that are globally distributed, including traffic management that ensures traffic is directed to the closest backend. This load balancer also enables high availability. Placing backends in multiple regions helps avoid failures in a single region. If one region's backends are down, traffic can fail over to another region. The internal passthrough Network Load Balancer is a regional private load balancing service for when you need to load balance TCP, UDP, ICMP, ICMPv6, SCTP, ESP, AH, and GRE traffic, or when you need to load balance a TCP port that isn't supported by other load balancers. In other words, this load balancer enables you to run and scale your services behind a private load balancing IP address. This means that it is only accessible through the internal IP addresses of virtual machine instances that are in the same region. Therefore, configure an internal passthrough Network Load Balancer IP address to act as the frontend to your private backend instances. Because you don't need a public IP for your load-balanced service, your internal client requests stay internal to your VPC network and region. This often results in lowed latency, because all your load-balanced traffic will stay within Google's network, making your configuration much simpler. Let's talk more about the benefit of using a software-defined internal passthrough Network Load Balancer service. Google Cloud internal load balancing is not based on a device or a VM instance. Instead, it is a software-defined, fully-distributed load balancing solution. In the traditional proxy model of internal load balancing, as shown on the left, you configure an internal IP address on a load balancing device or instances, and your client instance connects to this IP address. Traffic coming to the IP address is terminated at the load balancer, and the load balancer selects a backend to establish a new connection to. Essentially, there are two connections: one between the client and the load balancer, and one between the load balancer and the backend. Google Cloud internal passthrough Network Load Balancing distributes client instance requests to the backend using a different approach, as shown on the right. It uses lightweight load balancing built on top of Andromeda, Google's network virtualization stack, to provide software-defined load balancing that directly delivers the traffic from the client instance to a backend instance. For more information on Andromeda, refer to the link in the Course Resources. Now let's take a look at internal proxy Network Load Balancers. The Google Cloud internal proxy Network Load Balancer is a proxy-based load balancer powered by open-source Envoy proxy software and the Andromeda network virtualization stack. It load balances traffic within your VPC network or networks connected to your VPC network. The internal proxy Network Load Balancer is a layer 4 load balancer that enables you to run and scale your TCP service traffic behind a regional internal IP address that is accessible only to clients in the same VPC network or clients connected to your VPC network. The load balancer first terminates the TCP connection between the client and the load balancer at an Envoy proxy. The proxy opens a second TCP connection to backends hosted in Google Cloud, on premises, or other cloud environments. Internal proxy Network Load Balancers are available in regional internal or cross-region internal deployment modes. For more use cases, refer to the Proxy Network Load Balancer overview link in the Course Resources. A regional internal proxy Network Load Balancer is implemented as a managed service based on the open-source Envoy proxy. Regional mode ensures that all clients and backends are from a specified region, which helps when you need regional compliance. This diagram shows the components of a regional internal proxy Network Load Balancer deployment in Premium Tier. This diagram shows the components of a cross-region internal proxy Network Load Balancer deployment in Premium Tier within the same VPC network. Each global forwarding rule uses a regional IP address that the clients use to connect. This is a multi-region load balancer that is implemented as a managed service based on the open-source Envoy proxy. The cross region-mode lets you load balance traffic to backend services that are globally distributed, including traffic management that ensures traffic is directed to the closest backend. This load balancer also enables high availability. Placing backends in multiple regions helps avoid failures in a single region. If one region's backends are down, traffic can fail over to another region. Now, internal load balancing enables you to support use cases such as the traditional 3-tier web services. In this example, the web tier uses an external Application Load Balancer that provides a single global IP address for users in San Francisco, Iowa, Singapore, and so on. The backends of this load balancer are located in the us-west1, us-central1, and asia-east1 regions because this is a global load balancer These backends then access an internal Network Load Balancer in each region as the application or internal tier. The backends of this internal tier are located in us-west1-a, us-central1-b, and asia-east1-b. The last tier is the database tier in each of those zones. The benefit of this 3-tier approach is that neither the database tier nor the application tier is exposed externally. This simplifies security and network pricing.

#### Lab Intro: Configure an Internal Load Balancer

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567847

Let's apply some of the internal load balancer concepts that we just discussed in a lab. In this lab, you’ll create two managed instance groups in the same region. Then you configure and test an internal Network Load Balancer with the instances groups as the backends, as shown in this network diagram.

#### Configure an Internal Network Load Balancer

- https://www.cloudskillsboost.google/paths/11/course_templates/178/labs/567848

#### Lab Review: Configure an Internal Load Balancer

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567849

In this lab, you created two managed instance groups in the us-central1 region, along with firewall rules to allow HTTP traffic to those instances, and TCP traffic from the Google Cloud health checker. Then, you configured and tested an internal load balancer for those instance groups. You can stay for a lab walkthrough, but remember that Google Cloud’s user interface can change, so your environment might look slightly different. So here I'm in the GCP console and in this lab, similar to other labs, we've actually pre-created some resources for you. You can explore those, again, if you go to a navigation menu and then go to deployment manager, you'll see a deployment here. We created a network with two subnets and some firewall rules. We can also just explore those by navigating to VPC network, that's what the lab instructions actually mention. So I can click there. I already have the default network and here is that extra network I've with the two subnets. And I also have some firewall rules for those right here, to allow ICMP and SSH, and RDP. All right. So what we're going to do now, is we're going to create some more firewall rules, we're going to create one for HTTP, and then we're also going to create some for the health check. So let me just click create firewall rule, and this is going to be fairly similar to what we already did for the HTTP load balancer lab, the big difference is we now have our own network that we're going to apply this to. We're also going to have target tags, load balancer backend, IP ranges, we want to HTTP from anywhere and HTTP would be TCP-80, so we can click create, and then we're just going to repeat the same thing for the health checker. So let me copy the name of the firewall rule, apply it to the right network, have the load balancer backend as the target tag. Now the IP ranges, we're going to copy them one by one in here, so let me paste one, headspace, let me grab the other one, and paste that as well. And for now, I'm just going to do all ports under GCP, but you could be a little bit more specific depending on what you want your health checker to look for. So let's click create, and now we're going to configure instance templates and instance groups. So let me navigate to compute engine and then instance templates. And we're going to create a template in there and just call it instance template one then we click create. It's actually the name that's already in there. Then I can expand management security disnetworking. Now, a couple things, first of all, in the HTTP load balancer, we had a custom image, in this case, we're actually going to set up a startup script. So under the metadata, I'm going to provide as a key, the startup script URL and in a Cloud storage bucket, that's publicly accessible. We've placed a startup file and you could go in there and you could actually review that and the link to that is in the lab. Then I'm going to go to networking. I've created all these firewall rules, they apply to specific network tags, and they're also for a specific network. So let me make sure I have the right network selected and select denetwork tag, and this is going to be for subnet A. So now I can click create, and then we're going to create another instance template for subnet B. So let me just wait for this to be created, and then I'm just going to create another one from there by selecting it, and then clicking copy. It's going to change the name automatically, and the main difference is I now need to make sure I select the different subnet. This is going to be for subnet B, and then I click create as well. So once we have these up, we can now create the managed instance groups, so let me navigate to instance groups and start up by creating our first one, just call it instance group one. This is going to be a single zone. It's going to be in US central one A. We're going to use instance template one, and we're going to select the, this will be based on CPU usage, let's set 80 as a usage minimum of one maximum of five, and I could change the cool on period for example, to 45 seconds, and now I can click create. I could also attach a health check here or just attach that later to the load balancer. So let me click create, and we're going to repeat the same for the instant script two, and this is going to be now another one. It's going to be based on the other instance template also in US central one. Let's do that in B, for example, change the target CPU usage to match what we had earlier of 80 maximum of five, cool line of 45. And then we can go ahead and create that as well. So if I click on VM instances now, I should already have an instance from the first instance group and if I come back here, I had to refresh to see that other one, we can see that the other instances now being created for the instance group two. So we can, you know, verify, again, that they're being created here. So here we see, we have now one instance group each. So now what we can do is, we're going to create a utility VM to navigate to these instances. And so we can see also, by the way, if we look at their internal IP addresses, that they're both part of a different siter range, and if I click on nick zero here, we can see which network interface this is part of. You can see it's part of subnet A, that's correct. And if I click on the other one, we can see this is part of subnet B. Okay. So each subnet now has an instance group in it. So let me create another instance, this is going to be our utility VM. Now, an internal load balancer is regional, so I want to use the same region. We could use a different zone, let's say US central 1F, I need a very small machine only for this, and for, I want to make sure this is in the right network, so let me expand this option down here, networking and make sure that this is in my right network. I have the choice of the two different subnets I have in there, let's see if it's subnet A and if I want to match this to the network diagram that we have, I can specify the actual internal IP. So instead of Ephemeral automatic, I can choose Ephemeral custom and then just type in that IP address. And again, this is just to match that network diagram that we have. I can click down on that, and then I can go ahead and create that. All right. Now the lab instructions say, you know, make sure that the IP addresses that you have match the lab instructions, this is because these are the first available IP addresses. Again, the first IP, our first, and second is reserved as well as the last and second to last. So that's why we start with the .2 here and here we have a .50 because we defined that. So now I can go SSH to the utility VM and all the curl commands are based on these two IPS. So if yours are different, you maybe want to see if you have some other instances that you need to delete first. And I'm going to curl first to this first IP here. So let me just copy that directly from the lab instructions. And this is, what's displayed here is just the page that we've set up for these instances, this comes directly from the startup script, and it's just telling me the IP address where I'm coming from, while I'm coming from this utility VM, it has the name that's telling me that it's coming to this instance and it tells me the region and zone. And I can repeat the same for the other instance. And it's now telling me, again, from the same address, but different instance and a different zone. And this is going to be really useful for when we have the internal load balancers set up and we curl the load balancer [INAUDIBLE] itself, we should be able to see that if we curl several times, that we're kind of hopping between the different backends that we have established. So we can actually exit out of here for now, and what we're going to do now, is configure the internal load balancer. So to do that, I'm going to go to the navigation menu, go to network services, load balancing, we're going to create a load balancer, this is going to be a TCP load balancing. So we start that, it going to be only between VMs, this is an internal load balancer. When I do that, it restricts me to be regional. We covered that in the slides that the internal load balancer is regional. So we click continue, and then I'll give it a name, just call it my internal load balancer. We are going to configure the backend. Specifically, this is in a specific region, which is US central one. The network is my internal app, and then the instance group, we are going to click first instance group one, click done, and then add another backend, which is going to be instance group two, and click done for that. Now, we didn't create a health check earlier, we can do that here now. So let me just go create a health check, just call it my internal health check TCP 80. That's great. And here we again, have the health, the criteria, how often it's going to check what the internal is, the timeout and how it's going to define if a backend is healthy or unhealthy. So let's save and continue that. And we can see that we have a blue check mark, this is all set up. So now I can click on the frontend configuration, the subnetwork, let's, for example, put this in subnet B. For internal IP, we could actually reserve a static internal IP address. Let's give that a name, it's just called my internal advanced IP. And rather than assigning automatically, we could choose our own because it's just an internal IP and we could match this, again, to the network diagram. So it's going to be 10.10.30.5, let's reserve that. And then we're going to finish the configuration for the load balancer by setting the port here to 80, and then we're going to click done, and now we can review and finalize this. We have our two backends, we see the auto scaling on that, we see the zones, we see, and we have the frontend itself. So we have the exact IP address, the way we can then access this internal load balancer. So let me click create, and then let's wait for the load balancer to be created before we move on to the next step. So here we are, actually clicked refresh, and we can see that the load balancer is all set up. Now we specified the IP address, so I don't have to grab it from here, instead, I'm going to go back to my compute engine instances and use the utility VM to navigate to our load balancer IP. So I'm just going to curl that and since I had that startup script on the backends that defines which instance I'm looking at, this is now going to give me some more information. So I'm going to curl the IP address, and the first time I did it, you can see it targeted instant group two. Let's run that one more time, instance group two again, group two again. Let's maybe run the command a couple more times and let's see if we can get a couple different backends. So here, run it a couple times, we can see that's 2, 2, 2, 2, 2, then it's got 1, 2, 2, 2, 1. So we can certainly see that it is load balancing between the different backends that we have. And that's the end of the lab.

#### Choosing a load balancer

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567850

Now that we’ve discussed all the different load balancing services within Google Cloud, let me help you determine which load balancer best meets your needs. To determine which Cloud Load Balancing product to use, you must first determine what traffic type your load balancers must handle. As a general rule, you'd choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP(S) traffic. You'd choose a proxy Network Load Balancer to implement TLS offload, TCP proxy, or support for external load balancing to backends in multiple regions. You'd choose a passthrough Network Load Balancer to preserve client source IP addresses, avoid the overhead of proxies, and to support additional protocols like UDP, ESP, and ICMP, or if you need to expose client IP addresses to your applications. You can further narrow down your choices depending on your application's requirements: whether your application is external, (internet-facing), or internal, and whether you need backends deployed globally, or regionally. If you prefer a table over a flow chart, we recommend this summary table. The load-balancing scheme is an attribute on the forwarding rule and the backend service of a load balancer and indicates whether the load balancer can be used for internal or external traffic. The term MANAGED in the load-balancing scheme indicates that the load balancer is implemented as a managed service either on Google Front Ends or on the open source Envoy proxy. In a load-balancing scheme that is MANAGED, requests are routed either to the Google Front End or to the Envoy proxy. For more information on Network Service Tiers, refer to the documentation link in the Course Resources.

#### Quiz: Load Balancing and Autoscaling

- https://www.cloudskillsboost.google/paths/11/course_templates/178/quizzes/567851

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567852

In this module, we looked at the different types of load balancers that are available in Google Cloud, along with managed instance groups and autoscaling. You were able to apply most of the covered services and concepts by working through the two labs of this module. We also discussed the criteria for choosing between the different load balancers and looked at a flowchart and a summary table to help you pick the right load balancers. Remember, sometimes it's useful to combine an internal and an external load balancer to support three-tier web services.

### Infrastructure Automation

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567853

Person: Now that we've covered several of Google Cloud's services and features, it makes sense to talk about how to automate the deployment of Google Cloud Infrastructure. Calling the Cloud API from code is a powerful way to generate infrastructure. But writing code to create infrastructure also has some challenges. One issue is that the maintainability of the infrastructure depends directly on the quality of the software. For example, a program could have dozens of locations that call the Cloud API to create VMs. Fixing the problem with the definition of one VM would require you first identifying which of the dozen calls actually created it. Standard software development best practices will apply and it's important to note that applications undergo changes rapidly requiring maintenance on your code. Clearly another level of organization is needed, and that's the purpose of Terraform. Terraform uses a system of highly structured templates and configuration files to document the infrastructure in an easily readable and understandable format. Terraform conceals the actual Cloud API calls so you don't need to write the code and can focus on the definition of the infrastructure. In this module we will cover how to use Terraform to automate the deployment of infrastructure and how to use Google Cloud Marketplace to launch infrastructure solutions. You will use Terraform to deploy a VPC network, a firewall rule and VM instances in the lab for this module. Let's start by talking about Terraform.

#### Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567854

So far, you have been creating Google Cloud resources using the Google Cloud console and Cloud Shell. We recommend the console when you are new to using a service or if you prefer a UI. Cloud Shell works best when you are comfortable using a specific service and you want to quickly create resources using the command line. Terraform takes this one step further. Terraform is one of the tools used for Infrastructure as Code or IaC. Before we dive into understanding Terraform, let’s look at what Infrastructure as Code is. In essence, infrastructure as code allows for the quick provisioning and removing of infrastructures. The on-demand provisioning of a deployment is extremely powerful. This can be integrated into a continuous integration pipeline that smoothens the path to continuous deployment. Automated infrastructure provisioning means that the infrastructure can be provisioned on demand, and the deployment complexity is managed in code. This provides the flexibility to change infrastructure as requirements change. And all the changes are in one place. Infrastructure for environments such as development and test can now easily replicate production and can be deleted immediately when not in use. All because of infrastructure as code. Several tools can be used for IaC. Google Cloud supports Terraform, where deployments are described in a file known as a configuration. This details all the resources that should be provisioned. Configurations can be modularized using templates which allow the abstraction of resources into reusable components across deployments. In addition to Terraform, Google Cloud also provides support for other IaC tools, including: Chef Puppet Ansible Packer In this course we will focus on Terraform. Terraform lets you provision Google Cloud resources—such as virtual machines, containers, storage, and networking—with declarative configuration files. You just specify all the resources needed for your application in a declarative format and deploy your configuration. HashiCorp Configuration Language (HCL) allows for concise descriptions of resources using blocks, arguments, and expressions. This deployment can be repeated over and over with consistent results, and you can delete a whole deployment with one command or click. The benefit of a declarative approach is that it allows you to specify what the configuration should be and let the system figure out the steps to take. Instead of deploying each resource separately, you specify the set of resources which compose the application or service, allowing you to focus on the application. Unlike Cloud Shell, Terraform will deploy resources in parallel. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. This enables you to deploy almost everything we have seen so far, from instances, instance templates, and groups, to VPC networks, firewall rules, VPN tunnels, Cloud Routers, and load balancers. For a full list of supported resource types, a link to the Using Terraform with Google Cloud documentation page is included in the Course Resources. The Terraform language is the user interface to declare resources. Resources are infrastructure objects such as Compute Engine virtual machines, storage buckets, containers, or networks. A Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories. The syntax of the Terraform language includes: Blocks that represent objects and can have zero or more labels. A block has a body that enables you to declare arguments and nested blocks. Arguments are used to assign a value to a name. An expression represents a value that can be assigned to an identifier. Terraform can be used on multiple public and private clouds. Terraform is already installed in Cloud Shell. The example Terraform configuration file shown starts with a provider block that indicates that Google Cloud is the provider. The region for the deployment is specified inside the provider block. The resource block specifies a Google Cloud Compute Engine instance, or virtual machine. The details of the instance to be created are specified inside the resource block. The output block specifies an output variable for the Terraform module. In this case, a value will be assigned to the output variable "instance_ip." Let’s look at a simple example in Terraform. Before you get into the lab, let me walk you through how Terraform can be used to set up an auto mode network with an HTTP firewall rule. For this example we are going to define our infrastructure in a single file, main.tf. As our infrastructure becomes more complex we can build each element in a separate file to make the management easier. Let’s start with the main.tf file. The main.tf file is where we specify the infrastructure we wish to create. It is like a blueprint for our desired state. First we define the provider. Next we define our network, setting the auto create subnetworks flag to true which will automatically create a subnetwork in each region. We also set the mtu to 1460. Next, we define our firewall. Here we are allowing TCP access to port 80 and 8080. Terraform takes this main.tf file and uses it as the specification for what to create. Once we have completed the main.tf file, we can deploy the defined infrastructure in Cloud Shell. We use the command terraform init to initialize the new Terraform configuration. We run this command in the same folder as the main.tf file. The terraform init command makes sure that the Google provider plugin is downloaded and installed in a subdirectory of the current working directory, along with various other bookkeeping files. You will see an "Initializing provider plugins" message. Terraform knows that you're running from a Google project, and it is getting Google resources. The terraform plan command performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files. This command is a convenient way to check whether the execution plan for a set of changes matches your expectations without making any changes to real resources or to the state. The terraform apply command creates the infrastructure defined in the main.tf file. Once this command has completed you will be able to access the defined infrastructure.

#### Lab Intro: Automating the Deployment of Infrastructure Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567855

Person: Let's apply what you've just learned in a hands-on lab where you'll automate the deployment of VPC networks, firewall rules and VM instances. You deploy an auto mode network called My Network with a firewall rule to allow HTTP, SSH or DP and ICMP traffic. You also deploy the VM instances shown in this network diagram.

#### Automating the Deployment of Infrastructure Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/178/labs/567856

#### Lab Review: Automating the Deployment of Infrastructure Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567857

Philipp: In this lab, you created a Terraform configuration with a module to automate the deployment of GCP infrastructure. As your configuration changes, Terraform can create incremental execution plans, which allocates you to build your overall configuration step-by-step. The Instance module allowed you to reuse the same resource configuration for multiple resources while providing properties as input variables. You can leverage the configuration and module that you created as a starting point for future deployments. You can stay for a lab walk through, but remember that GCP's user interface can change. So your environment might look slightly different. So here I am in the GCP console, and the first thing we want to do is configure our Cloud Shell environment to use Terraform. Terraform is actually now integrated into Cloud Shell, so let's just start by verifying which version is installed. So I'm going to click and activate Cloud Shell, and then start Cloud Shell. And then we're going to run the Terraform version command to verify the version. Let me run that here, and then we'll see that this is the current version that is configured. You see that there's an even newer version here. That's fine. You could go, download that and their instructions in the lab on how to do that, but the lab instructions will work with the 12.2 or anything later. So we're ready to go. I'm going to set up a folder for us, and then we're going to launch the code editor, which is this little pencil icon up here. And we're going to use the code editor now to work in that folder that we just created and place all of our files in there. And that's going to be a much more interactive experience rather than using a command-line editor like Nano, so let's just wait for that to come up. In the meantime, I can clear this, and the first thing we're going to do once we're in here is, we're going to create a file called Provider TF, and this is going to help us initialize Terraform because Terraform uses a plug-in-based architecture to support many different infrastructure and service providers, so the provider file will specify that we're using Google as the provider. So let me right-click on TF Infra, create a new file. Plug in Provider TF, and then we're just going to copy in that the provider is Google. And I can save that, and autosave is actually enabled, so I won't have to click save all the time. And then within Cloud Shell, I'm going to navigate to that folder, and then I'm going to run the Terraform init command. And this is going to now initialize the provider, so we can see here this is the provider version. It's been initialized, so now we're ready to work with Terraform and Cloud Shell. So let's start off by configuring my network. I'm just going to now create a new file in this folder, call it My Network TF, and I'm going to copy the base code that we have in the lab instructions. So in here we have a comment. We have the resource, the type along with the name, and then we will also have resource properties. And this is a base template that's great for starting any resources in GCP, and we'll use the name and the field -- the name field as well as the type field and properties to really define what each of these resources do. So first things first, I want to replace the type with Google_compute_network, and what's important here is to also include these quotes for all of the resources that we're going to define. And this is just ABPC network. You can find more out about this in the two documentation links that are in the lab. One link is to the Google's hot platform documentation, and the other link is to the Terraform documentation. Now I also want to replace the name, so we're going to replace the resource name with My Network, again, quotes are important. And then we're going to create some properties. This is going to be an auto mode network, which means that all of these subnets are automatically created. I need to define that. Properties are optional for some resources, but in this case it's required for us to say that auto-create subnetworks is true, right? Now, I can verify that my file looks exactly like what's provided in the lab, and that seems to be true. It has moved around and spaced out some of these properties. There's a command we'll run later that will actually do that for us as well, so that's not really critical right now. I can go ahead and save this. Now next I want to configure the firewall rule. I have again some base code for that, so let me just paste that below my network resource. And we're going to create a file rule that will allow HTTP, SSH, RDP and ICMP, so I want to obviously find the right type. Now, you could look that up in the Terraform documentation or use what's in the lab instructions, and that is Google Compute Firewall. And again, we need to place the quotes around that and have a space between these two. I'm also going to have a name that's going to be the name of the firewall rule, the one that we'll actually see within GCP when we create this. And now a couple of different resource properties that I need to provide. If you think of a firewall rule, there are a couple of key things. There is the network to which the firewall rule applies. There are the source IP ranges and the protocols and ports. If you don't define the source IP ranges, it's just going to take 0.0.0/0, so in our case we're going to define the network. So let me paste that in here, and because this firewall rule depends on its network, we're using this self link reference here. And this instructs Terraform to resolve these resources in a dependent order, so in this case the network needs to be created before the firewall rule is created. We're going to do the same when we create the VM instances, so let me also now add the properties to allow and -- to allow a certain combination of protocols and ports. Specifically, I'm going to allow TCP22 for SSH, 80 for HTTP, 33 for RDP, and then the whole ICMP protocol. And then I can verify that this looks just like the instructions that are given to me, and that is the case. So I can go ahead and save that, but it's really being autosaved, so no need to hit save all the time in here. So now we're going to configure DVM instances, and what we're going to do is, we're going to create an instance module. And a module is just something that's a reusable configuration inside a folder, so we'll create one module. And we'll use it for both of the VM instances that we're going to create. To do that, we need to create a folder for the module, so let me create a new folder here, call it Instance Within the TF Infra Folder. And you see it created it outside, so I'm going to drag it in this folder. Alternatively, I could have right-clicked and created it, and the lab does show the hierarchy of these folders. And now within this folder, I'm going to create a file and call it Main TF. All right. And now within this file, we're going to again copy some base code to get us started. We have the resource type, the resource name and the type it's going to be a Google Compute instance, so let me replace that with the quotes. Now, rather than giving it a name and kind of hard-coding that, I'm going to now use a variable because I want to be able to create multiple instances with multiple different names, so I'm going to replace TF name with this construct. And then we'll later have to define from the parent configuration how to affect this module. We're also going to add some properties, which are the zone and machine type, and here again we are using variables that we'll have to define. We will also add a boot disk, and now the boot disk will just sort of hard code. We'll give it an image, and they'll be used for all of the instances that we create. And then we're also going to add a network interface, and in there we have to define a subnetwork. So where does this instance live? And if I just provide this construct here, it's going to allocate an external IP address or a public IP address to my instance. So now I need to define some input variables, right, so I'm using an input variable for name, zone, the type and the subnetwork. So let me add some stuff on top of my resource, and specifically I'm going to add a variable for the name and zone. I'm also going to define the instance type, and if I provide a value in these brackets, then that's going to be the default value. So if I don't provide another value from my configuration, it will just use this type, and that's kind of the default anyway. So that may be a good thing to do. We could have done something similar with the image, and that way we could control the image through an input variable. So now I'm just going to verify that my configuration, or I should say this module, looks exactly like the lab instructions, and that is true. So now I can go on and save this. And the next thing we need to do is we've defined the module, but now we need to use the module within my configuration. So in here I have a network and a file rule, but I also now need to say, "I want to create VM instances. I'm going to provide these input variables, and this is the module that I want you to use." So I'm just going to copy the lab instructions. Here I'm defining the module. I'm giving it the name, and then I'm defining the source. This lives in the instance folder, and then I'm just providing three of the four input variables because I already have a default value for one of them. Now, important again is I'm going to use the self link reference here because I cannot create these instances, nor the firewall rule, until the network is created. After that, all of these resources can and will be created in parallel , and we'll see that in a second, so let's go ahead and set this all up. I'm going to now just work from Cloud Shell. Let me clear this up here. We're going to run the Terraform FMT command, and this just rewrites the files into a canonical format and style. And if I do that, you might have just seen that everything got indented a little bit here and there. That's not really that critical, it's just telling us it did that, and specifically it touched the My Network TF file. And if you get an error here, you want to, you know, make sure that your configuration looks similar to the ones that we have so far. We also link the configuration to all of the three TS files the provider might not recommend in the lab instructions, so you can always refer to them and make sure that they align with what you have, and if not, you know, fix what's different. Now I'm going to need to run the Terraform init command again, and I need to mainly do that because I now have a module. It's going to say, "Oh, there's some modules that need to be used. Let me initialize those." So we've done that, and now we can go ahead and plan our configuration. We can say, "Okay, we're ready to go. Tell me what you would create when I run this command." So Terraform plan is going to run through this. It's going to tell me it's going to create these resources here. It's telling me that a lot of the values are provided, but some of the values won't be known until after it's created. And specifically it's going to add four different things, the VPC network, the firewall rule and the two instance, so if we're all good with that, we can run the Terraform apply command. It's actually going to walk us through those resources one more time, but now it's going to ask us if we're ready. So we just type yes in here. And it's going to start creating the resources, and you can see the network is the first resource that is being created here. And once the network is created, it's going to start creating all the other resources in parallel. It also gives us an update every 10 seconds saying it's still working on this, and that's pretty interesting. That way you can see that at least it's still working on this and it didn't get stuck on something. So let's wait for this to complete, and then we'll check back in. So here we can see that all of the resources were created. As I mentioned, the network gets created first, and once that's completed you can see one of the instances, the firewall rule and the other instances are starting to be created. Instances were created really quickly, and then we're just waiting for the firewall rule to be created. Now let's actually verify that all of these resources were created by navigating back to the GCP console, so is going to switch tabs here and go to the navigation menu. And first go to VPC network, and every network comes by default with a default network that is here. And here we can see the My Network that we created, which is an auto mode network. I can also go to the firewall rules, and I'll see that my custom firewall rule with the non-default firewall rule has been created. And that should allow me to ping between the two instances that I have in a network. I have ICMP traffic allowed, so I should be able to ping both in the external IP address, but even the internal IP address because both of these instances are on the same network. So let's try that out. I'm going to go back to the navigation menu, go to Compute Engine, and I'm going to grab the IP address of this first VM, and then SSH to this other VM. And then we'll try to ping that instance. So here I am. Let me run ping three times on that IP address, and we can see that all the packets were transmitted. So this should work again because both VM instances are in the same network, and the firewall rule that we created allows ICMP traffic. And that's the end of the lab.

#### Google Cloud Marketplace

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567858

Person: Let's learn a little more about Google Cloud Marketplace. Google Cloud Marketplace let's you quickly deploy functional software packages that run on Google Cloud. Essentially Cloud Marketplace offers production grade solutions from third-party vendors who have already created their own deployment configurations based on Terraform. These solutions are billed together with all of your project's Google Cloud Services. If you already have a license for a third-party service, you might be able to use a bring-your-own-license solution. You can deploy a software package now and scale that deployment when your application requires additional capacity. Google Cloud even updates the images of these software packages to fix critical issues and vulnerabilities, but doesn't update software that you've already deployed. You even can get direct access to partner support.

#### Demo: Launch Infrastructure Solutions on Google Cloud Marketplace

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567859

Philipp: Let me show you how to launch infrastructure solutions on GCP Marketplace. My goal is to deploy a LAMP stack to a single compute engine instance. A LAMP stack consists of Linux, Apache HTTP server, MySQL, and PHP. So here I am in the GCP Console. And let's go ahead and navigate to the GCP Marketplace. I'm going to go to the navigation menu and just go to Marketplace. Now I have lots of different options available. There's some filters on the left that could search directly. There's some featured solutions that are in here. So there's really a lot of stuff to choose from. In my case, I'm going to search for LAMP stack, because that's what I want to create. And here, I actually have different options. They're different providers, that's really what that means. The different providers will offer these services. I'm going to click the first one that's in here. Now I have the configuration page. I see the package contents. Tells me what LAMP is again. I can see that also here, the operating system is Linux. It has Apache installed. I have PHP, and I have MySQL. And we should also obviously have HTTP enabled and we'll see that in a second. There's no usage fee for the service. If there was, it would all be billed together. We have an instance billing. This is just an N1 standard one instance along with its persistent disk and there's a sustained use discount. So if I click on launch and compute engine, I get the actual VM configuration page. I could now change the instance type if I wanted. I could create a larger instance, a small instance, I could customize an instance. And now because this is an Apache HTTP, we can see that the HTTP firewall role is also set up. I also have some networking options if I want to, you know, place this somewhere else. I even have some extra options if I want to install phpMyAdmin. All that is available to me here. And I have logging options for Stackdriver to enable Stackdriver logging and monitoring and I can do that directly in here. So it's just like a regular VM instance page. So I'm going to go and click deploy. And when I do that, it's going to navigate us to Deployment Manager. And you can see all of the configuration, as well as all the imported files that were just displayed there that are used throughout this deployment. So we can see again that the solutions on Marketplace are just Deployment Manager configurations that are already set up for you to use so that you don't have to recreate them. I also see that a password is being generated, a VM is being generated. I can click on that and get some more information about it. We can see the recent software and we have that HTTP firewall role, so just TCP80 that's being enabled here. So we can just wait for that. The instance is up. It's just configuring some more software. And then once it's up and running, we can get some more information about that LAMP stack that we just have generated. So I can click back on LAMP. It's still pending, but once it's up and running, we'll have some more information here. Let's see. It doesn't have the address yet, it's still pending. And there we go. So we have an address, we have a user, we have a password, the instance. So all the type of information. We can visit the site, we can SSH to this. We have some next steps. We could open also HTTPS traffic, change the password, assign a static external IP address rather then the current default ephemeral IP address. We can learn more about the software that's being installed. But we can also look at this from a Compute Engine perspective. So if I navigate to Compute Engine, I'll also see the instance right here. That's how easy it is to launch infrastructure solutions on GCP Marketplace.

#### Quiz: Infrastructure Automation

- https://www.cloudskillsboost.google/paths/11/course_templates/178/quizzes/567860

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567861

Person: In this module we automated the deployment of infrastructure using Terraform, and we looked at infrastructure solutions in Cloud Marketplace. Now, you might say that going through all the effort to deploy a network, a firewall rule and two VM instances doesn't convince you to use Terraform. That's true if you only need to create those resources once and don't foresee ever needing to create them again. However, for those of us who manage several resources and need to deploy, update and destroy them in a repeatable way, an infrastructure automation tool like Terraform becomes essential.

### Managed Services

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567862

In the last module, we discussed how to automate the creation of infrastructure. As an alternative to infrastructure automation, you can eliminate the need to create infrastructure by leveraging a managed service. Managed services are partial or complete solutions offered as a service. They exist on a continuum between platform-as-a-service and software-as-a-service, depending on how much of the internal methods and controls are exposed. Using a managed service allows you to outsource a lot of the administrative and maintenance overhead to Google if your application requirements fits within the service offering. In this module, we give you an overview of BigQuery, Dataflow, Dataprep by Trifacta, and Dataproc. Now, all of these services are for data analytics purposes, and since that's not the focus on this course series, there won't be any labs in this module. Instead, we'll have a quick demo to illustrate how easy it is to use managed services.

#### BigQuery

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567863

Let’s start by talking about BigQuery. BigQuery is Google Cloud's serverless, highly-scalable, and cost-effective cloud data warehouse. It is a petabyte scale data warehouse that allows for super-fast queries using the processing power of Google's infrastructure. Because there is no infrastructure for you to manage, you can focus on uncovering meaningful insights using familiar SQL without the need for a database administrator. BigQuery is used by all types of organizations. You can access BigQuery by using the Google Cloud console, by using a command-line tool, or by making calls to the BigQuery REST API using a variety of client libraries such as Java, . NET, or Python. There are also several third-party tools that you can use to interact with BigQuery, such as visualizing the data, or loading the data. Here is an example of a Standard SQL query on a table called groceries. This query produces one output column for each column in the table groceries, aliased as g.

#### Dataflow

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567864

Let's learn a little bit about Dataflow. Dataflow is a managed service for executing a wide variety of data processing patterns. It's essentially a fully managed service for transforming and enriching data in stream and batch modes with equal reliability and expressiveness. With Dataflow, a lot of the complexity of infrastructure setup and maintenance is handled for you. It's built on Google Cloud infrastructure and autoscales to meet the demands of your data pipelines, allowing it to intelligently scale to millions of queries per second. Dataflow supports fast, simplified pipeline development via expressive SQL, Java, and Python APIs in the Apache Beam SDK, which provides a rich set of windowing and session analysis primitives, as well as an ecosystem of source and sync connectors. Dataflow is also tightly coupled with other Google Cloud services, like Google Cloud Observability, so you can set up priority alerts and notifications to monitor your pipeline and the quality of data coming in and out. This diagram shows some example use cases of Dataflow. As I just mentioned, Dataflow processes stream and batch data. This data could come from other Google Cloud services like Datastore or Pub/Sub, which is Google's messaging and publishing service. The data could also be ingested from third-party services like Apache Avro and Apache Kafka. After you transform the data with Dataflow, you can analyze it in BigQuery, Vertex AI, or even Bigtable. Using Looker Studio, you can even build real time dashboards for IoT devices.

#### Dataprep

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567865

Let's learn a little bit about Dataprep. Dataprep is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning. Because Dataprep is serverless and works at any scale, there is no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don't have to write code. With automatic schema, data type, possible joins, and anomaly detection, you can skip time-consuming data profiling and focus on data analysis. Dataprep is an integrated partner service operated by Trifacta and based on their industry-leading data-preparation solution, Trifacta Wrangler. Google works closely with Trifacta to provide a seamless user experience that removes the need for up-front software installation, separate licensing costs, or ongoing operational overhead. Dataprep is fully managed and scales on-demand to meet your growing data-preparation needs, so you can stay focused on analysis. Here's an example of a Dataprep architecture. As you can see, Dataprep can be leveraged to prepare raw data from BigQuery, Cloud Storage, or a file upload before ingesting it onto a transformational pipeline like Dataflow. The refined data can then be exported to BigQuery or Cloud Storage for analysis and machine learning.

#### Dataproc

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567866

Let's learn a little bit about Dataproc. Dataproc is a fast, easy-to-use, fully managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler way. You only pay for the resources you use with per-second billing. If you leverage preemptible instances in your cluster, you can reduce your cost even further. Without using Dataproc, it can take from 5 to 30 minutes to create Spark and Hadoop clusters on-premises or through other infrastructure-as-a-service providers. Dataproc clusters are quick to start, scale, and shut down, with each of these operations taking 90 seconds or less, on average. This means you can spend less time waiting for clusters and more hands-on time working with your data. Dataproc has built-in integration with other Google Cloud services such as BigQuery, Cloud Storage, Bigtable, Cloud Logging and Cloud Monitoring. This provides you with a complete data platform rather than just a Spark or Hadoop cluster. As a managed service, you can create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data. If you're already using Spark, Hadoop, Pig, or Hive, you don't even need to learn new tools or APIs to use Dataproc. This makes it easy to move existing projects into Dataproc without redevelopment. Now, Dataproc and Dataflow can both be used for data processing, and there’s overlap in their batch and streaming capabilities. So how do you decide which product is a better fit for your environment? Well, first, ask yourself whether you have dependencies on specific tools or packages in the Apache Hadoop or Spark ecosystem. If that's the case, you'll obviously want to use Dataproc. If not, ask yourself whether you prefer a hands-on or DevOps approach to operations, or a hands-off or serverless approach. If you opt for the DevOps approach, you want to use Dataproc, otherwise, use Dataflow.

#### Demo: Dataproc

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567867

Let’s look at how to create a Dataproc cluster, modify the number of workers in the cluster, and submit a simple Apache Spark job. So here I am in the GCP console. And the first thing I want to do is navigate to Cloud Dataproc. It's pretty far down, so let's navigate down to big data. We have Dataproc. And it's going to check if there is already a cluster, which we don't have, so we can go ahead and now create a cluster. And we can start off by defining the name. Let's just call it our example cluster. And then I'm not going to change any of the other settings. I'll just kind of highlight them. We can define where this is stored, what regions and zones, what kind of mode, which defines the relationship between nodes and workers. We want to have one master and workers. You can also have a high availability setting where you have three masters and then define the name of the workers. You have the machine types available for the master node, so four virtual CPUs. And then we also have the workers. There also can be four virtual CPUs, and they're going to be two of them. So in total, this itself is going to create 12 virtual CPUs. If we go to the advanced options, we could make some of these nodes preemptible. We can define the network [Indistinct] network tags in terms of firewall rules, make this internal IP only, Cloud storage bucket for staging, image. You can see there are lots of other options, all the way down to the specific encryption. So let me go ahead and just create this with the default configuration. And click create. And again, this is going to create a bunch of different machines for us now. If I open another tab and actually navigate to Compute Engine, we'll see all those instances being generated for us. So I can go to Compute Engine. So even though this is a managed service, we can see all the instances. They're all ready. So we have the master and we have our two worker nodes, and they just take the name that I specified. That attaches M for master, W for worker, and starts with a zero index. So if I come back here, I can refresh. The cluster itself is still being initialized. That's the software that's being installed and all the setup is happening in the back end. And once the cluster is ready, we can go ahead and we could maybe resize that. We see that we currently have two worker nodes. We could change that to something else that can be three worker nodes. And then after that, we're actually going to go ahead and submit a job for this. So here we are. Just took another minute or two. We have the cluster up and running. I can go click on the cluster itself and I can get more information about it. So here we have all sorts of monitoring set up. If I go to the VM instances, I'll see those. [Indistinct] master, any jobs I have, which currently we don't have any yet. And if I click on the configuration, we'll see that we currently have two worker nodes. And if I click on edit, I can change that. So let's say we want three worker node. We can change that to three and hit save, and it's now going to go ahead and request that update for us. So it's going to create another worker and it's also going to update the master and let the master know that there is another worker out there so when we submit jobs, all the workers are being leveraged. So if I change back to Compute Engine, here we see the new worker is already up and running. And if come back here and refresh, you can see that the cluster itself is still being updated. And this again should just take a minute or two. Pretty fast. Again, this is a managed service, but we can see the actual back-end instances that are being leveraged. So here we can see the cluster update is complete. I can click on it again and go to the configuration. We can see that we now have three worker nodes. So time to submit a job. Let's go to the job section and click on submit a job. I can leave the job ID, leave the region. Obviously you want to select the cluster, especially if I had multiple clusters. The job type in this case is going to be Spark. I'm going to define a main class. This is just from the example class. And what we're going to do actually is we're going to provide an example to calculate the value of pi. So arguments, I'm just going to give it 1,000. And jar file, I'm going to provide that as well. And then I can review that. There's lots of other things. I have properties, labels. So I'm all set, so I'm going to click submit on this job. So it's going to go ahead and submit that. And that job is now running. That's the status symbol that's on here right now. I can go click on that job itself. And here I can see the job actually running. I can also review the configuration one more time, so here you see all the different settings that I just specified. And we can go back to output. And again, this is now going to do a rough calculation for us to estimate the value of pi. So we'll just wait for that. And here we go. It says pi is roughly this. So the job is now complete. And if this is all that we wanted to do, we could go ahead and delete the cluster. Otherwise, we could submit more jobs. In our case, we're done. So let's go back to the cluster, select it and click delete. It's going to delete also all the data. Can't undo this. Okay. Click that. You can go to Compute Engine, refresh here. We can already see that all of these are now being stopped and will then be deleted. And that way you can easily spin up clusters and delete them so that you're only being charged for the uses of the cluster while you need it. So we can wait around for this to be deleted. So that just took another minute or two. We can see that the cluster itself is deleted. And if I go to the instances, we can also see that all the instances are gone. That's how easy it is to create a Dataproc cluster and submit a job to that cluster.

#### Quiz: Managed Services

- https://www.cloudskillsboost.google/paths/11/course_templates/178/quizzes/567868

#### Module Review

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567869

In this module, we provided you with an overview of managed services for data processing in Google Cloud, namely BigQuery, Dataflow, Dataprep, and Dataproc. Managed services allow you to outsource a lot of the administrative and maintenance overhead to Google, so you can focus on your workloads instead of the infrastructure. Speaking of infrastructure, most of the services that we covered are serverless. Now, this doesn't mean that there aren't any actual servers processing your data. Serverless means that servers, or Compute Engine instances, are obfuscated so that you don't have to worry about the infrastructure. Dataproc isn't a serverless service, because you were able to view and manage the underlying master and worker instances.

#### Course Series Review

- https://www.cloudskillsboost.google/paths/11/course_templates/178/video/567870

Thank you for taking the Architecting with Google Compute Engine course series. I hope you have a better understanding of the comprehensive and flexible infrastructure and platform services provided by Google Cloud. I also hope that the demos and labs made you feel more comfortable with using the different Google Cloud services that we covered. Now it's your turn. Go ahead and apply what you have learned by architecting you own infrastructure in Google Cloud. See you next time!

#### What’s Next? Get Certified

- https://www.cloudskillsboost.google/paths/11/course_templates/178/documents/567871

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/178/documents/567872

### Your Next Steps

## 07: Getting Started with Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/2

### Course Introduction

#### Welcome and getting started guide

- https://www.cloudskillsboost.google/paths/11/course_templates/2/documents/562154

#### Course introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562155

Welcome to the “Getting Started with Google Kubernetes Engine” course. If you’re interested in Kubernetes, a software layer that sits between your applications and your hardware infrastructure, then you’re in the right place! Google Kubernetes Engine brings you Kubernetes as a managed service on Google Cloud. The goal of this course is to introduce the basics of GKE, as it’s commonly referred to, and how to get applications containerized and running in Google Cloud. Through a combination of videos, quizzes, and hands-on labs, you’ll get comfortable interacting with GKE. The intended audience for this course will have a basic proficiency of command-line tools, Linux operating system environments, and web server technologies such as Nginx. It’s also helpful to have systems operations experience, including deploying and managing applications, either on-premises or in a public cloud environment. The course starts with a basic introduction to Google Cloud, is followed by an overview of containers and Kubernetes, Kubernetes architecture, and Kubernetes operations. Okay, let’s get started!

### Introduction to Google Cloud

#### Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562156

Before you jump into the world of Kubernetes, let’s take some time to introduce you to, or remind you of, some Google Cloud core concepts. In this first section of this course, you’ll explore: The definition of cloud computing. The services Google Cloud offers architects and developers to build solutions. How Google’s powerful global network can power Google Cloud services. How Google Cloud resources are structured and managed. Tools to ensure that your organization doesn't accidentally face a big Google Cloud bill. And four different ways to interact with Google Cloud to commence work. You’ll also get some hands-on practice accessing the Cloud console and Cloud Shell.

#### Cloud computing and Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562157

Let’s start at the beginning with an overview of cloud computing. Cloud computing is a way of using information technology, IT, that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they need with no need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they’re flexible, so customers can be. If they need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. That's it. That's the definition of cloud. Google Cloud offers a variety of services for architects and developers to use to build solutions. Some might sound familiar, like virtual machines, whereas others might represent a totally new paradigm, like Google Kubernetes Engine. A common first request that organizations make of Google Cloud is to run some code in the cloud. Google offers a range of computing services to help fulfill that request. We’ll explore these options next.

#### Google Cloud compute offerings

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562158

As organizations design for the future and start running more compute workloads in Google Cloud, it’s important to be aware of the options available. Google offers a range of computing services, let’s explore each. The services are: Compute Engine, GKE, App Engine, Cloud Run, and Cloud Run functions. At the end of this lesson, you’ll understand why people choose each. The first is Compute Engine. Compute Engine is an IaaS offering, or infrastructure as a service, which provides compute, storage, and network virtually that are similar to physical data centers. Compute Engine provides access to predefined and customized virtual machine configurations. At the time this training was developed, VMs could be as large as 416 vCPUs with more than 12 TB of memory. Virtual machines require block storage, and Compute Engine offers two main choices: Persistent disks and local SSDs. Persistent disks offer network storage that can scale up to 257 TB, and can disk snapshots for backup and mobility. Alternatively, local SSDs enable high input/output operations per second. Compute Engine workloads can be placed behind global load balancers that support autoscaling. They offer a feature called managed instance groups with which resources can be defined to automatically deploy to meet demand. Compute Engine costs can be controlled with the help of per-second billing. This means that when compute resources are deployed for short periods of time, like with batch processing jobs, costs can stay low. Compute Engine also offers preemptible virtual machines, which provide significantly cheaper pricing for workloads that can be interrupted safely. Compute Engine is a popular choice for developers because: It provides complete control over infrastructure since operating systems can be customized, and it can run applications that rely on a mix of operating systems. On-premises workloads can easily be lifted and shifted to Google Cloud without needing to rewrite applications or make any changes. And it’s the best option when other computing options don’t support your application or requirements. Google Cloud’s second compute offering, and the focus of this course, is Google Kubernetes Engine. GKE runs containerized applications in a cloud environment, as opposed to on an individual virtual machine, like Compute Engine. A container represents code packaged with all its dependencies. The third computing service offered by Google is App Engine. App Engine is a fully managed PaaS offering, or platform as a service. PaaS offerings bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic instead of deployment. This means developers can just upload code and App Engine will deploy the required infrastructure. It supports popular languages like Java, Node.js. , Python, PHP, C#, . NET, Ruby, and Go and can also be used to run container workloads. App Engine is closely integrated with Cloud Monitoring, Cloud Logging, Cloud Profiler, and Error Reporting. App Engine also supports version control and traffic splitting. App Engine is a good choice for developers that: Want to focus on writing code. Want to focus on building applications instead of deploying and managing the environment. And don’t need to build a highly reliable and scalable infrastructure. Some of the most common App Engine use cases include: websites, mobile app and gaming backends, and a method to present a RESTful API, which is an application program interface that resembles the way a web browser interacts with a web server, to the internet. App Engine makes them easy to operate. Another compute option offered by Google Cloud is Cloud Run. Cloud Run is a managed compute platform that runs stateless containers through web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It’s built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you never pay for over-provisioned resources. And finally, there is Cloud Run functions. Cloud Run functions is a lightweight, event-based, asynchronous compute solution for creating small, single-purpose functions that respond to cloud events, without the need to manage a server or a runtime environment. It executes code in response to events, like when a new file is uploaded to Cloud Storage. It’s also a completely serverless execution environment. Cloud Run functions is often referred to as functions as a service. Simply upload code written in Node.js, Python, Go, Java, . Net Core, Ruby, or PHP; and Google Cloud will automatically deploy the appropriate computing capacity to run that code. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also be used to connect and extend cloud services. You’re billed to the nearest 100 milliseconds, but only while your code is running. Cloud Run functions also provides a perpetual free tier, so many Cloud Run function use cases can be free of charge. What are common Cloud Run functions? It can be part of a microservices application architecture. It is used to build simple, serverless mobile or IoT backends or integrate with third-party services and APIs. Files uploaded to a Cloud Storage bucket can be processed in real time. Similarly, the data can be extracted, transformed, and loaded for querying and analysis. And it can be part of intelligent applications such as virtual assistants, video or image analysis, and sentiment analysis.

#### The Google network

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562159

Behind the services provided by Google Cloud, lies a huge range of resources, from physical assets like servers, to virtual resources like virtual machines and containers. It all runs on Google’s own global network. According to some publicly available estimates, Google’s network carries as much as 40% of the world’s internet traffic every day. Google’s network is the largest network of its kind, and Google has invested billions of dollars over the years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by using more than 100 content caching nodes worldwide. These are locations where high demand content is cached for quicker access, which allows applications to respond to user requests from the location that provides the quickest response time. Google Cloud’s infrastructure is based in seven major geographic locations: North America, South America, Europe, Africa, the Middle East, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you use Compute Engine to launch a virtual machine, it will run in the zone that you specify to ensure resource redundancy. You can also run resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection if issues with an entire region occur, such as a natural disaster. Google Cloud currently supports 124 zones in 41 regions, although this number is constantly increasing. You can find the most up-to-date numbers at cloud.google.com/about/locations.

#### Resource management

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562160

Every Google Cloud resource you use must belong to a project. But what is a project? A project is a container for all your Google Cloud resources. It provides a way to organize resources, manage billing, and control access. Each project even has a unique identifier. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. Resources are at the first level. These represent containers, virtual machines, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let’s look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they’re billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can’t be changed after creation. They’re what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don’t have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It’s helpful to know that these Google-generated numbers exist, but we won’t explore them much in this course. They’re mainly used internally by Google Cloud to keep track of resources. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. And when an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The “who” part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A “who” is also called a “principal.” Each principal has its own identifier, usually an email address. The “can do what” part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. So, who is responsible for security in the cloud? It’s a shared responsibility between you and Google Cloud. A general guideline for shared responsibility is that "if you configure or store it, you're responsible for securing it." This means that a cloud provider is responsible for securing the parts of the cloud that it directly controls, such as hardware, networks, and physical security. At the same time, the customer is responsible for securing anything that they create within the cloud, such as the configurations, access policies, and user data. No matter which cloud provider you use, there is shared responsibility.

#### Billing

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562161

Let’s explore how billing works in Google Cloud. Billing is established at the project level of the Google Cloud resource hierarchy. This means that when you define a Google Cloud project, you link a billing account to it. This billing account is where you will configure all your billing information, including your payment option. A billing account can be linked to zero or more projects, but projects that aren’t linked to a billing account can only use free Google Cloud services. Billing accounts are charged automatically and invoiced every month or at every threshold limit. Billing subaccounts can be used to separate billing by project. Some Google Cloud customers who resell Google Cloud services use sub accounts for each of their own clients. Now, you’re probably thinking, “How can I ensure that I don’t accidentally face a big Google Cloud bill?” We provide a few tools to help. You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month’s spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you’ll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud console that lets you monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack. This way both account owners and the Google Cloud community as a whole are protected. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources that you can have in your projects. For example, by default, each Google Cloud project has a quota that allows it no more than five Virtual Private Cloud networks. Although all projects start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

#### Interacting with Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562162

Now that you had a chance to explore how resources in Google Cloud run, are organized, and billed to you, it’s time to see how you actually interact with Google Cloud to start your work. You can use four Google products to access and interact with Google Cloud. The Google Cloud console, the Google Cloud SDK and Cloud Shell, the APIs, and the Google Cloud app. Let’s explore each. The first is the Google Cloud Console, which is Google Cloud’s graphical user interface, or GUI, and it helps you deploy, scale, and diagnose production issues in a simple web-based interface. With the Google Cloud console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Google Cloud console also provides a search facility to quickly find resources and connect to instances through SSH in the browser. The Google Cloud console is available from console.cloud.google.com. You’ll get some experience with the Cloud console during an upcoming lab. The second products are the Google Cloud SDK and Cloud Shell. Unlike the Google Cloud console, you can download and install the Google Cloud SDK locally onto a computer. The Google Cloud SDK is a set of tools you can use to manage resources and applications hosted on Google Cloud. These include the gcloud tool, which provides the main command-line interface for Google Cloud products and services, gcloud storage, which lets you access Cloud Storage from the command line, and bq, a command-line tool for BigQuery. When installed, all of the tools within the Google Cloud SDK are located under the bin directory. Cloud Shell provides command-line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent 5 gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. Each Cloud Shell VM is ephemeral, which means that it will be stopped whenever you stop using it interactively, and it’ll be restarted when you re-enter Cloud Shell. It also provides web preview functionality and built-in authorization for access to Cloud console projects and resources, including your GKE resources. With Cloud Shell, the Google Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The Cloud console’s GKE area has a web-based interface for administering GKE resources. The Cloud Shell is the place to launch commands to administer those GKE resources. Some of those commands are from the Google Cloud SDK, and others will be specific to your workload. Later in this course, you’ll learn about the kubectl command, and you can see it being launched from Cloud Shell here. The third way to access Google Cloud is through application programming interfaces, or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Cloud Console includes a tool called the Google APIs Explorer that shows which APIs are available, and in which versions. You can try these APIs interactively, even those that require user authentication. One important point to note is that developers often use APIs to build applications that allocate and manage Google Cloud resources. However, our present focus is on letting Kubernetes manage resources for us. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app isn’t relevant for the purposes of this course.

#### Lab Introduction: Accessing the Cloud console and Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562163

It’s time to gain some hands-on experience with some of the Google Cloud tools featured in this section of the course. In the lab titled “Accessing the Google Cloud console and Cloud Shell,” you’ll get practice creating buckets, virtual machines, and service accounts from both the Google Cloud console and Cloud Shell. You’ll also get practice executing other commands through Cloud Shell.

#### Accessing the Google Cloud Console and Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/2/labs/562164

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/2/quizzes/562165

### Introduction to Containers and Kubernetes

#### Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562166

Containerization helps development teams move fast, deploy software efficiently, and operate at an unprecedented scale. But what exactly are they, how do they work, and where does Kubernetes come into play? The goal of this second section of the course was designed to help answer those questions. You’ll explore containers, container images, Kubernetes, and Google Kubernetes Engine. You’ll also get hands-on practice with Cloud Build. Let’s get started!

#### Containers

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562167

So, what is a container? To answer that question, we’ll need to first explore how applications are deployed. Not long ago, the common way to deploy an application was on a local computer. To set one up, you needed physical space, power, cooling, and network connectivity. Then you needed to install an operating system, any software dependencies, and finally, the application. When you needed more processing power, redundancy, security, or scalability, you’d add more computers. And it was very common for each computer to have a single purpose, for example, for a database, web server, or content delivery. This practice wasted resources and took a lot of time to deploy, maintain, and scale. Then came virtualization, which is the process of creating a virtual version of a physical resource, such as a server, storage device, or network. Virtualization made it possible to run multiple virtual servers and operating systems on one local computer. The software layer that breaks the dependencies of an operating system on the underlying hardware and allows several virtual machines to share that hardware is called a a hypervisor. Kernel-based Virtual Machine, or KVM, is one well-known hypervisor. Today, you can use virtualization to deploy new servers fairly quickly. With virtualization, it takes less time to deploy new solutions. Fewer resources are wasted, and portability is improved because virtual machines can be imaged and easily moved. However, an application, all its dependencies, and operating system are still bundled together. It’s not easy to move a VM from one hypervisor product to another, and every time you start a VM, its operating system takes time to boot up. But running multiple applications within a single VM creates another problem: applications that share dependencies are not isolated from each other. The resource requirements of one application can starve other applications of the resources they need. Also, a dependency upgrade for one application might cause another to stop working. You can try to solve this problem with rigorous software engineering policies. For example, you can lock down the dependencies so that no application is allowed to make changes; but this can lead to new problems because dependencies need to be upgraded occasionally. You can also add integration tests to ensure applications work as intended. However, dependency problems can cause novel failure modes that are hard to troubleshoot. Plus, it really slows down development if you have to rely on integration tests to confirm the basic integrity of your application environment. The VM-centric way to solve this problem is to run a dedicated virtual machine for each application. Each application maintains its own dependencies, and the kernel is isolated so one application won't affect the performance of another. The result is that two complete copies of the kernel are running. Scale this to hundreds or thousands of applications and you see its limitations. A more efficient way to resolve the dependency problem is to implement abstraction at the level of the application and its dependencies. You don’t have to virtualize the entire machine, or even the entire operating system–just the user space. The user space is all the code that resides above the kernel, and it includes applications and their dependencies. This is what it means to create containers. Containers are isolated user spaces for running application code. Containers are lightweight because they don’t carry a full operating system, can be scheduled or integrated tightly with the underlying system, which is efficient, and can be created and shut down quickly, because they just start and stop operating system processes and do not boot an entire VM or initialize an operating system for each application. With containers, you can still develop application code in the usual ways–on desktops, laptops, and servers. However, the container can execute final code on VMs. The application code is packaged with all the dependencies it needs, and the engine that executes the container is responsible for making them available at runtime. But what makes containers so appealing to developers? First, they’re a code-centric way to deliver high-performing, scalable applications. Second, containers provide access to reliable underlying hardware and software. With a Linux kernel base, developers can be confident that code will run successfully regardless if it's on a local machine or in production. And if incremental changes are made to a container based on a production image, it can be deployed quickly with a single file copy. This speeds up development. And finally, containers make it easier to build applications that use the microservices design pattern–that is, with loosely coupled, fine-grained components. This modular design pattern allows the operating system to scale and upgrade components of an application without affecting the application as a whole.

#### Container images

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562168

An application and its dependencies are called an image, and a container is simply a running instance of an image. By building software into container images, developers can package and ship an application without worrying about the system it will run on. But to build and run container images, you need software. One option is Docker. Although this open-source technology can be used to create and run applications in containers, it doesn’t offer a way to orchestrate those applications at scale like Kubernetes does. Later in this course, you’ll use Google’s Cloud Build to create Docker-formatted container images. A container has the power to isolate workloads, and this ability comes from a combination of several Linux technologies. The first is the foundation of the Linux process. Each Linux process has its own virtual memory address space, separate from all others, and Linux processes can be rapidly created and destroyed. The next technology is Linux namespaces. Containers use Linux namespaces to control what an application can see, such as process ID numbers, directory trees, IP addresses, etc. It’s important to note that Linux namespaces are not the same thing as Kubernetes namespaces, which you'll learn more about later in this course. The third technology is Linux cgroups. Linux cgroups control what an application can use, such as its maximum consumption of CPU time, memory, I/O bandwidth, and other resources. And finally, containers use union file systems to bundle everything needed into a neat package. This requires combining applications and their dependencies into a set of clean, minimal layers. Let’s explore how this works. A container image is structured in layers, and the tool used to build the image reads instructions from a file called the container manifest. For Docker-formatted container images, that’s called a Dockerfile. Each instruction in the Dockerfile specifies a layer inside the container image. Each layer is read-only, but when a container runs from this image, it will also have a writable, ephemeral topmost layer. Let’s explore a simple Dockerfile. A Dockerfile contains four commands, each of which creates a layer. For the purposes of this training, this Dockerfile has been a little oversimplified for modern use. The FROM statement starts by creating a base layer, which is pulled from a public repository. This one happens to be the Ubuntu Linux runtime environment of a specific version. The COPY command adds a new layer, which contains some files copied in from your build tool’s current directory. The RUN command builds the application by using the “make” command and puts the results of the build into a third layer. And finally, the last layer specifies what command you should run within the container when it is launched. When you write a Dockerfile, the layers should start with those least likely to change at the top, and the layers most likely to change at the bottom. So I mentioned this Dockerfile example is oversimplified. Let me explain what I meant. Currently, it’s not a best practice to build your application in the same container where you ship and run it. After all, your build tools are at best just clutter in a deployed container, and at worst they are an additional attack surface. Today, application packaging relies on a multi-stage build process, where one container builds the final executable image, and a separate container receives only what is needed to run the application. When launching a new container from an image, the container runtime adds a new writable layer on top of the underlying layers. This layer is called the container layer. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer. And they’re ephemeral, which means that when the container is deleted, the contents of this writable layer are lost forever. The underlying container image remains unchanged. So when it comes to application design, this means that permanent data must be stored somewhere other than a running container image. Because each container has its own writable container layer, and all changes are stored in this layer, multiple containers can share access to the same underlying image and while still maintaining their own data state. This allows container images to get smaller with each layer. For example, a base application image might be 200 MB, but the difference to the next point release might only be 200 KB. When building a container, instead of copying the entire image, it creates a layer with just the difference. When running a container, the container runtime pulls down the layers it needs. When updating a container, only the difference needs to be copied. This is much faster than running a new virtual machine. So, how can you get or create containers? It’s common to use publicly available open-source container images as the base for your own images, or for unmodified use. Google maintains Artifact Registry at pkg.dev, which contains public, open source images. It also provides Google Cloud customers with a place to store their own container images and is integrated with Identity and Access Management (IAM). This allows storing container images that are private to your project. Container images are also available in other public repositories, like the Docker Hub Registry and GitLab. Google provides a managed service for building containers called Cloud Build. Cloud Build is integrated with Cloud IAM and was designed to retrieve the source code builds from different code repositories, including Cloud Source Repositories, or git-compatible repositories like GitHub and Bitbucket. To generate a build with Cloud Build, you must define a series of steps. For example, you can configure build steps to fetch dependencies, compile source code, run integration tests, or use tools such as Docker, Gradle, and Maven. Each build step in Cloud Build runs in a Docker container. From there, Cloud Build can deliver the newly built images to various execution environments including Google Kubernetes Engine, App Engine, and Cloud Run functions.

#### Lab introduction: Working with Cloud Build

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562169

Now it’s time to gain some hands-on experience with Cloud Build. In the lab titled, “Working with Cloud Build,” you’ll use provided code to build a Docker container image and a Dockerfile. From there, you’ll upload the container to the Google Cloud Artifact Registry, which is a private Docker repository to securely store and manage Docker images.

#### Working with Cloud Build

- https://www.cloudskillsboost.google/paths/11/course_templates/2/labs/562170

#### Kubernetes

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562171

So let’s say that your organization has implemented containers, and because containers are so lean, your coworkers are creating them in numbers that exceed the counts of virtual machines you used to have. Let’s also say that the applications that run in the containers need to communicate over the network, but you don’t have a network fabric that lets containers find each other. Kubernetes can help. So, what is Kubernetes? Kubernetes is an open source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud, which is a virtual machine that runs in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Kubernetes supports declarative configurations. When you administer your infrastructure declaratively, you describe the desired state you want to achieve, instead of issuing a series of commands to achieve that desired state. Kubernetes’s job is to make the deployed system conform to your desired state and to keep it there in spite of failures. Declarative configuration saves you work. Because the system’s desired state is always documented, it also reduces the risk of error. Kubernetes also allows imperative configuration, in which you issue commands to change the system’s state. One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state you declare. Therefore, experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool when building a declarative configuration. Now that you have a better understanding of what Kubernetes is, let’s explore some of its features. Kubernetes supports different workload types. It supports stateless applications, such as Nginx or Apache web servers, and stateful applications where user and session data can be stored persistently. It also supports batch jobs and daemon tasks. Kubernetes can automatically scale containerized applications in and out based on resource utilization. Kubernetes allows users to specify resource request levels and resource limits for workloads. Resource controls help Kubernetes improve the overall workload performance within a cluster. Kubernetes is extensible through a rich ecosystem of plugins and addons. For example, Kubernetes Custom Resource Definitions let developers define new types of resources that can be created, managed, and used in Kubernetes. And finally, because it’s open-source, Kubernetes is portable and can be deployed anywhere–whether on premises or on another cloud service provider. This means that Kubernetes workloads can be moved freely without vendor lock-in.

#### Google Kubernetes Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562172

What if you started using Kubernetes, but the infrastructure is too much to maintain? This is where Google Kubernetes Engine comes in. Google Kubernetes Engine is a managed Kubernetes service hosted on Google’s infrastructure. It’s designed to help deploy, manage, and scale Kubernetes environments for containerized applications. GKE is fully managed, which means the underlying resources don’t have to be provisioned, and a container-optimized operating system is used to run workloads. Google maintains these operating systems, which are optimized to scale quickly with a minimal resource footprint. Google Kubernetes Engine offers a mode of operation called GKE Autopilot, which is designed to manage your cluster configuration, like nodes, scaling, security, and other preconfigured settings. When you use GKE, you start by directing the service to create and set up a Kubernetes system for you. This system is called a cluster. The GKE auto-upgrade feature ensures that clusters are always upgraded with the latest stable version of Kubernetes. The virtual machines that host containers in a GKE cluster are called nodes. GKE has a node auto-repair feature that was designed to repair unhealthy nodes. It performs periodic health checks on each node of the cluster and nodes determined to be unhealthy are drained and recreated. Just like Kubernetes supports scaling workloads, GKE supports scaling the cluster itself. GKE is integrated with several services: Cloud Build uses private container images securely stored in Artifact Registry to automate the deployment. IAM helps control access by using accounts and role permissions. Google Cloud Observability provides an understanding into how an application is performing. And Virtual Private Clouds, which provide a network infrastructure including load balancers and ingress access for your cluster. And finally the Google Cloud console provides insights into GKE clusters and their resources, and a way to view, inspect, and delete resources in those clusters. Although open source Kubernetes provides a dashboard, it takes a lot of work to set it up securely. With the Google Cloud console, however, there is a more powerful dashboard for your GKE clusters and workloads that you don’t have to manage.

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/2/quizzes/562173

### Kubernetes Architecture

#### Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562174

Google Kubernetes Engine makes it easy to recognize the benefits of innovation initiatives without getting stuck troubleshooting infrastructure issues and managing daily operations related to enterprise-scale container deployment. But how does Kubernetes expect you to tell it what to do? And what choices do you have for describing your workloads? The third section of the course was designed to answer those questions. You’ll explore: Kubernetes concepts, like Kubernetes object model and the principal of declarative management. A list of Kubernetes components. Google Kubernetes Engine concepts, including the Autopilot and standard modes of operation. And Kubernetes object management. You’ll also get hands-on practice deploying a sample pod in GKE. Let’s get started!

#### Kubernetes concepts

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562175

To understand how Kubernetes works, it’s important to understand two related concepts. The first concept is the Kubernetes object model. Each item Kubernetes manages is represented by an object, and you can view and change these objects attributes and state. The second concept is the principle of declarative management. Kubernetes needs to be told how objects should be managed, and it will work to achieve and maintain that desired state. This is accomplished through a "watch loop." A Kubernetes object is defined as a persistent entity that represents the state of something running in a cluster: its desired state and its current state. Various kinds of objects represent containerized applications, the resources available to them, and the policies that affect their behavior. Kubernetes objects have two important elements. The first is an object spec for each object being created. It’s here that the desired state of the object is defined by you. The second is the object status, which represents the current state of the object provided by the Kubernetes control plane. By the way, “Kubernetes control plane” is a term to refer to the various system processes that collaborate to make a Kubernetes cluster work. You’ll learn about these processes later. Each object represents a certain type, or “kind,” as it’s referred to in Kubernetes. Pods are the foundational building block of the standard Kubernetes model, and they’re the smallest deployable Kubernetes object. Every running container in a Kubernetes system is in a Pod. A Pod creates the environment where the containers live, and that environment can accommodate one or more containers. If there is more than one container in a Pod, they are tightly coupled and share resources, like networking and storage. Kubernetes assigns each Pod a unique IP address, and every container within a Pod shares the network namespace, including IP address and network ports. Containers within the same Pod can communicate through localhost, 127.0.0.1. A Pod can also specify a set of storage volumes that will be shared among its containers. Let’s explore an example where you want three instances of an nginx Web server, each in its own container, to be always kept running. How can this be achieved in Kubernetes? You’ll recall that Kubernetes operates off of the principle of declarative management, which means that you’ll need to declare some objects to represent those nginx containers, and in this case, those objects should be Pods. From there, it’s Kubernetes’s job to launch those Pods and keep them in existence. Alright, so you’ve given Kubernetes a desired state that consists of three nginx Pods, to be always kept running. We did this by telling Kubernetes to create and maintain one or more objects that represent them. Now Kubernetes will compare the desired state to the current state. For this example, let’s say our declaration of three nginx containers is completely new, meaning the current state does not match the desired state. So Kubernetes, specifically its control plane, will remedy the situation. Because the number of desired Pods running for the object you declared is 3, and 0 are presently running, 3 will be launched. And the Kubernetes control plane will continuously monitor the state of the cluster, endlessly comparing reality to what has been declared, and remedying the state as needed.

#### Kubernetes components

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562176

The Kubernetes control plane is the fleet of cooperating processes that make a Kubernetes cluster work. Although you might only directly work with a few of these components, it’s important to understand what the fleet does and the role they each play. In this section of the course, you’ll get an opportunity to see how a Kubernetes cluster is constructed, part by part. This will help illustrate how a Kubernetes cluster that runs in GKE is easier to manage than one you provisioned yourself. First, a cluster needs computers, and these computers are usually virtual machines. They always are in GKE, but they could be physical computers too. One computer is called the control plane, and the others are called nodes. The node’s job is to run Pods, and the control plane’s is to coordinate the entire cluster. Let’s look at the control-plane components. Several critical Kubernetes components run on the control plane. First is the kube-APIserver component, which is the only single component that you'll interact with directly. The job of this component is to accept commands that view or change the state of the cluster. This includes launching Pods. Next is the kubectl command. The job of the kubectl command is to connect to the kube-APIserver and communicate with it using the Kubernetes API. The kube-APIserver also authenticates incoming requests, determines whether they are authorized and valid, and manages admission control. But it’s not just kubectl that talks with kube-APIserver. In fact, any query or change to the cluster’s state must be addressed to the kube-APIserver. There is the etcd component, which is the cluster’s database. Its job is to reliably store the state of the cluster. This includes all the cluster configuration data,along with more dynamic information such as what nodes are part of the cluster, what Pods should be running, and where they should be running. You’ll never directly interact with etcd, instead the kube-APIserver interacts with the database on behalf of the rest of the system. Next is kube-scheduler, which is responsible for scheduling Pods onto the nodes. Kube-scheduler evaluates the requirements of each individual Pod and selects which node is most suitable. However, it doesn’t do the work of actually launching Pods on nodes (that’s done by another component). Instead, whenever it discovers a Pod object that doesn’t yet have an assigned node, it chooses a node and writes the name of that node into the Pod object. How does kube-scheduler decide where to run a Pod? It knows the state of all the nodes, and also obeys constraints you define regarding where a Pod can run, considering hardware, software, and policy details. For example, you might specify that a certain Pod is only allowed to run on nodes with a specific amount of memory. You can also define affinity parameters, which specify when groups of Pods should run on the same node. Alternatively, you can define anti-affinity parameters, which ensure that Pods do not run on the same node. The kube-controller-manager component has a broader job–it continuously monitors the state of a cluster through the kube-APIserver. Whenever the current state of the cluster doesn’t match the desired state, kube-controller-manager will attempt to make changes to achieve the desired state. It’s called the controller manager because many Kubernetes objects are maintained by loops of code called controllers, which handle the process of remediation. You can use certain Kubernetes controllers to manage workloads. For example, remember our problem of keeping 3 nginx Pods always running? They can be gathered into a controller object called a deployment that runs, scales, and brings them together underneath a front end. Other types of controllers have system-level responsibilities. For example, the Node Controller’s job is to monitor and respond when a node is offline. The kube-cloud-manager component manages controllers that interact with underlying cloud providers. For example, if you manually launched a Kubernetes cluster on Compute Engine, kube-cloud-manager would be responsible for bringing in Google Cloud features like load balancers and storage volumes. Now let’s shift our focus to nodes. Each node runs a small family of control-plane components called a kubelet. You can think of a kubelet as Kubernetes’s agent on each node. When the kube-APIserver wants to start a Pod on a node, it connects to that node’s kubelet. Kubelet uses the container runtime to start the Pod and monitors its lifecycle, including readiness and liveness probes, and reports back to the kube-APIserver. The term container runtime, which was mentioned earlier in this course, is the software used to launch a container from a container image. Kubernetes offers several container runtime choices, but the Linux distribution that GKE uses for its nodes launches containers that use containerd, the runtime component of Docker. And finally, there is the kube-proxy component, which maintains network connectivity among the Pods in a cluster. In open source Kubernetes, network connectivity is accomplished by using the firewalling capabilities of iptables, which are built into the Linux kernel. We saw that the Kubernetes Control Plane is a complex management system. But how is GKE different from Kubernetes? From the user’s perspective, it’s a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE is responsible for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need for a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes.

#### GKE Autopilot and GKE standard

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562177

Now let’s explore the two available modes of operation, Autopilot and Standard mode, in more detail. At a high level, Autopilot mode optimizes the management of Kubernetes with a hands-off experience. However, less management overhead means less configuration options. And with Autopilot GKE you only pay for what you use. The Standard mode allows the Kubernetes management infrastructure to be configured in many different ways. This requires more management overhead, but produces an environment for fine-grained control. With GKE standard, you pay for all of the provisioned infrastructure, regardless of how much gets used. Let’s examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. As a Google-managed and optimized GKE instance, the job of Autopilot is to create clusters according to battle-tested and hardened best practices. Autopilot defines the underlying machine type for your cluster based on workloads, which optimizes both usage and cost for the cluster and adapts to changing workloads. And without the cluster management overhead, Autopilot lets you deploy production-ready GKE clusters faster. Autopilot also helps produce a strong security posture. Google helps secure the cluster nodes and infrastructure, and it eliminates infrastructure security management tasks. By locking down nodes, Autopilot reduces the cluster's attack surface and ongoing configuration mistakes. Autopilot promotes operational efficiency. Google monitors the entire Autopilot cluster, including control plane, worker nodes and core Kubernetes system components. This way, it ensures that Pods are always scheduled. This level of monitoring allows Google to always keep clusters up to date. Autopilot also provides a way to configure update windows for clusters to ensure minimal disruption to workloads. With Autopilot, Google is fully responsible for optimizing resource consumption. This means you only pay for Pods, not nodes. Now that you’ve seen many of the Autopilot mode benefits, let’s look at some restrictions presented with this operation mode. The configuration options in GKE Autopilot are more restrictive than in GKE Standard. This is because GKE Autopilot is a fully managed service and has a pod-scheduling service level agreement. Autopilot clusters also have restrictions on access to node objects. Features like SSH and privilege escalation were removed and there are limitations on node affinity and host access. However, all Pods in GKE Autopilot are scheduled with a Guaranteed class Quality of Service (or QoS). But this requires a minor configuration change. The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode.

#### Object management

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562178

Let’s finish up this section of the course by exploring Kubernetes object management. It’s important to know that all Kubernetes objects are identified by a unique name and a unique identifier. Recall the earlier example where you wanted the three nginx Web servers to run all the time. The simplest way to achieve this is by declaring three Pod objects and specifying their state. For each, a Pod must be created and an nginx container image must be used. Let’s see how to make this declaration. You define the objects you want Kubernetes to create and maintain with manifest files. These are ordinary text files that can be written in YAML or JSON. You’ll see YAML used in this course. This manifest file defines a desired state for a Pod: its name and a specific container image for it to run. Required fields include: apiVersion, which states the Kubernetes API version used to create the object, kind, which states the object you want (in this case, a Pod), and metadata, which identifies the object name, unique ID, and an optional namespace. If several objects are related, it’s a best practice to define them all in the same YAML file. This makes things easier to manage. We strongly recommend saving YAML files in version-control repositories so it’s easier to track and manage changes and to undo those changes when necessary. It’s also helpful when recreating or restoring a cluster. Cloud Source Repositories is a popular service for this purpose. Let’s look at the details of an object. First, all objects are identified by a name. Names must consist of a unique string under 253 characters. Numbers, letters, hyphens, and periods are allowed. Only one object can have a particular name at the same time in the same Kubernetes namespace. After an object is deleted, however, the name can be reused. Second, every object created throughout the life of a cluster has a unique identifier, or UID, generated by Kubernetes. And third, there are labels. Labels are key-value pairs to tag objects during or after their creation. Labels help identify and organize objects. A way to select Kubernetes resources by label is through the kubectl command. For example, this command can select all the Pods that contain a label called “app” with a value of “nginx.” Label selectors can be used to ask for all the resources that have a certain value for a label, all those that don’t have a certain value, or even all those with a value in a set you supply. Now let’s go back to our example. One way to create three nginx web servers is by declaring three Pod objects, each with its own section of YAML. In Kubernetes, a workload is spread evenly across available nodes by default. So how do you tell Kubernetes to maintain the desired state of three nginx containers? To maintain an application's high availability, you need a better way to manage it in Kubernetes than specifying individual Pods. One option is to declare a controller object. A controller object's job is to manage the state of the Pods. Because Pods are designed to be ephemeral and disposable, they don't heal or repair themselves and are not meant to run forever. Examples include Deployments, StatefulSets, DaemonSets, and Jobs. Deployments are a great choice for long-lived software components like web servers, especially when you want to manage them as a group. In our example, the practical effect of the Deployment controller is to monitor and maintain the three nginx Pods. When the kube-scheduler schedules Pods for a Deployment, it notifies the kube-APIserver. The Deployment controller creates a child object, a ReplicaSet, to launch the desired Pods. If one of these Pods fails, the ReplicaSet controller will recognize the difference between the current state and the desired state and will try to fix it by launching a new Pod. So, this means that instead of using multiple YAML manifests or files for each Pod, you used a single Deployment YAML to launch three replicas of the same container. Within a Deployment object spec, the number of replica Pods, which containers should run the Pods, and which volumes should be mounted the following elements are defined. Based on these templates, controllers maintain the Pod’s desired state within a cluster.

#### Lab introduction: Deploying GKE Autopilot Clusters

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562179

It’s time for some hands-on practice with GKE. In the lab titled “Deploying GKE Autopilot Clusters,” you’ll build and use GKE clusters, then you’ll deploy a sample Pod. Specifically, you’ll use the Google Cloud console to build GKE clusters, deploy a Pod, and then examine the cluster and Pods.

#### Deploying GKE Autopilot Clusters

- https://www.cloudskillsboost.google/paths/11/course_templates/2/labs/562180

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/2/quizzes/562181

### Kubernetes Operations

#### Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562182

kubectl is a command-line tool to interact with GKE clusters. Because it allows you to manage Kubernetes resources from the command line, it makes it easy to automate tasks and to troubleshoot problems. But how does it work? Does it need special configuration? How can it be used to gather information about the containers, Pods, services, and other engines running within the cluster? The goal of this final section of the course, titled “Kubernetes Operations” was designed to answer those questions. You’ll explore kubectl and how to configure it, and what introspection means and how it can be used to troubleshoot a cluster. You’ll also get hands on practice deploying Google Kubernetes Engine clusters from Cloud Shell. Okay, let’s get started!

#### The kubectl command

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562183

So, what exactly is the kubectl command and why is it important? kubectl is a utility used by administrators to control Kubernetes clusters. It’s used to communicate with the Kube API server on the control plane. This is important to users, because it allows them to make requests to the cluster and kubectl determines which part of the control plane to communicate with. Within a selected Kubernetes cluster, kubectl transforms command-line entries into API calls and sends them to the Kube API server. However, to work properly, kubectl must be configured with the location and credentials of a Kubernetes cluster. And before kubectl can be used to configure a cluster, it must first be configured. kubectl stores its configuration in a file in the home directory in a hidden folder named . kube, and contains the list of clusters and the credentials that will be attached to each of those clusters. But where do the credentials come from? GKE provides them through the gcloud command. To view the configuration, either open the config file or use the kubectl command: “config view”. Please note that the kubectl config shows the configuration of the kubectl command itself, whereas other kubectl commands show the configurations of cluster and workloads. For example, let’s say an administrator wants to see a list of Pods in a cluster. After connecting kubectl to the cluster with proper credentials, the administrator can issue the kubectl “get pod” command. kubectl then converts this command into an API call, which it sends to the Kube API server through HTTPS on the cluster’s control plane server. From there, the Kube API server processes the request by querying etcd. The Kube API server then returns the results to kubectl through HTTPS. Finally, kubectl interprets the API response and displays the results to the administrator at the command prompt. To connect kubectl to a GKE cluster, first retrieve the credentials for the specified cluster. This can be done with the “get-credentials” gcloud command in any other environment where the gcloud command-line tool and kubectl are installed. Note that both are installed by default in Cloud Shell. By default, the gcloud “get-credentials” command writes configuration information into a config file in the . kube directory in the $HOME directory. If this command is rerun for a different cluster, it’ll update the config file with the credentials for the new cluster. This configuration process only needs to be performed once per cluster in Cloud Shell, because the . kube directory and its contents stay in the $HOME directory. The gcloud command is how authorized users interact with Google Cloud from the command line. If authorized, the gcloud “get-credentials” command provides the credentials needed to connect with a GKE cluster. Although kubectl is a tool for administering the internal state of an existing cluster, it can’t create new clusters or change the shape of existing clusters. That’s done through the GKE control plane, which the gcloud command and the Google Cloud console interfaces to. After the config file in the . kube folder is configured, the kubectl command automatically references this file and connects to the default cluster without prompting for credentials. Now let’s explore how to use the kubectl command. kubectl’s syntax is composed of four parts: the command, the type, the name, and optional flags. The command specifies the action that you want to perform, such as get, describe, logs, or exec. Some commands show information, whereas others change the cluster’s configuration. The TYPE defines the Kubernetes object that the “command” acts upon, like Pods, deployments, nodes, or other objects, including the cluster itself. The TYPE used in combination with “command” tells kubectl what you want to do and the type of object you want to perform that action on. The NAME specifies the object defined in TYPE. The name field isn’t always needed, especially when using commands that list or show information. For example, if you run the command “kubectl get pods” without specifying a name, the command returns the list of all Pods. To filter this list, specify a Pod’s name, such as “kubectl get pod my-test-app” and kubectl will return only information on the Pod named ‘my-test-app’. Some commands let you append flags to the end, which you can think of as a way to make a special request. For example, to view the state of a Pod, use the command “kubectl get pod my-test-app -o=yaml”. Also, it’s worth mentioning that telling kubectl to produce a YAML output can be helpful for other tasks related to Kubernetes objects, for example, recreating an object in another cluster. Flags can also be used to display additional information. For example, run the command “kubectl get pods -o=wide” to display the list of Pods in “wide” format, which reveals Pods in the list. Wide format also displays which node each Pod is running on. The kubectl command has many uses, from creating Kubernetes objects, to viewing them, deleting them, and viewing or exporting configuration files. Just remember to configure kubectl first or to use the --kubeconfig or --context parameters, so that the commands you type are performed on the cluster you intended.

#### Introspection

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562184

Now that you’ve been introduced to the kubectl command, which is the way to specify the action that you want to perform on a Kubernetes cluster, let’s explore how to debug problems when an application is running. This process is called introspection. It’s the act of gathering information about the containers, pods, services, and other engines that run within the cluster. We’ll start with four commands to use to gather information about your app: get, describe, exec, and logs. A good place to start is with Pods, the basic units of Kubernetes. A simple kubectl “get pods” command tells you whether your Pod is running. It shows the Pod’s phase status as pending, running, succeeded, failed, or unknown, or CrashLoopBackOff. The phase provides a high-level summary, not the comprehensive details about a Pod or its containers. The pending status indicates that Kubernetes has accepted a Pod, but it’s still being scheduled. This means that the container images defined for the Pod have not yet been created by the container runtime. For example, when images are being pulled from the repository, the Pod will be in the pending phase. A Pod runs after it is successfully attached to a node, and all its containers are created. Containers inside a Pod can be starting, restarting, or running continuously. Succeeded means that all containers finished running successfully, or instead, that they terminated successfully and they won’t be restarting. Failed means a container terminated with a failure, and it won’t be restarting. Unknown is where the state of the Pod simply cannot be retrieved, probably because of a communication error between the control plane and a kubelet. And CrashLoopBackOff means that one of the containers in the Pod exited unexpectedly even after it was restarted at least once. This is a common error. Usually, this means that the Pod isn’t configured correctly. To investigate a Pod in detail, use the kubectl “describe pod” command. This command provides information about a Pod and its containers such as labels, resource requirements, and volumes. It also details the status information about the Pod and container. For Pods, the name, namespace, node name, labels, status, and IP address are displayed. For containers, the state–waiting, running, or terminated, images, ports, commands, and restart counts–are displayed. Single command execution using the exec command lets you run a single command inside a container and view the results in your own command shell. This is useful when a single command, such as ping, will do. And finally, the logs command provides a way to see what is happening inside a Pod. This is useful in troubleshooting, as the logs command can reveal errors or debugging messages written by the applications that run inside Pods. The logs contain both the standard output and standard error messages that the applications within the container have generated. The logs command is useful when you need to find out more information about containers that are failing to run successfully. And if the Pod has multiple containers, you can use the -c argument to show the logs for a specific container inside the Pod. There might be a scenario where you need to work inside a Pod, maybe to run a command. Let’s say you need to install a package, like a network monitoring tool or a text editor, before you can begin troubleshooting. To do so, you can launch an interactive shell using the -it switch, which connects your shell to the container that allows you to work inside the container. This syntax attaches the standard input and standard output of the container to your terminal window or command shell. The -i argument tells kubectl to pass the terminal’s standard input to the container, and the -t argument tells kubectl that the input is a TTY. If you don’t use these arguments then the exec command will be executed in the remote container and return immediately to your local shell. Now it’s important to note that it’s not a best practice to install software directly into a container, as changes made by containers to their file systems are usually ephemeral. Instead, consider building container images that have exactly the software you need, instead of temporarily repairing them at run time. The interactive shell will allow you to figure out what needs to be changed to solve a problem, but you should then integrate those changes into your container images and redeploy them.

#### Lab introduction: Deploying GKE Autopilot Clusters from Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562185

It’s time for the last hands-on lab of this course. In the lab titled “Deploying GKE Autopilot Clusters from Cloud Shell” you’ll use the command line to build GKE clusters. You’ll inspect the kubeconfig file and use kubectl to manipulate the cluster.

#### Deploying GKE Autopilot Clusters from Cloud Shell

- https://www.cloudskillsboost.google/paths/11/course_templates/2/labs/562186

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/2/quizzes/562187

### Course Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/11/course_templates/2/video/562188

This brings us to the end of the “Getting Started with Google Kubernetes Engine” course! Let’s do a quick recap. In this first section of the course, you explored: the definition of cloud computing, the services Google Cloud offers architects and developers to build solutions, how Google’s powerful global network can power Google Cloud services How Google Cloud resources are structured and managed, tools to ensure that your organization doesn't accidentally face a big Google Cloud bill, and four different ways to interact with Google Cloud to commence work. In the second section, you learned about: containers, container images, Kubernetes, and Google Kubernetes Engine. In the third section of the course, you were introduced to: Kubernetes concepts like Kubernetes object model and the principal of declarative management, a list of Kubernetes components, Google Kubernetes Engine concepts including the Autopilot and Standard modes of operation, and Kubernetes object management. And finally, in the last section, you examined: the kubectl and how to configure it, and what introspection means and how it can be used to troubleshoot a cluster. If you’re interested in learning more about Google Kubernetes Engine, please go to the next course in this series called “Architecting with Google Kubernetes Engine.” See you next time!

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/2/documents/562189

### Your Next Steps

## 08: Logging and Monitoring in Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/99

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533931

Welcome to the two-part course on Logging, Monitoring, and Observability in Google Cloud. The core operations tools in Google Cloud break down into two major categories. The operations-focused components and the application performance management tools. The first course is Logging and Monitoring in Google Cloud, which is this course, and covers the operations-focused components, including Logging, Monitoring, and Service Monitoring. This course tends to be more for personnel who are primarily interested in infrastructure, and keeping that infrastructure up, running, and error-free. The second course is Observability in Google Cloud, which covers the application performance management tools, including Error Reporting, Trace, and Profiler. This course in contrast, tends to be more for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute products. But it isn’t fair to think of these tools as belonging purely to either of these two groups. A developer would, of course, sometimes need access to logs or monitoring metrics, just like an operation team member might need to trace latency. This course is designed to equip Cloud Architects, Administrators, SysOps personnel, Cloud Developers, and DevOps personnel with the essential skills and knowledge needed to excel in logging and monitoring your applications and workloads on Google Cloud. The prerequisites for this course are: Basic scripting or coding ability, Google Cloud Fundamentals: Core Infrastructure or equivalent experience, and proficiency with command-line tools and Linux operating system environments. In this course, we will delve into the critical aspects of Cloud Logging and Cloud Monitoring on Google Cloud. Logging and monitoring play a pivotal role in maintaining the health and security of your cloud resources. You will learn: The purpose and capabilities of Google Cloud Observability, How to Implement monitoring for multiple cloud projects, Create alerting policies, uptime checks and alerts to identify and resolve problems quickly, And collect, analyse and export logging data. By the end of this course, you will have a solid grasp of how to implement and manage cloud logging and monitoring solutions within Google Cloud, enhancing your skills to maintain the reliability and security of your infrastructure. Let's get started!

### Introduction to Google Cloud Observability

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533932

Hey everyone, welcome to the first module of this course, Introduction to Google Cloud Observabilty. In this module I will provide you with a quick introduction to Google Cloud Observabilty. We will cover what it is and why it is important. Let us look at the objectives of the module. Google Cloud Observability consists of three broad categories, Logging, Monitoring, and Application Performance Management. In this module, we will start with an overview of why we need these tools, and then we get to know both the observabilty and the Application Performance Management products. We will explore what the Google Cloud Observability architecture consists of to understand how the three pieces Cloud Logging, Cloud Monitoring and APM are connected. So, let's get started!

#### Need for Google Cloud observability

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533933

We start this section with an overview of why we need these tools, and then we’ll spend a little time understanding the role of monitoring in product reliability. We will explore the significance of the four golden signals in measuring the system’s performance and reliability. We then move on to explore the products in Google Cloud Observability. If you've ever worked with on-premises environments, you know that you can physically touch the servers. If an application becomes unresponsive, someone can physically determine why that happened. In the cloud though, the servers aren't yours—they're Google’s—and you can’t physically inspect them. So the question becomes, how do you know what's happening with your server, or database, or application? The answer is by using Google’s integrated observability tools. In this slide, we will dive a little deeper to understand the four distinct recurring user needs for observability. Visibility into system health: Users want to understand what is happening with their application and system. They rely on a service that provides a clear mental model for how their application is working on Google Cloud. They need a report on the overall health of systems. The services should help answer questions such as “are my systems functioning?” or “”do my systems have sufficient resources available? ” Error reporting and alerting: Users want to monitor their service at a glance through healthy/unhealthy status icons or red/green indicators. Customers appreciate any proactive alerting, anomaly detection, or guidance on issues. Ideally, they want to avoid connecting the dots themselves. Efficient troubleshooting: Users don’t want multiple tabs open. They need a system that can proactively correlate relevant signals and make it easy to search across different data sources, like logs and metrics. If possible, the service needs to be opinionated about the potential cause of the issue and recommend a meaningful direction for the customer to start their investigation. It should allow users to immediately act on what they discover. For instance, a metric indicating insufficient quota should be accompanied by a button to increase quota. Performance improvement: Users need a service that can perform retrospective analysis. Generally, help them plan intelligently by analyzing trends and understand how changes in the system affect its performance. Let’s begin with monitoring. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and lessen their pain. In Google's Site Reliability Engineering book, which is available to read at landing.google.com/sre/books, monitoring is defined as: "Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes." An application client normally only sees the public side of a product, and as a result, developers and business stakeholders both tend to think that the most crucial way to make the client happy is by spending the most time and effort on developing that part of the product. However, to be truly reliable, even the very best products still must be deployed into environments with enough capacity to handle the anticipated client load. Great products also need thorough testing, preferably automated testing, and a refined continuous integration/continuous development (CI/CD) release pipeline. Postmortems and root cause analyses are the DevOps team's way of letting the client know why an incident happened and why it is unlikely to happen again. In this context we are discussing a system or software failure, but the term “incident” can also be used to describe a breach of security. Transparency here is key to building trust. We need our products to improve continually, and we need monitoring data to ensure that happens. We need dashboards to provide business intelligence so our DevOps personnel have the data they need to do their jobs. We need automated alerts because humans tend to look at things only when there's something important to look at. An even better option is to construct automated systems to handle as many alerts as possible so humans only have to look at the most critical issues. Typically, there's some triggering event: a system outage, data loss, a monitoring failure, or some form of manual intervention. The trigger leads to a response by both automated systems and DevOps personnel. Many times the response starts by examining signal data that comes in through monitoring. The impact of the issue is evaluated and escalated when needed, and an initial response is formulated. Throughout, good SREs will strive to keep the customer informed and respond when appropriate. Finally, we need monitoring tools that help provide data crucial to debugging application functional and performance issues. We’ll look more closely at Google’s integrated monitoring tools a bit later in this module. There are “four golden signals” that measure a system’s performance and reliability. They are latency, traffic, saturation, and errors. Latency measures how long it takes a particular part of a system to return a result. Latency is important because it directly affects the user experience. Changes in latency could indicate emerging issues. Its values may be tied to capacity demands. It can be used to measure system improvements. But how is it measured? Sample latency metrics include page load latency, number of requests waiting for a thread, query duration, service response time, transaction duration, time to first response and time to complete data return. The next signal is traffic, which measures how many requests are reaching your system. Traffic is important because it’s an indicator of current system demand. Its historical trends are used for capacity planning. It’s a core measure when calculating infrastructure spend. Sample traffic metrics include number of HTTP requests per second, number of requests for static vs. dynamic content, number of concurrent sessions, and many more. The third signal is saturation, which measures how close to capacity a service is. It’s important to note, though, that capacity is often a subjective measure, that depends on the underlying service or application. Saturation is important because it's an indicator of how full the service is. It focuses on the most constrained resources. It’s frequently tied to degrading performance as capacity is reached. Sample capacity metrics include percentage memory utilization, percentage of thread pool utilization, percentage of cache utilization and many more. The fourth signal is errors, which are events that measure system failures or other issues. Errors are often raised when a flaw, failure, or fault in a computer program or system causes it to produce incorrect or unexpected results, or behave in unintended ways. Errors might indicate configuration or capacity issues or service level objective violations. Sample error metrics include wrong answers or incorrect content, number of 400/500 HTTP codes, number of failed requests, number of exceptions and many more. Now, let's return to the observability concept. Observability starts with signals, which are metric, logging, and trace data captured and integrated into Google products from the hardware layer up. From those products the signal data flows into Google Cloud Observability tools where it can be visualized in dashboards and through the Metrics Explorer. Automated and custom logs can be dissected and analyzed in the Logs Explorer. Services can be monitored for compliance with service level objectives (SLOs), and error budgets can be tracked. Health checks can be used to check uptime and latency for external-facing sites and services. When incidents occur signal data can generate automated alerts to code or, through various information channels, to key personnel. Error Reporting can help operations and developer teams spot, count, and analyze crashes in cloud-based services. The visualization and analysis tools can then help troubleshoot what's happening in Google Cloud. Ultimately, you won't miss that easy server access, because Google provides more precise insights into your Cloud install than you ever had on-premises. Let’s explore the products most applicable for those in operations roles that work with Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace, and Cloud Profiler.

#### Cloud Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533934

We defined general monitoring and its benefits in the previous section. Let us take a look at Cloud Monitoring features and benefits. Cloud Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. It collects metrics, events, and metadata from projects, logs, services, systems, agents, custom code, and various common application components, including Cassandra, Nginx, Apache Web Server, Elasticsearch, and many others. Monitoring ingests that data and generates insights via dashboards, Metrics Explorer charts, and automated alerts. Cloud Monitoring provides many advanced capabilities that helps address the monitoring challenges and these include: Many free metrics: On 100+ monitored resources, over 1,500 metrics are immediately available with no cost. You can find out more about this at Google Cloud Observability pricing. Open source standards: Leverage Prometheus and Open Telemetry to collect metrics across compute workloads. Customization for key workloads: Cloud Monitoring offers custom visualization capabilities for GKE through Google Cloud Managed Service for Prometheus and for Compute Engine through Ops Agent. In-context visualizations and alerts: View relevant telemetry data alongside your workloads across Google Cloud.

#### Cloud Logging

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533935

The next service we will explore is Cloud Logging. Google's Cloud Logging allows users to collect, store, search, analyze, monitor, and alert on log entries and events. It provides automatic ingestion with simple controls for routing, storing, and displaying your log data. It leverages tools like Log Analytics to view trends, or Error Reporting and Log Explorer to quickly examine problems. Like I mentioned earlier, logging has multiple aspects such as collection, analysis, export and retention. Cloud Logging enables you to automatically collect cloud events and configuration changes. You can aggregate and centralize logs at a organizational level, project level and folder level based on your needs. Most log analysis start with Google Cloud’s integrated Logs Explorer. You can run queries and analyze log data with Log Analytics. Logging entries can also be exported to several destinations for alternative or further analysis. Export log data as files to Google Cloud Storage, or as messages through Pub/Sub, or into BigQuery tables. Pub/Sub messages can be analyzed in near-real time using custom code or stream processing technologies like Dataflow. BigQuery allows analysts to examine logging data through SQL queries. And archived log files in Cloud Storage can be analyzed with several tools and techniques. Logs-based metrics may be created and integrated into Cloud Monitoring dashboards, alerts, and service SLOs. Default log retention in Cloud Logging depends on the log type. Data access logs are retained by default for 30 days, but this is configurable up to a max of 3650 days. Admin logs are stored by default for 400 days. Alternatively you can also export logs to Google Cloud Storage or BigQuery to extend retention. Let us next look at a few use cases. A developer would love to get started quickly, thus we have out of the box collection of system metrics and logs and integration into popular logging SDKs and library. Cloud Logging also allows developers to do real time analysis, debugging and troubleshooting of your code. For convenient access and visibility, stack traces are automatically mapped to error types. Operators also take massive advantage of our offering. These include collecting telemetry that is not limited to Google Cloud, centralization of all the logs for users, teams and organizations. You are in control of retention periods and location of the logs. You can also understand log volume, cost and set alerts on important application metrics. You can export logs for storage, analysis and also integrate with third-party services. Lastly, security operations, or SecOps are in charge of ensuring that all access is authorized and that bad actors are not navigating your network. With audit logs, network telemetry and log analysis it can be achieved in a streamlined way.

#### Error Reporting

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533936

The next service we will explore is Error Reporting. Error Reporting counts, analyzes, and aggregates the crashes in your running cloud services. Error reporting enables you to perform a lot of advanced functionalities that ensures your application runs smoothly. These include: Real time processing: Application errors are processed and displayed in the interface within seconds. You can quickly view and understand errors as a dedicated page displays the details of the error: bar chart over time, list of affected versions, request URL and link to the request log. It provides instant notification. Do not wait for your users to report problems. Error Reporting is always watching your service and instantly alerts you when a new application error cannot be grouped with existing ones. Directly jump from a notification to the details of the new error. Crashes in most modern languages are exceptions which are not caught and are handled by the code itself. Its management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details: time chart, occurrences, affected user count, first- and last-seen dates, and a cleaned exception stack trace. You can also create alerts to receive notifications on new errors.

#### Application Performance Management Tools

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533937

The last section is about Application Performance Management. This include Cloud Profiler and Cloud Trace. Cloud Trace is a tracing system that collects latency data from your distributed applications and displays it in the Google Cloud console. Trace can capture traces from applications deployed on App Engine, Compute Engine VMs, and Google Kubernetes Engine containers. Now you can analyze changes to applications’ latency profiles through Google Cloud Console and on Android devices. Using the latency reports feature, you can view performance insights in near-real time. Automatically analyze all of your application's traces to generate in-depth latency reports to surface performance degradations. Continuously gather and analyze trace data to automatically identify recent changes to application performance. Poorly performing code increases the latency and cost of applications and web services every day, without anyone knowing or doing anything about it. Cloud Profiler changes this by using statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete CPU and heap picture of an application without slowing it down. With broad platform support that includes Compute Engine VMs, App Engine, and Kubernetes, it allows developers to analyze applications running anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Python, and Node.js. Cloud Profiler presents the call hierarchy and resource consumption of the relevant function in an interactive flame graph that helps developers understand which paths consume the most resources and the different ways in which their code is actually called. Overall, Google Cloud Observability helps you explore both the known and unknown issues underlying your workloads. The products are user focussed designed to understand a customer’s journey with SLO monitoring, uptime checks, tracing and more. They are open, flexible and leverage several popular open source projects like Prometheus, OpenTelemetry, and Fluentbit. Integrated for ease through automatic ingestion, connect data sets, in-context telemetry across Google Cloud service views. They also provide meaningful analysis and alerting through powerful analysis tools, leverage alerting for both automated and human-led resolutions.

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533938

In this module, we’ve explored the core observability tools in Google Cloud, including Cloud Logging, Cloud Monitoring, and Error Reporting, and the application performance management tools, including Cloud Trace, and Cloud Profiler. Now that we have a foundation, let’s move on to cover the various tools in greater detail.

#### Quiz - Introduction to Google Cloud Observability

- https://www.cloudskillsboost.google/paths/11/course_templates/99/quizzes/533939

### Monitoring Critical Systems

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533940

Let's spend a little time talking about how Google Cloud helps you monitor critical systems. Monitoring is all about keeping track of exactly what's happening with the resources that we launched inside of Google Cloud. In this module, let’s take a look at options and best practices as they relate to monitoring project architectures. It’s important to make some early architectural decisions before starting monitoring. We will examine some of the default dashboards created by Google, and see how to use them appropriately. We will create charts and use them to build custom dashboards to show resource consumption and application load. And, we will define uptime checks to track liveliness and latency. We will also cover the purpose of using MQL for monitoring.

#### Monitoring Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533941

We first start with an overview of monitoring. Then, we explore this concept in detail as we advance with the rest of the module. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and reduce their problems.

#### Cloud Monitoring achitecture patterns

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533942

Let's now learn more about some common monitoring architecture patterns. A typical Cloud Monitoring architecture includes 3 layers, a data collection layer, a data storage layer, and a data analysis and visualization layer. A data collection layer collects metrics, logs, and traces from cloud-based systems. In Cloud Monitoring, the data collection layer includes Google Cloud services such as Google Kubernetes Engine, Compute Engine, App Engine etc,. A data storage layer stores the collected data and routes to the configured visualization and analysis layer. In Cloud Monitoring, this layer includes the Cloud Monitoring API that helps triage the metrics collected to be stored for further analysis. A data analysis and visualization layer: This layer analyzes the collected data to identify problems and trends and presents the analyzed data in a way that is easy to understand. In Cloud Monitoring, this layer comprise of various features within Cloud Monitoring such as Dashboards to visualize data, Uptime checks to monitor applications, Alerting policies to configure alerts and notifications to notify of events that need attention. One of the most common uses of Cloud Monitoring is platform monitoring Blackbox monitoring of the platform enables users to get visibility into the performance of their Google Cloud services. With Google Cloud, this is enabled by default and system metrics are automatically collected without any user effort. Google Cloud Monitoring is the recommended solution for Platform monitoring. System metrics from Google Cloud are available at no cost to customers. These metrics provide information about how the service is operating. Over 1500 metrics across more than 100 Google Cloud services automatically. For example, Compute Engine reports over 25 unique metrics for each virtual machine (VM) instance. However, if customers, in traditional enterprise cohorts, are using third party products for monitoring and want to aggregate their Google Cloud metrics into those partner products, they can use Cloud Monitoring APIs to ingest these metrics. For applications or workloads deployed in GKE, many customers prefer a Prometheus-based solution for monitoring. We fully embrace that monitoring approach and provide customers a new way to leverage Prometheus based monitoring using Google Managed Prometheus (GMP). GMP is a part of Cloud Monitoring and it makes GKE cluster and workload metrics available as Prometheus data. It can ingest monitoring data exposed in Prometheus format, it supports PromQL compatible query language and has natively integrated the Prometheus expression browser, and Prometheus compatible rule evaluation. For application workloads in GKE, we recommend that customers use Google Managed Prometheus. For applications or workloads deployed in Compute Engine, customers should use the Ops Agent to collect in-process metrics and to collect metrics from third party applications that run in your VMs. Ops agent today supports more than 30 plugins for different open source and ISV software along with a collection of richer and more fine grained metrics at the OS level for Windows and Linux (many flavors). The Ops agent is based on OpenTelemetry standards so custom applications developed by customers can leverage OTEL client libraries for instrumenting their code and generate the needed telemetry. The Ops agent can collect these custom metrics and make them available in Cloud Monitoring as well. While this ecosystem of third party plugins will continue to expand, if users need support for other software products or services, consider using a partner product like Datadog or NewRelic. If you choose to use partner products, they can collect system metrics from the Google Cloud platform by using the native API-based integrations With Google's partner BindPlane by Blue Medora, you can import monitoring and logging data from both on-premises VMs and other cloud providers, such as Amazon Web Services (AWS), Microsoft Azure, Alibaba Cloud, and IBM Cloud into Google Cloud. The following diagram shows how Cloud Monitoring and BindPlane can provide a single pane of glass for a hybrid cloud. This architecture has the following advantages: In addition to monitoring resources like VMs, Blue Medora has built-in deep integration for over 150 popular data sources. There are no additional licensing costs for using BindPlane. BindPlane metrics are imported into Monitoring as custom metrics, which are chargeable. Likewise, BindPlane logs are charged at the same rate as other Cloud Logging logs.

#### Monitoring multiple projects

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533943

We have explored a few architecture patterns, let us next explore how you can monitor multiple projects from a single project by using metrics scope. When you go to monitoring settings for a project, you can see that the current metrics scope only has a single project in it, the one it is currently viewing. When you create a Google Cloud project, that project hosts a metrics scope and becomes the scoping project for that scope. It stores the alerts, uptime checks, dashboards, and monitoring groups that you configure for the scope. For example, if you create a staging project and then access monitoring, you can see the metrics for the resources in the staging project. This happens for every project you create. Each project creates a metrics scope for itself and hosts monitoring configuration for itself. But what if you want to centralize how that data is stored and how it's accessed? Since it's possible for one metrics scope to monitor multiple projects, and also a project can be monitored from only a single metrics scope, you will have to decide which relationship will work best for your organizational culture, and this particular project. Let us explore option one where every project is monitored locally, in that project. The advantages are clear and obvious separation for each project. If the project contains development-related resources, it's easy to provide access to the dev personnel. Project resources and monitoring resources all in the same place. Easy to automate, since monitoring becomes a standard part of the initial project setup. Let us discuss the disadvantage. If the application is larger than a single project, you will have limited visibility into application performance. Strategy B: Single metrics scope is used for monitoring large units of projects. You can add multiple projects to an existing scope. Now, monitoring data for all projects in that scope will be visible. This will let you create dashboards, showing resources from all the projects in the scope, or alerting policies that apply to resources in multiple projects as long as they're in the metrics scope. Note that the recommended approach for production deployments is to create a dedicated project to host monitoring configuration data and use its metrics scope to set up monitoring for the projects that have actual resources in them. This way, should a project not be necessary anymore and get deleted, the monitoring configuration for all the other projects won't be impacted. The advantage is a single pane of glass that provides visibility into the entire group of related projects. Compare non-prod and prod environments easily. The disadvantage is that anyone with IAM permissions to access Cloud Monitoring will be able to see metrics for all environments. Monitoring in prod is typically divided among different teams. This approach would not preserve that separation. Although the metric data and log entries remain in the individual projects, any user who has been granted the role Monitoring Viewer will have access to the dashboards and have access to all data by default. This means that a role assigned to one person on one project applies equally to all projects monitored by that metrics scope. Remember, metrics scope only affects and controls Google Cloud resources related to Cloud Monitoring. Other tools covered in this course, such as Cloud Logging, Error Reporting, and the Application Performance Management (APM) tools, are strictly project-based and do not rely upon the configuration of the metrics scope or the monitoring IAM roles.

#### Data model and dashboards

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533944

In this section we will explore more on the Monitoring Data model and see how we can visualize these metrics as charts in a dashboard. Let us start by understanding the Cloud Monitoring data model. In general terms, monitoring data is recorded in time series. Each individual time series includes four pieces of information relevant to this discussion: The metric field describes the metric itself and records two aspects: The metric-label that represents one combination of label values. The metric type specifies the available labels and describes what is represented by the data points. The resource field records: The resource-label represents one combination of label values. The specific monitored resource from which the data was collected. The metricKind and valueType fields tell you how to interpret the values. The value type is the data type for the measurements. Each time series records the value type (type ValueType) for its data points. Each time series includes the metric kind (type MetricKind) for its data points. The kind of metric data tells you how to interpret the values relative to each other. Cloud Monitoring metrics are one of three kinds: A gauge metric, a delta metric, or a cumulative metric. The points field is an array of timestamped values. The metric type tells you what the values represent. The sample time series has an array with a single data point; in most time series, the array has many more values. Navigate to the Time Series List API Explorer, which can be found in the reference documentation for Google Cloud Monitoring. Locate the API Explorer widget and edit the filter to include the metric type "logging.googleapis.com/log_entry_count" and resource type "gce_instance". Specify the desired start and end time for your data. Upon successful execution, you'll receive the time series data shown on the next slide. As mentioned earlier monitoring data is recorded in time series. The example shown here is a complete instance of a single time series. Most time series include a lot more data points; this one covers a one-minute interval. All time series have the same structure, with the following fields: The metric field indicates that the metric collected is a set of activity logs of the type log_entry_count with a severity level INFO. The resource field which indicates that the resource is a Compute Engine instance with the details on the specific instance and project ID. The metric kind is of the type DELTA and value type integer. Points are actual array of time stamp and value of the metric. First, identify the Google Cloud Monitoring resources you want to monitor. Next, check the Monitoring Dashboards for auto-created dashboard. When you can't find what you need, use the Metrics Explorer. You can monitor any of the more than 1500 metrics, custom metrics and can even build custom dashboards. Dashboards are a way for you to view and analyze metric data that is important to you. They give you graphical representations on the main signal data in such a way as to help you make key decisions about your Google Cloud-based resources. One of the changing aspects of monitoring is Google's commitment to providing more opinionated default information. Google Cloud sees that your project contains Compute Engine VMs, or a GKE Cluster, so Monitoring auto-creates dashboards for you that radiate the information that Google thinks is important for those two resource types. As you add more resources, Google will continue to add more default dashboards. These dashboards form a great monitoring foundation on which you can build. You can also use the Dashboard Builder to visualize application metrics that you are interested in. You can select the chart type and filter the metrics based on your requirements. Frequently, the easiest way to start chart creation is to build an ad-hoc chart with Google's Metrics Explorer. Metrics Explorer lets you build charts for any metric collected by your project. With it, you can Save charts you create to a custom dashboard. Share charts by their URL. View the configuration for charts as JSON. Most importantly, you can use Metrics Explorer as a tool to explore data that you don't need to display long term on a custom dashboard. As seen on this slide, the Metrics Explorer interface consists of three primary regions: A configuration region, where you pick the metric and its options, The chart that displays the selected metric The display panel, where you can configure the axis, set a threshold lines, and more. You define a chart by specifying both what data should display and how the chart should display it. Metric: To populate the chart, you must specify at least one pair of values, the monitored resource type (or monitored resource, or just resource), and the metric type (also called the metric descriptor, or just metric). Filter: You can reduce the amount of data returned for a metric by specifying a filter. Filtering removes data from the chart by excluding time series that don't meet your criteria. The result is fewer lines on the chart and, hopefully, a better signal to noise ratio. When you click in the Filter field, a panel that contains lists of criteria by which you can filter appears. In broad strokes, you can filter by resource group, by name, by resource label, and by the metric label. In this example, we filter by machine type. The zone can then be compared to a direct value, like "e2-medium," or by using the “=” operator, to any valid regular expression. You can check the documentation If you want to see the fully supported filter syntax. Grouping: You can reduce the amount of data returned for a metric by combining different time series. To combine multiple time series, you typically specify a grouping and a function. Grouping is done by label values. The function defines how all time-series data within a group are combined into a new time series. Alignment: Alignment creates a new time series in which the raw data has been regularized in time so it can be combined with other aligned time series. Alignment produces time series with regularly spaced data. A time series is a set of data points in temporal order. To align a time series is to break the data points into regular buckets of time, the alignment period. Multiple time series must be aligned before they can be combined. Alignment is a prerequisite to aggregation across time series, and monitoring does it automatically, by using default values. You can override these defaults by using the alignment options, which are the Alignment function and the Min alignment period. The alignment period determines the length of time for subdividing the time series. For example, you can break a time series into one-minute chunks or one-hour chunks in the Min interval field. The data in each period is summarized so that a single value represents that period. The default alignment period, which is also the minimum, is one minute. Although you can set the alignment interval for your data, time series might be realigned when you change the time interval displayed on a chart or change the zoom level. The alignment function determines how to summarize the data in each alignment period. The functions include the sum, the mean, and so forth. Valid alignment choices depend on the kind and type of metric data a time series stores. Click any of the legend column headers to sort by that field. The legend columns included in the chart's display are configurable. A chart's widget type and its analysis mode setting determine how the chart displays data. For example, when you create a line chart, each time series is shown by a line with a unique color. However, you can configure a line chart to display statistical measures such as the mean and moving average. There are three analysis modes: Standard mode displays each time series with a unique color. Stats mode displays common statistical measures for the data in a chart. X-Ray mode displays each time series with a translucent gray color. Each line is faint, and where lines overlap or cross, the points appear brighter. Therefore, this mode is most useful on charts with many lines. Overlapping lines create bands of brightness, which indicate the normal behavior within a metrics group. Compare to past: When you use Compare to Past mode on a chart, the legend is modified to include a second “values” column. The current Value column becomes Today, and the past values column is named appropriately—for example, Last Week. Threshold line: The Threshold option creates a horizontal line from a point on the Y-axis. The line provides a visual reference for the chosen threshold value. You can add a threshold that refers to a value on the left Y-axis or the right Y-axis. The Legend Alias field lets you customize a description for the time series on your chart. These descriptions appear on the tooltip for the chart and on the chart legend in the Name column. By default, the descriptions in the legend are created for you from the values of different labels in your time series. Because the system selects the labels, the results might not be helpful to you. To build a template for descriptions, use this field. You can enter plain text and templates in the Legend Alias field. When you add a template, you add an expression that is evaluated when the legend is displayed. To add a legend template to a chart, do the following: In the Display pane, expand Legend Alias. Click + (Plus) and select an entry from the menu. In this example, you see the mix of the variables. You can see the resulting output in the chart legend.

#### Query metrics

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533945

Before we wrap our discussion on dashboards, charts, and the Metrics Explorer, let’s examine a more versatile way of interacting with metrics by leveraging the query languages such as Monitoring Query Language (MQL) and PromQL. MQL: MQL is an advanced query language. It provides an expressive, text-based interface to Cloud Monitoring time-series data. By using MQL, you can retrieve, filter, and manipulate time-series data. PromQL provides an alternative to the Metrics Explorer menu-driven and Monitoring Query Language (MQL) interfaces for creating charts and dashboards. You can use PromQL to query and chart Cloud Monitoring data from the following sources: Google Cloud services, like Google Kubernetes Engine or Compute Engine, that write metrics described in the lists of Cloud Monitoring system metrics. User-defined metrics, like log-based metrics and Cloud Monitoring user-defined metrics. Google Cloud Managed Service for Prometheus, the fully managed multi-cloud solution for Prometheus from Google Cloud. For information about the managed service, including support from PromQL, see Google Cloud Managed Service for Prometheus. You can also use tools like Grafana to chart metric data ingested into Cloud Monitoring. Available metrics include metrics from Managed Service for Prometheus and Cloud Monitoring metrics documented in the lists of metrics. These are a few examples of when you can use MQL: Create ratio-based charts and alerts Perform time-shift analysis (compare metric data week over week, month over month, year over year, etc.) Apply mathematical, logical, table operations, and other functions to metrics Fetch, join, and aggregate over multiple metrics Select by arbitrary, rather than predefined, percentile values Create new labels to aggregate data by, using arbitrary string manipulations including regular expressions Whether you need to perform joins, display arbitrary percentages, or even make advanced calculations, the use cases for MQL are unlimited. MQL is built using operations and functions. Operations are linked together by using the common “pipe” idiom, where the output of one operation becomes the input to the next. Linking operations makes it possible to build complex queries incrementally. In the same way you compose and chain commands and data through pipes on the Linux command line, you can fetch metrics and apply operations by using MQL. For a more advanced example, suppose you built a distributed web service that runs on Compute Engine VM instances and uses Cloud Load Balancing, and you want to analyze error rate, which is one of the SRE “golden signals.” You want to see a chart that displays the ratio of requests that return HTTP 500 responses (internal errors) to the total number of requests; that is, the request-failure ratio. The loadbalancing.googleapis.com/https/request_count metric type has a response_code_class label, which captures the class of response codes. This query uses an aggregation expression built on the ratio of two sums: The first sum uses the if function to count 500-valued HTTP responses and a count of 0 for other HTTP response codes. The sum function computes the count of the requests that returned 500. The second sum adds up the counts for all requests, as represented by val(). The two sums are then divided, which results in the ratio of 500 responses to all responses. You can use queries by navigating to Metrics explorer and simply click the CODE button. You can use the radio button to switch between MQL and PromQL. We will cover PromQL in a later module.

#### Uptime checks

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533946

Another monitoring component before we wrap this module is uptime checks. Uptime checks can be configured to test the availability of your public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource you can check are App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. For each uptime check, you can create an alerting policy and view the latency of each global location. Uptime checks can help us ensure that our externally facing services are running and that we aren’t burning our error budgets unnecessarily. Here is an example of an HTTP uptime check. The resource is checked every minute with a 10-second timeout. Uptime checks that do not get a response within this timeout period are considered failures. So far, there is a 100% uptime with no outages. Uptime checks are easy to create. In Monitoring, navigate to Uptime Checks and click Create Uptime Check. Give the uptime check a name or title that is descriptive. Select the check type protocol, the resource type, and appropriate information for that resource type. A URL, for example, would need a hostname and an optional page path. A number of optional advanced options are available, including logging failures, narrowing the locations in the world from where test connections are made, the addition of custom headers, check timeout, and authentication. The interface also makes it easy to create an alert for failing uptime checks. Cloud Monitoring empowers users with the ability to monitor multiple projects from a single metrics scope. In this exercise, you start with three Google Cloud projects: two have monitorable resources, and you will use the third one to host a metrics scope. You attach the two resource projects to the metrics scope, build uptime checks, and construct a centralized dashboard.

#### Monitoring and Dashboarding Multiple Projects

- https://www.cloudskillsboost.google/paths/11/course_templates/99/labs/533947

#### Module summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533948

In this module, you learned how to: Use Cloud Monitoring to view metrics for multiple cloud projects, Explain the different types of dashboards and charts that can be built, Create an uptime check, Explain the purpose of using MQL for monitoring. Great job and keep up the good work.

#### Quiz - Monitoring critical systems

- https://www.cloudskillsboost.google/paths/11/course_templates/99/quizzes/533949

### Alerting Policies

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533950

In the previous module, we explored how to monitor resources using Cloud Monitoring. Let's us next learn how you can receive timely awareness to problems in your cloud applications so you can resolve the problems quickly. Upon completion of this module, you will be able to explain why SLI, SLO and SLA are important, define alerting policies and discuss alerting strategies. You will also be able to explain error budgets, identify types of alerts and common uses for each and also use Cloud Monitoring to manage services.

#### SLI, SLO, and SLA

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533951

The three terms that are frequently used in this course are SLI, SLO and SLA. Before we cover alerting strategy, it is important to understand what SLI, SLO and SLA are. Service level indicators, or SLIs, are carefully selected monitoring metrics that measure one aspect of a service's reliability. Ideally, SLIs should have a close linear relationship with your users' experience of that reliability, and we recommend expressing them as the ratio of two numbers: the number of good events divided by the count of all valid events. A Service level objective, or SLO, combines a service level indicator with a target reliability. If you express your SLIs as is commonly recommended, your SLOs will generally be somewhere just short of 100%, for example, 99.9%, or "three nines." You can't measure everything, so when possible, you should choose SLOs that are S.M.A.R.T. SLOs should be specific. "Hey everyone, is the site fast enough for you?" is not specific; it's subjective. "The 95th percentile of results are returned in under 100ms." That's specific. They need to be based on indicators that are measurable. A lot of monitoring is numbers, grouped over time, with math applied. An SLI must be a number or a delta, something we can measure and place in a mathematical equation. SLO goals should be achievable. "100% Availability" might sound good, but it's not possible to obtain, let alone maintain, over an extended window of time. SLOs should be relevant. Does it matter to the user? Will it help achieve application-related goals? If not, then it’s a poor metric. And SLOs should be time-bound. You want a service to be 99% available? That’s fine. Is that per year? Per month? Per day? Does the calculation look at specific windows of set time, from Sunday to Sunday for example, or is it a rolling period of the last seven days? If we don't know the answers to those types of questions, it can’t be measured accurately. And then there are Service Level Agreements, or SLAs, which are commitments made to your customers that your systems and applications will have only a certain amount of “down time.” An SLA describes the minimum levels of service that you promise to provide to your customers and what happens when you break that promise. If your service has paying customers, an SLA may include some way of compensating them with refunds or credits when that service has an outage that is longer than this agreement allows. To give you the opportunity to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA. For SLOs, SLIs and SLAs to help improve service reliability, all parts of the business must agree that they are an accurate measure of user experience and must also agree to use them as a primary driver for decision making. Being out of SLO must have concrete, well-documented consequences, just as there are consequences for breaching SLAs. For example, slowing down the rate of change and directing more engineering effort towards eliminating risks and improving reliability are actions that could be taken to get your product back to meeting its SLOs faster. Operations teams need strong executive support to enforce these consequences and effect change in your development practice. Here is an example of a SLA, which is to maintain an error rate of less than 0.3% for the billing system. Here error rate is a quantifiable measure which is the SLI and 0.3 is the specific target set which is the SLO in this case. If your service has paying customers, you probably have some way of compensating them with refunds or credits when that service has an outage. Your criteria for compensation are usually written into a service level agreement, which describes the minimum levels of service that you promise to provide and what happens when you break that promise. The problem with SLAs is that you're only incentivized to promise the minimum level of service and compensation that will stop your customers from replacing you with a competitor. When reliability falls far short of the levels of service that keep your customers happy, this contributes to a perception that your service is unreliable, customers often feel the impact of reliability problems before these promises are breached, Compensating your customers all the time can get expensive, so what targets do you hold yourself to internally? When does your monitoring system trigger an operational response? To give you the breathing room to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA.

#### Developing an alerting strategy

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533952

Now that we have a solid understanding of SLA, SLI and SLO, let us look at how to develop an alerting strategy next. An alert is an automated notification sent by Google Cloud through some notification channel to an external application, ticketing system, or person. If you are wondering why is the alert being sent? The answer perhaps is that a service is down, or an SLO isn't being met. Regardless, an alert is generated when something needs to change. The events are processed through a time series: a series of event data points broken into successive, equally spaced windows of time. Based on need, the duration of each window and the math applied to the member data points inside each window are both configurable. Because of the time series, events can be summarized, error rates can be calculated, and alerts can be triggered where appropriate. A great time to generate alerts is when a system is heading to spend all of its error budget before the allocated time window. An error budget is perfection minus SLO. SLIs are things that are measured, and SLOs represent achievable targets. If the SLO target is "90% of requests must return in 200 ms," then the error budget is 100% - 90% = 10%. Several attributes should be considered when attempting to measure the accuracy or effectiveness of a particular alerting strategy. Precision is the proportion of alerts detected that were relevant to the sum of relevant and irrelevant alerts. It’s decreased by false alerts. Recall is the proportion of alerts detected that were relevant to the sum of relevant alerts and missed alerts. It’s decreased by missing alerts. Precision can be seen as a measure of exactness, whereas recall is a measure of completeness. Detection time can be defined as how long it takes the system to notice an alert condition. Long detection times can negatively affect the error budget, but alerting too fast can generate false positives. Reset time measures how long alerts fire after an issue has been resolved. Continued alerts on repaired systems can lead to confusion. Error budgeting 101 would state that when the error count, or whatever is being measured, is trending to be greater than the allowed error budget, an alert should be generated. Both the SLO itself, and the idea of “trending toward” require windows of time over which they are calculated. In this subject space, the window term is used in two main ways: The SLO itself will be measured over a window of time, say an availability SLO of 99.9% over every 30-day period. Alert triggering will have a window over which it watches for trends. For example, an alert might fire if the percentage of errors over any 60-minute period exceeds 0.1%. One of the alerting decisions you and your team have to make is window length. The window is a regular-length subdivision of the SLO total time. Imagine you set a Google Cloud spend budget of $1,000 a month. When do you want to receive an alert? When the $1,000 is spent? Or when the predicted spend is trending past the $1,000? Of course, the latter. Now, the same concept, but this time imagine a 99.9% availability SLO over 30 days. You don't want to get an alert when your error budget is already gone. By then it's too late to do anything about the problem. One option is small windows. Smaller windows tend to yield faster alert detections and shorter reset times, but they also tend to decrease precision because of their tendency toward false positives. In our 99.9% availability SLO for over 30 days, a 10-minute window would alert in 0.6 seconds if a full outage occurs and would consume only 0.02% of the error budget. Longer windows tend to yield better precision, because they have longer to confirm that an error is really occurring. But reset and detection times are also longer. That means you spend more error budget before the alert triggers. In our same 99.9% availability for SLO over 30 days, a 36-hour window would alert in 2 minutes and 10 seconds if a full outage occurs, but would consume a full 5% of the error budget. One trick might be to use short windows, but add a successive failure count. One window failing won’t trigger the alert, but when three fail in a row the error is triggered. This way, the error is spotted quickly but treated as an anomaly until the duration or error count is reached. This is what you do when your car starts making a sound. You don't immediately freak out, but you pay attention and try to determine whether it's a real issue or a fluke. The downside is that precision typically has an inverse relationship to recall. As the precision goes up, as you avoid false positives, you let the problem continue to happen. If the "pay attention but don't alert yet" duration is 10 minutes, a 100% outage for 5 minutes is not detected. As a result, if errors spike up and down, they might never be detected. So how do we get good precision and recall? This is achieved with multiple conditions. Many variables affect a good alerting strategy, including the amount of traffic, the error budget, peak and slow periods, to name a few. The fallacy is believing that you have to choose a single option. Define multiple conditions in an alerting policy to get better precision, recall, detection time, and rest time. You can also define multiple alerts through multiple channels. Perhaps a short window condition generates an alert, but it takes the form of a Pub/Sub message to a Cloud Run container, which then uses complex logic to check multiple other conditions before deciding whether a human gets a notification. See the SRE Workbook for more information. And alerts should always be prioritized based on customer impact and SLA. Don't involve humans unless the alert meets some threshold for criticality. You can use severity levels as an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. You can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services. The notification channels were enhanced to accept this data—including Email, Webhooks, PubSub, and PagerDuty. This enables further automation and customization based on importance wherever the notifications are consumed. High-priority alerts might go to Slack, SMS, and/or maybe even a third-party solution like PagerDuty. You can even use multiple channels together for redundancy. Low-priority alerts might be logged, sent through email, or inserted into a support ticket management system.

#### Creating alerts

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533953

We've discussed some of the alerting concepts and strategies. Let's look at how to create alerts in Google Cloud. Google Cloud defines alerts by using alerting policies. An alerting policy has: A name One or more alert conditions Notifications Documentation For the name, use something descriptive so you can recognize alerts after the fact. Organizational naming conventions can be a great help. Alert policies can also be created from the gcloud CLI, the API and Terraform. It starts with an alert policy definition in either a JSON or YAML format. One neat trick when learning the correct file format is to create an alert using the Google Cloud console. Then use the gcloud monitoring policies list and the describe commands to see the corresponding definition file. The alerting API and gcloud CLI can create, retrieve, and delete alerting policies. [Reference: https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/channels/create] The alerting policies are of two types. Metric based alerting: Policies used to track metric data collected by Cloud Monitoring are called metric-based alerting policies. You can add a metric-based alerting policy to your Google Cloud project by using the Google Cloud console. A classic example of a metric-based alerting policy is to notify when the application is running on a VM that has high latency for a significant time period. Log based alerting: Log-based alerting is used to notify anytime a specific message occurs in a log. You can add a log-based alerting policy to your Google Cloud project by using the Logs Explorer in Cloud Logging or by using the Cloud Monitoring API. An example of log-based alerting policy is to notify when a human user accesses the security key of a service account. The alert condition is where you spend the most alerting policy time and make the most decisions. The alert condition is where you decide what's being monitored and under what condition an alert should be generated. Notice how the web interface combines the heart of the Metrics Explorer with a configuration condition. You start with a target resource and metric you want the alert to monitor. You can filter, group by, and aggregate to the exact measure you require. Then the yes-no decision logic for triggering the alert notification is configured. It includes the trigger condition, threshold, and duration. There are three types of conditions for metric-based alerts: Metric-threshold conditions trigger when the values of a metric are more than, or less than, a threshold for a specific duration window. Metric-absence conditions trigger when there is an absence of measurements for a duration window. Forecast conditions predict the future behavior of the measurements by using previous data. These conditions trigger when there is a prediction that a time series will violate the threshold within a forecast window. An alert might have zero to many notification options selected, and they each can be of a different type. There are direct-to-human notification channels (Email, SMS, Slack, Mobile Push), and for third-party integration use Webhook and Pub/Sub. Manage your notifications and incidents by adding user-defined labels to an alerting policy. Because user-defined labels are included in notifications, if you add labels that indicate the severity of an incident, then the notification contains information that can help you prioritize your alerts for investigation. If you send notifications to a third-party service like PagerDuty, Webhooks, or Pub/Sub then you can parse the JSON payload and route the notification according to its severity so that your team doesn't miss critical information. A notification channel decides how the alert is sent to the recipient. Alerts can be routed to any third-party service. Email alerts are easy and informative, but they can become notification spam if you aren't careful. SMS is a great option for fast notifications, but choose the recipient carefully. Slack is very popular in support circles. The Google Cloud app for mobile devices is a valid option. PagerDuty is a third-party on-call management and incident response service. Webhooks and Pub/Sub are excellent options when you want to alert users to external systems or code. The documentation option is designed to give the alert recipient additional information they might find helpful. Use the documentation section to guide your troubleshooting. Include internal playbooks, landing links and dynamic labels The default alert contains information about which alert is failing and why, so think of this more like an easy button. If there's a standard solution to this particular alert, adding a reference to it here might be a good example of proper documentation inclusion. Then again, if it was that easy, automate it! Here, you see an alert notification sent out through an email. Notice how many details about exactly what went wrong are automatically included in the email body. The bottom documentation section can also be used to augment the provided information. When one or more alert policies are created, the alerting web interface provides a summary of incidents and alerting events. An event occurs when the conditions for an alerting policy are met. When an event occurs, Cloud Monitoring opens an incident. In the Alerting window, the Summary pane lists the number of incidents, and the Incidents pane displays the ten most recent incidents. Each incident is in one of three states: Incidents firing: If an incident is open, the alerting policy's set of conditions is being met. Or there’s no data to indicate that the condition is no longer met. Open incidents usually indicate a new or unhandled alert. Acknowledged incidents: A technician spots a new open alert. Before one starts to investigate, they mark it as acknowledged as a signal to others that someone is dealing with the issue. Alert policies displays the number of alerting policies created. The Incidents pane displays the most recent open incidents. To list the most recent incidents in the table, including those that are closed, click Show closed incidents. Snooze displays the recently configured snoozes. When you want to temporarily prevent alerts from being created and notifications from being sent, or to prevent repeated notifications from being sent for an open incident, you create a snooze. For example, you might create a snooze when you have an escalating outage and you want to reduce the number of new notifications. Groups provide a mechanism for alerting on the behavior of a set of resources instead of individual resources. For example, you can create an alerting policy that is triggered if some resources in the group violate a condition (for example, CPU load), instead of having each resource inform you of violations individually. Groups can contain subgroups and can be up to six levels deep. One application for groups and subgroups is the management of physical or logical topologies. For example, with groups, you can separate your monitoring of production resources from your monitoring of test or development resources. You can also create subgroups to monitor your production resources by zone. Resources can belong to multiple groups. You define the one-to-many membership criteria for your groups. A resource belongs to a group if the resource meets the membership criteria of the group. Membership criteria can be based on resource name or type, Cloud Projects, network tag, resource label, security group, region, or App Engine app or service. Logs-based metrics are extracted from Cloud Monitoring and are based on the content of log entries. For example, the metrics can record the number of log entries that contain particular messages, or they can extract latency information reported in log entries. You can use logs-based metrics in Cloud Monitoring charts and alerting policies. As we covered earlier in this module, an alerting policy describes a set of conditions that you want to monitor. When you create an alerting policy, you must also specify its conditions: what is monitored and when to trigger an alert. The logs-based metrics serve as the basis for an alerting condition. To create alerting policies by using Terraform, start by creating a description of the conditions under which some aspect of your system is considered to be "unhealthy" and the ways to notify people or services about this state. Shown on slides is a basic monitoring alert policy with the name alert_policy. The code includes three required arguments: Display_name: This argument helps identify the policy with a name that can be seen on the dashboard. Combiner: This argument defines how the results of multiple conditions have to combined. Conditions: This argument defines a list of conditions that are combined based on the combiner. A policy can have up to six conditions. For some more policy examples, visit the links listed on this slide. These can be accessed from this module’s student PDF under Course Resources.

#### Alerting in Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/99/labs/533954

#### Service Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533955

Now that we’ve examined alerts, their use, and their creation, let’s see how the Service Monitoring console and API can help. Modern applications are composed of multiple services connected together, and when something fails, it often seems like many things fail at the same time. To help manage this complexity, SLO monitoring helps with SLO and alert creation. With Service Monitoring, you get the answers to the following questions: What are your services? What functionality do those services expose to internal and external customers? What are your promises and commitments regarding the availability and performance of those services, and are your services meeting them? For microservices-based apps, what are the inter-service dependencies? How can you use that knowledge to double check new code rollouts and triage problems if a service degradation occurs? Can you look at all the monitoring signals for a service holistically to reduce mean time to repair (MTTR)? Cloud Monitoring can identify potential or candidate services for the following types: GKE namespaces GKE services GKE workloads Cloud Run services The Service Monitoring consolidated services overview page is your point of entry. Services pane provides a summary of the health of your various services. Here, you can see the service name, type, SLO status, and whether any SLO-related alerts are firing. To monitor or view details for a specific service, click the service name. You can also filter by entering a value in the Filter text box to apply additional conditions. Service Monitoring can approach SLO compliance calculations in two fundamental ways. Request-based SLOs use a ratio of good requests to total requests. For example, we want a request-based SLO with a latency below 100 ms for at least 95% of requests. So It is convenient for us if 98% of requests were faster than 100 ms. Window-based SLOs use a ratio of the number of good versus bad measurement intervals, or windows. So each window represents a data point, instead of all the data points that comprise the window. For example, take a 95th percentile latency SLO that needs to be less than 100 ms for at least 99% of 10-minute windows. Here, a compliant window is a 10-minute period over which 95% of the requests were less than 100 ms. So It is convenient for us if 99% of 10-minute windows were compliant. Let's look at another pair of window-based versus request-based SLO examples. Imagine you get 1,000,000 requests a month, and your compliance period is rolling for 30 days. If you are looking for a 99.9% request-based SLO, that translates to 1,000 total bad requests every 30 days. However, a 99.9% windows-based SLO which is averaged across 1-minute windows, allows a total of 43 bad windows, or 43,200 total windows * 99.9% = 43,157 good windows. Windows-based SLOs can be tricky because they can hide burst-related failures. If the system returns nothing but errors, but only every Friday morning from 9:00-9:05, then you will never violate your SLO. However, not many people prefer to use the system first thing Friday morning. Service Monitoring makes SLO creation easy. On the Services overview page, select one of the listed services. If a service is built on a Google Cloud compute technology that supports Service Monitoring, it will be automatically listed. Next, click Create SLO. Select an option from the SLI metric. The options include: Availability is a ratio of the number of successful responses to the number of all responses. Latency is the ratio of the number of calls that are below the specified Latency Threshold to the number of all calls. The option Other gives you the Metrics Explorer window and lets you create your own SLI from the beginning. As previously discussed, you can also select request-based or windows-based SLOs here. In the Compliance Period section, select the Period Type and the Period Length. The two compliance period types are calendar-based and rolling. In the Performance Goal section, enter a percentage in the Goal field to set the performance target for the SLI. Service Monitoring uses this value to calculate the error budget you have for this SLO. You can create an alert by just clicking Create alerting policy. Click an individual service on the Services Overview page to view its details. There, you can see existing SLOs and, by expanding them, their details. The SLI status, the error budget remaining, and the current level of SLO compliance are all displayed. If alerts have been set, their status is also displayed. Service Monitoring can trigger an alert when a service is heading to violate an SLO. The alerting policy uses a lookback window to examine a period for trends. The burn rate threshold is then used to determine whether an alert should be raised. In this example, we are using a 60-minute window. A burn rate threshold of 1 would use 100% of the error budget by the end of the 7-day period. In this case, if we see a trend that would burn through our total error budget in 1/10th of those seven days or faster, then we should raise an alert. After the SLO is created, it’s easy to monitor the SLI status, error budget, compliance, and alert status. In this case, I’ve selected to view the Service level indicator, so it’s displaying the current SLI values. You could also examine the Error budget or Alerts firing tabs for more detail on those.

#### Service Monitoring

- https://www.cloudskillsboost.google/paths/11/course_templates/99/labs/533956

#### Module summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533957

In this module, you learned to explain why SLI, SLO and SLA are important, to define alerting policies and discuss alerting strategies. You also learnt to explain error budget, identify types of alerts and common uses for each and also use Cloud Monitoring to manage services.

#### Quiz - Alerting Policies

- https://www.cloudskillsboost.google/paths/11/course_templates/99/quizzes/533958

### Advanced Logging and Analysis

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533959

In this module, we examine some of Google Cloud's advanced logging and analysis capabilities. Specifically, in this module you learn to: Use Log Explorer features, Explain the features and benefits of log-based metrics, Define log sinks (inclusion filters) and exclusion filters, Explain how Big query can be used to analyze logs, Use Log Analytics on Google Cloud.

#### Cloud Logging overview and architecture

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533960

As we learned earlier, Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. In this section we will go through logging architecture and understand its use cases. Logs is one of the top most visited sections in Google Cloud console and one of most transitional, which indicated that it is an important component of many scenarios. End users need logs for troubleshooting and information gathering but don’t want to be overwhelmed with the data. Logs are the pulse of your workloads and application. Cloud Logging helps to: Gather data from various workloads:. This data is required to troubleshoot and understand the workload and application needs. Analyze large volumes of data: Tools like Error Reporting, Log Explorer, and Log Analytics let you focus from large sets of data. Route and store logs: Route your logs to the region or service of your choice for additional compliance or business benefits. Get Compliance Insights: Leverage audit and app logs for compliance patterns and issues. We will cover this in the Audit logs module in detail. Cloud Logging architecture consists of the following components: Log Collections: These are the places where log data originates. Log sources can be Google Cloud services, such as Compute Engine, App Engine, and Kubernetes Engine, or your own applications. Log Routing: The Log Router is responsible for routing log data to its destination. The Log Router uses a combination of inclusion filters and exclusion filters to determine which log data is routed to each destination. Log sinks: Log sinks are destinations where log data is stored. Cloud Logging supports a variety of log sinks, including: Cloud Logging log buckets: These are storage buckets that are specifically designed for storing log data. Pub/Sub topics: These topics can be used to route log data to other services, such as third-party logging solutions. BigQuery: This is a fully-managed, petabyte-scale analytics data warehouse that can be used to store and analyze log data. Cloud Storage buckets: Provides storage of log data in Cloud Storage. Log entries are stored as JSON files. Log Analysis: Cloud Logging provides several tools to analyze logs. Logs Explorer is optimized for troubleshooting use cases with features like log streaming, a log resource explorer and a histogram for visualization. Error Reporting help users react to critical application errors through automated error grouping and notifications. Logs-based metrics, dashboards and alerting provide other ways to understand and make logs actionable. Log Analytics feature expands the toolset to include ad hoc log analysis capabilities.

#### Log types and collection

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533961

Let us start with Log Collection and move our way forward to routing and storage and finally visualization. The Google Cloud platform logs visible to you in Cloud Logging vary, depending on which Google Cloud resources you're using in your Google Cloud project or organization. Let's explore the key log categories. Platform logs are logs written by Google Cloud services. These logs can help you debug and troubleshoot issues, and help you better understand the Google Cloud services you're using. For example, VPC Flow Logs record a sample of network flows sent from and received by VM instances. Component logs are similar to platform logs, but they are generated by Google-provided software components that run on your systems. For example, GKE provides software components that users can run on their own VM or in their own data center. Logs are generated from the user's GKE instances and sent to a user's Cloud project. GKE uses the logs or their metadata to provide user support. Security logs help you answer "who did what, where, and when." Cloud Audit Logs provide information about administrative activities and accesses within your Google Cloud resources. Access Transparency provides you with logs of actions taken by Google staff when accessing your Google Cloud content. User-written logs are logs written by custom applications and services. Typically, these logs are written to Cloud Logging by using one of the following methods: Ops Agent, Cloud Logging API, Cloud Logging client libraries. Multi-cloud logs and Hybrid-cloud logs refer to logs from other cloud providers like Microsoft Azure and logs from on-premises infrastructure. You can programmatically send application logs to Cloud Logging by using client libraries or by using one of the Logging agents. When you can't use them, or when you only want to experiment, you can write logs by using the gcloud logging write command or by sending HTTP commands to the Cloud Logging API endpoint entries.write. If you're using one of the agents, then your applications can use any established logging framework to emit logs. For example, in container environments like Google Kubernetes Engine or Container-Optimized OS, the agents automatically collect logs from stdout and stderr. On virtual machines (VMs), the agents collect logs from known file locations or logging services like the Windows Event Log, journald, or syslogd. Serverless compute services like Cloud Run and Cloud Run functions, include simple runtime logging by default. Logs written to stdout or stderr will appear automatically in the Google Cloud console.

#### Storing, routing and exporting the logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533962

Now that we understand the log collection, let's look at how logs can be routed and exported for long-term storage and analysis. What we call Cloud Logging is actually a collection of components exposed through a centralized logging API. Log Router: Entries are passed through the API and fed to Log Router. Log Router is optimized for processing streaming data, reliably buffering it, and sending it to any combination of log storage and sink (export) locations. By default, log entries are fed into one of the default logs storage buckets. Exclusion filters might be created to partially or totally prevent this behavior. Log sinks run in parallel with the default log flow and might be used to direct entries to external locations. Log storage: Locations might include additional Cloud Logging buckets, Cloud Storage, BigQuery, Pub/Sub, or external projects. Inclusion and exclusion filters can control exactly which logging entries end up at a particular destination, and which are ignored completely. For each Google Cloud project, Logging automatically creates two logs buckets: _Required and _Default, and corresponding log sinks with the same names. All logs generated in the project are stored in one of these two locations: _Required bucket holds Admin Activity audit logs, System Event audit logs, and Access Transparency logs, and retains them for 400 days. You aren't charged for the logs stored in _Required, and the retention period of the logs stored here cannot be modified. You cannot delete or modify this bucket. _Default bucket holds all other ingested logs in a Google Cloud project, except for the logs held in the _Required bucket. Standard Cloud Logging pricing applies to these logs. Log entries held in the _Default bucket are retained for 30 days, unless you apply custom retention rules. You can't delete this bucket, but you can disable the _Default log sink that routes logs to this bucket. The Logs Storage page displays a summary of statistics for the logs that your project is receiving, including: Current total volume: The amount of logs your project has received since the first date of the current month. Previous month volume: The amount of logs your project received in the last calendar month. Projected volume by EOM: The estimated amount of logs your project will receive by the end of the current month, based on current usage. You can view the total usage by resource type for the current total volume. The link opens Metrics Explorer, which lets you build charts for any metric collected by your project. For more information on using Metrics Explorer, refer to the documentation. Log Router sinks can be used to forward copies of some or all of your log entries to non-default locations. Use cases include storing logs for extended periods, querying logs with SQL, and access control. Here, you see we‚Äôve started creating a sink by generating a log query for a particular subset of entries. We will pass that subset to one of the available sink locations. There are several sink locations, depending on need: Cloud Logging bucket works well to help pre-separate log entries into a distinct log storage bucket. BigQuery dataset allows the SQL query power of BigQuery to be brought to bear on large and complex log entries. Cloud Storage bucket is a simple external Cloud Storage location, perhaps for long-term storage or processing with other systems. Pub/Sub topic can export log entries to message handling third-party applications or systems created with code and running somewhere like Dataflow or Cloud Run functions. Splunk is used to integrate logs into existing Splunk-based system. The Other project option is useful to help control access to a subset of log entries. The process for creating log sinks mimics that of creating log exclusions. It involves writing a query that selects the log entries you want to export in Logs Explorer, and choosing a destination of Cloud Storage, BigQuery, or Pub/Sub. The query and destination are held in an object called a sink. Sinks can be created in Google Cloud projects, organizations, folders, and billing accounts. Use Logs Explorer to build a query that selects the logs you want to exclude. Save the query to use when building the exclusion. Use the Log Explorer query to create an exclusion filter that filters the unwanted entries out of the sink. Give the exclusion a name and add the filter for log entries to exclude. It might be helpful to leave some representative events, depending on the exclusion. Create the exclusion and it will go into effect immediately. Use the Navigation menu to initiate editing of that entity. Take care here, because excluded log events will be lost forever. Over the next several slides, we will investigate some possible log export processing options. Here, for example, we are exporting through Pub/Sub, to Dataflow, to BigQuery. Dataflow is an excellent option if you're looking for real-time log processing at scale. In this example, the Dataflow job could react to real-time issues, while streaming the logs into BigQuery for longer-term analysis. Sink pipelines targeting Cloud Storage tend to work best when your needs align with Cloud Storage strengths. For example, long-term retention, reduced storage costs, and configurable object lifecycles. Cloud Storage features include automated storage class changes, auto-delete, and guaranteed retention. Here, we have an example organization that wants to integrate the logging data from Google Cloud, back into an on-premises Splunk instance. You can ingest logs into Splunk you can either stream logs using Pub/Sub to Splunk Dataflow or using the Splunk Add-on for Google Cloud. Pub/Sub is one of the options available for exporting to Splunk, or to other third-party System Information and Event Management (SIEM) software packages. A common logging need is centralized log aggregation for auditing, retention, or non-repudiation purposes. Aggregated sinks allow for easy exporting of logging entries without a one-to-one setup. The sink destination can be any of the destinations discussed up to now. There are three available Google Cloud Logging aggregation levels. We've discussed a project-level log sink. It exports all the logs for a specific project and a log filter can be specified in the sink definition to include or exclude certain log types. A folder-level log sink aggregates logs on the folder level and can include logs from children resources (subfolders, projects). And for a global view, an organization-level log sink can aggregate logs on the organization level and can also include logs from children resources (subfolders, projects). Security practitioners onboard Google Cloud logs for security analytics. By performing security analytics, you help your organization prevent, detect, and respond to threats like malware, phishing, ransomware, and poorly configured assets. One of the steps in security log analytics workflow is to create aggregate sinks and route those logs to a single destination depending on the choice of security analytics tool, such as Log Analytics, BigQuery, Chronicle, or a third-party security information and event management (SIEM) technology. Logs are aggregated from your organization, including any contained folders, projects, and billing accounts. There are a few naming conventions that apply to log entry fields: For log entry fields that are part of the LogEntry type, the corresponding BigQuery field names are precisely the same as the log entry fields. For any user-supplied fields, the letter case is normalized to lowercase, but the naming is otherwise preserved. For fields in structured payloads, as long as the @type specifier is not present, the letter case is normalized to lowercase, but naming is otherwise preserved. For information on structured payloads where the @type specifier is present, see the Payload fields with @type documentation. You can see some examples on the current slide. Here's a sample query over the Compute Engine logs. It retrieves log entries for multiple log types over multiple days. The query searches the last three days (today -2) of the syslog and apache-access logs. The query retrieves results for the single Compute Engine instance ID seen in the where clause. In this BigQuery example, we are looking for unsuccessful App Engine requests from the last month. Notice how the from clause is constructing the table data range. The status not equal to 200 is examining the HTTP status for anything that isn't 200. That is to say, anything that isn't a successful response.

#### Query and view logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533963

Once you have collected logs and routed to the right destination, now is the time to query and view logs. The Logs Explorer interface lets you retrieve logs, parse and analyze log data, and refine your query parameters. The Logs Explorer contains the following panes: Action toolbar to refine logs to projects or storage views, share a link and learn about logs explorer. Query pane is where you can build queries, view recently viewed and saved queries and a lot more. Results Toolbar can be used to quickly show or hide logs and histogram pane and create a log based metric or alert. Jump to now option helps query and view the current time results. Query results is the details of results with a summary and timestamp that helps troubleshoot further. Log fields pane is used to filter your options based on various factors such as a resource type, log name, project ID, etc,. Histogram is where the query result is visualized as histogram bars, where each bar is a time range and is color coded based on severity. Ultimately, it’s the query that selects the entries displayed by Logs Explorer. Queries may be created directly with the Logging Query Language (LQL), using the drop-down menus, the logs field explorer, or by clicking fields in the results themselves. Start with what you know about the entry you’re trying to find. If it belongs to a resource, a particular log file, or has a known severity, use the query builder drop-down menus. The query builder drop-down menu makes it easy to start narrowing your log choices. Resource: Lets you specify resource.type. You can select a single resource at a time to add to the Query builder. Entries use the logical operator AND. Log name: Lets you specify logName. You can select multiple log names at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. Severity: Lets you specify severity. You can select multiple severity levels at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. The next several slides are included for reference. Advanced queries support multiple comparison operators as seen here. The equal and not equal operators help filter values that match or not match a value assigned to a field name. These are useful when you search for a specific resource type or id that you want to evaluate. The numeric ordering operators are handy when searching for logs filtering a timestamp or duration. The colon operation helps check if a value exists. This is useful when you want to match a substring within a log entry field. To test if a missing or defaulted field exists without testing for a particular value in the field, use the :* comparison. If you’re looking for a specific set of log entries and have a rough idea when they would have been generated, start by narrowing to a specific time range. You can select one of the pre-created choices, set a custom range, or jump to a particular time. The Log fields panel offers a high-level summary of logs data and provides a more efficient way to refine a query. It shows the count of log entries, sorted by decreasing count, for the given log field. The log field counts correspond to the time range used by the Histogram panel. You can add fields from the Log fields panel to the Query builder to narrow down and refine a query by clicking a field. When a query is run, the log field counts are incrementally loaded as the log entries are progressively scanned. Once the query is complete, which is indicated by the completion of the blue progress bar, you see the total counts for all log fields. The histogram panel lets you visualize the distribution of logs over time. Visualization makes it easier to see trends in your logs data and troubleshoot problems. For example, the severity colors make it easy to spot an increasing number of errors even when the volume of requests is relatively constant. To analyze your log data, point to a bar in the Histogram panel and select Jump to time to drill into a narrower time range. A new query runs with that time-range restriction. Advanced queries support the AND, OR, and NOT Boolean expressions for joining queries. A couple of things to keep in mind include: Ensure to use the all caps for the operator name. The NOT operator has the highest precedence, followed by OR and AND in that order. The Boolean operators AND and OR are short-circuit operators. Here is a simple recipe for finding entries. When you’re trying to find log entries, as mentioned earlier, start with what you know: the log filename, resource name, even a bit of the contents of the logged message might work. Full text searches are slow, but they may be effective. For example, you might search for “/score called”. If possible, restrict text searches to an entry region, like jsonPayload:”/score called”, or even better, jsonPayload.message=”/score called”. You can use the built-in SEARCH function to find strings in your log data as shown on slide. SEARCH([query]) SEARCH([field], [query]) Both forms of the SEARCH function contain a query argument, which must be formatted as a string literal. In the first form, the entire log entry is searched. In the second form, you specify the field in the log entry to search. The Logging query language supports different ways that you can search your log data. When searching for a string, it is more efficient to use the SEARCH function than to perform a global search or a substring search. However, you can't use the SEARCH function to match non-text fields. Some tips on finding log entries quickly: Search for specific values of indexed fields, like the name of the log entry, resource type, and resource labels. Apply constraints on resource.type and resource.labels, resource.type = "gke_cluster" and resource.labels.namespace = "my-cool-namespace" These fields are preferentially indexed in our storage and can make a huge difference for query performance. As seen in the example, be specific on which logs you’re searching by referring to it or them by name. Limit the time range that you’re searching to reduce the log data that is being queried.

#### Using log-based metrics

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533964

Let's next focus about generating monitoring metrics from logging data. Logs-based metrics derive metric data from the content of log entries. For example, metrics can track the number of entries that contain specific messages or extract latency information that is reported in the logs. These metrics transform into time series data and use it in Cloud Monitoring Charts and Alerting Policies. There are two types of log-based metrics: System-defined log-based metrics, provided by Cloud Logging can be used by all Google Cloud projects. System-defined log-based metrics are calculated only from logs that have been ingested by Logging. If a log has been explicitly excluded from ingestion by Cloud Logging, it isn't included in these metrics. User-defined log-based metrics, created by you to track things in your Google Cloud project that are of particular interest to you. For example, you might create a log-based metric to count the number of log entries that match a given filter. Log-based metrics are suitable when you want to do any of the following: Count the occurrences of a message, like a warning or error, in your logs and receive a notification when the number of occurrences crosses a threshold. Observe trends in your data, like latency values in your logs, and receive a notification if the values change in an unacceptable way. Create charts to display the numeric data extracted from your logs. A refresher of the key IAM roles that relate to logging and monitoring. First, on the logging side: Logs Configuration Writers can list, create, get, update, and delete log-based metrics. Logs Viewers can view existing metrics. On the monitoring side, Monitoring Viewers can read the time series in log-based metrics. And finally, Logging Admins, Editors, and Owners are all broad-level roles that can create log-based metrics. There are three types of log-based metrics: counter or distribution. All predefined system log-based metrics are the counter type, but user-defined metrics can be either counter, distribution or boolean types. Counter metrics count the number of log entries matching an advanced logs query. So, if we simply wanted to know how many of our "/score called" entries were generated, we could create a counter. Distribution metrics record the statistical distribution of the extracted log values in histogram buckets. The extracted values are not recorded individually. Their distribution across the configured buckets is recorded, along with the count, mean, and sum of squared deviations of the values. Boolean metrics record where a log entry matches a specified filter System-defined log-based metrics apply at the Google Cloud project level. These metrics are calculated by the Log Router and apply to logs only in the Google Cloud project in which they're received. User-defined log-based metrics can apply at either the Google Cloud project level or at the level of a specific log bucket: Project-level metrics are calculated like system-defined log-based metrics; these user-defined log-based metrics apply to logs only in the Google Cloud project in which they're received. Bucket-scoped metrics apply to logs in the log bucket in which they're received, regardless of the Google Cloud project in which the log entries originated. With bucket-scoped log-based metrics, you can create log-based metrics that can evaluate logs in the following cases: Logs that are routed from one project to a bucket in another project. Logs that are routed into a bucket through an aggregated sink. Before we create a log-based metric, let's generate some logging entries. Here we see a basic NodeJS app built with the simple and lightweight Express web server. The app is run as a managed container on the Cloud Run service. The code watches for a request to come into the server on the '/score' path. When a /score request arrives, the code generates a random score between 1 to 100, and it then creates a log entry. Earlier code, not shown on this slide, created a unique identifier for the container serving this request in containerID and a random value called funFactor. The log entry contains the text "/score called‚Äù, the random score, the container ID, and the fun factor. Lastly, a basic message, also containing the score, is sent back to the browser. Use the Query builder to access project logs. In the list of entries, we've located one of the "/score called" entries. Now we can filter to select those entries by clicking "/score called", and selecting Show matching entries. Imagine we've generated some load on our Cloud Run sample application, and we'd like to use the log events to generate a log-based metric. As defined earlier there are two fundamental log-based metric types, System log-based metrics and User-defined log-based metrics. The latter is what we are creating now. Note the Create Metric button at the top of the interface. This is an example of a basic flow for creating log-based metrics: You start by finding the log with the requisite data. Then you filter it to the required entries. Create a metric. Pick your metric type (Counter or Distribution). If Distribution, then set configurations. And finally, add labels as needed. Like many cloud resources, labels can be applied to log-based metrics. Their prime use is to help with group-by and filtering tasks in Cloud Monitoring. Labels allow log-based metrics to contain multiple time series‚Äîone for each label value. All log-based metrics come with some default labels and you can create additional user-defined labels in both counter-type and distribution-type metrics by specifying extractor expressions. An extractor expression tells Cloud Logging how to extract the value of the label from log entries. You can specify the label's value as either of the following: The entire contents of a named field in the LogEntry object. A part of a named field that matches a regular expression (regexp). You can extract labels from the LogEntry built-in fields, such as httpRequest.status, or from one of the payload fields, textPayload, jsonPayload, or protoPayload. Label with care. A metric can support up to ten user-defined labels, and once created, a metric cannot be removed. Also, each log-based metric is limited to about 30,000 active time series. Each label can grow the time series count significantly. For example, if your log entries come from 100 resources, such as VM instances, and you define a label with 20 possible values, then you can have up to 2,000 time series for your metric. User-defined labels can be created when creating a log-based metric. The label form requires: Name is an identifier which will be used to label in Monitoring. Description describes the label. Try to be as specific as possible. Choose String, Boolean, or Integer for label type. For Field name, enter the name of the log entry field that contains the value of the label. This field supports autocomplete. Extraction regular expression: If the value of your label consists of the field's entire contents, then you can leave this field empty. Otherwise, specify a regular expression (regexp) that extracts the label value from the field value.

#### Log analytics

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533965

Next let us take a look at one of the new feature in Cloud Logging, Log Analytics. Log Analytics gives you the analytical power of BigQuery within the Cloud Logging console and provides you with a new user interface that's optimized for analyzing your logs. When you create a bucket and activate analytics on it, Cloud Logging makes the logs data available in both the new Log Analytics interface and BigQuery; you don't have to route and manage a separate copy of the data in BigQuery. You can still query and examine the data as usual in Cloud Logging with the Logging query language. Logs are written to the Logging API via client libraries, stdout/fluentbit agent or directly via API,. Logs Explorer helps with troubleshooting and getting to the root cause with search, filter, histogram and suggested search. Log Router routes logs to the Logging Sink for the Logs Bucket. Log Analytics, analyze application performance, data access and network access patterns. Log Analytics pipeline maps logs to BigQuery tables (JSON, STRING, INT64, RECORD, etc..) and writes to BigQuery. Use the same logs data in Log Analytics directly from BigQuery to report on aggregated application and business data found in logs. The logs data in your analytics-enabled buckets is different than logs routed to BigQuery via traditional export in the following ways: Log data in BigQuery is managed by Cloud Logging. BigQuery ingestion and storage costs are included in your Logging costs. Data residency and lifecycle are managed by Cloud Logging. You can query your logs on Log Analytics-enabled buckets directly in Cloud Logging via the new Log Analytics UI. The Log Analytics UI is optimized for viewing unstructured log data. You can also access your logs data in BigQuery using a read-only view if you want to combine your logs data with other data in BigQuery. You can turn on or off access to your analytics-enabled buckets in BigQuery by turning on or off the option that connects the logs to BigQuery. When the option is enabled, you can query the logs directly from BigQuery, including joining your logs data with other BigQuery datasets. To create an analytics-enabled bucket by using the console: Navigate to Logs Storage. Click Create log bucket. Select Upgrade to use Log Analytics. Note: Upgrading a bucket to use Log Analytics is permanent. You can't downgrade the log bucket to remove the use of Log Analytics. Log Analytics is useful in multiple aspects. Let's look at different fields' perspectives, starting with DevOps. DevOps: For a DevOps specialist it is important to quickly troubleshoot an issue that requires to reduce Mean Time to Repair (MTTR). Log Analytics includes capabilities to count the top requests grouped by response type and severity, which allows engineers to diagnose the issues. Security A security personal is interested in finding all the audit logs associated with a specific use over the past month. Log Analytics help better investigate the security -related attacks with queries over large volumes of security data. For more information, refer to the documentation. IT or Network Operations IT or Network Operations is interested in identifying network issues for GKE instances that are using VPC and firewall rules. Log Analytics in this case provides better network insights and management through advanced log aggregation capabilities. For more information, refer to the documentation.

#### Log Analytics on Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/99/labs/533966

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533967

Specifically, in this module you learn to: Use Log Explorer features, Explain the features and benefits of log-based metrics, Define log sinks, inclusion filters and exclusion filters, Explain how BigQuery can be used to analyze logs, Use Log Analytics on Google Cloud.

#### Quiz - Advanced Logging and Analysis

- https://www.cloudskillsboost.google/paths/11/course_templates/99/quizzes/533968

### Working with Audit Logs

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533969

In this module, we investigate the core Cloud Audit Logs that Google Cloud collects. In this module, you’ll learn to: Explain Cloud Audit Logs. List and explain different audit logs. Explain the features and functionalities of the different audit logs. List the best practices to implement audit logs.

#### Cloud Audit Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533970

We start with an overview of what audit logs do for us. Then we move to using the optional Data Access audit logs, explore the format of audit log entries, and wrap up with some logging best practices. In terms of sheer volume of useful information, probably the most important group of logs in Google Cloud are the Cloud Audit Logs. Cloud Audit Logs helps answer the question, "Who did what, where, and when? ” It maintains four audit logs for each Google Cloud project, folder, and organization: Admin Activity audit logs System Event audit logs Data Access audit logs Policy Denied audit logs All Google Cloud services will eventually provide audit logs. For now, see the Google services with audit logs documentation for coverage details. Admin Activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions. They are always on, are retained for 400 days, and are available at no charge. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. System Event audit logs contain log entries for Google Cloud administrative actions that modify the configuration of resources. System Event audit logs are generated by Google systems; they are not driven by direct user action. They are always enabled, free, and retained for 400 days. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. Data Access audit logs contain API calls that read the configuration or metadata of resources. Also, user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs don’t record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users). Data Access audit logs also don’t record the data-access operations on resources that can be accessed without logging into Google Cloud. They are disabled by default (except for BigQuery), and when enabled, the default retention is 30 days. To view these logs, you must have the IAM roles Logging/Private Logs Viewer or Project/Owner. When a security policy is violated, policy denied audit logs records when access to a user or service account is denied by Google Cloud service. Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs. However, you can use exclusion filters to prevent Policy Denied audit logs from being ingested and stored in Cloud Logging. To view and filter audit logs: Navigate to Logs Explorer Filter by using the Log name drop-down menu. Note: typing cloudaudit into the filter box is frequently quicker than scrolling. If one of the four audit logs is missing, that simply means it doesn’t currently have any entries. The example here filters the logs by a project and you can select the log entries you would like to audit. You can even use the query builder to filter audit logs. This query is auto populated in the query section when using the UI. For details check out the documentation page. Whether it's a hardware support engineer, or a rep working on a ticket, having dedicated experts manage parts of the infrastructure is a key benefit of operating in Google Cloud. Very similar to Cloud Audit logs, Access Transparency logs help by providing logs of accesses to your data by human Googlers (as opposed to automated systems). Enterprises with appropriate support packages can enable the logs, and receive the log events in near-real time. The log events are surfaced through the APIs, Cloud Logging, and Security Command Center. Access Transparency logs give you different information than Cloud Audit Logs. Cloud Audit Logs record the actions that members of your Google Cloud organization have taken in your Google Cloud resources, whereas Access Transparency logs record the actions taken by Google personnel.

#### Data Access audit logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533971

Let’s continue by looking at Data Access audit logs. Data Access audit logs can be enabled at various levels in the resource hierarchy. These levels include: Organization Folder Project, Resources, and Billing accounts You can also exempt principals from recording data access logs. The final configuration of Data Access audit logs is the union of the configurations. For example, at a project level, you can enable logs for a Google Cloud service. But you can't disable logs for a Google Cloud service that is enabled in a parent organization or folder. The added logging does add to the cost, currently: $0.50 per gigabyte for ingestion. Data Access audit logs are disabled by default, for everything but BigQuery. They may be enabled and configured at the organization, folder, project, or service level. You can control what type of information is kept in the audit logs. There are three types of Data Access audit logs information: Admin-read records operations that read metadata or configuration information. For example, you looked at the configurations for your bucket. Data-read records operations that read user-provided data. For example, you listed files and then downloaded one from Cloud Storage. Data-write records operations that write user-provided data. For example, you created a new Cloud Storage file. You can exempt specific users or groups from having their data accesses recorded. This functionality is useful when you want to reduce the cost and noise associated with the volume of logs that are not of your interest. Data Access audit logs can be of high volume, so cost associated is directly proportional to the volume of data logs. You can also use the Google Cloud CLI or the API to enable Data Access audit logs. If you're using the gcloud CLI frequently, the easiest way is to get the current IAM policies, as seen in step 1, and write them to a file. Then you can edit the /tmp/policy.yaml file to add or edit the auditLogConfigs. You can also add the log details per service, like this example is enabling logging for Cloud Run. You can even enable logging on all services. Then, as seen in step 3, you would set that as the new IAM policy.

#### Audit logs entry format

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533972

Now that you learned to enable the logs you need, let’s examine the logging entries themselves. Every audit log entry in Cloud Logging is an object of type LogEntry. What distinguishes an audit log entry from other log entries is the protoPayload field, which contains an AuditLog object that stores the audit logging data. Note the log name, which tells us that we’re looking at an example from Data Access audit logs. Identify the principal generating log by looking at the principalEmail. The operation field only exists for a large or long-running audit log entries. Google has a standard List of official service names. You can use this list as a handy reference. On this slide, you can tell we’re looking at a query that was run in BigQuery. If you expanded the serviceData field, you could actually see the query itself. So, when someone at your organization runs that unexpected, $40,000 query, you can see who ran it and what the query was.

#### Best practices

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533973

Let’s go over a few best practices before finishing this module. Like anything in the cloud, start by planning first. Spend time and create a solid plan for Data Access audit logs. Think organization, folder, then project. Like most organizations, some of your projects will be very specialized, but usually, they do break down into common organizational types. Then, create a test project and experiment to see if the logging works the way you expect. Then roll out the plan, and don't forget automation (Infrastructure as Code). Remember that Data Access audit logs can be enabled as high as the organization level. The advantage would be detailed information on exactly who accessed, edited, and deleted what, and when. The disadvantage is that Data Access logs can grow to be large, and are billed per gigabyte. This also results in higher queries per second based on the number of data access requests. Infrastructure as Code (IaC) is essentially the process of automating the creation and modifications to your infrastructure using a platform. The platform supports configuration files, which can be put through a CI/CD (Continuous integration and continuous deployment) pipeline, like with code. Terraform is an open source package from HashiCorp or paid for enterprise version. It isn't hosted directly in Google Cloud, though it’s installed by default in Cloud Shell. State management is a decision point for your organization. It can be remote or local. Remote options include using Cloud Storage or Terraform Cloud. Local storage involves setting up something local to your organization, or using the pay-to-use HashiCorp Enterprise service. Audit logs also inform you about the resources provisioned using an IaC tool. It is really useful if you want to control log sink filters at the project level. With terraform you can set default include/exclude filters and have them applied to every project. To manage your Google Cloud organization's logs, you can aggregate them from across your organization into a single Cloud Logging bucket. It is recommended to create user-defined buckets to centralize or subdivide your log storage. Based on compliance and usage requirements, customize your logs storage by choosing where your logs are stored and defining the data retention period. Some organization might have latency, compliance, and availability requirements in specific regions. Configure a default storage location to automatically apply a region in which buckets are created for log data. By default, Cloud Logging encrypts customer content stored at rest. Your organization might have advanced encryption requirements that the default encryption at rest doesn't provide. To meet your organization's requirements, instead of Google managing the key encryption keys that protect your data, configure customer-managed encryption keys (CMEK) to control and manage your own encryption. We've discussed the options and benefits of exporting logs. Again, make this part of your plan. Start by deciding what, if anything, you will export from Aggregated Exports at the organization level. Next, decide what options you will use, project by project, folder by folder, and so on. Then, carefully consider your filters—both what they leave in, and what they leave out. Filters apply to all logs, not just to exports. Lastly, carefully consider what, if anything, you will fully exclude from logging. Remember that excluded entries will be gone forever. Side-channel leakage of data through logs is a common issue. You need to be careful about who gets which kind of access and to which logs. Remember some of the discussions earlier in this course on monitoring metrics scope? And how to monitor a current project? That's where your security starts. Are you monitoring project by project, or are you selectively grouping work projects into higher-level monitored projects? Use appropriate IAM controls on both Google Cloud-based and exported logs, and only allow minimal access required to get the job done. Especially scrutinize the Data Access audit log permissions, because they often contain Personally Identifiable Information (PII). Log buckets store logs, including audit logs. Log views control access to logs in a log bucket. Custom log views can be created to control access to logs from specific projects or users. This helps protect sensitive data and ensures only authorized users have access. Lastly, a few access scenarios, starting with operational monitoring. Let’s explore your high-level teams and assignments. By job, a CTO will have the organization admin role, so they can assign permissions to the security team and service accounts. The CTO can then give the security team logging.viewer so they can view the Admin Activity audit logs. Also, logging.privateLogViewer, so they can view the Data Access audit logs. The view permissions are assigned at the organization level, so they are global. Access control to data exported to Cloud Storage or BigQuery will be secured selectively with IAM. You might also want to explore Sensitive Data Protection to redact the PII. Data in the Data Access audit logs is deemed as personally identifiable information (PII) for this organization. Integrating the application with Sensitive Data Protection gives the ability to redact sensitive PII data when viewing Data Access logs whether they are in the Data Access audit logs or from the historical archive in Cloud Storage. Moving on to development teams. The security team is unchanged from the last slide. They already have logging.viewer, and logging.privateLogViewer from the global assignment. The development team might get logging.viewer at the folder level so they can see the Admin Activity audit logs for the projects within their development control. They probably also need logging.privateLogViewer at the dev folder so they can see the Data Access audit logs. Limit data they test with though, so they aren't viewing actual customer information. Again, use Cloud Storage or BigQuery IAM to control access to exported logs. Prebuilding dashboards might also be a good option. For external auditors, provide pre-created dashboards where possible. If they need broad access, you can make them with Logs Viewer role at the organization level. For BigQuery, they could be BigQuery Data Viewer on the exported dataset. For Cloud Storage, again, you could use IAM, but also remember the temporary access URLs that Cloud Storage supports.

#### Cloud Audit Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/labs/533974

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533975

In this module, you learned to: Explain Cloud Audit Logs. List and explain different audit logs. Explain the features and functionalities of the different audit logs. List the Best Practices to implement audit logs.

#### Quiz - Working with Audit Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/99/quizzes/533976

### Course Summary

#### Course 1 Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/99/video/533977

This brings us to the end of Logging and Monitoring in Google Cloud course. In this course we learned the purpose and capabilities of Google Cloud Observability, how to implement monitoring for multiple cloud projects, create alerting policies, uptime checks and alerts to identify and resolve problems quickly, use Cloud Logging to collect logs and export for further analysis. If you’re interested in learning more about application performance management, please go to the next course ”Observability in Google Cloud”. See you next time!

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/99/documents/533978

### Your Next Steps

## 09: Observability in Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/864

### Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515449

Welcome to the second part of the two part course, Observability in Google Cloud. The first course covered the operations-focused components including Logging, Monitoring, and Service Monitoring. This course is all about application performance management tools, including Error Reporting, Cloud Trace, and Cloud Profiler which tend to be more for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute products. In this course, we will shift our focus to Application Performance Management (APM) and delve into monitoring various compute options available within Google Cloud and also cover profiling, analyzing, and optimizing application code and infrastructure to identify performance bottlenecks. In this course you will learn to: Install and manage Ops Agent to collect logs for Compute Engine Use Google Cloud Managed Service for Prometheus Analyze VPC Flow logs and Firewall Rules logs Analyze resource utilization cost for monitoring related components within Google Cloud This course is designed to equip Cloud Architects, Administrators, SysOps personnel, Cloud Developers, and DevOps personnel with the essential skills and knowledge needed to excel in logging and monitoring your applications and workloads on Google Cloud. The prerequisites for this course are: Basic scripting or coding ability Google Cloud Fundamentals: Core Infrastructure or equivalent experience Proficiency with command-line tools and Linux operating system environments Without further adieu, let us get started!

### Configuring Google Cloud Services for Observability

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515450

In the next part of our Google Cloud Observability discussion, let’s take some time to examine the art of configuring Google Cloud services for observability. In this module, we're going to spend a little time learning how to use Ops Agent with Compute Engine. We will also explain the benefits of using Google Cloud Managed Service for Prometheus and usage of Prometheus Query Language (or PromQL) to query Cloud Monitoring metrics. We will then finally cover custom metrics.

#### Introduction to Ops Agent

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515451

The first section of this module focuses on how to collect metrics from applications deployed on a Google Cloud Compute Engine. Monitoring data can originate from a number of different sources. With Google Compute Engine instances, because the VMs are running on Google hardware, Cloud Monitoring can access some instance metrics without the agent, including CPU utilization, some disk traffic metrics, network traffic, and uptime information, but that information can be augmented by installing agents into the VM operating system. Many mission critical services use compute infrastructure directly and run on Google Compute Engine instances. How can we improve observability for those workloads? The Ops Agent is the primary agent for collecting telemetry data from your Compute Engine instances. Combining logging and metrics into a single agent, the Ops Agent uses Fluent Bit for logs, which supports high-throughput logging, and the Open Telemetry Collector for metrics. These agents are required for security reasons, the hypervisor cannot access some of the internal metrics inside a VM, for example, memory usage. You can configure the Ops agent to monitor many third-party applications such as Apache, mySQL, Oracle database, SAP HANA, and NGINX. The Ops Agent collects metrics inside the VM, not at the hypervisor level. For a detailed list, refer to the documentation. The Ops Agent supports most major operating systems such as CentOS, Ubuntu and Windows. We learned what Ops Agent is, let us next understand why we need Ops Agent. Here is an example of an infrastructure summary dashboard for a project with a few Compute Engine VMs in it. Notice that we're not getting any data about how our instances are using memory. That's because the VM hypervisor only knows how much memory is allocated to each VM, not how much of the allocated memory the VM is actually using. This is just one example of data that can only be gathered by a process running in the guest VM, such as an agent. There are other benefits of running the Ops Agent inside the VM: It monitors your VM instances without the need for any additional configuration after the installation. It helps monitor 3rd party applications and also supports both Windows and Linux guest OS. It exposes many additional process metrics beyond memory, and gives you better visibility to CPU, disk, and network performance. It exposes metrics beyond the 80+ metrics that Cloud Monitoring already supports for Compute Engine. The Ops Agent unifies gathering of metrics and logs into a single agent. And also ingests any user defined (Custom) metrics in Prometheus format. You can install Ops Agent by using three different methods: Use the Google Cloud CLI or the Google Cloud console to install the agent on individual VMs. Use an Agent Policy that installs and manages agents on your fleet of VMs. Use automation tools, like Ansible, Chef, Puppet, and Terraform, to install and manage agents on your fleet of VMs. We will cover the first two methods. For the automation process, refer to the documentation. Installing the Ops Agent is well documented on the Google site. In this slide we will cover the process using the agent policy. The first step is to install the beta component. Then enable the APIs and finally create a policy. Here, you see an example to create a policy named ops-agents-test-policy. The policy targets a single CentOS 7 VM instance named test-instance. It also installs both Logging and Monitoring agents on that VM instance. On this slide, we will cover the process for installing the Ops Agent on individual VMs. Go to the VM instances page in the Google Cloud console. Click the name of the VM that you want to install the agent on. The Details page opens. Click the Observability tab. The Observability page opens. Click Install. A new window open. Click Run in Cloud Shell. Cloud Shell opens and pastes the installation command. Press Enter on your keyboard to run the command. Click Authorize to allow Cloud Shell to install the agent. You can install a specific version of the agent and the steps also vary based on the operating system. For details refer to the documentation. Normally, you won't have to perform this step, but if the agent is not sending logs to Cloud Logging: First, check the metrics module in syslog If there are no logs, then agent service is not running. If you see a 403 permission errors when writing to the Monitoring API, enable the Logging API and Logs Writer role Make sure to check the Google documentation if you have questions. We learned how to install the Ops agent. Let us next look at how the data collected can be previewed as Dashboards. You can preview the dashboards and charts for telemetry data collected from the third-party applications such as Apache Web Server, MySQL, and Redis for deployments that run on Compute Engine and Google Kubernetes Engine. To view the logs, metrics and dashboards for data collected through Ops agent, navigate to Monitoring, and select the Integrations page. Integrations brings together metrics, logs, dashboards, and alerts to give you quick access to rich data for your application stack. Built on an open-source foundation, you can customize the data to fit your needs.

#### Non-VM resources

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515452

In addition to Google Cloud virtual machines, there are a lot of Google Cloud resources that support some type of monitoring. Let's look at a few of these. When monitoring any of the following non-virtual machine systems in Google Cloud, the Ops Agent is not required, and should not be installed: App Engine standard environment has monitoring built-in. App Engine flexible environment is built on top of GKE and has the Monitoring agent pre-installed and configured. With Standard Google Kubernetes Engine nodes (VMs), Cloud Monitoring and Cloud Logging is an option which is enabled by default. Cloud Run and Cloud Run functions provide integrated monitoring support. Google's App Engine standard and flexible environments both support monitoring. Make sure to check Google's documentation for the metric details. They also both support logging by writing to stdout or stderr. For refined logging capabilities, review the language-specific logging APIs, such as Winston for Node.js. Also, the logs are viewable under the GAE Application resources. Cloud Run functions offers lightweight, purpose-built functions, typically invoked in response to an event. For example, you might upload a PDF file to a Cloud Storage bucket, the new file triggers an event that invokes a Cloud Run functions instance, which translates the PDF from English to Spanish. Cloud Run functions monitoring is automatic and can provide you with access to invocations, execution times, memory usage, and active instances in the Google Cloud console. These metrics are also available in Cloud Monitoring, where you can set up custom alerting and dashboards for these metrics. Cloud Run functions also support simple logging by default. Logs written to stdout or stderr will appear automatically in the Google Cloud console. The logging API can also be used by extending log support. Cloud Run is Google’s managed container service. It can run in a fully managed version, in which it acts as a sort of App Engine for containers, and it can also run on GKE, in which case it’s a managed version of the open-source KNative. Cloud Run is automatically integrated with Cloud Monitoring with no setup or configuration required. This means that metrics of your Cloud Run services are captured automatically when they are running. You can view metrics either in Cloud Monitoring or on the Cloud Run page in the console. Cloud Monitoring provides more charting and filtering options. The resource type differs for fully managed Cloud Run and Cloud Run for Anthos: For fully managed Cloud Run, the monitoring resource name is "Cloud Run Revision" (cloud_run_revision). For Cloud Run for Anthos, the monitoring resource name is "Cloud Run on GKE Revision" (knative_revision). Cloud Run has two types of logs which are automatically sent to Cloud Logging, request logs and container logs. Request logs are logs of requests sent to Cloud Run services. And container logs are logs emitted from the container instances from your own code, written to stdout or stderr streams, or using the logging API.

#### Cloud Operations for GKE

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515453

We looked at monitoring for Compute Engine and other compute options. Now lets look at monitoring options explicitly available for GKE. Google Kubernetes Engine (GKE) includes integration with Cloud Logging and Cloud Monitoring and Google Cloud Managed Service for Prometheus. When you create a GKE cluster that runs on Google Cloud, Cloud Logging and Cloud Monitoring are enabled by default and provide observability specifically tailored for Kubernetes. You also enable Google Cloud Managed service for Prometheus to collect Prometheus metrics and monitor workloads running on GKE and non-GKE compute workloads. We will explore this in detail in the next section. You can configure Cloud Logging and Cloud Monitoring for GKE clusters either during creation or after creation. During cluster creation, navigate to Standard mode under Operations. For new cluster, Cloud Logging and Cloud Monitoring are enabled by default. You can change the components for which the metrics and logs are collected under the Components section. Enable managed collection by selecting Managed Service for Prometheus.

#### Google Cloud Managed Service for Prometheus

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515454

Next, let’s look at what Google Cloud Managed Service for Prometheus is all about and how it helps collect metrics. Google Cloud Managed Service for Prometheus is a fully managed service that makes it easy to collect, store, and analyze Prometheus metrics. Managed Service for Prometheus lets users collect metrics from both Kubernetes and VM environments at incredible scale without operational overhead by leveraging Monarch, Google's own globally available and scalable time series database. For those getting started with Prometheus for the first time, Managed Service for Prometheus helps make Google Kubernetes Engine even easier to use with maintained metric collectors. It's built on top of Monarch, the same globally-scalable data store as Cloud Monitoring. Monarch is an end-to-end monitoring system with high-level data modeling, data collection, querying, alerting and data management features. You can replace your existing Prometheus deployment to collect cluster and workload metrics and then query the data across multiple clusters by using PromQL. Managed Service for Prometheus splits responsibilities for data collection, query evaluation, rule and alert evaluation, and data storage into multiple components. Monarch handles the query evaluation and data storage. Monarch can execute queries and union results across all Google Cloud regions. It also supports two years of metric retention by default at no additional cost. When it comes to data collection, there are multiple choices available for data collection, which includes self-deployed, Ops Agent, OpenTelemetry, and managed collection. These collectors are responsible for scraping local exporters and forwarding that data to Monarch. Rule evaluation on the other hand is handled by locally run and configured rule evaluators. Refer to the documentation for latest information on the storage granularity and retention timeline. Another important Query Validator activity of Prometheus monitoring is making queries. Any UI that can call the Prometheus query API is also supported in the managed service for Prometheus. That includes Grafana and Cloud Monitoring. Your existing dashboards in Grafana continue to work just as before and you can keep using any PromQL found in popular open source repositories and forums. PromQL can be used to query: 1,500 free metrics in Cloud Monitoring Free Kubernetes metrics, custom metrics, and log-based metrics For collecting the data Prometheus will monitor, you can use the service in one of four ways: Managed data collection Self-deployed data collection Using Ops Agent OpenTelemetry collection Managed Service for Prometheus Managed Service for Prometheus offers an operator for managed data collection in Kubernetes environments. We recommend that you use managed collection; because it eliminates the complexity of deploying, scaling, sharding, configuring, and maintaining Prometheus servers. Managed collection is supported for both GKE and non-GKE Kubernetes environments. With self-deployed data collection, you manage your Prometheus installation as you always have. The only difference from upstream Prometheus is that you run the Managed Service for Prometheus drop-in replacement binary instead of the upstream Prometheus binary. You can configure the Ops Agent on any Compute Engine instance to scrape and send Prometheus metrics to the global data store. Using an agent simplifies VM discovery and eliminates the need to install, deploy, or configure Prometheus in VM environments. OpenTelemetry Collector uses a single collector to collect metrics from any environment and then sends them to any compatible backend. It is deployed either manually or by using Terraform in any compute or Kubernetes environment. When you choose between collection options, consider the following aspects: Managed collection is a recommended approach for all Kubernetes environments and is especially suitable for more hands-off fully managed experience. Self-deployed collection is suitable for quick integration into more complex environment. Using the Ops Agent is the easiest way and is recommended to collect and send Prometheus metric data originating from Compute Engine environments, including both Linux and Windows distros. The OpenTelemetry Collector is best to support cross-singal workflows such as exemplars. Managed Service for Prometheus provides a standalone rule evaluator for evaluating, recording, and alerting rules against all Monarch data accessible in a metrics scope. No need to co-locate the data in a single Prometheus server or on a single Google Cloud project. The rule evaluator uses the standard Prometheus rule files format, which makes migration to Managed Service for Prometheus easier. You can enable a managed collection for your resource by selecting the GKE cluster you need and clicking ENABLE SELECTED. After enabling managed collection, the in-cluster components are running, but no metrics are generated yet. You must deploy a PodMonitoring resource that scrapes a metrics endpoint to see any data in the Query UI. The manifest shown on slide defines a PodMonitoring resource, prom-example, in the namespace. The resource uses a Kubernetes label selector to find all the pods in the namespace that have the label app with the value prom-example. The matching pods are scraped on a port named metrics, every 30 seconds, on the /metrics HTTP path. To apply this resource, run the command on screen. To configure a horizontal collection that applies to a range of pods across all namespaces, use the ClusterPodMonitoring resource. The ClusterPodMonitoring resource provides the same interface as the PodMonitoring resource but does not limit discovered pods to a given namespace. When working with metric data, including data from Managed Service for Prometheus, in Cloud Monitoring, you can use the following query tools provided by Cloud Monitoring: PromQL Monitoring Query Language (MQL) Monitoring filters The simplest way to verify that your Prometheus data is being exported is to use the Cloud Monitoring Metrics Explorer page in the Google Cloud console: In the Google Cloud console, Go to Monitoring. In the Monitoring navigation pane, click Metrics Explorer. Select the PromQL tab. Enter the query. Here we are running a simple up query. And simply click Run Query. For MQL and filter options, refer to the documentation. Here is another example of using PromQL. This PromQL query show the average CPU utilization of the compute instances in your Google Cloud environment. The chart shows the visual representation of the utilization within a span of 1 hour.

#### Exposing user-defined metrics

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515455

In addition to the more than 1,000 metrics that Google automatically collects, you can use code to create your own. Application-specific metrics, also known as user or custom metrics, are metrics that you define and collect to capture information that the built-in Cloud Monitoring metrics cannot. You capture such metrics by using an API provided by a library to instrument your code, and then you send the metrics to Cloud Monitoring. Custom metrics can be used in the same way as built-in metrics. That is, you can create charts and alerts for your custom metric data. . There are two fundamental approaches to creating custom metrics for Cloud Monitoring: You can use the classic Cloud Monitoring API. Or you can use the OpenTelemetry protocol and Ops Agent. The OpenTelemetry Protocol (OTLP) receiver is a plugin installed on the Ops Agent that helps collect the user-defined metrics from the application and send those metrics to Cloud Monitoring for analysis and visualization. These metrics can then be used to create dashboards, uptime checks, and altering policies. The ingestion and authorization is not required as it is handled at the agent level. To configure OTLP, you must install an Ops Agent and modify the user configuration file to include the OTLP file. By default, the receiver uses the Prometheus API; the default value for the metrics_mode option is googlemanagedprometheus. To receive the custom metrics from the OTLP receiver, set the OTLP receiver metrics_mode to googlecloudmonitoring. The steps used to create a custom metric using the API are well documented. To begin, the data you collect for a custom metric must be associated with a descriptor for a custom metric type. In this example, we create a gauge double metric named my_metric. It's a gauge metric of type double, with the description "Custom metric example." Once you collect the information you need for creating your custom metric type, call the create method, passing into a MetricDescriptor object. You write data points by passing a list of TimeSeries objects to create_time_series. You write data points by passing a list of TimeSeries objects to the function create_time_series. Each time series is identified by the metric and resource fields of the TimeSeries object. These fields represent the metric type and the monitored resource from which the data was collected. In this example, we use the my_metric described on the last slide to link our metric to the specified Compute Engine instance. Next, we create the point by adding it to the series and adding the details. Each TimeSeries object must contain a single Point object. Finally, we report our metric. After you configure the metrics_mode to Prometheus API or the Cloud Monitoring API in the OTLP receiver, you can then query the metrics by using Metrics Explorer, dashboards or even alternate interface. Here we see an example from the Metrics explorer, where you query the user defined metrics ingested by the monitoring API.

#### Monitoring a Compute Engine by using Ops Agent

- https://www.cloudskillsboost.google/paths/11/course_templates/864/labs/515456

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515457

In this module, you learned how to: Use Ops Agent with Compute Engine. Explain the benefits of using Google Cloud Managed Service for Prometheus. Explain the usage of PromQL to query Cloud Monitoring metrics. Explain custom metrics.

#### Configuring Google Cloud Services for Observability

- https://www.cloudskillsboost.google/paths/11/course_templates/864/quizzes/515458

### Monitoring Google Cloud Network

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515459

In this module, let’s spend some time analyzing Google’s Virtual Private Cloud. Specifically, you learn to: Collect and analyze VPC Flow Logs, Firewall Rules Logging, load balancer logs, and Cloud NAT logs so you can see what's happening to the traffic across your network. Enable Packet Mirroring so you can replicate packets at the virtual machine network interface, and forward it for further analysis. We will also cover the capabilities of the Network Intelligence Center.

#### VPC Flow Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515460

Let’s start with monitoring the network. VPC Flow Logs records a sample (about one out of ten packets) of network flows sent from and received by VM instances, including Google Kubernetes Engine nodes. These logs can be used for network monitoring, traffic analysis, forensics, real-time security analysis, and expense optimization. VPC Flow Logs is part of Andromeda, the software that powers VPC networks. VPC Flow Logs introduces no delay or performance penalty when enabled. This slide shows an example of a VM to external traffic flow pattern. VPC Flow Logs provides visibility into traffic, which helps monitor the flow between zones and IP addresses. In this example, the traffic flows between a VM and an external network connected either through a Cloud VPN or Cloud Interconnect. Traffic flows are reported from the VM only and include: Igness traffic: The logs reported with VM as its destination. In this example, the ingress traffic is reported from the source VM 10.30.0.2 to 10.10.0.2 Egress traffic: The logs reported with VM as its source. In this example, the egress traffic is reported from the source VM 10.10.0.2 to 10.30.0.2 These are some of the main properties you must remember when working with VPC Flow Logs: VPC Flow Log samples are from a VM’s perspective. For this reason, if an egress firewall is denied, those packets are sampled by VPC Flow Logs. Similarly, the ingress blocked packets are not logged because they are sampled after the ingress firewall rules. VPC Flow Logs samples TCP, UDP, ICMP, ESP and GRE flows from each VM. It records inbound and outbound flows for each VM, thus capturing traffic between VM’s, VM to on-premises, VM to another host on the internet. VMs support multiple network interface and can be enabled at the subnet level. You can activate or deactivate VPC Flow Logs per VPC subnet. When enabled for a subnet, VPC Flow Logs collects data from all VM instances in that subnet. To enable VPC Flow Logs, during subnet creation, select On next to Flow Logs. You can optionally adjust log sampling and aggregation to adjust the metadata and sample rate that is written to logs. Each log entry contains a record of different fields. For example, this table illustrates the IP connection information that is recorded. Information consists of the source IP address and port, the destination IP address and port, and the protocol number. This set is commonly referred to as 5-tuple. Other fields include the start and end time of the first and last observed packet, the bytes and packets sent, instance details including network tags, VPC details, and geographic details. For more information on all data recorded by VPC Flow Logs, see the documentation. Logs Explorer can be used to access the VPC Flow Logs. The entries will be vpc_flows below the Compute Engine section. Searching the log names for vpc_flows works well. Log Analytics powered by BigQuery provides new capabilities to analyze flow log data and generate useful insights. With Log Analytics: You can analyze ad-hoc query-time without complex pre-processing as before. You can use BigQuery to query data and upgrade buckets to use Log Analytics and then create a linked dataset. Refer to the documentation for curated sample queries to get started with Flow Log Analysis.

#### Firewall Rules Logging

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515461

Another essential part of knowing what's happening at the VPC network level is knowing what the firewall rules are doing. VPC firewall rules let you allow or deny connections to or from your virtual machine (VM) instances based on a configuration that you specify. Enabled VPC firewall rules are always enforced, and protect your instances regardless of their configuration and operating system, even if they didn’t start. Firewall Rules Logging lets you audit, verify, and analyze the effects of your firewall rules. It can help answer questions like: Did my firewall rules cause that application outage? How many connections match the rule I just created? Are my firewall rules stopping (or allowing) the correct traffic? See the Firewall Rule Logging documentation for details. By default, Firewall Rules Logging is disabled. You can enable it on a per-rule basis. In the slide screenshot, you’re editing the firewall rule named enable-rdp. Selecting the radio button will enable firewall rules. Note: Firewall Rules Logging can only record TCP and UDP connections. For other protocols, use Packet Mirroring. Caution: Firewall Rules Logging can generate a lot of data, which might have a cost implication. Firewall Rules Logging can also be activated on existing firewall rules by using the CLI. See these two examples on this slide. In both, the [NAME] tag will be the name of your firewall rule. Like all Google Cloud logs, use Logs Explorer to view logs in real time or to configure exports. To filter for firewall logs and network policy firewall logs, below the Compute Engine resource, select firewall. Many are familiar with classic segmentation or gateway-centric firewalls. In this example, you can see a private network, possibly at your office or home. At the network boundary, where the private network meets the outside internet, sits a firewall. A segmentation firewall is designed to segment and secure a protected network from an outside insecure network. Google Cloud VPC firewalls are micro-segmentation firewalls. These firewalls function more like a bunch of micro-firewalls, each operating over the Network Interface Controller (NIC) of every VM connected to the VPC. The micro-firewalls can then grant or deny any configured incoming or outgoing traffic. Now, imagine we have an issue. We have two different web servers. After some configuration changes by a particular DevOps team, the web servers can no longer access the application server they both share. How can we tell if the issue is firewall-related? Let's see. If the connectivity issue is related to a firewall, then there are two major possibilities: A firewall rule is actively blocking the incoming connections from the web servers. Or Network traffic is blocked by default in most networks. A firewall rule might not be allowing the traffic from the web servers as it should. Logging all denied connections could generate significant data that would take time and effort to monitor. So, instead of starting with option one, start with option two. Create a temporary high-priority rule designed to allow the web server traffic through to the app server. Enable Cloud logging on it so you can examine the entries. Suddenly the traffic is getting through, so you know it's firewall related. Now examine the log entries. Also, find the existing rule supposed to be allowing the traffic and see what you can find. Hey, look at that! The rule that's supposed to allow the traffic is based on a network tag named webserver. The web server machines are actually using the network tag web-server. There it is, that's your problem.

#### Load balancer logs

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515462

Several of Google Cloud load balancers support monitoring or logging. While all the Google Cloud load balancers support Cloud Logging and Cloud Monitoring, the log type and log fields supported vary based on the type of the load balancers. These include: Internal and external Application Load Balancers, Internal and external Network Load Balancers, and internal and external Proxy Load Balancers. Cloud Logging for load balancing logs all the load balancing requests sent to your load balancer. These logs can be used for debugging and analyzing your user traffic. You can view request logs and export them to Cloud Storage, BigQuery, or Pub/Sub for analysis. For example, in network load balancer, per-connection logging gives you insight into how each connection is routed to serving backends. For external Application Load Balancers with backend buckets, logging is automatically enabled and cannot be disabled. You can activate logging on a per backend service basis. A single internal Application Load Balancer URL map can reference more than one backend service. You might need to enable logging for more than one backend service, depending on your configuration. It will be enabled by default for all new load balancers backends. But backends created before the Globally Available (GA) release of load balancer logging might require manual configuration. Application Load Balancing log entries contain information useful for monitoring and debugging your HTTP(S) traffic. Make sure to check the documentation for further details. Log entries contain the following types of information: LogEntry format includes general information shown in most logs, such as severity, project ID, project number, timestamp, and so on. However, HttpRequest.protocol is not populated for Application Load Balancing logs. This can include a method, a URL, remote IP address, a protocol, a latency string, or a user agent. resource contains the monitored resource type associated with the log entry. jsonPayload contains the statusDetails field. This field holds a string that explains why the load balancer returned the HTTP status that it did. Redirects (such as HTTP response status code 302 Found) issued from the load balancer are not logged. Redirects issued from the backend instances are logged. Let’s take an example of how to use log record information to troubleshoot a load balancing issue. Consider a scenario where the load balancer generates an HTTP error resource code 5XX and sends the same error code to the client. Refer to the load balancer logs to determine the source of an error: Within the statusDetails field: the response_sent_by_backend indicates it is a backend issue. Whereas, failed_to_pick_backend indicates that the load balancer failed to pick a healthy backend to handle a request.

#### Cloud NAT Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515463

Another piece of the network telemetry features in Google Cloud is Cloud NAT logs. Cloud NAT is the Google-managed Network Address Translation service. It lets you provision your application instances without public IP addresses, and it also lets them access the internet in a controlled and efficient manner. With Cloud NAT, your private instances can access the internet for updates, patching, configuration management, and more. There are many Cloud NAT benefits. VMs without external IP addresses can access destinations on the internet. For example, you might have VMs that only need internet access to download updates or complete provisioning. Cloud NAT lets you configure these VMs with an internal IP address. Thus, your organization needs fewer external IP addresses. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses. Cloud NAT supports VMs that belong to managed instance groups, including those with autoscaling enabled. Cloud NAT is not dependent on a single, physical gateway device. Cloud NAT is a distributed, software-defined managed service. You configure a NAT gateway on a Cloud Router, which provides the control plane for Cloud NAT. Cloud Router contains the NAT configuration parameters. Cloud NAT logging lets you log NAT TCP and UDP connections and errors. When Cloud NAT logging is enabled, a log entry can be generated when a network connection that uses Cloud NAT is created, and/or when an egress packet is dropped because no port was available for Cloud NAT. You can opt to log both kinds of events, or just one or the other. Logs contain TCP and UDP traffic only, and the log rate threshold will reach a maximum of 50-100 log events per vCPU before log filtering. Cloud NAT logging might be enabled when a new Cloud NAT gateway is first created, or by editing the settings of an existing gateway. To view the collected logs in Logs Explorer, filter to the Cloud NAT Gateway resource and optionally, restrict to a particular region or Gateway.

#### Packet Mirroring

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515464

Another way to monitor the network traffic flowing in and out of your Compute Engine virtual machines is to use Packet Mirroring. Packet Mirroring clones the traffic of specific instances in your Virtual Private Cloud (VPC) network and forwards it for examination. Packet Mirroring captures all ingress and egress traffic and packet data, such as payloads and headers. The mirroring happens on the virtual machine (VM) instances, not on the network. Therefore, Packet Mirroring consumes additional bandwidth on the hosts. Packet Mirroring is useful when you need to monitor and analyze your security status. It exports all traffic, not only the traffic between sampling periods. For example, you can use security software that analyzes mirrored traffic to detect all threats or anomalies. Also, you can inspect the full traffic flow to detect application performance issues and to provide network forensics for Payment Card Industry Data Security Standards (PCI DSS) compliance and other regulatory use cases. We will elaborate on this further in the next few slides. Obviously, Packet Mirroring can generate significant data, so collector destination is generally an instance group behind a TCP/UDP load balancer or equivalent technology. One of the major limitations of Packet Mirroring is bandwidth consumption. Packet Mirroring consumes the egress bandwidth of the mirrored instances. However, there is a work around. Use filters to reduce the traffic collected for mirrored instances. This filter can be used for IP address ranges, protocols, traffic directions and lot more. The current maximum number of filters that can be used for Packet mirroring is 30. For more information, refer to the documentation. Two main use cases where Packet Mirroring is useful in security and monitoring. Let’s explore each of these use cases in detail. Network and application monitoring: Network engineers can use the data from Packet Mirroring to: Maintain integrity of deployment. Troubleshoot packet loss issues by analyzing protocols. Troubleshoot reconnection and latency issues by analyzing real time traffic patterns. Security and compliance: Implement zero-trust by monitoring network traffic across and within the trust boundaries without any network re-architecture. Packet Mirroring helps capture multiple packets for a single flow. This information can be quite useful for the implementation and usage of the following security tools: Intrusion detection systems match signatures with multiple packets of a single flow. Deep Packet Inspection engines inspect payloads for anomalies. Network forensics for PCI compliance: Packet mirroring help capture, process and preserve forensic of different attack vectors. Packet Mirroring exports monitoring data about mirrored traffic to Cloud Monitoring. You can use monitoring metrics to check whether traffic from a VM instance is being mirrored as intended. For example, you can view the mirrored packet or byte count for a specific instance. You can also view the monitoring metrics of mirrored VM instances or instances that are part of the collector destination (internal load balancer). For mirrored VM instances, Packet Mirroring provides metrics specific to mirrored packets such as mirrored packets count, mirrored bytes count and dropped packets count. Monitoring can also spot where packet mirroring is being used unnecessarily or unexpectedly. Remember that, as noted earlier, mirroring generates significant data that requires storage and processing. Also, note that it slows the network throughput of the virtual machines being monitored and might accidentally expose sensitive data.

#### Network Intelligence Center

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515465

This section is a bit of a detour, but let’s at least mention the Network Intelligence Center and how it helps with network analysis. Network Intelligence Center gives you centralized monitoring and visibility into your network. It reduces troubleshooting time and effort and increases network security, all while improving the overall user experience. Currently, it offers five modules: Network Topology, Connectivity Tests, Performance Dashboard, Firewall Insights, and Network Analyzer. Network Topology visualizes your Google Cloud network as a graph. You can use the graph to explore your existing configurations and quickly troubleshoot networking issues. You can select network entities, filter, see lines of communication with bandwidth information, expand and collapse hierarchies, and select time boundaries. The Connectivity Tests tool in Network Intelligence Center helps you to quickly diagnose connectivity issues and prevent outages. These tests let you self-diagnose connectivity issues within Google Cloud or from Google Cloud to an external IP address (the connectivity issue could be on-premises or in another cloud). The results help to isolate whether the issue is in Google Cloud. Run tests to help verify the effect of configuration changes and ensure that network intent captured by these tests is not violated, proactively preventing network outages. These tests also help assure network security and compliance. Performance Dashboard gives you visibility into the performance of your VPC. The Packet Loss tab shows the results of active probing between your VMs in a given VPC. To get this data, it runs workers on the physical hosts that house your VMs. These workers insert and receive probe packets that run on the same network as your traffic, revealing issues on that network. Workers run on the physical host and not on your VM. Therefore, these workers do not consume VM resources and the traffic is not visible on your VMs. Packet loss is aggregated for all zone pairs. The Latency tab aggregates latency information based on a sample of your actual Transmission Control Protocol (TCP) VM traffic. The method used is similar to the one used for VPC Flow Logs. The latency is calculated as the time that elapses between sending a TCP sequence number (SEQ) and receiving a corresponding Acknowledgement (ACK) that contains the network Round Trip Time (RTT) and TCP stack related delay. The latency metric is only available if TCP traffic is around 1,000 packets per minute or higher. Firewall Insights, a component product of Network Intelligence Center, produces metrics and insights that let you make better decisions about your firewall rules. It provides data about how your firewall rules are being used, exposes misconfigurations, and identifies rules that could be made more strict. Firewall Insights uses Cloud Monitoring metrics and Recommender insights. Cloud Monitoring collects measurements to help you understand how your applications and system services are performing. A collection of these measurements is generically called a metric. The applications and system services being monitored are called monitored resources. Measurements might include the latency of requests to a service, the amount of disk space available on a machine, the number of tables in your SQL database, the number of widgets sold, and so forth. Resources might include virtual machines, database instances, disks, and so forth. Recommender is a service that provides recommendations and insights for using resources on Google Cloud. These recommendations and insights are per-product or per-service, and are generated based on heuristic methods, machine learning, and current resource usage. You can use insights independently from recommendations. Each insight has a specific insight type. Insight types are specific to a single Google Cloud product and resource type. A single product can have multiple insight types, where each provides a different type of insight for a different resource. Firewall Insights metrics let you analyze the way that your firewall rules are being used. Firewall Insights metrics are available through Cloud Monitoring and the Google Cloud console. Metrics are derived through Firewall Rules Logging. With Firewall Insights metrics, you can perform the following tasks: Verify that firewall rules are being used in the intended way. Over specified time periods, verify that firewall rules allow or block their intended connections. Perform live debugging of connections that are inadvertently dropped because of firewall rules. Discover malicious attempts to access your network, in part by getting alerts about significant changes in the hit counts of firewall rules. Network Analyzer automatically monitors your VPC network configurations and detects misconfigurations and suboptimal configurations. It provides insights on Network Topology, firewall rules, routes, configuration dependencies, and connectivity to services and applications. It identifies network failures, provides root cause information, and suggests possible resolutions. Network Analyzer runs continuously and triggers relevant analyses based on near real-time configuration updates in your network. If a network failure is detected, it tries to correlate the failure with recent configuration changes to identify root causes. Wherever possible, it provides recommendations to suggest details on how to fix the issues. Network Analyzer provides insights that help identify common issues such as connectivity blockage, load balancing errors, external IP address that are not used but allocated, invalid next hop, GKE network misconfiguration and lot more. It also identifies the root cause of the insights and also provides recommended fixes. In the example above, an insight of the type Error, a GKE node to control plane connectivity is generated. The insight page also describes the following: The root cause: an ingress firewall rule is blocking the connection between the node and the plane. This indicated that the default drywall rules were modified, removed, or shadowed by another firewall rule. A solution: if the root of the problem is a deleted firewall, create a new firewall rule. If it's a shadowed firewall rule, then increase the priority.

#### Analyzing Network Traffic with VPC Flow Logs

- https://www.cloudskillsboost.google/paths/11/course_templates/864/labs/515466

#### Module summary

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515467

After completing this module, you know how to: Collect and analyze VPC Flow Logs, Firewall Rules Logging, load balancer logs, and Cloud NAT logs so you can see what's happening to the traffic across your network. Enable Packet Mirroring so you can replicate packets at the virtual machine network interface and forward it for further analysis. And explain the capabilities of the Network Intelligence Center.

#### Monitoring Google Cloud Network

- https://www.cloudskillsboost.google/paths/11/course_templates/864/quizzes/515468

### Investigating Application Performance Issues

#### Module Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515469

When deploying applications to Google Cloud, the Application Performance Management products (Cloud Trace and Cloud Profiler) provide insight into how your code and services are functioning. The tools also help troubleshoot when needed. In this module, you will learn to: Explain the features and benefits of Error Reporting, Cloud Trace, and Cloud Profiler. List and explain the functionalities of the Error Reporting, Cloud Trace, and Cloud Profiler components.

#### Error Reporting

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515470

Application Performance Management (APM) combines the monitoring and troubleshooting capabilities of Cloud Logging and Cloud Monitoring with Error Reporting, Cloud Trace, and Cloud Profiler. APM helps you reduce latency and cost so that you can run more efficient applications. Let's start with Error Reporting. Error Reporting looks through all the logs that your application and infrastructure has reported. It then counts, analyzes, and aggregates the exceptions to report them on your preferred notification channel such as email, mobile app, slack, or through web hooks. Error Reporting can only analyze log entries that are stored in Cloud Logging buckets that are in the global region. The source and destination Google Cloud projects must be the same, and customer-managed encryption keys (CMEK) must be disabled. If you route logs to a different Cloud project, regionalized buckets, or enable CMEK, then Error Reporting doesn't capture and analyze those logs. Error Reporting has several features. It helps understand errors. See at a glance the top or new errors for your application in a clear dashboard. Looking at a log stream to find important errors can slow you down when you’re troubleshooting. Error Reporting brings you the processed data directly to help you understand and fix the root causes faster. Real production problems can be hidden in mountains of data. Error Reporting helps you see the problems through the noise by constantly analyzing your exceptions. Problems are intelligently aggregated into meaningful groups tailored to your programming language and framework. It provides instant error notification. You do not wait for your users to report problems. Error Reporting is always watching your service and instantly alerts you when a new application error cannot be grouped with existing ones. Directly navigate from a notification to the details of the new error. Error Reporting is available on desktop and in the Google Cloud app for iOS and Android. It also provides popular language and product support. Support is available for many popular languages, including Go, Java, Node.js, PHP, Python, Ruby, or . NET. Use our client libraries, REST API, or send errors with Cloud Logging. Error Reporting can aggregate and display errors for: App Engine standard environment and flexible environment, Cloud Run functions, Apps Script, Cloud Run, Compute Engine, Amazon EC2, and Google Kubernetes Engine (or GKE). Setting up Error Reporting is simple and dependent on the language and compute environment. Here’s an error-catching example written in Node.js and run on Cloud Run. To report errors, the code needs the Error Reporting Writer Identity and Access Management role. Enable the Error Reporting API, and install the client library by using npm. The easiest way to manually log errors to Error Reporting in Node.js is to import the Error Reporting library. You then instantiate a client to start reporting errors to Error Reporting. Optionally, you can also customize the behavior of the Error Reporting library for Node.js. These can be configured by passing objects to options. Use error message builder to customize all fields. Call the report method to manually report an error. You can also integrate the Error Reporting library for Node.js to web frameworks like Express.js. Refer to the documentation for more details. Error Reporting Library for Node.js can be configured on many Google Cloud environments. Let’s explore the process for all. For App Engine flexible environment and standard environment, Cloud Run, Cloud Run functions, and Apps Script, Error Reporting is automatically enabled. For Google Kubernetes Engine, add cloud-platform access scope during cluster creation. For Compute Engine, ensure the service account used has the Error Reporting Writer role. Outside Google Cloud, provide the Google Cloud project ID and service account credentials to the Error Reporting library for Node.js. To see your errors, open the Error Reporting page in the Google Cloud console. By default, Error Reporting shows you a list of recently occurring open and acknowledged errors in order of frequency. Errors are grouped and de-duplicated by analyzing their stack traces. When Auto reload is turned on, Error Reporting automatically reloads the error list every 10 seconds. Error Reporting recognizes the common frameworks used for your language and groups errors accordingly. You can sort errors based on occurrences (first and last seen). You can also link an issue tracker link to an error group by clicking the + icon below Insert link. Selecting an error entry will let you expand into the Error Details page. On this page, you can examine information about the error group, including the history of a specific error, specific error instances, and diagnostic information. To view the log entry associated with a sample error, click View logs from any entry in the Recent samples panel. You’re taken to Logs Explorer in the Cloud Logging console.

#### Trace

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515471

Now, let’s have a look at Cloud Trace. Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud console. You can track how requests propagate through your application and receive detailed near-real time performance insights. Trace automatically analyzes all of your application traces to generate in-depth latency reports to surface performance degradations. It helps with Issue detection. Trace continuously gathers and analyzes trace data from your project to automatically identify recent changes to the performance of your application. These latency distributions, available through the Analysis Reports feature, can be compared over time or versions. If Trace detects a significant shift in the latency profile of your app, you’re automatically alerted. It also helps with identification of performance bottlenecks. Cloud Trace helps inspect detailed latency information for a single request or view aggregate latency for your entire application. Using the various tools and filters provided, you can quickly find where bottlenecks are occurring and more quickly identify their root cause. Trace is based off the tools used at Google to keep our services running at extreme scale. Trace offers broad platform support. The language-specific SDKs of Trace can analyze projects that run on VMs (even VMs not managed by Google Cloud). The Trace SDK is available for Java, Node.js, Ruby, and Go. The Trace API can be used to submit and retrieve trace data from any source. A Trace consists of a tracing client, which collects spans and sends them to Cloud Trace. You can then use the Google Cloud console to view and analyze the data collected by the agent. A trace describes the time that it takes an application to complete a single operation. A trace is a collection of spans. A span describes how long it takes to perform a complete suboperation. For example, a trace might describe how long it takes to process an incoming request from a user and return a response. A span might describe how long a particular RPC call requires. If an OpenCensus library is available for your programming language, you can simplify the process of creating and sending trace data by using OpenCensus. In addition to being simpler to use, OpenCensus implements batching that might improve performance. If an OpenCensus library doesn't exist, instrument your code by importing the Trace SDK library and using the Cloud Trace API. The Cloud Trace API collects trace data and sends it to your Google Cloud project. There are two ways to send trace data to Cloud Trace: The first one is automatic tracing. Some configurations support automatic tracing. These include: App Engine standard environment with Java 8, Python 2, and PHP 5 applications. HTTP requests and latency data from Cloud Run functions and Cloud Run. And the second one is by instrumenting the application. You can do this by using Google client libraries or OpenTelemetry, which is the recommended option. Trace will need to offload tracing metrics to Google Cloud. As far as the required IAM permissions are concerned, for external systems, or Compute Engine and GKE environments that don't run under the default service account, ensure that they run under a service account with at least the Cloud Trace Agent role. App Engine, Cloud Run, Cloud Run functions, Google Kubernetes Engine, and Compute Engine have default access. Compute Engine and GKE get that access through the default Compute Engine service account. Analysis reports in Trace show you an overall view of the latency for all requests, or a subset of requests, to your application. Analysis reports will be similar to the daily report viewed on the Trace Overview main page. Custom reports can be created for particular request URIs and other search qualifiers. The report can show results as both a density distribution, as in the screenshot, or as a cumulative distribution.

#### View application latency with Cloud Trace

- https://www.cloudskillsboost.google/paths/11/course_templates/864/labs/515472

#### Profiler

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515473

Finally, let's look at Cloud Profiler. Understanding the performance of production systems is notoriously difficult. Attempting to measure performance in test environments usually fails to replicate the pressures on a production system. Continuous profiling of production systems is an effective way to discover where resources like CPU cycles and memory are consumed when the service operates in its working environment. Cloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the source code that generated it, which helps you identify the parts of your application that are consuming the most resources. The insights provided illuminate the performance of your application characteristics. Cloud Profiler has many features. It provides low-impact production profiling Although it's possible to measure code performance in development environments, the results don‚Äôt often show what happens in production. Many production profiling techniques slow down code execution or can only inspect a small subset of a codebase. A comprehensive application performance view is provided without slowing it down. It offers practical application profile creation Poorly performing code increases latency and costs for web applications and services every day, without anyone knowing or doing anything about it. Cloud Profiler changes this situation by continually analyzing the performance of CPU or memory-intensive functions that run in an application. Cloud Profiler presents the call hierarchy and resource consumption of the corresponding function in an interactive flame graph. This graph helps developers figure out which paths are consuming the most resources and what are the different ways your code is called. It also offers broad language and product support Cloud Profiler enables developers to analyze applications that run anywhere, including Google Cloud and other on-premises or cloud platforms that support Java, Go, Node.js, and Python. You can find a full explanation of language compatibility in the documentation. The profiling types available vary by language. Check the Cloud Profiler documentation for the most recent options. For the CPU metrics, you will find the following: CPU time is the time that the CPU spends executing a block of code. The time it was waiting or processing instructions for something else is not included. Wall time is the time that it takes to run a block of code, including all wait time, including that for locks and thread synchronization. The wall time for a block of code can never be less than the CPU time. For heap, you have the following: Heap is the amount of memory allocated in the heap of the program when the profile is collected. Allocated heap is the total amount of memory that was allocated in the heap of the program. Allocated heap includes memory that has been freed and is no longer in use. And for threads you have: Contention, which provides information about threads stuck waiting for other threads. Threads, which contain thread counts. Profiler instruments applications that run in most Google and non-Google compute technologies. Note that Windows guest OS is not supported. Like with other Google application performance management products, the exact setup steps vary by language, so check the documentation to find more information. Here, we are sticking with our Python application, which will run on App Engine. Before you start, ensure that the Profiler API is enabled in your project. Start by importing the googlecloudprofiler package. Install the C/C++ compiler and development tools, pip, and Profiler package. Then, early as possible in your code, start the profiler. In this example, we are setting the logging level (verbose) to 3, or debug level. That setting will log all messages. The default would be 0 or error only. At the top of the Profiler interface, you can select the profile that you want to analyze. You can select by options including Timespan, Service, Profile type, Zone, and Version. The Weight will limit the subsequent flame graph to particular peak consumptions. Top 5%, for example. Compare to allows the comparison of two profiles. Profiler then randomly selects a maximum of 250 profiles from this set, and uses those to construct the flame graph. As mentioned earlier, cloud Profiler displays profiling data by using Flame Graphs. Unlike trees and standard graphs, flame graphs use screen space efficiently by representing a large amount of information in a compact and readable format. Look at this example. We have a basic application with a main method, which calls foo1, which in turn calls bar. Then main calls foo2, which also calls bar. As you move through the graphic left to right, you can see how the information is collapsed. First, by removing arrows, then by creating frames, and finally by removing spaces and left-aligning. In the bottom view, you see the result as it appears in the Profiler. If you look at CPU time, then you can see that the main method takes a total of 9 seconds. Beneath the main bar, you can see how that CPU time was spent: some in main itself, but most in the calls to foo1 and foo2. And most of the foo time was spent in bar. So, if we could make bar faster, we could really save some time in main. Here we have a full example. When you hold the pointer over a frame, a tooltip opens and displays additional information, which includes: The function name The source file location And some metric consumption information If you click a frame, the graph is redrawn, which makes the call stack of the selected method more visible.

#### Module summary

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515474

In this module, you learned to: Explain the features and benefits of Error Reporting, Cloud Trace, and Cloud Profiler. List and explain the functionalities of the Error Reporting, Cloud Trace, and Cloud Profiler components.

#### Quiz - Investigating Application Performance Issues

- https://www.cloudskillsboost.google/paths/11/course_templates/864/quizzes/515475

### Optimizing the Costs for Google Cloud Observability

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515476

In our final module, we discuss how to optimize the costs for Google Cloud Observability. Specifically, you learn to analyze resource utilization costs for operations-related components within Google Cloud and implement best practices for controlling the cost of operations within Google Cloud.

#### Costs and pricing

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515477

Let’s start with costs and pricing. Because Google Cloud Observability services are managed services, their cost is usage-based and not infrastructure-based. Although Cloud Profiler is offered at no cost, Cloud Logging, Cloud Monitoring, and Cloud Trace has associated costs. Error Reporting supports errors sent by using Cloud Logging and incurs costs associated with this service. Logging pricing is based on the volume of chargeable logs ingested. Logging incurs cost when using: Cloud Load Balancing logs, Custom logs, Error reporting costs (if your errors are ingested by Cloud Logging), The write operation in the Cloud Logging API, Logs stored beyond 30 days will incur a retention charge for non-required buckets. Refer to the documentation for updated pricing details. Cloud Monitoring prices are based on the: Volume of chargeable metrics ingested. Number of chargeable API calls. Execution of Cloud Monitoring uptime checks. Metrics ingested by using Google Cloud Managed Service for Prometheus. Example product usage that generates cost through metric volume and API calls includes using Cloud Monitoring custom metrics. AWS Metrics. The read operation in the Monitoring API, except from Google Cloud console. Trace prices are based on the number of spans ingested and eventually scanned. The free allotment was 2.5 million spans. Example product usage that generates cost through spans ingested includes adding instrumentation for your: Spans for App Engine apps outside of the default spans, Cloud Load Balancing, Custom apps. Many functions of Google Cloud Observability are free, including Using Cloud Profiler. Collecting and using the Cloud Audit Logs, Access Transparency logs, BigQuery Data Access logs and anything excluded from logs. Creating and using dashboards. Visualizing Google Cloud and Anthos metrics and log streams. App Engine standard trace spans. Uptime checks. Logs analytics when queries are running in Cloud Logging. After the free tier finishes, you will be charged for the various operation-related fees. For the latest pricing information, always check the Google Documentation. The networking logs, including VPC Flow logs, Firewall Rules Logging, and Cloud NAT, will cost you the standard log storage fees. However, if you store them in Cloud Logging, they won't cost you anything extra to generate. If you export the network telemetry logs to an external service, cost is incurred to generate logs. The cost is in addition to any destination or networking fees. Network Intelligence Center incurs costs for metrics overlaid on the network topology, Network Analyzer, and performance dashboard. Network Intelligence Center also incurs a cost for running connectivity tests and Firewall insights. Refer to the documentation for more information on pricing models for Firewall insights.

#### Bill Estimation

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515478

We covered some of the pure costs. Now let’s talk about bill estimation. When you try to estimate prices in Google Cloud, the page of choice should always be the Google Cloud Pricing Calculator. The Pricing Calculator is accurate, but it's only as accurate as the data that you provide it. If your operation services are already running on Google Cloud, start by pulling the prices of what you're spending on them. You can look for the requisite data in several places, let us explore these as we move forward in this section. You can also use the Cost Estimation API. It provides customer-specific estimates that include your discounts. For example, those negotiated as part of a contract and those based on committed usage. These cost estimates can help you make more informed business decisions. For more information, refer to the documentation linked in the reference material section. Start by going to your billing account Reports page. Set your date range, and then filter by Stock Keeping Unit (SKU). The SKUs you want are: Log volume Spans ingested Metric volume Monitoring API Requests Here's another view of the same page. In the chart, you can change the view to display the data. When you select Daily cumulative, the cost trend line effectively shows where you’re headed based on current spend trends. If you recently added logging, adding Daily cumulative might be a good way to estimate what your bill might do. Then, check your usages and costs. Note, if SKU usage is 0 for a metric, then it won’t appear in the list. You can do something similar to see how money was spent in the past month. In this case, the biggest item is log volume. This insight might raise the question: if money is spent on logging data, where exactly is that data coming from? To find the answer, check Metrics Explorer. Open Monitoring and then Metrics Explorer, and set the Metric to Global. Then, depending on what you want, select one of the following: Log bytes ingested provides Log bytes ingested in Cloud Logging. Monthly log bytes ingested provides a graph where each point represents the month-to-date sum of log bytes ingested in Cloud Logging. The monthly total is available on the last day of the month, when it also resets. Metric bytes ingested provides chargeable number of bytes of metric data ingested in Cloud Monitoring. Trace spans ingested: provides chargeable trace spans ingested in Cloud Trace. Monthly trace spans ingested: provides a graph where each point represents the month-to-date sum of trace spans ingested in Cloud Trace. It resets on the last day of the month; the monthly total is found on the last day of the month. In this example, the orange line represents the highest logging data. Thus, the gae_app is where all the logging data coming from Migrate for Compute Engine. You can find more information on Migrate for Compute Engine in the documentation. We learned that a metrics scope is used in Cloud Monitoring to monitor the resources you care about. The resources could be in a Google Cloud project, an AWS account, or multiple Google Cloud projects and AWS accounts. To view your Monitoring usage by metrics scope, go to Monitoring and then Settings. On the Summary tab, the Metrics Ingested table displays a summary of your metrics ingestion data by resource. This data includes the previous month total usage, the current month to-date usage, and projected usage for the current month. To get your project-level usage in detail, in the Metrics Ingested table, click View Bill. You’re taken to the Cloud Billing Reports page. After you know the projects where you’re spending on Cloud Monitoring, we want it to be easy to understand which metrics are driving these observability costs. We also want to provide insights on how to reduce spend on unused and noisy metrics. To get started, go to Monitoring > Metrics Diagnostics. This page provides many tools to understand metric ingestion and Monitoring API usage. One tool is a “Metrics” table where you can sort and filter metrics by volume or samples ingested, metric cardinality, metric name and domain, metric labels, project, error rate, and more. We recommend sorting metrics by “Metric Data Ingested” in descending order to identify exactly which metrics are primarily driving ingestion volume. Whether intended or not, our customers often find that only a few metrics or metric types drive most consumption. These are the ones that are ripe for cost reduction and optimization. To see your logs-based metrics usage, go to Logging > Logs-Based Metrics. Previous Month Usage represents the sum of bytes ingested in the logs-based metric in the previous calendar month. Usage (MTD) represents the sum of bytes ingested in the logs-based metric in the current calendar month. Clicking any of the column names lets you sort data in ascending or descending order. For example, if you want to review which metrics ingest the most data, sorting data is helpful.

#### Cost Control Best Practices

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515479

Finally, let’s discuss some billing best practices. We start with what we already discussed in this module, mostly in the last section. You learn to calculate costs, become aware of your operations-related spend and where it goes, and learn what the more expensive items are. If you exclude log entries, you don't pay for them. However, they will be gone once excluded, so be careful with the log entries you keep. Here are some of the common exclusions: Google has several different load balancers, and they all support various forms of monitoring and logging. Frequently, for Cloud Load Balancing that support Cloud Logging, all you need is a small percentage. Exclude 90% or more. VPC Flow Logs is another good example. Again, you can often exclude more than 95% of the entries and still get enough to monitor the VPC. Also, remember that your exclusion can filter on entry contents. Perhaps you only want to retain logs from sources with a CIDR outside your network. If you send your VPC Flow Logs to Cloud Logging, the charges for the generation of VPC Flow Logs instances are waived. Only logging charges apply. However, if you send them and then exclude your VPC Flow Logs from Cloud Logging, VPC Flow Logs charges apply. To lower the bill, they have to be deactivated completely. For more details, refer to the documentation provided at the end of this module. Another common exclusion is web applications and services that are logging HTTP 200 OK from requests. Frequently, OK messages don't provide much insight, and they can generate numerous entries. Log exports are free, but not the target resource. You can export logs yet exclude them from being ingested into Cloud Logging. You can retain the logs in Cloud Storage and BigQuery or use Pub/Sub to process the logs while excluding the logs from Cloud Logging, which can reduce costs. The fee associated with log exports are: Storage fees in Cloud Storage. Storage, streaming, and query fees in BigQuery. Pub/Sub message and networking egress fees. In fact, 2 TiBs of data access log data stored in Logging would cost about $1,000. The same 2 TiBs stored in a regional, standard class bucket would cost about $40. With archival, it would be much cheaper. You can reduce log volumes by not sending the additional logs generated by the Ops Agent to Cloud Logging. For example, you might reduce log volumes by choosing not to add the Ops Agent to VMs in your development or other nonessential environments to Cloud Logging. Your virtual machines continue to report the standard logs to Cloud Logging, but don't report logs from third-party apps nor the syslog. How you use labels on Monitoring custom metrics can affect the volume of time series that are generated. Given a custom metric with two labels (cost_center and env values), you can calculate the maximum number of time series by multiplying the cardinality of both labels. If there are 11 cost_center values and 5 env values, that means that up to 55 time series can be generated. Adding additional metric labels can add significant metric volume and, therefore, increase the cost. Where possible, limit the number of custom metric labels. Select labels thoughtfully to avoid label values with high cardinality. For example, using user_id as a label results in at least one time series for each user. If you have significant traffic, the number could be very large. Metrics sent from the Ops Agent are chargeable metrics. Although installing them is undoubtedly a best practice, there will be exceptions. Weigh the advantages and disadvantages, and remember that both the agents are customizable through configuration files. If you don't need the detailed system metrics or metrics from the third-party apps for certain VMs, reduce the volume by not sending these metrics. You can also reduce the metric volumes by reducing the number of VMs using the Ops Agent. For example, you can reduce metric volumes by choosing not to add Google Cloud projects in your development or other nonessential environments to Cloud Monitoring. Also, you can choose not to include the monitoring agent in VMs in development or other nonessential environments. Custom metrics can increase spend. Newer metrics created using OpenTelemetry support sampling to help reduce volume. When you instrument more apps to send metrics, more custom monitoring metrics are generated. If you want to reduce metric volumes, you can reduce the number of custom monitoring metrics that your apps send. Open source Prometheus documentation rarely recommends filtering metric volume, which is reasonable when costs are bounded by machine costs. But when paying a managed-service provider on a unit basis, sending unlimited data can cause high bills. To reduce the number of metrics, you can do the following: Modify your scrape configs to scrape fewer targets. Filter exported metrics when you using managed collection or self-deployed collection. Managed Service for Prometheus charges on a per-sample basis. Here are some of the ways in which you can reduce the number of samples ingested: Increasing the length of the sampling period. For example: Changing a 10-second sampling period to 30 seconds can reduce your sample volume by 66%, without much loss of information. Setting the scraping interval on a per-job or a per-target basis. For managed collection, you set the scrape interval in the PodMonitoring resource by using the interval field. If you’re configuring the service by using self-deployed collection, for example with kube-prometheus, prometheus-operator, or by manually deploying the image, then you can reduce your samples sent to Managed Service for Prometheus by aggregating high-cardinality metrics locally. Use recording rules, flags, and environment variables to aggregate data to Monarch. Refer to the documentation for more information. Trace charges are based on the number of trace spans ingested and scanned. Use sampling to reduce the volume of traces ingested. Sampling is a crucial part of a tracing system, because it provides insight into the breakdown of latency caused by app components, such as RPC calls. Sampling is not only a best practice for using Cloud Trace: you might reduce your span volume for cost-reduction reasons too. For example, with a popular web application with 5000 queries/second, you might gain enough insight from sampling 5% of your app traffic instead of 20%. A smaller sample reduces the number of spans ingested into Trace to one-fourth. The OpenTelemetry lets you specify a sampling rate. You can enforce span quotas with the API-specific quota page in the Google Cloud console. Setting a quota that is lower than the default product quota means that you guarantee that your project won't go over the specific quota limit. Quotas are a way to ensure that your costs are expected. Your app might be called by another app. If your app reports spans, the number of spans reported by your app might depend on the incoming traffic that you receive from the third-party app. For example, if you have a frontend microservice that calls a checkout microservice, and both are instrumented with OpenTelemetry, the sampling rate for the traffic is at least as high as the frontend sampling rate. Understanding how instrumented apps interact lets you assess the effect of the number of spans ingested.

#### Module summary

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515480

In this module, you learned how to: Analyze resource utilization costs for operations-related components within Google Cloud. And implement best practices for controlling the cost of operations within Google Cloud.

#### Quiz

- https://www.cloudskillsboost.google/paths/11/course_templates/864/quizzes/515481

### Course Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/864/video/515482

This brings us to the end of the “Observability in Google Cloud” course! Let’s do a quick recap of what was covered. In this course we learned how to: Install and manage Ops Agent to collect logs for Compute Engine Use Google Cloud Managed Service for Prometheus Analyze VPC Flow logs and Firewall Rules logs Analyze resource utilization cost for monitoring related components within Google Cloud. Thank you for taking this course. I hope you feel more comfortable with using the different operations suite products that we covered. Now it’s your turn. Go ahead and apply what we have learned by monitoring your applications in Google Cloud. See you next time!

### Course Resources

#### Course Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/864/documents/515483

### Your Next Steps

## 10: Getting Started with Terraform for Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/443

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508936

– (intro splash music) Rekha: So what is Terraform, and how can it be used to help you manage your infrastructure? Eoin: What is infrastructure as code and how can it benefit the business? If you're asking yourself any of these questions, you're in the right place. Rekha: Terraform is an open source, Infrastructure as Code tool by HashiCorp, for provisioning resources – including Google Cloud resources – with declarative configuration files. Eoin: These files can be shared amongst team members, treated as code, edited, reviewed, and even versioned. This enables you to build and deploy complex cloud infrastructure by running a few simple commands. Eoin: In the Getting Started with Terraform for Google Cloud course, you will learn how to use Terraform – an infrastructure as Code tool – to create and manage your Google Cloud infrastructure. Rekha: Hello! My name is Rekha Eoin: And I’m Eoin, we're both Course Developers at Google Cloud and we want to welcome you to the “Getting Started with Terraform for Google Cloud course. Rekha: The purpose of this course is to explore the Terraform workflow in detail. Eoin: You will become familiar enough with the Terraform workflow that you will know the purpose of each phase and how it is implemented. Rekha: This content is designed for DevOps Engineers, Cloud Architects, Cloud Engineers and anyone who is interested in using Terraform to create and manage their Google Cloud Infrastructure. This is a fundamental course, that covers the basics of Terraform for Google Cloud. Eoin: The prerequisite for this course is Google Cloud Fundamentals: Core Infrastructure. You’ll also need to have basic programming skills, familiarity with using CLI and general familiarity with Google Cloud. Eoin: Through a series of videos and hands-on labs, you will explore the business need for Terraform and Infrastructure as Code and the benefits of using it in your environment. Rekha: You’ll learn about Terraform features and functionalities. Eoin: You’ll also be equipped to use Terraform resource blocks, variables, and output values to create Google Cloud infrastructure resources. Rekha: You will also learn to use modules to build reusable configuration. Eoin: In addition, you’ll be equipped to explain the Terraform state file and its importance. Eoin: Let’s get started!

### Introduction to Terraform for Google Cloud

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508937

Hello and welcome to the “Introduction to Terraform for Google Cloud” module. Cloud computing has played a major role in how companies build, scale, and maintain technology products. It has transformed how products and services are provisioned; the ability to provision cloud infrastructure in just a few clicks has proven to be simple and highly productive. So, why would you want to use lines of code to provision your infrastructure? What is Infrastructure as Code, and what are the benefits of using it? What is the difference between Infrastructure as Code and Terraform, and how do they relate? In this module, you’ll find the answers to these questions. Starting with the basics, this module provides an overview of infrastructure as code – or IaC – which is the basic concept for Terraform. You’ll learn about the features and benefits of using Terraform, and also explore how it can be used as an IaC tool for Google Cloud. You’ll then investigate how Terraform transforms lines of code into infrastructure. This module concludes with a short quiz and recap of topics covered. Let’s jump in! This is an introductory module that covers the business need for Terraform. We’ll start with the basics by providing an overview of infrastructure as code (IaC), which is the basic concept for Terraform. We'll cover the features and benefits of using Terraform, and also explore how Terraform can be used as an IaC tool on Google Cloud. We’ll then look at how Terraform transforms lines of code into real infrastructure on Google Cloud. This module concludes with a short quiz and recap of topics covered. Let’s jump in!

#### Infrastructure as Code

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508938

Let’s start with the fundamental concept required to understand Terraform: Infrastructure as code, or IaC. With IaC you can write code using a configuration language to define, provision, and manage your infrastructure. Gone are the days where a system administrator had to manually configure hundreds of servers, networks, and firewall rules by interacting with a user interface. With IaC, you only declare the desired end state of the infrastructure –IaC handles management and provisioning. The rapid increase of business demand has manifested a cultural shift known as DevOps. DevOps emphasizes collaboration and communication between developer and operations teams, and automates software delivery and infrastructure changes. Let’s explore some challenges and the role IaC plays in addressing them. With competition growing among industries to benefit from the cloud, DevOps teams are struggling to manage growing environments and rapidly expanding businesses. This struggle involves four major challenges. First, high business demand requires rapid scaling of IT infrastructure across industries. Due to rapid scaling of IT infrastructure, Ops teams must overcome new operational and technical bottlenecks, such as managing infrastructure consistently at scale. When infrastructure is changed, DevOps teams often struggle to collaborate and audit changes. Closing the communication gap is imperative for successful deployments. Increased quantity and scale have also led to human, manual error that can potentially have significant effects. Now, let’s discuss the benefits of IaC. First, its declarative. Instead of having to specify the exact sequence of steps to make a particular change, you can focus on the desired state of the infrastructure. For example, you might use a production label to specify three subnets across two regions. Terraform will update the live state to match the desired state by adding a label to the existing subnet, and creating two new subnets in additional regions. Through declarative abstractions, IaC tools manage the implementation details so you can focus on changes made to your desired state. This declarative approach allows anyone to read the state of the infrastructure, as opposed to asking a system admin. Another benefit of IaC is that it’s managed in the same way as application source code. Typically, if you make changes directly in the Google Cloud Console that cause an outage, you’d have to manually identify the breaking changes - an error-prone process. With IaC, you can see a full history of versions and infrastructure changes. If something breaks, you can roll back to a previous state. The entire history of your infrastructure is captured in the commit log. Version control also enables developers to collaborate on changes. For example, perhaps a developer needs a port opened on a firewall to support their application. They can simply submit a pull request to open the port instead of waiting in a queue for an administrator to manually open it. Changes can be discussed, reviewed, and audited without creating information silos, which can lead to more collaborative ownership as infrastructure evolves. IaC also features an auditable history of your infrastructure. Typically, infrastructure doesn’t contain comments. You might see changes in audit logs, but it’s often difficult to understand why those changes were made. With IaC, you can include explanations. For example, you might comment that a specific subnet allocation is required for a certain application. This auditable history gives a clear view into how infrastructure has evolved over time. When proposing new changes, you can also preview them and inspect exactly how they will affect live infrastructure before they’re applied. These previews let you detect drift and ensure that your infrastructure remains robust. Portability is another benefit of IaC. You can build reusable modules that encapsulate conventions and let you consistently build infrastructure from a shared template. Instead of having to rebuild infrastructure manually, you can have many instances of the same template deployed for different applications or regions. Google Cloud has developed reusable modules that can be found in the Cloud Foundation Toolkit. Using the library of reusable, documented, tested infrastructure code makes it easier to scale and evolve your infrastructure. Provisioning and configuring are two terms that are sometimes misinterpreted. Fundamentally, IaC is used for provisioning and managing cloud resources, and configuration management is used for VM OS-level configuration. For example, infrastructure as code creates and provisions a VM instance, and configuration management configures the internals of the VMs. Configuration management is a broad topic that covers more than we can discuss in this section. To simplify, activities like configuring a VM for application dependencies are considered configuration management. Configuration often consists of manual tasks such as starting services, installing dependencies, installing applications, and running updates. To elaborate, the term Infrastructure as code refers to frameworks that manipulate Google Cloud APIs to deploy the infrastructure required to run application code. In contrast, configuration management refers to package configurations and software maintenance. Let’s look at another common example to differentiate between provisioning and configuration. IaC automates tasks involved in launching a GKE cluster into Google Cloud, and configuration management automates tasks involved in deploying containers into the GKE cluster. An important principle to understand about infrastructure as code is that it’s declarative. Most programming languages use an imperative model, where you specify the exact action you want to take, such as create five servers. This model is challenging for infrastructure because if you run the script again, it might create five new servers even if some exist. Imperative workflows make it difficult to determine the difference between live infrastructure and your desired state, leading to repeated resource creation. With IaC, you declare the desired state of your infrastructure and let the tool determine the details. For example, you might declare that you should have five servers. The first time Terraform runs, it might create all five servers. Then let’s say someone accidentally deletes a server. The next time Terraform runs, it would recognize that four servers exist and restore only the missing server. We encourage adopting declarative infrastructure tools like Terraform. Focus on how the infrastructure should be, and let automation handle the details of updating the infrastructure to match your desired state. Declarative management focuses on the WHAT rather than the HOW.

#### Terraform Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508939

You now know the basics of infrastructure as code, but what exactly is Terraform? Terraform is an infrastructure as code tool created by HashiCorp, and is currently licensed under the HashiCorp Business Source License v1.1. It lets you provision Google Cloud resources with declarative configuration files. Examples of resources include virtual machines, containers, storage, and networks. Terraform allows infrastructure to be expressed as code in a simple, human-readable language called HashiCorp Configuration Language, or HCL. Terraform reads configuration files and provides an execution plan of changes, which can be reviewed, applied, and provisioned. At a high level, Terraform lets operators author files containing resource definitions on the Google Cloud provider, and automates the creation of those resources. Terraform features include: Multi-cloud and multi-API support. Terraform supports all major cloud providers, in addition to Google Cloud and API-exposed services such as GitHub and Kubernetes. Three different editions that range from self-hosted to fully managed, including enterprise support. A large community, including a registry with publicly available modules for Google Cloud deployments. And infrastructure provisioning, rather than configuration. You can use Terraform for Google Cloud to provision resources, meaning you can use resource blocks to define infrastructure elements such as VMs, networks, and firewalls. You can create explicit dependencies between resources, so that a given resource can only be created after the creation of another resource. You can standardize how a given resource is created by creating reusable modules. And you can limit the values provided for a given resource argument using validation rules. Next let’s view the standard IaC configuration workflow. Before jumping into the Terraform workflow, you must determine what resources should be created for a project. For example, for a common 2-tier architecture, you need a pool of web servers that use a database tier. Therefore during the “scope” phase, you scope the requirements for the Google Cloud resources and plan how they should be connected. In this case scope the type of compute and database instances needed. The Terraform workflow begins with the author phase, where you author the configuration code for the infrastructure you want to create. Referring to our example, during this phase, you code the configuration of the instances and the VPC network. You then organize your code in configuration files, such as variables, main, and tfvars. The next phase is initialize, where any plugins or modules are installed. During this phase, you run the terraform init command, which initializes the Terraform configuration directory and installs Google Cloud as the provider. In the plan phase, you run the terraform plan command. The terraform plan command provides an execution plan for the resources created, changed, or destroyed as per the configuration defined in the author phase. You can then review the plan before applying it to your Google Cloud infrastructure. After you review the configuration described in the generated plan, apply it to create the infrastructure and a state file. Later in the course you will explore the Terraform workflow in more detail. Now let’s examine common Terraform use cases. Terraform is used to manage infrastructure. It takes an immutable approach, meaning you write code to reduce the complexity involved in upgrading or modifying the services and infrastructure. Terraform is also used to track infrastructure changes. Whenever a new change is planned or applied, you will be prompted to approve the change before Terraform modifies the state of the infrastructure. When an infrastructure is created using Terraform, a state file is automatically generated. State files reflect the current state of your infrastructure, and provide the amount and type of Google Cloud resources modified in your configuration. Terraform is used to automate changes. Because configuration files are declarative in nature, you do not have to write detailed instructions to build the infrastructure; you only define the end state. Terraform manages the dependencies and provisions the resources. It’s also used to standardize configurations. You can use modules to save time and implement best practices. You can also leverage publicly available modules from the Terraform Registry. Terraform can also be used to automate the enforcement of policies on the types of resources teams can provision and use.

#### Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508940

So, how can you use Terraform to create, update, or destroy infrastructure resources? First, you must write your infrastructure as code in configuration files. Configuration files describe to Terraform the resources you want to provision. Terraform generates an execution plan for the desired state, and then executes the plan to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied. Terraform is available in three editions: Community Edition, Cloud, and Enterprise. Community Edition is a free version of the software available for download on a local machine or compute resource in the cloud. With this version, many features are available with no license cost. You can provision several different types of resources and codify your infrastructure. Community Edition grants access to publicly available templates so you can apply code writing best practices. You can only use the Community Edition version on your local machine, and it does not support concurrent deployments. Terraform Community Edition does not have version control, so you can't track changes or ensure that your commits do not affect the infrastructure setup. With the Community Edition version, Terraform can only be interacted with through the CLI, whereas Terraform Cloud and Enterprise come with a GUI option. Terraform Cloud is useful for collaborative environments. Terraform Cloud has three plans: free, standard and plus. Unlike Terraform Community Edition, Terraform Cloud and Terraform Enterprise support concurrent deployment. With Terraform Cloud – like most other SaaS solutions – the operational overhead is low. Terraform Enterprise is hosted on-premises or on an infrastructure controlled by you. Due to infrastructure maintenance and its integration with Terraform, Terraform Enterprise has a high operational overhead. Terraform is pre-installed on Cloud Shell, but you can also install it on your local machine. You can install Terraform as a binary package or use popular package managers. To manually install Terraform on a Windows machine: First, download the package that meets your system requirements. Extract the package. Terraform includes a single binary called terraform. Edit the PATH variable to include Terraform. And then verify the installation by entering terraform -help in a new terminal. The authentication mechanism used for Google Cloud varies based on where Terraform is run. The Google Cloud provider – not Terraform – will require authentication to communicate to the resource API for resource creation. Terraform on Cloud Shell is pre-authenticated. Cloud Shell is a Compute Engine virtual machine, and the service credentials are automatic, so there’s no need to set up a service account key. If you’re running Terraform on your workstation, authenticate by using the Cloud SDK. You first must install gcloud CLI, and then run the gcloud auth application-default login command to authenticate with Google Cloud. If you’re running Terraform in a VM on Google Cloud, configure that VM to use a Google Service Account. A Google Service Account allows Terraform to authenticate without a separate credential or authentication file. Ensure that the VM has the Google Cloud API enabled. If you’re running Terraform outside Google Cloud, you can use a Google Cloud Service Account with Terraform. From the service account key page in the Cloud Console, choose an existing account or create a new one. Then download the JSON key file. Name it something you can remember, and store it somewhere secure on your machine. Supply the key to Terraform using the environment variable GOOGLE_APPLICATION_CREDENTIALS, setting the value to the location of the file. Terraform can then use this key for authentication. This method does have a few limitations. Service account keys are short-lived. Key rotation is not allowed, and the keys must be protected. Workload identity and workload identity federation are tools for mitigating short-lived identity tokens when running Terraform outside of Google Cloud. Once you’ve downloaded Terraform and authenticated successfully, you’re ready to start authoring a Terraform configuration. Let’s create a simple VPC network. We won’t get into the details of the code just yet. For now, let’s focus on the workflow. First, create a configuration file with a . tf extension. and define Google Cloud as the provider. Then add HCL code to create a Google Compute instance. After you save the code, navigate to the directory where you saved the file and run terraform init, terraform plan and terraform apply. After you run terraform apply, a new VPC network named mynetwork will be created on Google Cloud. Once you’ve downloaded Terraform and authenticated successfully, you’re ready to start authoring a Terraform configuration. Let’s create a simple VPC network. We won’t get into the details of the code just yet. For now, let’s focus on the workflow. First, create a configuration file with a . tf extension. Then, define Google Cloud as the provider and run the terraform init command to initialize the provider. Add the HCL code to create a Google Compute instance. After you save the code, navigate to the directory where you saved the file and run the terraform plan and terraform apply. After you run terraform apply, a new VPC network named mynetwork will be created on Google Cloud.

#### Quiz : Introduction to Terraform for Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/443/quizzes/508941

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508942

This brings you to the end of the first module of the Getting Started with Terraform for Google Cloud course. This module covered the basics of Terraform and Infrastructure as Code. We defined Infrastructure as code and covered the business case for IaC. We explained the features and benefits of using Terraform. We also discussed common use cases, how to use Terraform, and the consumption model for Terraform.

### Terms and Concepts

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508943

Hi there. Welcome to Module 2: Terms and Concepts. This module elaborates on the Terraform workflow that was introduced in Module 1. Now this is going to be a very short module; while there's a lot to cover about how to author infrastructure, this module explores the foundation concepts you’ll need to be familiar with before getting into the details of code constructs such as resources variables and output values. This module includes key concepts and terminology and the basics of the HashiCorp Language – or HCL. Wondering what HCL language looks like? What do resources mean in the Terraform world? This module is for you. This module will cover each phase of the Terraform workflow, from author to apply. You’ll learn to create basic configuration files and explore the HCL language. HCL is similar to JSON, easy to learn, and powerful for configuring infrastructure resources. You’ll learn about the initialize phase, which includes providers. Next comes the plan and apply phases, where you’ll examine the purpose of a few important terraform commands. This module will also cover an optional phase called validate, where you’ll learn about the Terraform validator tool. And it concludes by describing how to create, update, and destroy Google Cloud resources using Terraform. This module concludes with a short demo on creating infrastructure objects, a hands-on lab to apply your knowledge, and a recap of topics covered in the module. Let’s get started!

#### Terraform Configurations and the HashiCorp Language

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508944

Let’s get started with an overview of Terraform configurations and the Hashicorp Language. This course focuses on the core Terraform workflow: author, initialize, plan, and apply. In this module, you’ll explore how these individual phases fit together to transform code to cloud resources. Let’s start with the author phase, where you write Terraform code in . tf files. Before writing code, start by creating directories for a Terraform configuration. A Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. The configuration is stored with a . tf extension. A Terraform directory can consist of multiple files and directories. A Terraform configuration consists of: A root module, also referred to as the root configuration file, And an optional tree for child modules. Child modules are optional, and can be variables, outputs, providers, and so forth. You can have a series of resources and other code constructs within a single root configuration, but the best practice is to logically separate your files. The root module is the working directory in which Terraform commands are run. In that directory, Terraform will look for any . tf files and use them to create a plan and infrastructure elements. Now that you know how to create a directory structure, let’s explore the language used for Terraform configurations. The language used to write configurations is the HashiCorp Configuration language, or HCL. Generic HCL syntax is shown in this example. HCL is used to create and manage API-based resources, predominantly in the cloud. Resources are infrastructure objects such as virtual machines, storage buckets, containers, and networks. Terraform uses HCL to define resources in your Google Cloud environment, create dependencies with those resources, and define the data to be fetched. Despite some commonalities, HCL is a configuration language, not a programming language. It’s a JSON-based variant that’s human and machine friendly. The simplicity of HCL makes Terraform accessible to developers. HCL includes a limited set of primitives such as variables, resources, outputs, and modules. It does not include any traditional statements or control loops. The logic is expressed through assignments, count, and interpolation functions. Let’s explore HCL syntax in detail. Blocks are lines of code that belong to a certain type. Examples include resource, variable, and output. A block can be simple or nested to include another block type. Arguments are part of a block and used to allocate a value to a name. Some blocks have mandatory arguments, while others are optional. Identifiers are names of an argument, block type, or any Terraform-specific constructs. Identifiers can include letters, underscores, hyphens, and digits, but cannot start with a digit. Expressions can be used to assign a value to an identifier within a code block. These expressions can be simple or complex. Comment syntax start with a # for a single-line comment. Remember, HCL is declarative by nature, which means you define the end state of an infrastructure. Therefore the order of the blocks or files does not matter.

#### Author Phase Terms and Concepts

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508945

We’ve covered Terraform configurations and the HashiCorp Language. Now let’s discuss common terms and concepts that you encounter during the author phase of the Terraform workflow. Resources are code blocks that define the infrastructure components. A resource is identified by the keyword resource, followed by the resource type and a custom name. The resource type depends on the provider defined in your configuration. Inside the curly brackets are the resources arguments, where you specify the inputs needed for the configuration. Terraform uses the resource type and the resource name to identify an infrastructure element. The keyword resource identifies the block as a cloud infrastructure component. The resource type is google_storage_bucket, which identifies the Google Cloud resource. This terminology is Terraform-specific and the convention cannot be customized. The resource type varies based on the provider defined. The resource name is example-bucket. Terraform uses the resource type and the resource name together as an identifier for the resource. This example assumes that Google Cloud is defined as the provider and shows two resource blocks: a Cloud Storage bucket and a compute instance. The arguments differ based on the resource type being defined. For the google_storage_bucket resource, you only must specify the name and location to successfully create the resource. For the google_compute_instance resource, you must specify the name, machine_type, and the network_interface. Zone and tags are optional. You can use separate files – for example, files for instances, storage buckets, and datasets – if you have lengthy resource configurations. Providers implement every resource type; without providers, Terraform can't manage any kind of infrastructure. In the providers.tf file, you specify the Terraform block that includes the provider definition you will use. Terraform downloads the provider plugin in the root configuration when the provider is declared. Providers expose specific APIs as Terraform resources, and manage their interactions. Provider configurations belong in the root module of a Terraform configuration. An example of a provider block is shown on screen. The source argument provides the global source address for the provider that you intend to use. In the above example, the source argument is assigned to the Terraform Registry URL hashicorp/google. The name Google is the local name of the provider to be configured. To ensure that the local name is configured correctly, the provider must be included in the required provider block. The arguments – such as project and region – are specific to the Google provider. When a provider block is not included within a Terraform configuration, Terraform assumes an empty default configuration. You can also assign a version to each provider. The version argument is optional, but recommended. Version arguments constrain the provider to a specific version or a range of versions to prevent downloading a new provide that may contain breaking changes. If the version isn't specified, Terraform will automatically download the most recent provider during initialization. Variables are used to parameterize your configuration. Input variables serve as parameters for Terraform, allowing easy customization and sharing without having to alter source code. Once a variable is defined, there are different ways to set its values at runtime: environment variables, CLI options, key or value files, and so forth. You can define a resource attribute at run time or centrally in a file with a . tvars extension. You can also easily separate attributes from deployment plans. In this example, main.tf declares a Cloud Storage bucket. The location attribute has been parameterized because it’s declared as a variable in the variables.tf file. By parameterizing the attribute, you can define the values of these variables at run time. You’ll learn more about variables in the next module. The outputs.tf file holds output values from your resources. Resource instances managed by Terraform each export attributes whose values can be used elsewhere in configuration. If needed, output values are a way to expose some of that information. Some resource attributes are computed upon their creation. For example, a self-link of the resource or the URL of a bucket is generated upon bucket creation. These computed attributes might be required for accessing the bucket or uploading objects. With an output value, you can output this information and make it accessible. The label after the output keyword is the name, which must be a valid identifier. In a root module, this name is displayed to the viewer. In a child module, it can be used to access the value. The value argument takes an expression that returns the results to the user. Terraform saves the state of resources that it manages in a state file. By default, the state file is stored locally, but it can also be stored remotely. Remote storage is often the preferred method when working in a team environment. Do not modify or touch this file; it’s created and updated automatically. States are covered in detail in an upcoming module. Finally, a Terraform module is a set of Terraform configuration files in a single directory. Even a simple configuration consisting of a single directory with one or more . tf files is considered a module. Modules are the primary method for code reuse in Terraform. They are reused by specifying the source from which the code can be retrieved. Sources can either be local or remote. You can use an upstream module from the HashiCorp module registry or create your own.

#### Terraform Commands

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508946

Once Terraform is installed on your machine, you can use commands at your root module to interact with Terraform. Common commands covered in this module include terraform init, Terraform plan, Terraform apply, Terraform destroy And terraform fmt. terraform init is used to initialize the provider with a plugin. terraform plan provides a preview of the resources that will be created after terraform apply. terraform apply creates infrastructure resources. terraform destroy destroys infrastructure resources. And terraform fmt auto formats to match canonical conventions. The terraform init command is run during the initialize phase. It’s the first command to run after creating a configuration or checking out an existing configuration from version control. Terraform init ensures that the Google provider plugin is downloaded and installed in a subdirectory of the current working directory, along with various other bookkeeping files. The provider block includes a source attribute specifying where the provider plugins will be downloaded from. Terraform uses a plugin-based architecture to support the numerous infrastructure and service providers available. Each "provider" is its own encapsulated binary that is distributed separately from Terraform itself. The terraform init command will automatically download and install any provider binary for the providers to use within the configuration. In this case, the provider is Google. After you run terraform init, a hidden directory called . terraform is created inside the current working directory. You will see an "Initializing provider plugins" message, letting you know that Terraform will find the latest plugin and download the associated files. The output of the command tells you the provider version that Terraform has installed, in this case version 4.21. Terraform plan creates an execution plan detailing all the resources that will be created, modified, or destroyed upon executing terraform apply. When terraform plan is run, Terraform: Reads the current state of existing remote objects to ensure that Terraform state is up to date. Compares the current configuration to the prior state and notes any differences. And builds an execution plan that only modifies what is necessary to reach your desired state. Terraform plan does not actually create or change any infrastructure resources, but instead provides you with an opportunity to preview your infrastructure changes before applying them. For example, you might want to run this command before committing a change to version control to be sure that it will behave as expected. You can use the optional -out=FILE option to Optionally, you can save the generated plan to a file on disk, which you can later execute by passing the file to terraform apply as an extra argument. Terraform apply executes the actions proposed in a Terraform plan, creates the resources, and establishes the dependencies. The symbols next to the resources and arguments indicate the action performed on the resource. The plus next to the resource means that Terraform will create this resource. Terraform also shows the attributes that will be set. Minus slash plus means that Terraform will destroy and recreate the resource, rather than updating it in-place. The tilde means that Terraform will update the resource in-place. The minus indicates that the instance and the network will be destroyed. As with terraform apply, Terraform shows its execution plan and waits for approval before making any changes. If the plan is created successfully, Terraform pauses and waits for approval before proceeding. If anything in the plan is incorrect or unsafe, you can abort here without changing your infrastructure. If terraform apply fails, read the error message and troubleshoot the issues. Just like with terraform plan, Terraform determines the order in which things must be destroyed. For example, Google Cloud won't allow a VPC network to be deleted if it still has resources. Terraform waits until the instance is destroyed before destroying the network. Let’s cover some code formatting best practices: Separate the meta arguments from the other arguments by placing them first or last in the code with a blank line. Indent your arguments with two spaces from the block definition When two or more arguments are defined in a given block, align the values at the equal sign When a block includes a nested block, place the block following all the arguments. Finally, when your code includes multiple blocks, separate them with a black line for readability. Running terraform fmt on your modules and code automatically applies all formatting rules and recommended styles to assist with readability and consistency. Terraform fmt automatically maintains consistent formatting, so you don't have to manually change configuration to ensure it meets the standards. The last command to cover in this module is terraform destroy, which destroys resources. Terraform destroy is similar to terraform apply, but it behaves as if all resources have been removed from the configuration. Terraform is sometimes used to manage ephemeral infrastructure for development purposes. In this case, you can use terraform destroy to conveniently clean up temporary objects once you’re finished with your work. You can also destroy specific resources by specifying a target in the command. Destroying your infrastructure is a rare event in production environments. But if you're using Terraform to create multiple environments such as development, testing, and staging, then destroying is often a useful action. Use terraform destroy carefully - it will destroy any resource and the data associated with it. For example, if there’s data in a bucket, be careful when running terraform destroy, as that data cannot be recovered.

#### The Terraform Validator

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508947

Let’s think back to the Terraform workflow, and explore the validate phase. Between the plan and apply phases, is the option to validate. During this phase, pre-deployment checks are run against organizational policies. The Terraform validator is a tool for enforcing policy compliance as part of an infrastructure CI/CD pipeline. It’s also incredibly useful in an infrastructure-as-code environment, as it helps mitigate configuration errors that can cause security and governance violations. Although validation is optional, The Terraform Validator is a useful tool in an IaC environment. Businesses are shifting toward infrastructure-as-code, and with that change comes the risk that configuration errors can cause security and governance violations. Furthermore, many organizations have compliance and governance policies in place that must be adhered to. For example, perhaps there are data residency laws in place, and your governance team must ensure that resource creation is only allowed in certain regions. To address this policy, the security and governance teams can set up guardrails. These guardrails are in the form of constraints. Constraints define the source of truth for security and governance requirements. Constraints must be compatible with tools across every stage of the application lifecycle, from development, to deployment, to auditing. The Terraform Validator is run by executing the gcloud beta terraform vet. Executing the command enforces policy compliance as part of an infrastructure CI/CD pipeline. The tool retrieves project data with Google Cloud APIs, so you can accurately validate your plan. You can use the Terraform Validator to detect policy violations and provide warnings or halt deployments before they reach production. The same set of constraints that you use with the tool can also be used with any other tool that supports the same framework. gcloud beta terraform vet is different from the terraform validate command. Terraform validate is used for testing syntax and the structure of your configuration without deploying any resources. The Terraform Validator is used to ensure that the configuration adheres to the set of constraints. These constraints automate the enforcement of the organization policies. With The Terraform Validator you can: Enforce policies at any stage of application development Remove manual errors by automating policy validation And reduce learning time by using a single paradigm for all policy management Let’s examine a few Terraform Validator use cases, and show how it can help different teams. Platform teams can easily add guardrails to infrastructure CI/CD pipelines, to ensure that all requests for infrastructure are validated before deployment to the cloud. This limits platform team involvement by providing failure messages to end users during their pre-deployment checks. These messages tell you which policies they have violated. Application teams and developers can validate their Terraform configurations against a central policy library to identify misconfigurations early in the development process. Before submitting to a CI/CD pipeline, you can ensure that your Terraform configurations are in compliance with organizational policies, thus saving time and effort. Security teams can create a centralized policy library that is used by all teams across the organization to identify and prevent policy violations. Depending on how your organization is structured, the security team or other trusted teams can add the necessary policies needed to meet compliance requirements.

#### Demo - Terraform Workflow

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508948

Hi, this demonstration will help you become familiar with the Terraform configuration to create a Compute Engine instance. We will show you how to create configuration files and use the Terraform CLI, to execute a few Terraform commands such as terraform init, terraform plan, terraform apply and terraform destroy. While watching this demonstration, you will discover how easy it is to use and adopt Terraform to manage your Google Cloud infrastructure. Let's get started. For demonstration purposes, we are using Terraform on Cloud Shell. Terraform comes preinstalled on Cloud Shell. With Cloud Shell, you have command line access to your cloud resources directly from your browser, so you can easily manage projects and resources without having to install any tools on your local machine. Further, command line tools suggest Terraform and other utilities are automatically authenticated so you can use them setup free. To verify the Terraform installation, enter terraform in the command line. You can check the version of Terraform running on Cloud Shell by running terraform version command. As a first step, let's define the provider. To do this, we first create a folder called infra. Right-click underneath your project to create New Folder. Give it the name of your choice. Create a new file called main.tf to author the terraform code. Before creating the resources, we define the provider so Terraform knows which provider to download. At this point, because you might not be familiar with the code and syntax, we leverage Terraform Registry. Navigate to the registry. Under Google Cloud Provider, click the Use Provider button and copy the code underneath it. In the configuration define, the Google providers source is defined as hashicorp/google, which is shortened for Terraform Registry for Google Cloud. In this example, we already have a project created. Let's edit the configuration options to include the project and assign the project ID to it. Save the configuration changes. Switch to Cloud Shell. Change to the current directory. Execute terraform init to initialize Terraform and download the provider plug-ins. Let's next define our first Google Cloud resource, a Google Compute Engine. Let's edit the project ID to our current project. Save the code and return to the Cloud Shell window. Next, execute terraform plan. Authorize when prompted. You can see a summary of Compute Engine instances created. Execute terraform apply, to apply your configuration. The output has a plus next to google_compute_instance, which means that Terraform will create this resource. The sign varies based on the configuration applied. Beneath the resource update status, the attributes that will be set are displayed. When the value says ‘known after apply’ the value cannot be determined until the resource is provisioned. Review the output of the apply command to ensure that the resource is configured as per your expectations. If anything in the output seems incorrect or unsafe, enter ‘no’ so that no changes are made to your infrastructure. The plan looks acceptable here so we type ‘yes’. While we wait for the resources to be created, we switch to the editor to see that the terraform state file is automatically created. The terraform state defines the current state of the infrastructure. Let's verify the resource created on the Google Cloud console. You can see a Terraform resource is now being created in the region defined. We can also destroy the resources created by simply executing the terraform destroy command. Let's navigate to the Cloud Shell and delete the resource we just created. The minus prefix indicates that the resource will be destroyed. As with apply, Terraform shows its execution plan and waits for approval before making any changes. Enter ‘yes’. You can see that the resources are now deleted. Let's verify the deletion on the Google Cloud console. Navigate to Compute Engine. Click VM instances. You can now see the Terraform instance is no more available in the instance list. This completes the demonstration. In this demonstration, we defined a provider to download the provider plug-ins. We created a Google Compute Engine using Terraform. We used the command terraform plan to review the resource creation. We used the command terraform apply to create infrastructure resources. We then finally executed terraform destroy to destroy the resources we just created. Thanks for watching. Hope you enjoyed it.

#### Lab Intro: Infrastructure as Code with Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508949

In this lab, you will use Terraform to create, update, and destroy Google Cloud resources. You will start by defining Google Cloud as the provider. You will then create a VM instance without mentioning network to see how terraform parses the configuration code. You will then edit the code to add network and create a VM instance on Google Cloud. You will explore how to update the VM instance. You will edit the existing configuration to add tags and then edit the machine type. You will then execute terraform commands to destroy the resources created.

#### Infrastructure as Code with Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/labs/508950

#### Quiz: Terms and Concepts

- https://www.cloudskillsboost.google/paths/11/course_templates/443/quizzes/508951

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508952

This concludes Module 2: Terms and Concepts. Before writing code, you‚Äôll need to have a strong understanding of the HCL and Terraform concepts that were covered in this module. Let‚Äôs review. This module described terms and concepts relating to each phase of the Terraform workflow. You learned how to create basic configuration files within Terraform, and how to describe a Terraform provider. This module also explained the purpose of a few important Terraform commands. In addition, you learned about an optional phase in the Terraform workflow called validate. You also learned how to create, update, and destroy Google Cloud resources. Check out the next module to learn more about writing infrastructure code.

### Writing Infrastructure Code for Google Cloud

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508953

Hello again! Welcome to Module 3 of the Getting Started with Terraform for Google Cloud course, where you’ll dive deeper into resources, variables, and output values, and learn how to write infrastructure code for Google Cloud. This module focuses on the first phase of the Terraform workflow: the author phase. During this phase, you’ll write your code in terraform language, HCL. This module begins by exploring how to create infrastructure components using resources, and then discusses how Terraform handles dependencies within resources. You’ll learn how to parameterize a configuration using variables, and walkthrough the syntax used to declare and define them within your configuration. You’ll then learn how to use output values to export resource attributes outside the resource declaration. This module also explains how to simplify code using the Terraform registry and Cloud Foundation Toolkit. You’ll conclude with a lab to apply what you’ve learned. Let’s get started!

#### Introduction to Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508954

The first topic in this module is resources. Let’s get started with an introduction to resources, before jumping into meta-arguments and dependencies. In the Terraform world, resources are infrastructure elements, such as Compute Engine instances and Cloud Storage buckets. These elements can be configured with Terraform code. Real-world infrastructure has a diverse set of resources and resource types. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. Resources include instances, instance templates, groups, VPC networks, firewall rules, VPN tunnels, Cloud Routers, and load balancers. Resources are defined within a . tf file. It’s recommended that you place similar types of resources in a directory and define resources in the main.tf file. In this example, main.tf is the root configuration. The resource block is used to declare a single infrastructure object. The resource type identifies the type of resource being created, and depends on the provider being declared within a Terraform module. A provider is a plugin that provides a collection of resource types. Generally, a provider is a cloud infrastructure platform. In this course, Google Cloud is the provider. Resource arguments use expressions to declare attributes. Some resource arguments are mandatory for resource creation, and others are optional. Attributes can be used to define any advanced features associated with a resource. You can include multiple resources of the same or different types within the same Terraform configuration file. These resources can even span across multiple providers. Here, the first resource block declares the VPC network, and another that declares the subnet. Both resources are in the same main.tf file. Let’s examine a couple more resource blocks. A resource is identified by the resource keyword, followed by the resource type. This example shows a VPC resource named vpc_network, which is of the type google_compute_network. The name is required for the google_compute_network block, but the other arguments are optional. You can also define advanced features, such as versioning, within the same block. This example defines a VPC subnet called subnetwork-ipv6. For the subnetwork block, the name, the network in which the subnet should be created, and the IP CIDR range are required. Notice that the resource arguments depend on the resource type. This means that the network includes arguments such as name, project, and auto_create subnetworks, and the subnetwork includes arguments such as name, IP_cidr_range, and network. When accessing a resource attribute from another resource block, use the format ... In this example, when a subnet is created, the subnet requires the network ID of the VPC network it belongs to. The network ID is a computed resource attribute of a google_compute_network block. The attribute is generated when the network is created. The subnet resource block uses the network ID created from the vpc_network block and uses the format google_compute_network.vpc_network.id. This method can be used only when resources are defined within the same root configuration. When defining a resource block, there are important factors to consider. First, a declared resource is identified by its type and name. Therefore the resource name must be unique within the module. The resource type is a keyword associated with the provider and cannot be user-defined. A Cloud Storage bucket is associated with the Google Cloud provider and is defined by the keyword google_storage_bucket. Any user-defined type would result in an error when terraform plan or terraform apply is executed. All configuration arguments must be enclosed within the resource block body, which is between the curly brackets. An infrastructure element and its associated attributes are defined within a resource block. A Terraform configuration will not pass through the plan and apply phases successfully without all the required arguments defined within the configuration.

#### Meta-arguments for Resources

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508955

Let’s transition and examine resource meta-arguments. The Terraform language defines several meta-arguments, which can be used with any resource type to change the behavior of resources. Count creates multiple instances, depending on the value you define. for_each, creates multiple instances according to a map or set of strings. Depends_on is used to specify explicit dependencies. Lifecycle defines the lifecycle of a resource. With the lifecycle argument you can prevent destruction of a resource for compliance purposes, and create a resource before destroying the replaced resource. This approach is often used for high availability. Provider selects a non-default provider configuration. You can have multiple configurations for a provider, including default. This course covers count and for_each in detail. Let’s start with the count meta-argument. Perhaps you must deploy multiple VM instances. In the resource definition here, each Compute Engine instance is created with a separate google_compute_instance resource block. Writing code in this way is redundant. Instead, replace redundant code by adding the count argument at the beginning of the resource definition. The count argument tells Terraform to create three instances of the same kind. The expression on the second line, count.index, represents the index number of the current count loop. The count index starts at 0 and increments by 1 for each resource. Include the count index variable in strings using interpolation. When deployed, Terraform names the instances dev_VM1, dev_VM2, and dev_VM3. If your instances are almost identical, count is appropriate. If some of their arguments need distinct values that can't be directly derived from an integer, it's safer to use for_each. The for_each argument can be assigned to a string of values or a map. Terraform will create one instance for each member of the string. Consider a scenario where you need three similar instances configured in three specific zones and you want the names to have zones as prefixes for identification. Instead of writing lengthy repetitive code, you can use the for_each argument to assign specific values. The example code creates three instances in their respective zones: dev-us-central1-a, dev-asia-east1-b, and dev-europe-west4-a.

#### Resource Dependencies

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508956

Next up are resource dependencies. While building an infrastructure, you may prefer to have a visual representation of how your infrastructure is connected and interdependent. A dependency graph helps you understand your infrastructure before deploying it. Terraform builds a dependency graph from your configurations to generate plans and refresh state. The attributes are interpolated during run time, and primitives such as variables, output values, and providers are connected in a dependency tree. Terraform creates a dependency graph to determine the correct order of operations. In more complicated cases with multiple resources, Terraform will perform operations in parallel when it's safe to do so. Terraform can handle two kinds of dependencies: implicit and explicit. Implicit dependencies are known to Terraform, whereas explicit dependencies are unknown. Sometimes, one resource creation depends on the information generated from another. For example, you cannot create a compute instance unless the network is created. Similarly, you cannot assign a static IP address for a Compute Engine instance until a static IP is reserved. These dependencies are implicit. There are other scenarios where you induce a dependency. A given resource can only be created upon creation of another resource. In such cases, you would want to explicitly mention dependencies that Terraform cannot see. For example, let’s say you use a specific Cloud Storage bucket to run an application. That dependency is configured inside the application code and not visible to Terraform. In this scenario, you can use depends_on to explicitly declare the dependency. Let’s take a closer look at how Terraform handles implicit dependencies using another example. Here, the compute instance my_instance is created in a custom VPC called my_network. In Google Cloud, you cannot create a compute instance without a network. Terraform is informed of these relationships by interpolation expressions. Interpolation expressions should be used whenever possible. The reference to my_network in the network argument creates an implicit dependency on the network mentioned in the google_compute_network block. When you run terraform apply, you can view the order in which the resources are created. Terraform by default knows this order. Because of implicit dependency, Terraform is able to infer a dependency and knows it must create the network before creating the instance. In this example, Terraform creates the VPC my_network before creating the compute instance named my_instance. When Terraform reads the configuration, it will: First ensure that my_network is created before my_instance. Then save the properties of my_network in the state. And set the network argument in google_compute_instance to the value of the name argument in the google_compute_network block. Next, let’s examine explicit dependencies, which are not visible to Terraform. Explicit dependencies are defined with the depends_on argument within the dependent resource block. The depends_on argument gives you the flexibility to control the order in which Terraform processes the resources in a configuration. Depends_on can be used within the module block regardless of the resource type. The value can be an expression that directs to the resource. For example, let’s say you need to create two VMs—server and client —and want the client VM to only be created upon the successful creation of the server VM. This dependency is not visible to Terraform and has to be explicitly mentioned. You can use depends_on to explicitly declare the dependency of the client VM on the server VM. So, where in your configuration should these resource dependencies be defined? The order in which the resources are defined has no effect on how Terraform applies your changes, so organize your configuration files in a way that makes the most sense for you and your team. Upon executing terraform apply, you will notice that the client VM is created after the server VM. During interpolation of expression, Terraform processes the resource specified in the meta argument depends_on, and creates the server VM before creating the client VM.

#### Variables

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508957

So far in this module you’ve been hardcoding resource argument values, but what if you want to parameterize your configuration, and define argument values during the apply phase of the Terraform workflow? Perhaps you want to standardize your code but maintain the flexibility to customize resource attributes at runtime. This is where variables come in. With variables, you can parameterize values shared between resources. Input variables serve as parameters for Terraform, allowing easy customization and sharing without having to alter the source code. After a variable is defined, there are different ways to set its values at runtime, including environment variables, CLI options, and key-value files. In the code shown in the example, the name, location, and storage class are hardcoded. You can declare any of these attributes as a variable, and specify the details at run time. Variables separate source code from value assignments. Let’s explore how to declare an input variable. Variables must be declared in the variable block. It’s recommended that you save all variable declarations within a separate file named variables.tf. The label next to the keyword variable gives the variable a name. There are two rules for naming variables. First, the name of the variable must be unique within a module. Second, variable names cannot be keywords. There are no required arguments for a variable, so a variable block can be empty. Terraform can automatically deduce the type and default values. The type argument specifies value types that are accepted for the variable. Terraform supports the following primitive variable types: Bool, which is used for binary values such as true or false without the quotes. Number, which is used for numeric variables. And string, which is used for a sequence of unicode characters. Default is another meta argument, used to assign a default value to an attribute. To access the value of a variable declared within the module, you can use the expressions var. . In the example on the slide, the variable bucket_storage_class is formatted as var.bucket_storage_class in the resource block. The name of the variable in the variable block has to match the reference made within the resource block. The default value can be overridden by assigning a value in environment values, or . tfvars files or -var option. The folder structure shows that the variable declaration is written within the variables.tf file, which is a recommended best practice. The variable is assigned to the storage_class argument. The default value in quotes is used when creating the resource. The description documents the purpose of the variable. The description is displayed during the apply phase when the variable doesn't have a defined value. The description should explain the purpose of the variable and the expected value. The description string is often included in documentation, so it should be written from the perspective of the user rather than its maintainer. Comments can be used by the maintainer. Sensitive, as the name suggests, is a variable argument used to protect sensitive information from being displayed in command outputs or log files. The acceptable value for sensitive is true. When set to true, the value is marked sensitive in the output of terraform plan or terraform apply. This argument is beneficial when dealing with information such as database credentials or API tokens. Marking variables as sensitive will eliminate the accidental exposure of confidential information. In this example, the user_information variable is marked sensitive. The resource foo uses name and address, which are both sensitive variables. When terraform plan or apply is run, values are not displayed because they are marked as sensitive. There are several ways to set variable values at run time. First, you can use . tfvars files to quickly switch between and version sets of variables. You can also use CLI options. CLI options are useful for running quick examples on simple files. Environment variables are useful in scripts and pipelines. If a required variable has not been set by using one of the described methods, you can use the CLI prompt When you have many variable definitions to input, providing the value in the command line might not be feasible. Instead, specify the variables in a definitions file with either a . tfvars or . tfvars.json extension. Variable definitions follow the same syntax as HCL, but include only variable name assignments. Terraform automatically loads the variable definitions files as long as they are exactly named terraform.tfvars, terraform.tfvars.json, . auto.tfvars, or . auto.tfvars.json. The definition provided in the . tfvars file overrides the definition in the default argument and environment variable. If you want to specify the value of a variable individually on the command line, you can use the -var option. These values can be entered when running either terraform plan or terraform apply. This method is frequently used for automation, where the -var is sourced from another environment variable. It’s also useful when running quick examples on simple files. If you use a . tf extension, you must specify the filename using the -var-file option on the command line. Set variables can be overridden at deployment. You can reuse the variable file and still customize the configuration at deployment. The -var option takes the highest precedence over all other variable assignment methods. If the variable value is assigned using multiple methods, the value defined using the -var option is the value that is assigned to the variable. If a required variable has not been set using a described method, Terraform prompts you on the CLI. In this example, the variable is not parameterized and has not been assigned a value. Therefore the CLI prompts you to enter a value during the plan phase. You can also validate the value assigned to a variable by including a validation subblock with the variable block. The validation block includes a condition argument for which the validation rule is assigned. In this example, contains is used as the condition argument, and validates that the storage class value is upper case and an accepted value.

#### Variables Best Practices

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508958

Now that you learned how variables can be used in your configuration, let’s explore some best practices for using them. First, only parameterize values that vary for each instance or environment. When deciding whether to expose a variable, ensure that you have a concrete use case for changing it. If there's only a small chance that a variable might be needed, don't expose it. Changing or adding a variable with a default value is backward-compatible, removing a variable is not. For root modules, provide values to variables by using a . tfvars variables file. Avoid alternating between var-files and command-line options. Command-line options are ephemeral and easy to forget, and they cannot be checked into source control. Default variable files are more predictable. Give variables descriptive names that are relevant to their usage or purpose. Variables representing numeric values—such as disk sizes or RAM size—must be named with units. Google Cloud APIs don't have standard units, so following this naming convention clears the expected input unit for configuration maintainers. To simplify conditional logic, give boolean variables positive names, for example, enable_external_access. Finally, variables must have descriptions. Descriptions are automatically included in documentation, and add additional context for new developers.

#### Output Values

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508959

We’ve covered resources, meta-arguments, dependencies, and variables. Let's wrap up by exploring output values. Output values are similar to return values in common programming languages. With outputs, you can view information about the infrastructure resources you created on the command line. Output values are used for several purposes. The most common use case is to print root module resource attributes in the CLI after its deployment. Most of the server details are calculated at deployment and can only be inferred post-creation. Output values are also used to pass information generated by one resource to another. For example, you can extract server-specific values – such as an IP address – to another resource that requires this information. Output values are declared using the output block. The keyword ‘output’ indicates that the label associated with the keyword is the name of the output value. You can declare an output value anywhere within a configuration, but the recommended best practice is to declare them in a separate file named output.tf. The arguments that can be included within an output block are: value, which is a required argument that returns a value to the user of the module. Description, which is an optional argument used to provide an explanation of the purpose of the output and the value expected. The description is often used for documentation purposes. And sensitive, which is another optional argument used to mask the value to a resource attribute. This argument is useful to hide the accidental display of a resource attribute that is meant to be confidential, such as password information. In this example, we have declared an output value for the object named picture. After the object is successfully uploaded, the object URL is displayed on the screen. We recommend that you use output values, instead of user-supplied inputs, for the computed attributes of a resource. You can query all output values used in a project by running the terraform output command. Let’s explore best practices for output values. Only output useful information, such as computed information. Avoid outputting values that simply regurgitate variables or provide known information. For example, for a network resource, the following computed attributes are useful: id, which is an identifier for the resource Gateway_ipv4, which is the gateway address for default routing out of the network. And self_link - the URI of the created resource. As with variables, provide meaningful names and descriptions. Organize your code to include all output values in a file named output.tf. Finally, mark sensitive outputs. Instead of attempting to manually encrypt sensitive values, rely on built-in support for sensitive state management. When exporting sensitive values to output, make sure that the values are marked as sensitive. When a value is marked sensitive, the data is masked from the output of terraform plan or terraform apply command.

#### Terraform Registry and Cloud Foundation Toolkit

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508960

There are a couple resources available to help you write infrastructure code for Google Cloud. The Terraform Registry is an interactive resource for discovering a wide selection of integrations and configuration packages, otherwise known as providers and modules. The Registry includes solutions developed by HashiCorp, third-party vendors, and the Terraform community. The Registry provides plugins to manage any infrastructure API, pre-made modules to quickly configure common infrastructure components, and examples of how to write quality Terraform code. In addition, the Cloud Foundation Toolkit – also known as CFT – is available for you to use. CFT provides a series of reference modules for Terraform that reflect Google Cloud best practices. These modules can be used to quickly build a repeatable foundation in Google Cloud. CFT modules are also referred to as Terraform blueprints. We also have Cloud Foundation Fabric (CFF), a collection of Terraform modules and end to end examples meant to be cloned as a single unit and used for fast prototyping or decomposed and modified for usage in organizations. The repository on github provides end to end blueprints, and a suite of Terraform modules for Google Cloud, which support different use cases. For more information on the Cloud Foundation Toolkit, Terraform blueprints, and Cloud Foundation Fabric, links have been added to the student PDF in the Course Resources. With the Terraform Registry you can surface modules for all providers, such as Google, AWS, and so forth. CFT is a collection of Google Cloud Terraform modules built and maintained by Googlers. These modules are published to the Terraform Registry. CFT modules let you maintain the IAM roles for multiple projects within the same module, as opposed to updating roles for each project individually. This example shows a standard Terraform IAM binding. The binding is used to: Assign the network user, network group, and network admin role on project-one and project-two. And assign the App Engine user, App Engine group, and the App Engine admin role on project-one and project-two. The equivalent CFT module defines the role and user bindings for multiple projects within the same module. Infrastructure Manager, or Infra Manager, is a managed service that automates the deployment and management of Google Cloud infrastructure resources. Infrastructure is defined using Terraform and deployed onto Google Cloud by Infra Manager, enabling you to manage resources using Infrastructure as Code. Infra Manager does not manage the deployment of applications onto your resources. To manage application deployment, you can use Google Cloud products like Cloud Build and Cloud Deploy. You can also use third-party tools or your own toolchain. For more information, refer to the Infrastructure Manager documentation.

#### Lab Intro: Creating Resource Dependencies with Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508961

In this lab, you will create two VMs in the default network. You will use variables to define the VM's attributes at runtime, and use output values to print a few resource attributes. You will then add a static IP address to the first VM to examine how terraform handles implicit dependencies. Finally, you will create a Google Cloud Storage bucket by mentioning explicit dependency to the VM to examine how terraform handles explicit dependency. Writing infrastructure code is a fundamental skill needed to manage your infrastructure on Google Cloud. In this module, you learned how to declare resources: infrastructure elements you can configure using Terraform. In addition, you examined resource blocks. You also learned how to specify resource dependencies, and how and why to use variables and output values within a configuration. This module also explained the Terraform Registry and the Cloud Foundation Toolkit, two tools for simplifying the code writing process.

#### Creating Resource Dependencies with Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/labs/508962

#### Quiz: Writing Infrastructure Code for Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/443/quizzes/508963

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508964

Writing infrastructure code is a fundamental skill needed to manage your infrastructure on Google Cloud. In this module, you learned how to declare resources: infrastructure elements you can configure using Terraform. In addition, you examined resource blocks. You also learned how to specify resource dependencies, and how and why to use variables and output values within a configuration. This module also explained the Terraform Registry and the Cloud Foundation Toolkit, two tools for simplifying the code writing process.

### Organizing and Reusing Configuration with Terraform Modules

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508965

Welcome to Module 4: Organizing and Reusing Configuration with Terraform Modules. Abstracting and modularizing your code is a common practice In the programming world. As your infrastructure grows, so does your code base, and your team will have to spend a fair amount of time to understand the code, change it, test it, and then deploy it. The convenience that abstraction brings along with the flexibility to standardize code is undeniable. Developers commonly refer to the “Don’t Repeat Yourself” principle, also known as DRY. The idea of DRY is to avoid repeating the same set of code multiple times. In this module, we’ll explore how Terraform implements the DRY principle using modules. With modules, you can group sets of resources together so you can reuse them later. Should you have to update code, you’ll only need to do so in one location. This module explores use cases for modules and how to use them when writing your infrastructure code. We’ll start by describing a scenario in which modules play a critical role in building efficient code. Upon completion of this module, you’ll be able to define Terraform modules and use them to reuse configuration. To simplify the code writing process, you’ll learn how to use publicly available modules on Terraform or GitHub. You’ll also explore how to use input variables to parameterize configurations, and output values to access resource attributes outside the module. In addition, you’ll learn best practices for using modules, and examine how modules can be useful when working in different environments, such as development, testing, and production. You’ll conclude with a lab to apply the concepts that were covered. Ready? Let’s dive in!

#### Introduction to Terraform Modules

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508966

Let’s get started with an introduction to Terraform modules. Before defining modules, let’s view a simple scenario demonstrating why modules are necessary. Let’s say you must create a web server in a custom network. Typical attributes that represent the web server include: The machine type, disk, a static IP address, and a Google service account. The associated code to deploy a server with these attributes is similar to the code shown in this example. What if you want to deploy several servers of the same kind? You might have to manually copy the code. What if you have to update one attribute for all these servers? You have to manually find and update that attribute in every occurrence. When new resources are added or the code is updated, the code becomes bigger, unmanageable, and harder to read. Copying the code becomes a cumbersome process where every little change has to be applied error-free across all environments. It also makes the code inefficient. Similar, duplicated blocks can cause discrepancies when updated. In the programming world, disadvantages of code repetition are addressed with the “Don’t Repeat Yourself” principle, also known as DRY. The idea of DRY is to avoid repeating the same set of codes multiple times. Instead, replace it with abstraction to avoid redundancy. This practice encourages building efficient code that is readable and reusable. In general-purpose programming languages like Ruby, Java, and Python, functions are used to implement the DRY principle. Any piece of code that has to be manually copied is placed inside a function and reused across the main code. With Terraform, you can place your reusable code inside a module and reuse that module in multiple places. Define the reusable code in a module named server, so that any changes you make to the module are reflected across all the environments that you plan to reuse. The definition of a module ia a group of one or more Terraform configuration files in a directory. Modules allow you to group a set of resources together and reuse them later, and they can be referenced from other modules. So far, we have unintentionally written our terraform configuration in a module, called the root module. The root module is the directory from which you run terraform commands. Notice that the root module consists of . tf files that are stored in your working directory. This working directory is where terraform plan and terraform apply are run. Other modules and resources are instantiated in the root module. Now that you know what modules are and why they’re useful, let’s examine the creation process. In this example, let’s create two modules, one called “server, and the other called “network”. The network module will contain networking resources to host the server. The server module will contain necessary compute resources that define the server. To create a module, first create a directory of Terraform files. In this example, create two directories, named network and server. Each directory has its own main.tf file. Next, enter the code in the main.tf file to create a custom network within the network directory. Notice that multiple resources are grouped in the network module. In the main.tf file of the server directory, enter the code to create a virtual server. And that’s all there is to it! Two modules have been created. At this point, you created the modules and wrote the configuration code, but this process does not create the infrastructure resources. These modules are instantiated when they are called with the terraform apply command, which you’ll learn about in the next lesson.

#### Modules use cases and benefits

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508967

Let’s next look into where and how modules can be useful. Terraform is a tool for managing your infrastructure as code. But imagine having a single Terraform configuration file for managing your entire cloud environment. Sounds challenging, right? If you have a single Terraform file for all the infrastructure components within your environment, the code will be unreadable. As your business scales, your code inevitably will too. Thus you should break your code into reusable modules so that it’s readable and easy to manage. Modules are used to eliminate repetition. When a set of resources is repeated, the code becomes longer and more difficult to read. Modules are used to eliminate repetition. You can standardize the configuration by placing code in a module. This is useful for when a set of resources must be created in a specific way. When the module is called, the same source code will be used. For example, if you want to deploy database servers with encrypted disks, you can ensure encryption by hardcoding the encryption configuration in a module. You can then call the module when a disk has to be created. Now let’s explore the benefits of using modules in your configuration. First, modules enhance the readability of code by eliminating lines with a call to the source module. They’re reusable. You can write code once and reuse it multiple times across various environments. They’re abstract. You can separate configurations into logical units, reducing their dependency and making them easier to debug. With modules, you can also package sets of resource configurations for consistent resource replication.

#### Reuse Configurations with Modules

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508968

Now that modules are created, the next step is to call the modules from the parent main.tf file. To call a module means to reuse it in your configuration. You can call the module to reference the code in the module block. In this example, main.tf is the calling module. The parent main.tf file uses the source argument to call the server and network modules. Run the terraform init command to download any modules referenced by a configuration. The source argument determines the location of the module source code. Every module you call within Terraform requires a mandatory source argument, which is a meta-argument within the module block. This value can either be a local path within the root directory or a remote path to a module source that Terraform downloads. Different source types supported by Terraform include Terraform Registry, GitHub, Bitbucket, HTTP URLs, and Cloud Storage buckets This course covers local paths, Terraform Registry, and GitHub. Let’s start with the local path. Local path is used to reference a module stored within the same directory as the calling module. For example, if the module that you want to call is stored in a directory named “servers”, located in the same directory as your root module, your root configuration will look like the example on screen. A local path starts with either . / or . ./. Local paths are unique when compared to other module sources, because they do not require any installation. The files are locally referenced from the child module to the parent module directly. Therefore no explicit update is required. Terraform Registry contains a directory of publicly available modules for various infrastructure components, such as load balancers, SQL instances, and so forth. These publicly available modules are incredibly useful for complex deployments. The registry source address has to be in the format //. To use the code published in a Terraform Registry for the Google Cloud provider, use the format terraform-google-modules/gcloud/google. To avoid any unwanted changes to your Terraform configuration, it’s recommended that you use version constraint. The version argument can be assigned a version string. The version string allows Terraform to automatically upgrade the module to new patch releases while still keeping a solid target. Only the modules installed from the Terraform Registry support this constraint. After Terraform Registry, the most commonly used remote source is the GitHub repository. Similar to Terraform Registry, you can directly enter the GitHub URL where the source code is located.

#### Variables and Outputs

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508969

Now let’s examine how you can use variables to parameterize a module, and outputs to pass resource attributes outside a module. The examples covered so far all had hardcoded resource attributes. For example, the name of the network is hardcoded in the main.tf file within the server module. If you use the server module more than once, you will receive name conflict errors when you execute terraform plan or terraform apply. In such scenarios, you must have the flexibility to configure a few attributes differently in different environments, and standardize them in others. With variables, you can customize aspects of modules without altering the source code. Variables are also useful when working with different environments such as development, production, and staging. It’s a best practice to run a small machine type in the staging environment and a larger machine type in the production environment. Let’s view an example of how to parameterize the name argument with input variables. The first step is to replace the hardcoded values within your module with a variable. Assign the argument that you want to parameterize – in this case network name – by using the format var. . This assignment provides the flexibility to configure the name argument when the module is called. Then declare the attributes that are parameterized in the variables.tf file. Place the declared variables in the same directory as the defined resource. Be sure to specify the appropriate variable type. Pass the value to the input variable when you call the module. Since you parameterized the network name, you can now reuse the same network module twice in the main configuration and provide a different name for each instance. Remember to run the terraform init command to download any modules referenced in a configuration. Unlike root configuration, you cannot pass values to variables at run time. To pass resource arguments from one module to another, the argument must be configured as an output value in Terraform. For example, the network module includes the network name. The server module should receive the network name from the module that defines the network. If configured in two separate modules, pass this value by using the output values. To pass the network name from the network module, declare the network name as the output value to expose the resource attribute outside of the module. Then define the network name as a variable in the server module, so that it can accept the values passed outside the module. Now, in the main configuration, reference the output value when calling the server module. You can refer to the output value by using the format module... In our example, we refer to the output value of the my_network module using module.my_network_1.network_name. Notice that both modules are called from the root configuration. Each time a module is instantiated, terraform init has to be executed. After you run terraform init, you can pass the network name from the network module to the server module.

#### Best Practices and a Real Time Scenario

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508970

Finally, let’s examine some best practices for using modules. Don’t over-use modules. Modularize your code to keep your codebase DRY. Try not to fixate on eliminating all the duplications in your configurations. Duplication allows for more explicit configuration, and the infrastructure is easier to visualize. If you want to loop over multiple values, use Terraform features such as count or for_each instead of building your own custom scripting. Parameterize modules intelligently only if they make sense for end users to change. For example, in a custom network module, the MTU and routing_mode can be standardized, but its name has to be parameterized to ensure reusability. When considering parameterization, focus on the values that you must change. If a value is always fixed in your environment, hardcoding is okay. Use local modules to organize and encapsulate your code. Using local modules to organize your configuration from the beginning will significantly reduce the burden of maintenance as your infrastructure grows in complexity. Use the public Terraform Registry to find modules. By using modules from the Registry, you can quickly and confidently implement your configuration. Publish and share modules with your team. Modules can be used across your team to create and maintain infrastructure. You can publish modules either publicly or privately. To wrap up the module, let’s view a scenario describing how modules can help manage complex infrastructure in Terraform. In a typical application development environment, when a new feature is added or if an existing feature is modified, code is approved by passing it from development to staging, and then to production. Each environments has the same type of resources but differ in their quantity. Let’s assume that the current directory structure for this application includes development, staging, and production folders. When creating a new resource, the developer will add code to the development environment first. The code is then approved and tested. Without modules, the code is manually copied to staging and then to production. The solution is to define the reusable code within a module named server. Then, any change you make to the module is reflected across all the environments that you plan to reuse. You can now reference the code without having to make the changes manually across all the environments. In the example shown, the development, staging, and production environment can reuse the same module, but have different names and deploy a different number of servers.

#### Lab Intro: Automating the Deployment of Infrastructure Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508971

In this lab, you create a Terraform configuration with a module to automate the deployment of Google Cloud infrastructure. Specifically, you deploy one auto mode network with a firewall rule and two VM instances. The instance module allowed you to re-use the same resource configuration for multiple resources while providing properties as input variables. You can leverage the configuration and module that you created as a starting point for future deployments.

#### Automating the Deployment of Infrastructure Using Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/443/labs/508972

#### Quiz: Organizing and Reusing Configuration with Terraform Modules

- https://www.cloudskillsboost.google/paths/11/course_templates/443/quizzes/508973

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508974

This brings us to the end of Module 4. This module was all about – well – modules! In Terraform, modules are used to reuse configuration so you can write efficient, readable, and reusable code. Let’s review what was covered. You learned the definition of a module, viewed a few examples, and learned how you can use them to reuse configurations. In addition, you learned how to use input variables to parameterize configurations, and output values to access resource attributes outside the module. This module concluded with best practices and use cases for modules.

### Introduction to Terraform State

#### Module Overview

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508975

Hello, and welcome to the final module of this course: Terraform States. This one will be relatively short, introducing you to the basics of Terraform State. To ensure efficient Terraform workflows and foster collaboration, it is imperative that infrastructure changes are tracked and monitored. In Terraform, you can use the state to achieve these objectives. The Terraform State stores metadata for your infrastructure configuration, so you can keep track of your configuration. Put simply, as its name suggests, a state describes the state of your infrastructure resources. The module starts with an introduction to Terraform state. You’ll then learn about the different ways to store Terraform state. Later in the module you’ll explore the benefits of storing the state file in a remote location. While there are many remote locations in which you can store the state file, this module describes how to store it in a Google Cloud Storage Bucket. You’ll wrap up the module by learning best practices for working with state files. Let’s dive in.

#### Introduction to Terraform State

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508976

Let’s get started with an introduction to terraform state. Terraform state is a metadata repository of your infrastructure configuration. Terraform saves the state of the resources that it manages in a state file. By default, state is stored locally in a file named "terraform.tfstate", but it can also be stored remotely, which is recommended for team environments. Terraform uses the local state to create plans and change your infrastructure. Before any operation, Terraform does a refresh to update the state with the real infrastructure. The primary purpose of Terraform state is to store bindings between objects in a remote system and resource instances declared in your configuration. When Terraform creates a remote object in response to a change of configuration, it will record the identity of that remote object against a particular resource instance. Terraform then potentially updates or deletes that object in response to future configuration changes. Every infrastructure component created within the resource block is identified by its name within the terraform state. When a terraform configuration is applied for the first time, infrastructure resources are created and a state file is automatically generated with a reference to the name mentioned in the resource block. If a resource already has an entry within the Terraform state file, then Terraform compares the configuration with the state file and the current live state. Based on the comparison, a plan is generated. When the plan is applied, it updates the resource to match the configuration defined. Once the configuration is applied, the Terraform state file is updated to reflect the current state of the infrastructure. If arguments cannot be updated in-place due to remote API limitations, then the resource is destroyed and re-created. If a resource is removed from the current configuration but has an entry in the state file, then terraform compares the configuration and destroys the resources that no longer exist.

#### Storing State Files

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508977

Now that you’re familiar with state files and their purpose, let’s transition to state file storage. Terraform, by default, saves local state files in the current working directory with a . tfstate extension, so they don’t require additional maintenance. Local states work well when there’s only one developer working on a project. The default configuration can get tricky when multiple developers run Terraform simultaneously and each machine has its own understanding of the current infrastructure. If you’re working in a team where multiple developers are writing code to manage the infrastructure, then you should store state files remotely in a central location. That way when your infrastructure is changed, your Terraform state file is updated and synced, and your team will always be working with up-to-date infrastructure. Let’s examine some issues you may face when using local states for team environments. Local states do not have shared access. To update your infrastructure using Terraform, each member of your team needs access to the same state files. That means that those files must be stored in a shared location. You can’t lock local state files. If two team members are running Terraform at the same time, they might encounter RACE conditions because multiple Terraform processes are making updates to the state files. These conditions can lead to conflicts, data loss, and state file corruption. Local state files are not confidential. The information is in plain text, often exposing sensitive data such as database credentials. Now let’s observe how storing the state file in a remote location addresses these issues. First, when stored remotely, the state file is automatically updated. Once you configure a remote backend, Terraform will automatically load the state file from the backend every time you run the plan or apply commands. It will also automatically store the state file in that backend after each apply, so there’s no chance of manual error. Remote Cloud Storage buckets natively support state locking. The file can be locked so that if multiple developers run terraform apply simultaneously, it won’t be corrupted by simultaneous updates. Remote file storage is also more secure than local. Cloud Storage buckets natively support encryption in transit and encryption on disk. In addition, Cloud Storage includes several ways to configure access permissions, so you can control who has access to your state files. For example you can use IAM policies with a bucket. Though Cloud Storage buckets are encrypted at rest, you can use customer-supplied encryption keys to provide an added layer of protection. You can provide an extra layer of protection with the GOOGLE_ENCRYPTION_KEY environment variable. Even though secrets shouldn’t be in the state file to start with, always encrypt the state as an extra measure of defense. Let’s walk through how to store a Terraform state remotely in a Cloud Storage bucket. First, add the google_storage_bucket resource to a Terraform configuration file, such as main.tf. In the code snippet, the location field is hardcoded to US, meaning a multi-region bucket in the US will be created. You can change this field to a location of your choice. After you’ve added the code, run terraform apply to create the storage bucket. Next, add the code to a new Terraform configuration file called backend.tf. Ensure that the bucket name is updated to match the name of your new Cloud Storage bucket. Run terraform init to configure your Terraform backend. Terraform detects that you already have a state file locally and prompts you to copy it to the new Cloud Storage bucket. Enter yes. After you run terraform init, your Terraform state is stored in the Cloud Storage bucket. Terraform pulls the latest state from this bucket before running a command, and pushes the latest state to the bucket after running it. Here is a snippet of the state file from the Cloud Storage bucket. The state includes the metadata of the resources created, such as the resource type, resource name and the provider name.

#### Terraform State Best Practices

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508978

Let’s explore some best practices for Terraform state optimization and security. First, use remote state for team environments, so that you can lock and version state files. Google Cloud customers should use the Cloud Storage state backend to lock the state, separate sensitive information from version control, and ensure that only the build system and highly privileged administrators have access to the remote state bucket. To prevent accidentally committing development state to source control, use gitignore for Terraform state files. Don't store secrets in a state. Many resource and data providers store secret values in plain text in the state file. Where possible, avoid storing secrets in a state file. To add an extra layer of defense, always encrypt the state file. Even though Cloud Storage buckets are encrypted at rest, customer-supplied encryption keys provide an added layer of protection. Don’t modify Terraform state manually. The state file is critical for maintaining mapping between the Terraform configuration and Google Cloud resources. Corruption can lead to major infrastructure problems.

#### Lab Intro: Creating a Remote Backend

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508979

This is going to be a short lab. In this lab, you will create a local backend and then create a Cloud Storage bucket to migrate the state to a remote backend.

#### Creating a Remote Backend

- https://www.cloudskillsboost.google/paths/11/course_templates/443/labs/508980

#### Quiz: Introduction to Terraform State

- https://www.cloudskillsboost.google/paths/11/course_templates/443/quizzes/508981

#### Module Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508982

This brings us to the end of the final module in the Getting Started with Terraform for Google Cloud course. You are now ready to use Terraform to write your own IaC configuration to manage your Google Cloud infrastructure. This short module covered the Terraform state and listed the benefits of storing the state in a remote location, including in a Google Cloud Storage Bucket. You also learned some best practices for states and applied what you learnt in the lab.

### Course Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/11/course_templates/443/video/508983

Rekha: Congratulations! You’ve reached the end of the Getting Started with Terraform for Google Cloud course. Eoin: This course covered a lot, from Terraform and its workflow, to Infrastructure as Code and configuration files, to modules and state files. Let’s recap the key points. Eoin: You learned how to describe Terraform, Infrastructure as Code, and the HashiCorp Language. All are tools for provisioning resources and managing infrastructure on Google Cloud. This course explained the features and functionalities of Terraform, as well as the benefits of IaC – a declarative, manageable, auditable, and portable approach to infrastructure management. You explored the Terraform workflow, from author to apply. This course also explored how to use Terraform code constructs such as resources, variables and output values to create Google Cloud infrastructure resources. We also explored how to build reusable components with modules. In addition to being reusable across your code, modules are a great way to improve readability. Finally, this course included an introduction to the state file, where infrastructure metadata is stored. Eoin: That’s it for this course. You are now familiar with Terraform, how to use it, and its benefits. Feel free to get started writing your own code to provision and manage your Google Cloud infrastructure using Terraform.

#### Course Resource: All Slides

- https://www.cloudskillsboost.google/paths/11/course_templates/443/documents/508984

### Your Next Steps

## 11: Implementing Cloud Load Balancing for Compute Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/648

### Implementing Cloud Load Balancing for Compute Engine

#### Set Up Network Load Balancers

- https://www.cloudskillsboost.google/paths/11/course_templates/648/labs/567898

#### Set Up Application Load Balancers

- https://www.cloudskillsboost.google/paths/11/course_templates/648/labs/567899

#### Using an Internal Application Load Balancer

- https://www.cloudskillsboost.google/paths/11/course_templates/648/labs/567900

#### Implement Load Balancing on Compute Engine: Challenge Lab

- https://www.cloudskillsboost.google/paths/11/course_templates/648/labs/567901

### Your Next Steps

## 12: Set Up an App Dev Environment on Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/637

### Perform Foundational Infrastructure Tasks in Google Cloud

#### Cloud Storage: Qwik Start - Cloud Console

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526668

#### Cloud Storage: Qwik Start - CLI/SDK 

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526669

#### Cloud IAM: Qwik Start

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526670

#### Cloud Monitoring: Qwik Start

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526671

#### Cloud Run Functions: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526672

#### Cloud Run Functions: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526673

#### Pub/Sub: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526674

#### Pub/Sub: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526675

#### Pub/Sub: Qwik Start - Python

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526676

#### Set Up an App Dev Environment on Google Cloud: Challenge Lab

- https://www.cloudskillsboost.google/paths/11/course_templates/637/labs/526677

### Your Next Steps

## 13: Develop your Google Cloud Network

- https://www.cloudskillsboost.google/paths/11/course_templates/625

### Set up and Configure a Cloud Environment in Google Cloud

#### Cloud IAM: Qwik Start

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566599

#### Introduction to SQL for BigQuery and Cloud SQL

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566600

#### Multiple VPC Networks

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566601

#### Cloud Monitoring: Qwik Start

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566602

#### Managing Deployments Using Kubernetes Engine

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566603

#### Develop your Google Cloud Network: Challenge Lab

- https://www.cloudskillsboost.google/paths/11/course_templates/625/labs/566604

### Your Next Steps

## 14: Build Infrastructure with Terraform on Google Cloud

- https://www.cloudskillsboost.google/paths/11/course_templates/636

### Build Infrastructure with Terraform on Google Cloud

#### Terraform Fundamentals

- https://www.cloudskillsboost.google/paths/11/course_templates/636/labs/555699

#### Infrastructure as Code with Terraform

- https://www.cloudskillsboost.google/paths/11/course_templates/636/labs/555700

#### Interact with Terraform Modules

- https://www.cloudskillsboost.google/paths/11/course_templates/636/labs/555701

#### Managing Terraform State

- https://www.cloudskillsboost.google/paths/11/course_templates/636/labs/555702

#### Build Infrastructure with Terraform on Google Cloud: Challenge Lab

- https://www.cloudskillsboost.google/paths/11/course_templates/636/labs/555703

### Your Next Steps


# Associate Data Practitioner Learning Path

- https://www.cloudskillsboost.google/paths/1336

[TOC]

## 01: Introduction to Data Engineering on Google Cloud

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157

### Course Introduction

#### Course Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521317

Hello, and welcome to Introduction to Data Engineering on Google Cloud. I'm Damon, and I am a curriculum developer at Google. Whether you already work in data engineering and want to learn how to be successful on Google Cloud, or you are looking to progress in your career, this course will help you get started. Through a series of lectures, quizzes, and hands-on labs, you will learn the fundamentals of data engineering on Google Cloud. This course was designed for data engineers or anyone interested in preparing and sorting datasets for further usage in their organization. This involves using tools such as, but not limited to, Dataflow, Dataproc, Cloud Composer, BigQuery, Bigtable, and Datastream. In this course, you learn about the duties and responsibilities of a data engineer, identify data engineering tasks and core components used on Google Cloud to accomplish those tasks, understand how to create and deploy data pipelines of varying patterns on Google Cloud, and identify and utilize various automation techniques on Google Cloud to complete data engineering tasks. The course is divided into six modules designed to address the learning objectives. First, you look at data engineering tasks and components on Google Cloud. Next, you explore data replication and migration. Then you explore each of the three main data pipeline patterns; extract and load, extract, load, and transform, and extract, transform, and load. You conclude the course by examining automation techniques important to a data engineer.

### Data Engineering Tasks and Components

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521318

In this module, first you'll learn about the role of a data engineer. Second, we will cover the differences between a data source and a data sync. Then, we will review different types of data formats that a data engineer will encounter. The next topic addresses the options for storing data on Google Cloud. Then we cover the choices available for metadata management. Finally, we will look at the features of Analytics Hub, that allow you to easily share datasets both within and outside your organization.

#### The Role of a Data Engineer

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521319

What does a data engineer do? At a basic level, a data engineer builds data pipelines. Why does the data engineer build data pipelines? Because they want to get their data into a place such as a dashboard, or report, or machine learning model, from where the business can make data-driven decisions. The data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is by itself not very useful. Once data becomes useful, the data engineer will often apply updates or transformations to add new value to the data. Of course, new data environments require data management practices to ensure currency and accuracy. Finally, data engineers create processes and operations to move data usage into production settings. In the most basic sense, a data engineer moves data from data sources to data syncs in four stages: replicate and migrate, ingest, transform, and store. The replicate and migrate stage of a data pipeline focuses on the tools and options to bring data from external or internal systems into Google cloud for further refinement. There are a wide variety of tools and options at your disposal. They will be covered in more detail throughout this course.

#### Data Sources Versus Data Sinks

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521320

The ingest stage of a data pipeline is the point where data becomes a data source and is available for usage downstream. Think of a data source as the starting point of your data journey. It is raw, unprocessed data waiting to be transformed into valuable insights. Any system, application, or platform that creates, stores, or shares data can be considered a data source. Two examples of Google Cloud products used in the ingest phase are Cloud storage, a data lake holding various types of data sources, and Pub/Sub, an asynchronous messaging system delivering data from external systems. The transform stage of a data pipeline represents action taken on a data source to adjust, modify, join, or customize a data source so that it matches a specific downstream data or reporting requirement. There are three main transformation patterns: extract and load, extract, load, and transform, and extract, transform, and load. You explore each of these patterns in their own modules later in the course. The store stage of a data pipeline represents the last step when we deposit data in its final form. A data sync is the final stop in the data journey. It's where processed and transformed data is stored for future use, analysis, and decision-making. Think of it as the reservoir at the end of the river, where valuable information is collected and readily available. Two examples of Google Cloud products used in the store phase are BigQuery, a serverless data warehouse, and Bigtable, a highly scalable no SQL database.

#### Data Formats

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521321

Data exists in two primary formats, unstructured and structured. Unstructured data is information stored in a non-tabular form, such as documents, images, and audio files. Unstructured data is usually suited for Cloud Storage, but BigQuery also offers the capability to store unstructured data via object tables. There is also structured data, which represents information stored in tables, rows, and columns.

#### Storage Solution Options on Google Cloud

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521322

There are several key products on Google Cloud that are used by data engineers. One main product is Cloud storage. Unstructured data is usually well suited to be stored in Cloud Storage. Within Cloud Storage, objects are accessed by using HTTP requests, including ranged GETS to retrieve portions of the data. The only key is the object name. There is object metadata, but the object itself is treated as unstructured bytes. The scale of the system allows for serving large static content and accepting user-uploaded content including videos, photos, and files. Objects can be up to five terabytes each. Cloud Storage is built for availability, durability, scalability, and consistency. It's an ideal solution for hosting static websites and storing images, videos, objects, and blobs, and any unstructured data. Cloud Storage has four primary storage classes; standard storage, nearline storage, coldline storage, and archive storage. The classes are differentiated by the expected period of object access. You have a full range of cost effective storage services for structured data to choose from when developing with Google Cloud. No one size fits all, and your choice of storage and database solutions will depend on your application and workload. Cloud SQL is Google Cloud's managed relational database service. AlloyDB is a fully managed, high- performance PostgreSQL database service from Google Cloud. Spanner is Google Cloud's fully managed relational database service that offers both strong consistency and horizontal scalability. Firestore is a fast, fully managed, serverless, NoSQL document database built for automatic scaling, high performance, and ease of application development. BigQuery is a fully managed, serverless enterprise data warehouse for analytics. Bigtable is a high-performance NoSQL database service. Bigtable is built for fast key-value lookup and supports consistent sub-10 millisecond latency. The two key concepts in data engineering are that of the data lake and the data warehouse. A data lake is a vast repository for storing raw unprocessed data in various formats, including unstructured, semi-structured, and structured. It serves as a centralized storage solution for diverse data types, enabling flexible use cases like data science, applications, and business decision making. A data warehouse is a structured repository designed for storing pre-processed and aggregated data from multiple sources. Primarily used for long term business analysis, it enables efficient querying and reporting for informed decision making. Data warehouses often operate as standalone systems, independent of other data storage solutions. BigQuery is a fully managed, serverless enterprise data warehouse for analytics. BigQuery has built-in features like machine learning, geospatial analysis, and business intelligence. BigQuery can scan terabytes in seconds and petabytes in minutes. BigQuery is a great solution for online analytical processing, or OLAP, workloads for big data exploration and processing. BigQuery is also well-suited for reporting with business intelligence tools. BigQuery has several easy to use options for accessing data. The first is via the Google Cloud console's SQL editor. The second is via the bq command line tool which is part of the Cloud SDK. The last is via a robust REST API which supports calls in seven programming languages. BigQuery organizes data tables into units called datasets. These datasets are scoped to your Google Cloud project. When you reference a table from the command line in SQL queries or code, you refer to it by using the construct, project.dataset.table. Access control is through IAM and is at the dataset, table, view, or column level. In order to query data in a table or view, you need at least read permissions on the table or view.

#### Metadata Management Options on Google Cloud

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521323

Metadata is a key element to making data more manageable and useful across an organization. Dataplex is a comprehensive data management solution that allows you to centrally discover, manage, monitor, and govern distributed data across your organization. With Dataplex, you can break down data silos, centralize security and governance, while enabling distributed ownership, and easily search and discover data based on business contexts. Dataplex also offers built-in data intelligence, support for open-source tools, and a robust partner ecosystem, helping you to trust your data and accelerate time to insights. Dataplex lets you standardize and unify metadata, security policies, governance, classification, and data life cycle management across this distributed data. Another common use case is when your data is accessible only to data engineers, and is later refined and made available to data scientists and analysts. In this case, you can set up a lake to have the following: A raw zone for the data, which is accessed by data engineers and data scientists. A curated zone for the data, which is accessed by all users.

#### Sharing Datasets using Analytics Hub

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521324

Sharing data is challenging especially outside of your organization. You need to consider security and permissions, destination options for data pipelines, data freshness and accuracy, and finally, usage monitoring. Analytics Hub was created to meet these data sharing challenges. Analytics Hub helps organizations unlock the value of data sharing, leading to new insights and business value. With Analytics Hub, you create a rich data ecosystem by publishing and subscribing to analytics-ready datasets. Because data is shared in place, data providers are able to control and monitor how their data is being used. Analytics Hub provides a self-service way to access valuable and trusted data assets, including data provided by Google. Finally, Analytics Hub provides an opportunity to monetize data assets. Analytics Hub removes the tasks of building the infrastructure required for monetization.

#### Lab Intro: Loading Data into BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521325

In this lab, you practice loading data into BigQuery. The primary objective of this lab is to load data into BigQuery using both the command-line interface and the Google Cloud console. You also experience loading several datasets into BigQuery and using the Data Description Language, or DDL.

#### Loading data into BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521326

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521327

### Data Replication and Migration

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521328

In this module, first, you review the baseline Google Cloud data replication and migration architecture. Second, you will cover the options and use cases for the gcloud command line tool. Then you will review the functionality and use cases for the Storage Transfer Service. The next topic addresses the functionality and use cases for the Transfer Appliance. Finally, you will look at the features and deployment of Datastream.

#### Replication and Migration Architecture

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521329

The replicate and migrate stage of a data pipeline focuses on the tools and options to bring data from external or internal systems into Google Cloud for further refinement. Google Cloud provides a comprehensive suite of tools to migrate and replicate your data. Start replicating and migrating data by using tools like the 'gcloud storage' command, Transfer Appliance, Storage Transfer Service, or Datastream. You can then transform the data as needed before finally storing it within Google Cloud. Data can originate from on-premises or multi-cloud environments, including file systems, object stores, HDFS, and relational databases. Google Cloud offers options for one-off transfers, scheduled replications, and change data capture, ultimately landing data in Cloud Storage or BigQuery. Google Cloud provides additional workload migration with options for various database types. Leverage Database Migration Service for seamless transitions from Oracle, MySQL, PostgreSQL, and SQL Server. For other data formats or complex migrations, use ETL tools like DataFlow with a wide range of templates that handle NoSQL or non-relational databases. Your target destination can be Cloud SQL, AlloyDB, or BigQuery, depending on your needs. The ease of migrating data depends heavily on data size and network bandwidth. With one terabyte of data, a 100 gigabits per second network takes about two minutes to transfer, while the same size on a 100 megabits per second network takes 30 hours. The 'gcloud storage' command or Storage Transfer Service are suitable for smaller datasets. For larger datasets, consider Transfer Appliance for faster offline transfer.

#### The gcloud Command Line Tool

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521330

You can use the 'gcloud storage' command to transfer small to medium-sized datasets to Cloud Storage. The data can originate from various on-premise sources like file systems, object stores, or HDFS. The 'cp' command, shown in the code snippet, facilitates these ad-hoc transfers directly to Cloud Storage.

#### Moving Datasets

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521331

To move larger datasets, consider using Storage Transfer Service. Storage Transfer Service efficiently moves large datasets from on-premises, multicloud file systems, object stores (including Amazon S3 and Azure Blob Storage), and HDFS into Cloud Storage. It boasts high transfer speeds (up to tens of gigabits per second) and supports scheduled transfers for convenient data migration. Transfer appliance is Google's solution for moving massive datasets offline. Google provides the hardware, you transfer your data onto it, then ship it back. It is ideal for scenarios with limited bandwidth or very large transfers, and it comes in multiple sizes to suit your needs.

#### Datastream

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521332

Datastream enables continuous replication of your on-premises or multi-cloud relational databases such as Oracle, MySQL, PostgresSQ,L or SQL Server into Google Cloud. Datastream offers change data capture options for historical backfill or allows you to just propagate new changes with data landing in Cloud Storage or BigqQuery for analytics. You have flexibility in connectivity options and can selectively replicate data at the schema, table, or column level. Datastream enables real-time data replication from source systems for various use cases. It supports direct replication into BigQuery for analytics, allows custom data processing in Dataflow before loading into BigQuery, and facilitates event-driven architectures. Additionally, Datastream can be used with Dataflow templates for seamless database replication and migration tasks, making it a versatile tool for integrating data into Google Cloud. Datastream taps into the source database's write-ahead log (WAL) to capture and process changes for propagation downstream. Datastream supports reading the logging mechanisms for the specific source database such as LogMiner for Oracle, binary log for MySQL, PostgreSQL's logical decoding, and transaction logs from SQL Server. These change events such as inserts, updates, and deletes are then processed by Datastream and transformed into structured formats like Avro or JSON, ready for storage in Google Cloud, typically in BigQuery tables, enabling near real-time data replication for analytics and other use cases. Datastream event messages contain two main sections: generic metadata and payload. Metadata provides context about the data, like source table, timestamps, and related information. Payload contains the actual data changes in a key-value format, reflecting column names and their corresponding values. This structure allows for efficient and organized data replication and tracking of changes. Datastream event messages also include source-specific metadata in addition to generic metadata and payload. This metadata provides context about the data's origin within the source system, including details like the database name, schema, table, change type (such as INSERT), and other system-specific identifiers. This additional information helps track data lineage and understand the context of changes replicated from the source database. Datastream simplifies data replication by using unified data types to map between different source and destination databases. This means that regardless of whether your source data is in Oracle as number, MySQL as decimal, PostgreSQL, as numeric, or SQL Server as decimal, Datastream will consistently represent it as decimal during replication. When this data lands in Google Cloud, it can be further transformed into format-specific data types in different file types or destinations, such as Avro as decimal, JSON as number, or stored natively in BigQuery tables as numeric. This ensures data type consistency and compatibility across different database systems, streamlining the data replication process. In summary, Google Cloud offers several data migration and replication options. The 'gcloud storage' command is suitable for smaller online transfers. Storage Transfer Service handles larger online transfers efficiently. Transfer Appliance is ideal for massive offline data migrations, and Datastream provides continuous online replication of structured data, supporting both batch and streaming velocities. Choose the option that best fits your data size, transfer type, and data availability requirements.

#### Lab Intro: Datastream: PostgreSQL Replication to BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521333

In this lab, you use Datastream to replicate data from PostgreSQL to BigQuery. You prepare and load a Cloud SQL for PostgreSQL instance. You create Datastream connection profiles for the source and target. You then create a Datastream processing stream and start replication. Finally, you validate the replication in BigQuery.

#### Datastream: PostgreSQL Replication to BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521334

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521335

### The Extract and Load Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521336

In this module, first, you review the baseline extract and load architecture diagram. Second, you explore the options of the bq command line tool. Then, you review the functionality and use cases for the BigQuery Data Transfer Service. Finally, you look at the functionality and use cases for BigLake as a non extract-load pattern.

#### Extract and Load Architecture

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521337

The extract and load data pipeline pattern focuses on the tools and options to bring data into BigQuery by eliminating the need for upfront transformation. Extract and load greatly simplifies data ingestion into BigQuery. Extract and load leverages tools like 'bq load' and Data Transfer Service to directly load data from various sources or uses external tables and BigLake tables to make data accessible via BigQuery. This pattern also offers scheduling capabilities and eliminates the need for data copying, promoting efficiency in data pipelines. BigQuery provides extensive flexibility in data handling. It supports loading data from various formats like Avro, Parquet, ORC, CSV, JSON, as well as Google Cloud Firestore exports. Similarly, you can export BigQuery artifacts, including query results and table data, into formats like CSV, JSON, Avro, and Parquet, facilitating easy integration with other tools and systems. BigQuery offers two ways to load data: through its friendly user interface for file uploads, or via the LOAD DATA SQL statement. The UI simplifies the process, allowing you to select files, specify formats, and even auto-detect schema. LOAD DATA provides more control, ideal for automation and appending or overriding existing tables.

#### The bq Command Line Tool

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521338

The Cloud SDK's bq command offers a programmatic way to interact with BigQuery. You can create BigQuery objects like datasets and tables, put the familiar Linux command, bq mk. The bq load command efficiently loads data into BigQuery tables. Key parameters for bq load include specifying the source format such as CSV, skipping header rows, and defining the target dataset and table. You can load data from multiple files in Cloud Storage using wildcards and optionally provide a schema file for the table structure. These options provide flexibility and control for loading data into BigQuery from various sources.

#### BigQuery Data Transfer Service

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521339

The BigQuery Data Transfer Service enables seamless loading of structured data from diverse sources, like SaaS applications, object stores, and other data warehouses into BigQuery. The service provides scheduling options for recurring or on-demand transfers, along with configuration options for data source details and destination settings. It is a managed and serverless solution, eliminating infrastructure management overhead. In addition, its no code approach simplifies data transfer setup and management.

#### BigLake

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521340

BigQuery's data access capabilities extend beyond its own storage. BigQuery allows you to query data residing in sources like Cloud Storage, Google Sheets, and Bigtable using external tables. Additionally, BigLake tables provide a way to query data across Cloud Storage, and even other cloud object stores, expanding BigQuery's reach and flexibility for data analysis. BigQuery offers flexibility in analyzing structured data. You can load data into permanent BigQuery tables for high-performance analytics, but with data movement involved. External tables allow you to query data directly in Cloud Storage without loading it into BigQuery, which is suitable for less frequent access. BigLake tables provide the best of both worlds: high-performance analytics on data in Cloud Storage without the need to load it into BigQuery, and without data movement. BigQuery external tables bridge the gap between Google Sheets and BigQuery, enabling direct querying of sheets data within BigQuery. By specifying the Google Sheets URL and format, users can treat the sheet as a table in BigQuery, simplifying data analysis across platforms. However, be aware that querying external tables may have limitations, like slower performance and the unavailability of cost estimation, table preview, and query caching. BigLake extends BigQuery's capabilities, providing a unified interface to query data directly from your data lake and other sources without moving or copying it. BigLake leverages Apache Arrow for efficient data handling and offers fine-grained security and metadata caching. With BigLake, you can seamlessly access data across data lakes and data warehouses using familiar BigQuery tools. BigLake tables provide a seamless querying experience, allowing you to interact with data stored in external sources, like Cloud Storage, just like you would with data in native BigQuery tables. You can use standard SQL queries to access and analyze the data within BigLake tables, including SELECT statements and joins. Behind the scenes, BigLake leverages metadata caching to enhance query performance, even though the data physically resides outside BigQuery. However, some features like query cost estimation and table preview are not available for BigLake tables due to the external nature of the data. BigLake maintains a metadata cache. The cache stores details about external data. For example, it can contain details about Parquet files stored in Cloud Storage, such as file size, row count, and column statistics like minimum/maximum values. This cache allows querying via BigQuery to skip listing all objects, prune files, and partitions faster, and enable dynamic predicate pushdown, resulting in improved query performance. The cache allows querying by Spark to access metadata statistics that the Spark-BigQuery connector can leverage to speed up queries. The metadata cache has configurable staleness from 30 minutes to seven days, and it can be refreshed automatically or manually. External tables in BigQuery require users to have separate permissions for both the table itself and the underlying data source. This can lead to more complex access management. BigLake tables offer a streamlined approach. Access is delegated through a service account decoupling table access from the data source. This simplifies permission management and enhances security. In summary, both external and BigLake tables enable querying data residing outside of BigQuery, but BigLake offers broader capabilities. BigLake supports a wide range of data formats and storage locations, including object stores across multiple cloud providers, and provides advanced security features like column-level and row-level security. External tables are simpler to set up, but lack fine-grained security controls. BigLake tables offer enhanced performance, security, and flexibility for querying external data, making them suitable for enterprise data lake use cases.

#### Lab Intro: BigLake: Qwik Start

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521341

In this lab, you use BigLake to connect to various external data sources. You configure a connection resource and set up access to a Cloud Storage data lake. You create and query a BigLake table and set up access control policies. Finally, you upgrade an existing external table to be a BigLake table.

#### BigLake: Qwik Start

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521342

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521343

### The Extract, Load, and Transform Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521344

In this module, first, you review the baseline extract, load, and transform architecture diagram. Second, you look at a common ELT pipeline on Google Cloud. Then you review BigQuery's SQL scripting and scheduling capabilities. Finally, you look at the functionality and use cases for Dataform.

#### Extract, Load, and Transform (ELT) Architecture

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521345

Extract, load, and transform centers around data being loaded into BigQuery first. Once data is loaded, there are multiple ways to transform it. Procedural languages like SQL can be used to transform data. Scheduled queries can be used to transform data on a regular basis. Scripting and programming languages like Python can be used to transform data. And a tool like Dataform simplifies transformation beyond basic programming options. In an extract, load, and transform pattern pipeline, structured data is first loaded into BigQuery staging tables. Transformations are then applied within BigQuery itself using SQL scripts or tools like Dataform with SQL workflows. The transformed data is finally moved to production tables in BigQuery ready for use. This approach leverages BigQuery's processing power for efficient data transformation.

#### SQL Scripting and Scheduling with BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521346

With support for procedural language, BigQuery allows the execution of multiple SQL statements in sequence with shared state. This enables the automation of tasks like table creation, implementation of complex logic using constructs like IF and WHILE, and the use of transactions for data integrity. You can also declare variables and reference system variables within your procedural code. BigQuery supports user-defined functions, or UDFs, for custom data transformations using SQL or JavaScript. These UDFs can be persistent or temporary, and it is recommended to use SQL UDFs when possible. JavaScript UDFs offer the flexibility to use external libraries, and community-contributed UDFs are available for reuse. Stored procedures are pre-compiled SQL statement collections, streamlining database operations by encapsulating complex logic for enhanced performance and maintainability. Benefits include reusability, parameterization for flexible input, and transaction handling. Stored procedures are called from applications or within SQL scripts, promoting modular design. BigQuery supports running stored procedures for Apache Spark. Apache Spark stored procedures on BigQuery can be defined in the BigQuery PySpark editor or using the CREATE PROCEDURE statement with Python, Java, or Scala code. The code can be stored in a Cloud Storage file or defined inline within the BigQuery SQL editor. Remote functions extend BigQuery's capabilities by integrating with Cloud Run functions. This enables complex data transformations using Python code. You define the remote function in BigQuery, specifying the connection and endpoint to your Cloud Run function. This function can then be called directly within your SQL queries, similar to a UDF, allowing for seamless integration of custom logic. The example here shows a Python function that gets a list of signed URLs pointing to Cloud Storage objects and returns the length for these objects. In BigQuery, we just need to register the function. We call it object_length(). Then, we can use it as is in SQL. Jupyter Notebooks coupled with BigQuery DataFrames facilitate efficient data exploration and transformation. This integration emphasizes the ability to handle large datasets that exceed runtime memory, perform complex data manipulations using SQL or Python, and schedule notebook executions. The seamless integration of BigQuery DataFrames and popular visualization libraries further streamlines the entire process. BigQuery offers the option to save and schedule queries for repeated use. You can save queries, manage versions, and share them with others. Scheduling allows you to automate query execution by setting frequency, start and end times, and result destinations. Dataform is recommended for more complex SQL workflows. Often, there are needs to perform additional tasks after a scheduled query is executed in BigQuery. These include tasks such as triggering subsequent SQL scripts, running data quality tests on the output, or configuring security measures. In an ideal situation, these actions can be automated, ensuring data pipelines remain efficient and reliable.

#### Dataform

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521347

Dataform is a serverless framework that simplifies the development and management of ELT pipelines using SQL, Dataform enables data transformation within BigQuery, ensuring data quality and providing documentation, This approach streamlines the process of moving data from source systems to production tables in BigQuery, making operations more efficient and manageable, Dataform streamlines data operations in BigQuery by unifying transformation, assertion, and automation, Without Dataform, tasks like defining tables, managing code, testing data quality, and scheduling pipelines would be time-consuming and prone to errors, They could also involve multiple tools and manual processes, Dataform simplifies these tasks within BigQuery, improving efficiency and data reliability, Dataform and BigQuery work together to manage SQL workflows, With Dataform, developers create and compile SQL workflows using SQL and JavaScript, Dataform then performs real-time compilation, including dependency checks and error handling, Finally, the compiled SQL workflows are executed within BigQuery, enabling SQL transformations and materialization either on-demand or through scheduled runs, Development using Dataform utilizes workspaces containing default files and folders, Key folders include 'definitions' for sqlx files and 'includes' for JavaScript files, The ,gitignore file is used for managing Git commits, Developers may also use package,json and package-lock,json for handling JavaScript dependencies, The 'workflow settings,yaml' file stores, project compilation settings, and custom files like README,md can also be added, The sqlx file structure provides a clear framework for organizing SQL code and associated tasks, It begins with a config block for metadata and data quality tests, followed by a js block to define reusable JavaScript functions. The pre_operations block handles SQL statements executed before the main SQL body, which defines the core SQL logic. Finally, the post_operations block contains SQL statements to be run after the main execution, ensuring a structured and efficient workflow. sqlx development streamlines SQL code by replacing repetitive patterns with concise definitions. The code example demonstrates how a complex CASE statement for categorizing countries can be replaced with a simple function call $(mapping.region("country")). This approach improves code readability and maintainability by reducing boilerplate code and promoting reusability. With Dataform, table and view definitions should be created in a specific manner so that they can be compiled into SQL statements. Key configuration types are: declaration for referencing existing BigQuery tables. table for creating or replacing tables with a SELECT statement, incremental for creating tables and updating them with new data, and view for creating or replacing views, which can optionally be materialized. Dataform offers assertions to define data quality tests, ensuring data consistency and accuracy. Assertions can be written in SQL or JavaScript, providing flexibility for complex checks. Operations allow you to run custom SQL statements before, after, or during pipeline execution. These two options enable custom data transformations, data quality checks, and other tasks within your workflows. By combining assertions and operations, Dataform empowers you to create robust and reliable data pipelines in BigQuery. Dataform provides two methods to manage dependencies, implicit declaration and explicit declaration. Implicit declaration is when you reference tables or views directly within your SQL using the ref() function. Explicit declaration is when you list dependencies within a config block using the dependencies array. It is also possible to use the resolve() function to reference without creating a dependency. Dataform allows you to compile user-defined table definitions into executable SQL scripts. The sample code shows a customer_details table being created or replaced based on a customer_source table using a SELECT statement. Dataform manages the dependencies between these tables and orchestrates their execution within a workflow. This process streamlines data transformation and ensures efficient data pipeline management. Dataform SQL workflows are best visualized in graph format. The sample workflow starts with a declaration of customer_source followed by a customer_intermediate table, likely derived from a source system as a pre-processed data source. Next, customer_rowConsistency applies assertions for data quality checks. The graph then splits into two paths. In one path, an operation named customer_ml_training is invoked. It performs operations on the validated data. In the other path, a view named customer_prod_view is created. There are several scheduling and execution mechanisms for Dataform SQL workflows. One path is through internal triggers. These include manual execution in the Dataform UI or scheduled configurations within Dataform itself. The other path is through external triggers. These include tools like Cloud Scheduler and Cloud Composer. Ultimately, all workflows are executed within BigQuery, showcasing its central role in this process.

#### Lab Intro: Create and Execute a SQL Workflow in Dataform

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521348

In this lab, you use Dataform to create and execute a SQL workflow. First, you create a Dataform repository. Second, you create and initialize a Dataform development workspace, then you create and execute a SQL workflow. Finally, you view execution logs in Dataform to confirm completion.

#### Create and execute a SQL workflow in Dataform

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521349

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521350

### The Extract, Transform, and Load Data Pipeline Pattern

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521351

In this module, first, you review the baseline extract, transform, and load architecture diagram. Second, you look at the GUI tools on Google Cloud used for ETL data pipelines. Third, you review batch data processing using Dataproc. Then, you examine using Dataproc Serverless for Spark for ETL. Next, you review streaming data processing options on Google Cloud. Finally, you examine the role Bigtable plays in data pipelines.

#### Extract, Transform, and Load (ETL) Architecture

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521352

The extract, transform, and load data pipeline pattern focuses on data being adjusted or transformed prior to being loaded into BigQuery. Google Cloud offers a variety of services to handle distributed data processing. For developers who prefer visual interfaces, there are user-friendly tools like Dataprep and Data Fusion. If you favor open-source frameworks, Dataproc and Dataflow are an option. And to streamline your workflows, template support is provided across various stages of data processing, from extraction and loading to full-fledged transformation.

#### Google Cloud GUI Tools for ETL Data Pipelines

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521353

Dataprep by Trifacta simplifies data transformation by offering a serverless, no-code solution. It connects to various data sources, provides pre-built transformation functions, and allows users to chain these functions together into recipes. The resulting transformation flows can be executed seamlessly with Dataflow, while also providing capabilities for scheduling and monitoring. Dataprep provides a visual interface where you can see the impact of your data transformations before applying them. It also offers intelligent suggestions to help you refine your data cleaning and preparation process, like extracting specific values or replacing patterns within your data. Data Fusion is a user friendly, GUI-based tool designed for enterprise data integration. Data Fusion seamlessly connects to various data sources, both on-premises and in the cloud. You can build data pipelines without coding using its drag-and-drop interface and pre-built transformations. The platform is extensible, allowing for custom plugins, and it executes on powerful Hadoop/Spark clusters for efficient processing. Data Fusion Studio easily allows the creation of data pipelines with visual tools. The example pipeline shows two SAP tables being used as data sources. The two tables are joined together and then one outbound leg is written to a Cloud Storage bucket. The other leg undergoes an Add-Datetime transformation and is outputted to BigQuery. The pipeline also highlights the ability to preview data at different stages.

#### Batch Data Processing Using Dataproc

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521354

Dataproc allows you to seamlessly run your Apache Hadoop and Spark workloads on Google Cloud. You can leverage HDFS data stored on Cloud Storage and use Dataproc to perform transformations with Spark jobs. The results can then be easily stored in various destinations like Cloud Storage, BigQuery, or NoSQL databases like Bigtable, all within the Google Cloud ecosystem. Dataproc is Google Cloud's managed service for data processing using Hadoop and Spark. It offers flexibility with runtimes on GCE, GKE, and Serverless Spark, and provides a rich, open-sourced ecosystem. Dataproc simplifies cluster management with workflow templates, autoscaling, and the option for both permanent and ephemeral clusters. It also integrates seamlessly with other Google Cloud storage services, eliminating the need for disk-based HDFS. Dataproc clusters on Google Compute Engine offer flexible storage options. Clusters can utilize HDFS on persistent disks for cluster storage, or leverage other Google Cloud storage services like Cloud Storage for persistent data. Additionally, Dataproc integrates with BigQuery and Bigtable using connectors, enabling seamless interaction with these data stores. This setup allows users to choose the most suitable storage solution for their specific needs while taking advantage of Dataproc's processing capabilities. Dataproc Workflow Templates allow you to define and manage complex data processing workflows with dependencies between jobs. You can specify these workflows in a YAML file, providing details about the jobs like Hadoop or Spark, their order of execution, and any required parameters. These templates can then be submitted to Google Cloud using the gcloud command line tool, where they will be executed on either a managed ephemeral cluster or an existing predefined cluster. Apache Spark is a versatile framework for data processing, offering various capabilities through its components like Spark SQL for structured data, Spark Streaming for real-time data, MLlib for machine learning, and GraphX for graph processing. Spark supports multiple languages including R, SQL, Python, Scala and Java, making it accessible to a wide range of users. With these features, Spark excels in tasks like data engineering, machine learning, analytics, and many more. Dataproc Serverless for Spark simplifies Spark workload execution by eliminating cluster management. It offers automatic scaling, cost efficiency with pay-per-execution pricing, faster deployment, and no resource contention. Users can focus solely on writing and executing their code, making it ideal for various Spark use cases like batch processing, interactive notebooks, and Vertex AI pipelines. Dataproc Serverless for Spark offers two main execution modes: Serverless for batches and Serverless for interactive notebook sessions. Batches are submitted using the gcloud command-line tool and are ideal for automated or scheduled jobs. Interactive sessions leverage JupyterLab, either locally or within the Google Cloud environment, for interactive development and exploration. The platform also supports features like BigQuery external procedures, templates, custom containers, and a pay-as-you-go pricing model. Dataproc Serverless for Spark seamlessly integrates with various Google Cloud services, enhancing its functionality and usability. It leverages Dataproc History Server and Dataproc Metastore for persistent storage and metadata management. It interacts with BigQuery for data warehousing and analytics, and with Vertex AI workbench for machine learning tasks. Additionally, it utilizes Cloud Storage and other storage services for data storage and retrieval. Behind the scenes, it creates and manages ephemeral clusters for efficient job execution. The lifecycle of an interactive notebook session begins with its creation, where various configurations like runtime version and network settings are defined. Once active, the session allows for code development and execution, with the kernel transitioning between idle and busy states. The session eventually reaches a shutdown phase, either manually triggered or due to inactivity, leading to the kernel being shut down and its state becoming unknown.

#### Lab Intro: Use Dataproc Serverless for Spark to Load BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521355

In this lab, you use Dataproc Serverless for Spark to load BigQuery. First, you configure the environment. Next, you download lab assets. You then configure and execute the Spark code. Finally, you view the data in BigQuery.

#### Use Dataproc Serverless for Spark to Load BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521356

#### Streaming Data Processing Options

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521357

Batch processing involves analyzing a fixed set of store data, suitable for tasks like payroll or billing systems. On the other hand, streaming data processing handles a continuous flow of data from various sources, making it ideal for real-time applications like fraud or intrusion detection. Streaming ETL workflows on Google Cloud involve the continuous ingestion of event data, often through Pub/Sub. This data is often processed in real-time using Dataflow, allowing for transformations and enrichment. Finally, the processed data is loaded into various destinations like BigQuery for analytics, enabling near real-time insights, or a Bigtable for NoSQL storage. Pub/Sub can efficiently manage high volumes of event data. Pub/Sub acts as a central hub, receiving events like 'New employee' or 'New contractor' from various sources. Pub/Sub then distributes these events to relevant systems like badge activation, facilities, and account provisioning, ensuring reliable delivery and enabling decoupled, asynchronous communication between systems. Dataflow leverages the Apache Beam programming framework to efficiently process both batch and streaming data. This unified approach simplifies development, allowing you to use languages like Java, Python, or Go. Dataflow seamlessly integrates with other Google Cloud services and offers features like a pipeline runner, serverless execution, templates, and notebooks for a streamlined experience. This code example demonstrates how to use Apache Beam to stream messages from Pub/Sub, transform them using a parsing function, and then write the results into BigQuery. The ReadFromPubSub function retrieves messages, Beam. Map() applies the parsing transformation, and WriteToBigQuery loads the transformed data into a specified BigQuery table, creating the table if necessary and appending new data to it. Dataflow templates allow you to create reusable pipelines for recurring tasks. You can separate the pipeline design from its deployment, making it easier to manage and update. By using parameters, you can customize the pipeline for different inputs, increasing its versatility. These templated pipelines can be easily deployed through various methods, and Google provides pre-built templates for common scenarios.

#### Bigtable and Data Pipelines

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521358

Bigtable is an excellent choice for handling streaming data pipelines that require millisecond-level latency analytics. Bigtable utilizes a wide-column data model with column families, allowing for flexible schema design. Row keys serve as efficient indexes for quick data access. Bigtable's high-throughput and low latency capabilities make it suitable for applications like time series data, IoT, financial data, and machine learning, especially when dealing with large datasets. In summary, Google Cloud provides various services for ETL processing. Dataprep is ideal for data wrangling tasks and offers a serverless option. Data Fusion excels at data integration, particularly in hybrid and multicloud environments, utilizing the open-source CDAP framework. Dataproc handles ETL workloads with support for Hadoop, Spark, and other open source tools, with Serverless Spark as a serverless option. Lastly, Dataflow, built on Apache Beam, is recommended for both batch and streaming ETL workloads, and provides a serverless architecture.

#### Lab Intro: Creating a Streaming Data Pipeline for a Real-Time Dashboard with Dataflow

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521359

In this lab, you create a streaming data pipeline for a real-time dashboard with Dataflow. You create a Dataflow job from a template. You then monitor a pipeline loading data into BigQuery. After that, you examine the data loaded using SQL. Finally, you visualize key metrics using Looker Studio.

#### Creating a Streaming Data Pipeline for a Real-Time Dashboard with Dataflow

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521360

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521361

### Automation Techniques

#### Module Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521362

In this module, first, you review the automation patterns and options available for pipelines. Second, you explore Cloud Scheduler and workflows. Then, you review the functionality and use cases for Cloud Composer. Next, you review the capabilities of Cloud Run functions. Finally, you look at the functionality and automation use cases for Eventarc.

#### Automation Patterns and Options for Pipelines

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521363

On Google Cloud, ELT and ETL workloads can be automated for recurring execution. For example, in a scheduled ELT, a defined schedule triggers data extraction from BigQuery, transformation via Dataform, and loading back into BigQuery. Meanwhile, in the example shown for an event-driven ETL, a file upload to Cloud Storage initiates a batch process using Dataproc, culminating in data landing in Cloud Storage. Google Cloud offers a suite of services to automate and orchestrate your workloads. For scheduled tasks or one-off jobs, you can leverage Cloud Scheduler and Cloud Composer. If your workflows require orchestration, Cloud Composer is the ideal choice. To trigger actions based on events, consider using Cloud Run functions or Eventarc.

#### Cloud Scheduler and Workflows

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521364

Cloud Scheduler empowers you to automate tasks by invoking your workloads at specified recurring intervals. It grants you the flexibility to define both the frequency and precise time of day for job execution. Triggers can be based on HTTP/S calls, App Engine HTTP calls, Pub/Sub messages, or Workflows. Cloud Scheduler can be used to trigger a Dataform SQL workflow. In the example code, a scheduled job in Cloud Scheduler initiates the process defined in a YAML config file. The workflow involves two main steps: creating a compilation result from your Dataform code, and then triggering a workflow invocation using that result, ensuring only specific parts of your Dataform project execute based on included tags.

#### Cloud Composer

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521365

Cloud Composer acts as a central orchestrator, seamlessly integrating your pipelines across diverse systems, whether on Google Cloud, on-premises, or even multicloud environments. Cloud Composer leverages Apache Airflow, incorporating essential elements like operators, tasks, and dependencies to define and manage your workflows. Additionally, Cloud Composer offers robust features for triggering, monitoring, and logging, ensuring comprehensive control over your pipeline executions. Developing and executing workflows using Apache Airflow and Cloud Composer is easily done using Python. First, you leverage Apache Airflow operators to craft your directed acyclic graph, or DAG, defining the tasks and their dependencies. Next, the DAG is deployed to Cloud Composer, which handles the parsing and scheduling of your workflow. Cloud Composer further manages the execution of your tasks, incorporating features like error handling, retries, and monitoring to ensure smooth operation. With minimal effort, Cloud composer can be used to run a data analytics DAG. In the example code, the workflow retrieves a file from Cloud storage, loads it into BigQuery, and then performs a JOIN operation with an existing BigQuery table. The joined results are then inserted into a new BigQuery table. Finally, Dataproc is used for further data transformation.

#### Cloud Run Functions

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521366

Cloud Run functions allow you to execute code in response to various Google Cloud events. These events can originate from sources like HTTP requests, Pub/Sub messages, Cloud Storage changes, Firestore updates, or custom events through Eventarc. When triggered, a Cloud Run function provides a serverless execution environment where your code runs, supporting multiple programming languages for flexibility. Cloud Run functions easily automate routine tasks on Google Cloud. In the example code, a Dataproc workflow template is triggered after a file is uploaded to Cloud Storage. A Cloud Run function is used to capture the Cloud Storage new file event and call the Dataproc API. The Dataproc API then executes the specified workflow template, using the uploaded file as an input parameter. The final result of the workflow execution is stored in Cloud Storage.

#### Eventarc

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521367

Eventarc enables the creation of a unified event-driven architecture for loosely coupled services. Eventarc connects various event sources, including Google Cloud services, third-party systems, and custom events via Pub/Sub to a range of event targets like Cloud Run functions, and more. By using a standardized CloudEvent message format, Eventarc simplifies the integration of diverse systems and facilitates the development of responsive scalable applications. Eventarc enables deep monitoring of logging and other events which occur less frequently on Google Cloud. In the example code, Eventarc is used to trigger actions in response to data insertion events in BigQuery. When an insert operation occurs in a BigQuery table, it generates a Cloud Audit Log event. Eventarc can capture this event and initiate various actions such as rebuilding a dashboard, retraining an ML model, or executing any other custom action based on the specific requirements. In summary, there are various Google Cloud data-related automation options. Cloud Scheduler and Cloud Composer are suitable for scheduled or manual triggers, while Cloud Run functions and Eventarc are event-driven. Cloud Scheduler offers low coding effort with YAML, and Cloud Composer requires medium effort with Python. Cloud Run functions support multiple languages, while Eventarc is language agnostic. As a final note, all options except Cloud Composer are serverless.

#### Lab Intro: Use Cloud Run Functions to Load BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521368

In this lab, you create a Cloud Run function to load BigQuery. You create a Cloud Run function using the Cloud SDK. You then deploy and test the Cloud Run function. Finally, you view data in BigQuery and review Cloud Run function logs.

#### Use Cloud Run Functions to Load BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/labs/521369

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/quizzes/521370

### Course Summary

#### Course Summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/video/521371

This concludes our course, Introduction to Data Engineering on Google Cloud. In this course, you learned about the duties and responsibilities of a data engineer, how to accomplish data engineering tasks, and core components used on Google Cloud to accomplish those tasks. How to create and deploy data pipelines of varying patterns on Google Cloud, and how to identify and utilize various automation techniques on Google Cloud to complete data engineering tasks. We hope you had a great learning experience.

#### Course Resources

- https://www.cloudskillsboost.google/paths/1336/course_templates/1157/documents/521372

### Your Next Steps

## 02: Derive Insights from BigQuery Data

- https://www.cloudskillsboost.google/paths/1336/course_templates/623

### Derive Insights from BigQuery Data

#### Introduction to SQL for BigQuery and Cloud SQL

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551908

#### BigQuery: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551909

#### BigQuery: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551910

#### Explore an Ecommerce Dataset with SQL in BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551911

#### Troubleshooting Common SQL Errors with BigQuery

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551912

#### Explore and Create Reports with Looker Studio

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551913

#### Derive Insights from BigQuery Data: Challenge Lab

- https://www.cloudskillsboost.google/paths/1336/course_templates/623/labs/551914

### Your Next Steps

## 03: Prepare Data for Looker Dashboards and Reports

- https://www.cloudskillsboost.google/paths/1336/course_templates/628

### Prepare Data for Looker Dashboards and Reports

#### How to earn a Looker Skill Badge

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/video/551063

#### Looker Data Explorer - Qwik Start

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/labs/551064

#### Filtering and Sorting Data in Looker

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/labs/551065

#### Merging Results from Different Explores in Looker

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/labs/551066

#### Looker Functions and Operators

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/labs/551067

#### Prepare Data for Looker Dashboards and Reports: Challenge Lab

- https://www.cloudskillsboost.google/paths/1336/course_templates/628/labs/551068

### Your Next Steps

## 04: Introduction to AI and Machine Learning on Google Cloud

- https://www.cloudskillsboost.google/paths/1336/course_templates/593

### Introduction

#### Course introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565154

[MUSIC] Artificial intelligence, AI, and machine learning, ML are rapidly evolving, especially in the field of generative AI. Generative AI allows machines to produce content based on human input, which opens a wide range of possibilities that were not available even a few months ago. If you are a developer, you might want to incorporate new AI capabilities in your applications or use them to improve productivity. If you are a data scientist, you might want to train the machine learning model with your own data to solve a business problem. If you are a machine learning engineer, you might want to build an ML pipeline and deploy it to production. Even if you are not an AI professional, you might be curious about cutting edge advancements such as generative AI and how they can spark new business ideas. If you are any of these people, we have a course for you. Introduction to AI and machine learning on Google Cloud. I'm Doctor Yoanna Long, an AI and machine learning educator at Google Cloud. I was a college professor for 15 years and I'm passionate about making complex concepts simple so everyone can understand and use AI. I'm here to help you embark on the learning journey and succeed in this course, so what is this course about? This course presents a toolbox which is full of AI technologies and tools offered by Google. These technologies and tools are organized into layers to make navigation easier. You begin with the AI foundation layer where you learn about cloud essentials like compute, storage, and network and data tools such as data pipelines and data analytics. These tools help you start your journey from data to AI. You then move on to the AI development layer where you explore different options to build machine learning project, including out of the box solutions, low code or no code, and DIY, do it yourself. You also walk through the workflow to train and serve a machine learning model using Vertex AI, the AI development platform provided by Google Cloud. Finally, you are introduced to generate AI and you learn how generative AI empowers the AI development and AI solutions layer. After completing this course, you will be able to recognize the data to AI technologies and tools offered by Google Cloud. Leverage generative AI capabilities in applications, choose between different options to develop an AI project on Google Cloud, and build machine learning models end-to-end using Vertex AI. How can you succeed in this course? Here are some suggestions, write down three keywords after each lesson, lab and module. These keywords can be points you learned, use cases to apply, and new business ideas. Look at the list and see how the terms relate to each other. You can then draw lines to connect the terms or group them together in a way that makes sense to you. This practice will strengthen your understanding of the concepts. Apply what you learned to your own work. This is the best way to develop your skills as an AI practitioner. The evolution and the capabilities of AI are fascinating. As you learn more about this technology, you are better prepared to meet the challenges of today and tomorrow, let's get started.

### AI Foundations

#### Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565155

Welcome to the first module of this course, AI Foundations. The first question you may have is quickly, how does AI help you innovate business processes and improve business efficiency? Lesson 1 answers this question by showing you a use case powered by recent AI technologies like generative AI. You may also wonder why Google and how to start an AI project on Google Cloud. Lesson 2 illustrates an AI ML framework and helps you navigate through the whole course. Next, you explore Google Cloud's infrastructure, focusing on compute and storage. You then examine the products that support your journey from data to AI on Google Cloud. After that, you advance to more AI and ML content, starting with ML model categories, which provides context to understand ML model building. You then explore Big Query and specifically Big Query ML and walk through the steps to build an ML model with SQL commands. Finally, you complete a hands on lab to build your first ML model on Google Cloud with Big Query ML. Let's get started.

#### Why AI

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565156

Why AI and why Google? These might be your first questions. Let's explore an example to understand how AI can enhance business efficiency and transform operations. Coffee on Wheels, an international company that sells coffee on trucks in cities like London, New York, San Francisco, and Tokyo, provides a compelling case study. Coffee on Wheels is facing three main challenges: Coffee on Wheels is facing three main challenges: Location selection and route optimization Predicting popular locations for truck placement, and optimizing routes based on weather and traffic conditions. Sales forecast and real-time monitoring Forecasting sales and monitoring performance in real-time. Marketing campaign automation Automating marketing campaigns to increase efficiency and effectiveness. Recognizing the potential of AI, Coffee on Wheels sought assistance from Data Beans, a digital native company, to leverage data and AI technologies to resolve their business challenges. Let's take a tour of the demo. Choose one of the four current locations, such as London. The dashboard displays overall statistics across cities, including revenue, operating margin, and the number of trucks. This information is generated by data tools like BigQuery and Looker, as well as AI tools and models like Gemini and Vertex AI. On the right, you can view the final data for London with a summary. It shows how London's revenue compares to the average, and provides insights into revenue per truck and customer loyalty. In the top left corner, the dashboard displays the weather and generates route suggestions based on weather conditions. For example, if lower temperatures are forecasted, it might suggest a new itinerary that focuses on covered areas. You can click "show updated route" and "publish route" to implement these changes. By clicking on a specific time on the timeline, you can see route suggestions based on city events. For example, if there's a football game happening, it might suggest rerouting trucks to avoid congestion. Clicking on the truck sign provides a detailed dashboard with information such as street view and revenue forecast. To monitor the performance of the business in real-time, you can access a dashboard by clicking "show menu." If an item is underperforming, you can click the "generate" button to get suggestions for a new item. Additionally, you have the option to generate marketing campaigns by selecting "yes" to "save the suggestion." This feature enables you to automatically create campaigns that include both text and images. You can further streamline your marketing efforts by sending campaign emails to targeted customers with just a click of the "post" button. Finally, you can generate an operational report and export the insights to any format, such as Google Slides. To customize the application using code, click "how it's made." This reveals the tools and technologies, including BigQuery, Gemini, and Vertex AI, that were used to create the app. You can also click "open in notebook" to access the sample code in the code development environment. Isnt it remarkable? The process is actually straightforward: Multimodal input: this involves incorporating various forms of data, such as text (customer reviews), images (coffee and dessert pictures), and videos (real-time street view). 2. Prediction and generation: this is powered by data analytics like customer segment analysis, predictive AI like sales forecasting, and generative AI like marketing campaign automation. 3. Visual output: the insights and reports are then presented visually, empowering businesses to make real-time data-driven decisions and optimize their operations. Behind the scenes, many Google products collaborate to make this application possible. For example, Gemini multimodal enables data acquisition, BigQuery provides data analytics, Vertex AI handles ML development, and Looker and Google APIs contribute to data visualization and app creation. The course will explore these tools in more depth later, giving you the chance to learn about them in detail. This application's development encompasses the entire data to AI lifecycle. It includes data ingestion, data analytics, data engineering, model training, testing, and deployment. These processes are supported by Google's unified development platforms. Additionally, Google's AI development platform enables the utilization of various types of AI, including predictive AI for tasks like sales forecasting, generative AI for tasks like automating marketing campaigns, and hybrid approaches that combine both. By leveraging the application, Coffee on Wheels gained the following benefits: Streamlined business processes in key areas such as marketing, digital commerce, and back-office operations. Modernized customer service through features such as automated comment replies and actionable consumer insights and predictions. Enhanced employee productivity through the utilization of GenAI for code assistance and marketing content generation.

#### AI/ML architecture on Google Cloud

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565157

You might be excited about the capabilities and potentials of AI, but how to start your AI projects with Google? You explore the AI and ML toolbox in this lesson. So, why should you trust Google for AI? Firstly, Google has been an AI-first company and used AI to power its products since the beginning. Second, Google is a leader in AI and ML innovations. Thirdly, Google is a believer in responsible AI. Let's take a brief look at how Google began innovating with data and AI in its products. Data is the basis of AI. Historically speaking, Google experienced challenges related to data processing quite early. As a search engine, Google needed to constantly invent new data processing methods to index the World Wide Web and keep up with the rapidly growing internet. The innovation started in the early 2000s from Google File System, or GFS, which is the foundation for Cloud Storage, and MapReduce, which aims to manage large-scale data processing. To address the challenges of different types of data, whether structured or unstructured, and different processing requirements, either streaming or batch, Google has continued to invent multiple products and technologies in data analytics and engineering. For example, BigQuery, the data warehouse widely used on Google Cloud, can analyze large amounts of data and build ML models at the same time with SQL, Structured Query Language. In 2015, Pub/Sub was invented to help build data pipelines for streaming analytics. In the field of AI, Google has also contributed with many key technologies. For example, the widely-used ML library in Python for general purposes scikit-learn was a Google summer coding project back in 2007. TensorFlow, an open-source ML platform for training deep learning neural networks was developed by Google in 2015. And it has evolved in multiple versions. The transformer, the basis of all generative AI (or gen AI) applications seen today, was invented by Google in 2017. From this, large language models evolved rapidly, including Bidirectional Encoder Representations from Transformers (BERT) in 2018, Language Model for Dialogue Applications (LaMDA) in 2021, Pathways Language Model (PaLM) in 2022, Gemini, the most recent cutting-edge genAI model to process multimodal data released at the end of 2023, and Gemma, an open model released at the beginning of 2024. In terms of AI products, Google built an end-to-end AI development platform that evolves from AutoML in 2018, to Al Platform in 2019, and Vertex AI in 2021. In 2023, Google also announced a series of generative AI products on Vertex AI such as Vertex AI Studio, Model Garden, and Search and Conversation. Youll explore all of these AI technologies and products in depth later in this course. Google is a leader in AI and ML innovations. Here are four major reasons: The ML development platform is empowered by Google's state-of-the-art ML models. So you build on excellence. Google provides an end-to-end development platform to convert an ML model from experiment to production so you can be more efficient. Google provides a unified data-to-AI platform so you can develop data and AI projects with ease. Like Google Cloud itself, AI services are based on an efficient and scalable infrastructure so you can get more for less. AI comes with its own set of unique challenges. Google integrates responsible AI into its AI principles. Responsible AI refers to the development and use of artificial intelligence systems in a way that prioritizes ethical considerations, fairness, accountability, safety, and transparency. We have three AI principles that guide our work. These are concrete standards that actively govern our research and product development and affect our business decisions. Heres an overview of each one. 1. Bold innovation. We develop AI that assists, empowers, and inspires people in almost every field of human endeavor, drives economic progress, and improves lives, enables scientific breakthroughs, and helps address humanitys biggest challenges. 2. Responsible development and deployment. Because we understand that AI, as a still-emerging transformative technology, poses evolving complexities and risks, we pursue AI responsibly throughout the AI development and deployment lifecycle, from designing to testing to deployment to iteration, learning as AI advances and uses evolve. 3. Collaborative progress, together. We make tools that empower others to harness AI for individual and collective benefit. Establishing principles were a starting point rather than an end. They are a foundation that establishes what Google stands for, what to build, and why to build it, and they are core to the success of Googles enterprise AI offerings. To learn more about responsible AI and how Google implements it in research and product design, please check the Google doc in the reading list. In sum, Googles AI principles revolve around being an AI-first company with a rich history of AI development, a leader in AI innovations, and a practitioner of responsible AI ethics. Now you know why you can trust Google for artificial intelligence and machine learning. The next question you may have is : How do I start? Assuming you are a data scientist and you want to develop an ML model from beginning to end, what options do you have? Say you are an ML engineer, and you want to build an ML pipeline to automatically monitor the model performance. What tools do you have? Or perhaps you are an AI developer who would like to leverage generative AI capabilities in an application. What products can you use? Youll find all the answers in this course. This course presents a toolbox that is based on the AI/ML framework on Google Cloud. The toolbox is organized into three layers to make navigation easier. You begin with the AI foundations layer in this module, module one. Here, you learn about cloud essentials like compute and storage. Also about data and AI products that support your journey from data to AI. You then advance to the AI development layer. In module 2, you focus on the options to develop an ML model from beginning to end on Google Cloud. There, you explore out-of-the-box solutions like pre-built APIs, low or no-code solutions like AutoML, and do-it-yourself approaches like custom training. In module 3, you walk through the workflow to build an ML model using Vertex AI, the end-to-end AI development platform. From data preparation, to model training, and finally model serving. Additionally, you learn how to automate this ML workflow using the Vertex AI Pipelines SDK. Finally, in module 4, you are introduced to generative AI, how it works, the tools to develop generative AI projects, and how generative AI empowers AI solutions. Regardless your role, module one provides you with an overview of the AI/ML framework on Google Cloud the cloud infrastructure, and data and AI products. If you are an AI developer, a data scientist, or an ML engineer who would like to build your own ML models, module two to four will walk you through all the details you need to know. If you are a business user and would like to leverage AI/ML in your applications, AI solutions in module 4 help you navigate through a use case, tools, and technologies.

#### Google Cloud infrastructure

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565158

Lets explore Google Cloud infrastructure. Google has been working with data and artificial intelligence since its early days as a company in 1998. Ten years later, in 2008, Google Cloud was launched to provide secure and flexible cloud computing and storage services. You can think of the Google Cloud infrastructure in terms of three layers. At the base layer is networking and security, which lays the foundation to support all of Googles infrastructure and applications. On the next layer sit compute and storage. Google Cloud separates, or decouples, as its technically called, compute and storage so they can scale independently based on need. The top layer includes data and AI/machine learning products, which enable you to perform tasks to ingest, store, process, and deliver business insights, data pipelines, and ML models. Thanks to Google Cloud, these tasks can be accomplished without a need to manage and scale the underlying infrastructure. Lets begin with compute. Organizations with growing data needs often require lots of compute power to run data and AI jobs. And as organizations design for the future, the need for compute power only grows. Google offers a range of computing services. The first is Compute Engine. Compute Engine is an Iaas, or infrastructure as a service offering, which provides compute, storage, and network resources virtually that are similar to a physical machine. You use the virtual compute and storage resources the same as you manage them locally. Compute Engine provides maximum flexibility for those who prefer to manage server instances themselves. The second is Google Kubernetes Engine, or GKE. GKE runs containerized applications in a cloud environment, as opposed to on an individual virtual machine like Compute Engine. A container represents code packaged up with all its dependencies. The third computing service offered by Google is App Engine, a fully managed PaaS, or platform as a service, offering. PaaS offerings bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. Then there is Cloud Run, a fully managed compute platform that enables you to run requests or event-driven stateless workloads without having to worry about servers. It abstracts away all infrastructure management so you can focus on writing code, and it automatically scales up and down from zero, so you never have to worry about scale configuration. Cloud Run charges you only for the resources you use, so you never pay for over-provisioned resources. And finally, there is Cloud Run functions, which executes code in response to events, like when a new file is uploaded to Cloud Storage. Its a completely serverless execution environment, which means you dont need to install any software locally to run the code and you are free from provisioning and managing servers. Cloud Run functions is often referred to as Functions as a Service. Where does the processing power come from? Its from the hardware: from computer chips. However, traditional computer chips, like a CPU, or central processing unit, and even the more recent GPU, or graphics processing unit, may no longer scale to adequately reach the rapid demand for ML. To help overcome this challenge, in 2016, Google introduced the Tensor Processing Unit, or TPU. TPUs are Googles custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. TPUs act as domain-specific hardware, as opposed to general-purpose hardware like CPUs and GPUs. This allows for higher efficiency by tailoring the architecture to meet the computation needs in a domain, such as the matrix multiplication in machine learning. TPUs are generally faster than current GPUs and CPUs for AI and ML applications. They are also significantly more energy-efficient. Cloud TPUs have been integrated across Google products, making this state-of-the-art hardware and supercomputing technology available to Google Cloud customers. Lets now examine storage. For proper scaling capabilities, compute and storage are decoupled. That is one major difference between cloud and desktop computing. With cloud computing, compute and storage can scale separately. Most applications require a database and storage solution of some kind. Google Cloud offers fully managed database and storage services. These include: Cloud Storage Cloud Bigtable Cloud SQL Cloud Spanner Firestore And BigQuery How do you choose from these products and services? Well, it depends on the data type and business needs. Lets look at the data type, which includes unstructured versus structured data. Unstructured data is information stored in a non-tabular form such as documents, images, and audio files. Unstructured data is usually suited to Cloud Storage. Cloud Storage has four primary storage classes. The first is standard storage. Standard storage is considered best for frequently accessed, or hot, data. Its also great for data that is stored for only brief periods of time. The second storage class is nearline storage. This is best for storing infrequently accessed data, like reading or modifying data once per month or less, on average. Examples include data backups, long-tail multimedia content, or data archiving. The third storage class is coldline storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to nearline storage, coldline storage is meant for reading or modifying data at most once every 90 days. The fourth storage class is archive storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. Its the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Alternatively, there is structured data, which represents information stored in tables, rows, and columns. Structured data comes in two types: transactional workloads and analytical workloads. Transactional workloads stem from online transaction processing systems, which are used when fast data inserts and updates are required to build row-based records. This is usually to maintain a system snapshot. They require relatively standardized queries that impact only a few records. Then there are analytical workloads, which stem from online analytical processing systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. Once youve determined if the workloads are transactional or analytical, you then need to identify whether the data will be accessed using SQL or not. So, if your data is transactional and you need to access it using SQL, then two options are Cloud SQL and Spanner. Cloud SQL works best for local to regional scalability, while Spanner works best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional, NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery is likely the best option. BigQuery, Googles data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. Its best for real-time, high-throughput applications that require only millisecond latency.

#### Data and AI products

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565159

In this lesson, let's investigate the primary data and AI products on Google Cloud. In the last lesson, you examined Google Cloud Infrastructure, specifically compute and storage. The final layer of the Google Cloud infrastructure that is left to explore is Data and AI products. As you explored in the earlier lesson, Google offers a range of Data and AI tools, so how do you know which is best for your business needs? Let's look closer at the list of products, which can be divided into four general categories along the data to AI workflow. You gather data from multiple sources through ingestion and process. The data is then saved in different types of storage based on data type and business needs. You analyze the data and visualize the results, and you can further train an ML model with historical data to either predict future trends or generate new content. These tools are seamlessly connected on Google Cloud, making it easy for data scientists and AI developers to transition from data to AI. For example, Big Query provides embedded AI features that allow you to directly call SQL commands to train an ML model. Additionally, on Vertex AI, the AI development platform, you can use SQL commands in a notebook to import data from Big Query and further train ML models. Let's cover the products available in each stage of the data to AI workflow. The first category is ingestion and process, which includes products that are used to digest both real time and batch data. The list includes Pub Sub, Data Flow, data PC and Cloud data fusion. Please check cloud.google.com/training for more training on data ingestion and process. The second product category is data storage, and you'll recall from earlier that there are six storage products, Cloud storage, which saves unstructured data such as text image, audio and video, Big Query, Cloud SQL, Spanner, Big Table, and Fire Store. Big Query, Cloud SQL, and Spanner focus on SQL databases, while Big Table and Fire Store are no SQL databases. The third product category is analytics. The major analytics tool is big query, which you'll explore more later in this module. Big Query is a fully managed data warehouse that can be used to analyze data through SQL commands. In addition to big query, you have business intelligence, BI tools to analyze data and visualize results. For example, the Looker family, which includes comprehensive BI tools to visualize, analyze, model, and govern business data. The final product category is AI and machine learning, which includes both AI development tools and AI solutions. These products are either integrated with generative AI or embedded with generative AI capabilities. The major product to support AI development is Vertex AI, which is a unified platform and includes multiple tools such as AutoML for predictive AI, workbench, and Colab Enterprise for coding, and Vertex AI Studio and Model Garden for generative AI. AI solutions are built on the ML development platform and include state of the art technologies to meet both horizontal and vertical market needs. These include document AI, Contact Center AI, Vertex AI search for retail and healthcare data engine. These products unlock insights that only large amounts of data can provide. Many of them have been recently embedded with generative AI capabilities. For example, Contact Center AI is equipped with chat bots that are powered with large language models to understand natural language and engage in conversations like a human agent. You'll explore the machine learning options and workflow together with these products in greater detail later.

#### ML model categories

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565160

Before you start building ML models, let's take a look at model categories. First, let's pause to clarify two terms, artificial intelligence and machine learning. You may note that people often use the terms interchangeably, but they do have some difference. Artificial intelligence, or AI, is an umbrella term that includes anything related to computers mimicking human intelligence. Some examples of AI applications include robots and self-driving cars. Machine learning is a subset of artificial intelligence that allows computers to learn without being explicitly programmed. This is in contrast to traditional programming, where the computer is told explicitly what to do. Machine learning mainly includes supervised and unsupervised learning. You might also hear the terms deep learning or deep neural networks. This is a subset of machine learning that adds layers in between input data and output results to make a machine learn at more depth. You'll learn more about neural networks and deep learning later in this course. Lastly is generative AI, which produces content and performs tasks based on requests. Generative AI relies on training extensive models like large language models. These models are a type of deep learning model. So what's the difference between supervised and unsupervised learning? Imagine two types of problems. In problem one, you are asked to classify dogs and cats from a very large set of pictures. You already know the difference between dogs and cats, so you label each picture and pass the labeled pictures to a machine. By learning from the data, in this case, pictures with the answers or labels, supervised learning is being enacted, allowing the machine to tell if a new picture represents a dog or a cat in the future. In problem two, you are asked to classify breeds of dogs. Unfortunately, this time you don't know how many of them and are not able to label the pictures, so you send these unlabeled pictures to a machine. In this case, the machine learns from the data without answers and finds underlying patterns to group the animals, this is an example of unsupervised learning. Put simply, supervised learning deals with labeled data, is task-driven, and identifies a goal. Unsupervised learning, however, deals with unlabeled data, is data-driven, and identifies a pattern. An easy way to distinguish between the two is that supervised learning provides each data point with a label or an answer, while unsupervised does not. There are two major types of supervised learning, the first is classification, which predicts a categorical variable, such as determining whether a picture shows a cat or or a dog. In ML, you use models like a logistic regression model to solve classification problems. The second type of supervised learning is regression, which predicts a numerical variable like forecasting sales for a product based on its past sales. You use ML models like a linear regression model to solve regression problems. There are three major types of unsupervised learning. The first is clustering, which groups together data points with similar characteristics and assigns them to clusters, like using customer demographics to determine customer segmentation. You use ML models like k-means clustering to solve clustering problems. The second type is association which identifies underlying relationships like a correlation between two products to place them closer together in a grocery store for a promotion. You use association rule learning techniques and algorithms like a priori to solve association problems. And the third type of unsupervised learning is dimensionality reduction, which reduces a number of dimensions or features in a data set to improve the efficiency of a model. For example, combining customer characteristics like age, driving violation history, or car type to create a simplified rule for calculating and and insurance quote. You use ML techniques like principal component analysis to solve these problems. All right, time to test your learning. You are asked to predict customer spending based on purchase history. Is this supervised or unsupervised learning? Yes, that's supervised learning because you have the labeled data, the amount the customers have spent, and you want to predict their future purchases. Is this a classification or regression problem? Yes, it's a regression problem because it predicts a continuous number future spending. Which ML model should you use, a logistic regression or a linear regression? Yes, a linear regression. A logistic regression model is for classification problems while a linear regression model is for regression problems. Lets look at another scenario. Imagine you are using the same dataset, however, this time you are asked to identify customer segmentation. You don't want to base your judgment on stereotypes such as age or gender, so you use a computer for help. Is this supervised or unsupervised learning? Yes, it's unsupervised learning because you don't have each customer labeled as belonging to a certain segment, instead, you want the computer to discover the underlying pattern. Is it a clustering association or dimensionality deduction problem? Yes, identifying customer segmentation is a clustering problem. Which ML model should you use? Logistic regression, linear regression, or k-means clustering analysis? Right, it's a clustering analysis scenario. You will find these models within BigQuery ML, auto ML and custom training later on in this course.

#### BigQuery ML

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565161

With the different types of ML models in your mind. Let's apply concept to practice, in this lesson, you explore BigQuery ML and walk through the steps to build an ML model with SQL commands. You learned about BigQuery, the primary data analytics tool on Google Cloud, from the previous lesson. BigQuery provides two services in one, it's a fully managed storage facility to load and store datasets, and it's a fast SQL-based analytical engine. The two services are connected by Googles high speed internal network. Its this super fast network that allows BigQuery to scale both storage and compute independently based on demand. Although it started out solely as a data warehouse, over time it has evolved to provide features that support the data to AI lifecycle, meaning you can perform both data analytics and build ML models within BigQuery. In this lesson, you explore BigQuery capabilities to build ML models and walk through the steps and key SQL commands to do so. If you've worked with ML models before, you know that building and training them can be very time intensive. You must import and prepare the data, then experiment with different ML models and tune the parameters to improve the model performance. You also need to go back and forth to train the model with new data and features. And finally, you need to deploy the model to make predictions. This is an iterative process that requires a lot of time and resources. Now, with BigQuery ML, you can manage tabular data and execute ML models in one place with just a few steps. BigQuery ML tunes the parameters for you and helps you manage the ML workflow. Let's walk through the phases of a machine learning project and the key SQL commands. In phase 1, you extract, transform, and load data into BigQuery. If it isn't there already. If you're already using other Google products like YouTube for example, look out for easy connectors to get that data into BigQuery. Before you build your own pipeline. You can enrich your existing data warehouse with other data sources by using SQL joins. In phase 2, you select and preprocess features. You can use SQL to create the training dataset for the model to learn from BigQuery ML does some of the preprocessing for you like one hot encoding of your categorical variables? One hot encoding converts your categorical data into numeric data that is required by a training model. In phase 3, you create the model inside BigQuery. This is done by using the create model command. In this example, you want to create an ML model to predict customer purchasing behavior, specifically if they will buy the product in the future, you give the model a name ecommerce classification. You then specify the model type. Remember the previous lesson about ML model types. If you want to predict whether a customer will buy or not, which ML model should you use? That's right, a logistic regression model is the answer because you are solving a classification problem other than the logistic regression model, Bigquery ML supports other popular ML models. Including regression models such as linear regression and other models such as k means clustering and time series forecasting models. In addition to providing different types of machine learning models, BigQuery ML supports ML OPs machine learning operations. MLOPs turns your ML experiment to production and helps deploy, monitor, and manage the ML models. You'll learn more about MLOPs later in this course. You are recommended to start with simple options such as logistic regression and linear regression. And use the results as a benchmark to compare against more complex models such as DNN deep neural networks, which take more time and computing resources to train and deploy. After specifying the model type, you also need to define the label column. Why, remember the two major categories of ML models, supervised and unsupervised. The former deals with labeled data and predicts a goal, whereas the latter handles unlabeled data and identifies a hidden pattern. Is this a supervised or unsupervised model? Of course its a supervised classification problem, thus a labeled column. From there, you can run the query. In phase 4, after your model is trained, you can execute an ML evaluate query to evaluate the performance of the trained model on your evaluation data set. It's here that you specify which evaluation metrics the model will access, such as accuracy, precision, and recall. You'll explore these metrics later in the next module. Finally, in phase 5, when you're happy with your model performance, you can then use it to make predictions. To do so, invoke the ML predict command on your newly trained model to return with predictions and the model's confidence in those predictions. With the results your label field will have predicted added to the field name. This is your model's prediction for that label. You'll practice all these steps and the SQL commands using BigQuery ML in the hands on lab.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565162

Now it's time to get some hands-on practice building a machine learning model in BigQuery. In the lab that follows this video, you'll use e-commerce data from the Google Merchandise Store, shop.merch.google. The site's visitor and order data has been loaded into BigQuery, and you'll build a machine learning model to predict whether a visitor will return for more purchases later. You'll get practice creating a dataset in BigQuery, training an ML model, evaluating the model, and using the model for prediction. Let's get started.

#### Predict Visitor Purchases with BigQuery ML

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/labs/565163

#### Summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565164

In the first module of this course, you learned about AI foundations. Let's have a quick recap. You started with the story of coffee on wheels, which demonstrates how AI enables business processes, transformation and efficiency enhancement. You were then introduced to the toolbox of AI and ML on Google Cloud, which consists of three layers, AI foundations, AI development and AI solutions. This course focuses on the second layer which delves into both predictive AI and generative AI. Next, you explored the Google Cloud infrastructure, specifically compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. Additionally, you examined data and AI products which enable you to perform tasks to support the data to AI journey from data ingestion, storage and analytics to AI and machine learning. After that, you advance to the fundamental ML concepts, including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. With these concepts in mind, you can choose from different models and follow the steps to build an ML model to fit your needs with BigQuery ML. Finally, you had a hands on lab where you applied those steps to build your own ML model using SQL commands. This concludes the overview of the AI foundations module. In the next module, you'll advance to AI development and explore the different options to build an AI and ML project. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/quizzes/565165

#### Reading

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/documents/565166

### AI Development Options

#### Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565167

In the first section of this course, you learned about AI Foundations, specifically on Cloud essentials like compute, storage, and networking. You also explored data tools like BigQuery, which helps you start the journey from data to AI. Now you'll explore how to develop an AI project on Google Cloud, specifically, the options available to build a machine learning, or ML model. You begin by comparing AI development options on Google Cloud, from pre-made to low-code and no-code, and finally a do-it-yourself approach. You then delve into the first option, pre-trained APIs, which use pre-trained ML models that don't require any training data. Next, you are introduced to Vertex AI, Google's unified platform to build an ML model end-to-end and support both no-code and code development. After that, you explore AutoML on Vertex AI, a low or no-code option to automate the ML development from data preparation to model training and model serving with your own training data. Finally, you explore the last option, custom training. Which allows you to manually code ML projects with tools like Python, Keras and TensorFlow. Consolidating the background knowledge gained throughout this module, you conclude with a hands on practice using the natural language API to identify subjects and analyze sentiment in text. Let's get started

#### AI developement options

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565168

Let's begin with Google Cloud's AI development options, which include out-of-the-box, low-code and no-code, and do it yourself. You'll explore the pros and cons of each and examine a decision tree, to help you consider which option might be best for your business problem. Imagine that you're helping your organization use AI to transform your business model and operation. Where and how should you start? Let's say you are a business user or an application developer, and you don't have training data and lack experience developing an ML model. However, you really want to use AI to automatically label and classify customer feedback. What can you do? Or perhaps you're a data analyst and you have some training data and experience using SQL. How can you build a custom ML model with your existing skills? Maybe you're a data scientist and you have a large amount of data, and want to train a custom ML model. However, you don't want to spend hours tuning ML parameters from the beginning. What choices do you have? Or what if you're an ML engineer or scientist, and enjoy using do-it-yourself models and code to operationalize the ML pipeline. What tools can you use? Well, you can find tools on Google Cloud to help you achieve your goals. From pre-configured solutions such as pre-trained APIs, to low or no code solutions such as BigQuery ML and AutoML, to a completely DIY approach using a code based solution by using custom training. Let's look at each of them. Google Cloud offers four options for building machine learning models. The first option is to use pre-trained APIs. API stands for application programming interface. This option lets you use pre-trained machine learning models, so you don't need to build your own if you don't have training data or machine learning expertise in the house. The second option is BigQuery ML, which you learned about in the previous module. This option uses SQL queries to create and execute machine learning models in BigQuery. If you already have your data in BigQuery, and your problems fit the predefined ML models offered by BigQuery ML, this could be your choice. The third option is AutoML. Which is a no-code solution that helps you build your own machine learning models on Vertex AI, through a point and click interface. Finally, there is custom training, through which you can code your very own machine learning environment, training and deployment. This option allows you flexibility and control over the ML pipeline. Let's compare the four options to help you decide which one to use for building your ML model. Note that the technologies change constantly and this is only a brief guideline. BigQuery ML only supports tabular data, whereas the other three support tabular image, text, and video. Pre-trained APIs also process audio. In terms of training data size, pre-trained APIs do not require any training data. Whereas BigQuery ML and custom training, require a large amount of data. Pre-trained APIs and AutoML, are user-friendly with low requirements for machine learning and coding expertise. Whereas custom training has the highest requirement, and BigQuery ML requires you to understand SQL. At the moment, you can't tune the hyperparameters with pre-trained APIs or AutoML. However, you can experiment with hyperparameters by using BigQuery ML and custom training. Pre-trained APIs require no time to train a model, because they directly use pre-trained models from Google. The time to train a model for the other three options depends on the specific project. Normally, custom training takes the longest time, because it builds the ML model from the beginning unlike AutoML and BigQuery ML. The best option depends on your business needs and ML expertise. Budget is also an important consideration. Visit Google Cloud's website for detailed pricing information. If you have little ML experience and no intention to train your own ML models, using pre-trained APIs might be the best choice. Pre-trained APIs address common perceptual tasks such as vision, video, and natural language. They are ready to use without any model development effort. If your data engineers, scientists, or analysts are familiar with SQL and already have data in BigQuery, BigQuery ML lets you use SQL queries to build predefined ML models. If you wish to build custom models with your own training data while you spend minimal time coding, then AutoML on Vertex AI is your choice. AutoML allows you to focus on business problems instead of the underlying model architecture and provisioning. If your ML engineers and data scientists want full control of ML workflow, Vertex AI custom training, lets you train and serve custom models with code on Vertex AI Workbench or Google Colab. Let's walk through pre-trained APIs, AutoML, and custom training one-by-one later in this module.

#### Pre-trained APIs

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565169

Now that youve been introduced to the different AI development options available with Google Cloud, lets now focus on the first: pre-trained APIs. Good machine learning models require lots of high-quality training data. You should aim for hundreds of thousands of records to train a custom model. But what if you don't have that kind of data? How can you use AI to serve your purposes? Pre-trained APIs are a great place to start. API stands for application programming interface, and they define how software components communicate with each other. Imagine APIs are electric sockets. Different regions have different standards, for example, the US has type A and B whereas Europe has type F. As a traveller, you only need to know which adapter to use without worrying about whats behind the wall and how to build the electric network. The same principle applies to APIs. As a user, you only need to know which API to fit your code without worrying about the implementation of the APIs. Or in other words, how to train and deploy ML models. After you plug the API to your code, you can directly use the functions. Pre-trained APIs are offered as services. In many cases, they can act as building blocks to create the application you want without the expense or complexity of creating your own models. They save the time and effort of building, curating, and training a new dataset so you can just directly deal with predictions. So, what are the pre-trained APIs provided by Google Cloud? Lets explore a short list. Speech, text, and language APIs For example, the Natural Language API derives insights from text using pre-trained large language models. It recognizes the entities and sentiment of a sentence. Image and video APIs For example, the Vision API recognizes content in static images, and the Video Intelligence API recognizes motion and action in video. Document and data APIs For example, the Document API processes documents like text extraction and form parser. It can be used in specialized use cases like lending, contracts, procurement, and identity documents. Conversational AI APIs For example, the Dialogflow API builds conversational interfaces. Lets try out the Natural Language API in a browser. Scroll down to try the API by uploading a paragraph of text, which can be in over ten different languages including Chinese, English, and Spanish. Feel free to view the full list by clicking See supported languages. For example, if you use the sample paragraph about Google, and click ANALYZE, You find four types of analysis: entity, sentiment, syntax, and category. Entity analysis identifies the subjects in the text including: A proper noun, such as the name of a particular person, place, organization, or thing. In this case, the Natural Language API automatically detects Google as an organization, Mountain View as a location, and Sundar Pichai as a person. Common nouns such as goods are also addressed. In this case, It identifies Android and phone as consumer goods. How can entity analysis be applied to solve your business problems? How about automatic tagging? Say you have tons of legal documents and you want to auto-tag the main words of each document. How about document classification? Say you want to classify your documents to different categories based on key information in the text. Or what about information extraction, so you can generate summaries based on the key entities in the doc? There are many other usages of entity analysis. Take a minute to think about how to apply it to your use cases. Sentiment analysis is used to identify the emotions indicated in the text such as positive, negative, and neutral. Score ranges between -1.0 (negative) and 1.0 (positive) and magnitude indicates the overall strength of emotion within the given text, between between zero and positive infinity. The Natural Language API can analyze the sentiment for the entire document and at entity level. How can sentiment analysis be used solve your business problems? Well, it can be used to analyze the emotion of customer feedback, social network comments, and conversations. You can then use this data as feedback to adjust your offerings. In addition to entity and sentiment analysis, the Natural Language API can analyze syntax and extract linguistic information for further language model training in a specific field. It can also do category analysis for the entire text. For example, this text is about an internet and telecommunications company. The UI, or user interface, demonstrates the major features of the Natural Language API. When youre ready to build a production model, you can integrate the APIs into your code. Well show you how in the hands-on lab later in this module. In summary, you can build AI projects and applications without training your own ML models or providing training data. Instead, you can use pre-trained AI models through APIs provided by Google. In addition to the above APIs, generative AI APIs have rapidly evolved recently. These APIs allow you to use different foundation models to generate various types of content. Some examples include Gemini multimodal, processing data in multiple modalities like text, image, and video. Embeddings for both text and multimodal, converting multimodal data into numerical vectors that can be processed by ML models, especially gen AI foundation models. Gemini for text and chat, performing language tasks and conducting natural conversations. Imagen for Image, generating images and captions. Chirp for speech, building voice-enabled applications. And Codey for code, generating, completing, and chatting about code. As gen AI advances every day, youll find new APIs and models emerging in the near future. In the latter part of this course, you get a chance to delve deeper into the multifaceted realm of generative AI.

#### Vertex AI

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565170

In this lesson, you'll explore Vertex AI, which is the unified platform that supports various technologies and tools on Google Cloud to help you build an ML project from end to end. For years now, Google has invested time and resources into developing Data and AI. Google had developed crucial technologies and products from scikit-learn as a Google Summer Coding project back in 2007 to Vertex AI and generative AI today. As an AI first company, Google has applied AI technologies to many of its products and services, like Gmail, Google Maps, Google Photos, and Google Translate, just to name a few. But developing these technologies doesn't come without challenges. Some traditional challenges include handling large quantities of data, determining the right machine learning model to train the data and harnessing the required amount of computing power. Then there are challenges around getting ML models into production. Some of the challenges can be scalability, monitoring, and continuous integration, delivery, and training. In fact, according to Gartner, only half of enterprise ML projects get past the pilot phase. There are also ease of use challenges. Many tools on the market require advanced coding skills, which can take a data scientists focus away from model configuration. Without a unified workflow, data scientists often have difficulties finding tools. Google's solution to many of the production and ease of use challenges is Vertex AI, a unified platform that brings all the components of the machine learning ecosystem and workflow together. What exactly does a unified platform mean? There are two primary aspects. Firstly, it means that Vertex AI provides an end-to-end ML pipeline to prepare data and create, deploy, and manage models over time and at scale. For instance, during the data readiness stage, users can upload data from wherever it's stored, Cloud Storage, BigQuery, or a local machine. Then during the feature readiness stage, users can create features, which are the process data that will be put into the model and then share them with others by using the feature store. After that, it's time for training and hyperparameter tuning. This means that when the data is ready, users can experiment with different models and adjust hyperparameters. Finally, during deployment and model monitoring, users can set up the pipeline to transform the model into production by automatically monitoring and performing continuous improvements. You'll learn how to do this later in this course when you explore MLOps. Second, vertex AI is a unified platform that encompasses both predictive AI and generative AI. Predictive AI allows for sales forecasting and classification, while generative AI enables the creation of multimodal content. Vertex AI allows users to build ML models with either AutoML a no code solution, or custom training a code based solution. AutoML provides an easy to navigate UI. It lets data scientists focus on what business problems to solve instead of how to code and deploy an ML solution. Custom training gives data scientists and ML engineers more control over the development environment and process. They can use tools like Vertex AI workbench and Colab to DIY their ML projects. One convenient feature is that data scientists can now write SQL with workbench on Vertex AI to seamlessly connect BigQuery and Vertex AI. Being able to perform such a wide range of tasks in one unified platform has many benefits. This can be summarized with four Ss. It's seamless. Vertex AI provides a smooth user experience from uploading and preparing data all the way to model training and production. It's scalable. The machine learning operations or MLOps provided by Vertex AI helps to monitor and manage the ML production and therefore scale the storage and computing power automatically. It's sustainable. All of the artifacts and features created using Vertex AI can be reused and shared. It's speedy. Vertex AI produces models that have 80% fewer lines of code than competitors. In addition to AutoML and custom training, Vertex AI also provides tools for generative AI. You can use these tools to generate content and embed generative AI into your applications. We will discuss generative AI technologies and tools later in this course.

#### AutoML

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565171

In the last lesson you learned about Vertex AI, a unified platform that supports both AutoML, a no code solution, and custom training a code based solution. In this lesson, you'll explore AutoML in depth, including the technologies used to power automated ML development. Auto ML, which stands for automated machine learning aims to automate the process to develop and deploy an ML model. If you've worked with ML models before, you know that building them can be extremely time consuming because you need to repeatedly add new data and features. Try different models and tune parameters to achieve the best results. When AutoML was first introduced in January of 2018, the goal was to save the manual work from data scientists and automate machine learning pipelines from preprocessing data to model training and deployment. Since 2021, AutoML features are embedded in vertex AI and have become part of the platform. But how could this be done? How can you trust AutoML to generate the best results without bias and do so in a speedy way? Let's look deeper to explore how AutoML works and the main technologies behind it. AutoML is powered by the latest research from google, it's an ongoing endeavor. There are four distinct phases, phase 1 is data processing. After you upload a dataset, AutoML provides functions to automate part of the data preparation process. For example, it can convert numbers, date time, text categories, arrays of categories, and nested fields into a certain format of data so that it can be fed into an ML model. Phase 2 includes searching the best models and tuning the parameters. Two critical technologies support this auto search, the first is called neural architect search, which helps search the best models and tune the parameters automatically. And the second is called transfer learning, which helps speed the searching by using pre trained models. Let's look at transfer learning first, machine learning is similar to human learning it learns new things based on existing knowledge. AutoML has already trained many different models with large amounts of data, these trained models can be used as a foundation model to solve new problems with new data. A typical example are large language models, LLMs which are general purpose and can be pre trained and fine tuned for specific purposes. LLMs are trained for general purposes to solve common language problems, such as: text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields, such as: retail, finance and entertainment, using a relatively small size of field datasets. Transfer learning is a powerful technique that lets people with smaller data sets or less computational power, achieve great results by using pre trained models trained on similar larger datasets. Because the model learns through transfer learning it doesn't have to learn from the beginning, so it can generally reach higher accuracy with much less data and computation time than models that don't use transfer learning. Now, let's look at neural architecture search, the goal of neural architecture search is to find optimal models among many options specifically, AutoML tries different architectures and models and compares against the performance between models to find the best ones. For instance, auto ML can search through multiple advanced ML models and automatically tune the parameters to find the best fit for your data. In phase 3 the best models are assembled from phase 2 and prepared for prediction in phase four. Note that AutoML does not rely on one single model, but on the top number of models. The number of models depends on the training budget, but is typically around ten. The assembly can be as simple as averaging the predictions of the top number of models relying on multiple top models instead of one greatly improves the accuracy of prediction. By applying these advanced ML technologies, AutoML automates the pipeline from feature engineering to architecture search, to hyperparameter tuning and to model assembly. It might seem that AutoML can do a better job than a human to find the optimal models that fit your data, perhaps, the best feature of auto ML is that it provides a no code solution. You can point and click through a UI to build an ML model with your own data. You'll walk through the details from preparing training data to train your model and finally get predictions in the next module.

#### Custom training

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565172

Now, let's look at custom training. You do it yourself solution to build an ML project. You explored the options that Google Cloud provides to build machine learning models by using pre-trained APIs, where you directly call Google's train models to solve your problems. BigQuery ML, where you write a few lines of SQL code to train your own model, and AutoML, where you build your ML model with your own data through a UI. But if you want to create your own machine learning environment to experiment with and build your own pipeline, you need custom training. Before any coding begins, you must determine what environment you want your ML training code to use. There are two options, a pre-built container or a custom container. A pre-built container is like a furnished kitchen with cabinets, appliances, and cookware. If your ML training needs a platform like Python, TensorFlow, and PyTorch, and you're not particular about the underlying infrastructure to run on or to use our kitchen analogy, which oven or knife you use, a pre-built container is probably your best choice. A custom container alternatively, is like an empty room. You define the exact appliances and tools that you prefer to cook with. That means you must determine the details like the environment, machine type, and discs when creating the custom container. In terms of the tools to code your ML model, you can use Vertex AI Workbench. You can think of Vertex AI Workbench as a Jupiter Notebook deployed in a single development environment that supports the entire data science workflow from exploring to training, and then deploying a machine learning model. You can also use Colab Enterprise, which was integrated into Vertex AI platform in 2023, so data scientists can code in a familiar environment. After you decide the working environment, the next step is to start writing code. These days, you don't have to code from scratch. Instead, you can leverage ML libraries. An ML library is a collection of pre written code that can be used to perform machine learning tasks. These libraries can save developers time and effort by providing them with the tools they need to build machine learning models without having to write everything from the beginning. As a data scientist, you might already be familiar with popular ML libraries like TensorFlow, Scikit-learn, and PyTorch. They are open-source and widely used by a large community of users and developers. Let's explore TensorFlow, an end to end open platform for machine learning supported by Google. TensorFlow contains multiple abstraction layers. You use TensorFlow APIs to develop and train ML models. The TensorFlow APIs are arranged hierarchically with the high level APIs built on the low level APIs. The lowest layer is hardware. TensorFlow can run on different hardware platforms, including CPU, GPU, and TPU. The next layer is the low level TensorFlow APIs, where you can write your own operations in C++ and call the core basic and numeric processing functions written in Python. The third layer is the TensorFlow model libraries, which provide the building blocks such as neural network layers and evaluation metrics to create a custom ML model. The high level TensorFlow APIs like Keras sit on top of this hierarchy. They hide the ML building details and automatically deploy the training. They can be your most used APIs. Note that Vertex AI fully hosts TensorFlow from low level to high level APIs. Regardless of which abstraction level you are writing your TensorFlow code at, Vertex AI gives you a managed service. Now let's look at an example of using TF Keras, a high level TensorFlow library commonly used to build a simple regression model. Typically, it takes three fundamental steps. In Step 1, you create a model where you piece together the layers of a neural network. In Step 2, you compile the model, where you specify hyper parameters such as performance evaluation and model optimization. Finally, you train your model to find the best fit. Assume you already imported necessary packages like TensorFlow and uploaded the data. The first step is to create a model by using TF Keras sequential. To demonstrate, you can define your model as a three layer neural network. You'll explore more details about neural networks such as activation functions and the next module. The next step is to compile the model by specifying how you want to train it by using the method compile. For instance, you can decide how to measure the performance by specifying a loss function. You can also optimize the training by pointing to an optimizer. The last step is to train the model by using the method fit. For instance, you can define the input, the training data, and the output, the predicted results. You can also decide how many iterations you want to train the model by specifying the number of epoxs. After you train the model and are satisfied with the performance, you can then deploy the model and make predictions. Apart from TensorFlow, Google is constantly introducing new frameworks. One of the most promising frameworks is JAX. JAX is a high performance numerical computation library that is highly flexible and easy to use. It offers new possibilities for both research and production environments.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565173

Now it's time for some hands-on practice. In the following lab, you'll use the Natural Language API to analyze texts. Specifically, you'll identify entities and analyzeSentiment with code. Before you begin, let's briefly review the main features of the Natural Language API you learned in the previous lesson. You can identify entities which are subjects in the inputted texts, such as Google as a company name, and Mountain View as a location. You can identify the sentiment which indicates a motion at both the overall document and individual subject level. You can analyzeSyntax and extract linguistic information, such as the relationship between words, and you can also classify the text to categories based on topics or keywords. Similar to assigning a tag to a piece of text. You perform all of this analysis through a UI, which is a quick and efficient way to demonstrate and test these features. However, if you want to incorporate these features in production, you must embed the APIs into code. Using APIs in your code is similar to ordering a sandwich at a Deli. You order from the menu and get your food without worrying about how it was made in the kitchen. The same concept applies to using APIs. You only need to know three things. The features, like the menu, the input, like the order, and the output, like the sandwich. Like a menu, features are the types of requests that you can make to the Natural Language API. Like a food order, the input is how you construct the requests. Then the sandwich you receive after you place the order is the response or output. With this, you can determine next steps. What are the different types of requests that you can make? The Natural Language API provides several methods for performing analysis and annotations on your text. You'll practice with most of them in the lab. For entity analysis, you can use the analyzeEntities method. The sentiment analysis is performed through the analyzeSentiment method at the entire texts level, and analyzeEntitiesSentiment at the individual entity and subject level. The syntactic analysis is performed with the analyzeSyntax method. The content classification is performed by using the classifyText method. Now, how do you construct those requests? The Natural Language API is a rest API and consists of JSON requests and responses. A simple JSON request for entity analysis looks like the code shown here, where you define the type of the document. For example, plain text, the language like EN, which stands for English. The content, which can be the text itself or the file location and Cloud Storage, and finally, the encoding type like UTF8. After you construct the request, you need to call the API. Just like after you decide what you want at a Deli, you then need to place the order with a counter person. Here is an example to call the API with curl. Curl stands for client URL, and is a command line tool to transfer data between client and server. You can also use other programming languages such as Python and Java SDKs to call the APIs. Typically, the vendors of the products and services you are using to find the APIs and provide the SDKs in different languages for you to choose. In this example, you call the Natural Language API feature analyzeEntities. Pass the request.json file that you just constructed and save the response to result.json file. Finally, how should you handle the responses? You can review the result by using a command like cat result.json, or parse it for further usage. Equipped with the technical details in this lab, you'll use the Natural Language API to extract entities, analyzeSentiment and analyzeSyntax. By completing the lab, you'll get practice creating a Natural Language API request and calling the API with curl, extracting entities and running sentiment analysis on text, performing linguistic analysis on text, and creating a Natural Language API request in a different language. Let's start

#### Entity and Sentiment Analysis with the Natural Language API

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/labs/565174

#### Summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565175

This concludes the module on AI development options. Let's do a quick recap. You compared AI development options from a ready-made approach to low-code and no-code to do it yourself. You started with pre-trained APIs, which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google's unified platform to build a machine learning project from end-to-end. You learned about AutoML, which is a low or no-code tool on Vertex AI that lets you automate ML development from data preparation to model training and model serving with your own data. Then you learned about custom training, which lets you manually code ML projects by using tools like Python, TensorFlow, and Vertex AI workbench. Finally, you've got hands-on practice with a lab on the Natural Language API, which identifies subjects and analyzes sentiment in text. In the next module, you'll explore the AI development workflow, where you walk through the ML pipeline and build an ML model end-to-end. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/quizzes/565176

#### Reading

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/documents/565177

### AI Development Workflow

#### Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565178

In the previous section of this course, you learned about different options to develop an AI project on Google Cloud, from ready to use approaches like pre-trained APIs to low or no-code solutions like AutoML, and to DIY solutions such as custom training. In this section, you further investigate how to develop an AI project on Google Cloud. Specifically, you walk through the ML workflow and explore how to create an automated pipeline. You begin with an overview of the ML workflow and then focus on the 1st stage of the workflow, data preparation, which includes data uploading and feature engineering. After that, you advance to the 2nd stage, model development. This includes model training and evaluation. You then proceed to the 3rd stage, model serving. This includes model deployment and monitoring. Next, you learn about machine learning operations, or MLOps, which takes ML models from development to production. You'll also be shown an example of how to build a pipeline to automate the training, evaluation, and deployment of an ML model by using vertex AI pipelines. You conclude this module with a hands on lab, where you walk through the three stages to build an ML model with auto ML on vertex AI. Gaining a solid grasp of ML terminology requires a clear understanding of how a neural network learns. This module offers an optional lesson that delves into the neural networks learning process along with the key terminologies. If you're already familiar with the ML theories, feel free to skip this lesson. Let's get started.

#### ML workflow

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565179

Let's look at the ML workflow and walk through the main stages. Building an ML model is actually not too different from serving food in a restaurant. You start by preparing raw ingredients and finish by serving the dishes on the table. There are three main stages to the ML workflow with Vertex AI. The first stage is data preparation, which includes two steps, data uploading, and feature engineering. A model needs a large amount of data to learn from. The quality and quantity of the data, decide how much and how well the machine learns. The data used in machine learning can be real time streaming data or batch data. The data can also be structured or unstructured. Structured data is data that can be easily stored in tables, such as numbers and text. Unstructured data is data that cannot be easily stored in tables, such as images and videos. The second stage of the ML workflow is model development. A model needs a tremendous amount of iterative training. This is when training and evaluation form a cycle to train a model and then evaluate the model and then train the model more. The third and final stage is model serving. A model needs to actually be used in order to predict results. This is when the machine learning model is deployed and monitored. If you don't move an ML model into production, it has no use and remains only a theoretical model. Compare this process to serving food in a restaurant. Data preparation is when you prepare the raw ingredients. Model development is when you experiment with different recipes, and model serving is when you finalize the menu to serve the meal to customers. Now, it's important to note that an ML workflow is not linear. It's iterative. For example, during model training, you might need to return to the raw data and generate more useful features to feed the model. When monitoring the model during model serving, you might find data drifting, or the accuracy of your prediction might suddenly drop. You might need to check the data sources and adjust the model parameters. Fortunately, these steps can be automated with ML Ops. You'll learn more about this later in this module. Although the main stages remain the same, you have two options to set up the workflow with Vertex AI. The first choice is to use AutoML, a no code solution that lets you build an ML model through UI. It's user friendly and doesn't require a lot of ML expertise. Also, no coding skills are needed. Alternatively, you can code the workflow with Vertex AI Workbench or Colab using Vertex AI Pipelines. Vertex AI Pipelines is essentially a toolkit that includes pre-built SDKs and software development kits, which are the building blocks of a pipeline. Coding the pipeline is a good option if you're an experienced ML engineer or data scientist and want to automate the workflow programmatically. Let's focus on AutoML next and then explore the code-based approach later in this module.

#### Data preparation

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565180

Let's start with the first stage of the ML workflow, data preparation. During this stage, you must upload data and then prepare it for model training and feature engineering. The data can come from Cloud storage, big query, or even your local machine. AutoML supports four types of data; image, tabular, text, and video. For each data type, AutoML solves different types of problems called objectives. For image data, you can train the model to classify images, either single label or multi label. Single label means that you can only assign one tag to an image like dog or cat. Whereas multi label means that you can assign multiple tags to one image, like dog, large, and brown. You can also train the model to detect objects and discover image segmentation. For tabular data, you can train the model to solve regression, classification, or forecasting problems. Forecasting is vital to many industries like retail. To learn more about how to build a forecasting model, please check the course titled, Introduction to Vertex Forecasting and Time Series and Practice in the reading list. For text data, you can train the model to classify text, extract entities, and conduct sentiment analysis. Do these tasks sound familiar? Right. You learned about entity extraction and sentiment analysis when you explored natural language APIs in the previous module. Finally, for video data, you can train the model to recognize video actions, classify videos, and track objects. Although the tasks can be similar, pre trained APIs rely on Google's pre trained ML models with no customer data, whereas AutoML trains a custom model using your own data. In reality, you might not be restricted to just one data type and one objective. Instead, you might need to combine multiple data types and different objectives to solve a business problem. AutoML is a powerful tool that can help you do this. After the data is uploaded to AutoML, the next step is preparing it for model training with feature engineering. Imagine you're preparing a meal. Your data is like your ingredients, such as carrots, onions, and tomatoes. Before you start cooking, you'll need to peel the carrots, chop the onions, and rinse the tomatoes. This is what feature engineering is like. The data must be processed before the model starts training. A feature refers to a factor that contributes to the prediction. It's like an independent variable in statistics or a column in a table. Preparing features can be both challenging and tedious. To help, Vertex AI provides a service called Vertex AI Feature Store, which is a centralized repository to manage, serve, and share features. It aggregates the features from different sources in Big Query and makes them available for both real time, often called online, and batch, often called offline serving. Vertex AI automates the feature aggregation to scale the process with low latency. Additionally, Vertex AI features store is ready for the challenge of generative AI. It can manage and serve embeddings, which is the crucial data type in gen AI. It also supports retrieving similar items in real time, ensuring low latency. The workflow to setup and start real time online serving using Vertex AI Feature store can be summarized as follows. First, prepare the data source in big query. Optionally, register the data sources by creating feature groups and features. Set up online store and feature view resources to connect the feature data sources with online serving clusters and serve the latest feature values online from a feature view. So what are the benefits of vertex AI feature store? First, features are shareable for training and serving. They are managed and served from a central repository, maintaining consistency across your organization. Second, features are reusable. This helps to save time and reduces duplicated efforts. Third, features are scalable. They automatically scale to provide low latency serving so you can focus on developing the logic to create them without worrying about deployment. Fourth, features are easy to use. Vertex AI feature store is built on an easy to navigate user interface.

#### Model development

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565181

Let's advance to the second stage model development, where you train the model and evaluate the result. Now that our data is ready, which, if you return to the cooking analogy is the ingredients, it's time to train the model. This is like experimenting with the recipes. This stage involves two steps, model training, which is like cooking the recipe, and model evaluation, which is like testing how good the meal is, this process might be iterative. To set up an ML model, you need to specify a few things. First of all, is the training method where you tell Vertex AI the dataset you just uploaded from the preparation stage. Depending on the data type, whether it's tabular, image, text or video, you specify the training objective. This is the goal of the model training and the task you want to solve. Then you decide the training method, AutoML, without code or custom training using code. The next step is to determine the training details. For example, if you are training the model to solve a supervised learning problem such as regression and classification, you must choose the target column from your dataset. In training options, you can choose certain features to participate in the training and transform the data type if needed. Finally, you specify the budget in pricing and then click "Start Training." AutoML will train the model for you and choose the best performed models amongst thousands of others. Do you recall the powerful technologies behind AutoML? The credit there goes to neural architecture search and transfer learning. While you're experimenting with a recipe, you need to keep tasting it to ensure that it meets expectations. This is the evaluation portion of the model development stage. Vertex AI provides extensive evaluation metrics to help determine a model's performance. Let's focus on the metrics of recall and precision when evaluating the performance of classification models. To do this, you'll use a confusion matrix. A confusion matrix is a specific performance measurement for machine learning classification problems. It's a table with combinations of predicted and actual values. To keep things simple, we assume the output includes only two classes. Let's explore an example. The first is true positive, which can be interpreted as the model predicted positive, and that's true. The model predicted that this is an image of a cat and it actually is. The opposite of that is true negative, which can be interpreted as the model predicted negative, and that's true. The model predicted that the image is not a cat and it actually isn't. Then there is false positive, otherwise known as a Type 1 error, which can be interpreted as the model predicted positive and that's false. The model predicted that the image is a cat, but it actually isn't. Finally, there is false negative, otherwise known as a Type 2 error, which can be interpreted as the model predicted negative and that's false. The model predicted that the image is not a cat, but it actually is. A confusion matrix is the foundation of many other metrics used to evaluate the performance of a machine learning model. Let's look at the two popular matrix, recall and precision that you will encounter in the lab. Recall refers to all the positive cases and looks at how many were predicted correctly. This means that recall is equal to the true positives, divided by the sum of the true positives and false negatives. Precision refers to all the cases predicted as positive and how many are actually positive. This means that precision is equal to the true positives, divided by the sum of the true positives and false positives. Imagine you're fishing with a net, using a wide net, you caught both fish and rocks, 80 fish out of 100 total fish in the lake plus 80 rocks. The recall in this case is 80%, which is calculated by the number of fish caught 80, divided by the total number of fish in the lake, 100. The precision is 50%, which is calculated by taking the number of fish caught 80, and dividing it by the number of fish and rocks collected 160. Let's say you wanted to improve the precision, so you switched to a smaller net. This time, you caught 20 fish and zero rocks. The recall becomes 20%, 20 out of 100 fish collected, and the precision becomes 100%, 20 out of 20 total fish and rocks collected. Precision and recall are often a trade off. Depending on your use case, you might need to optimize for one or the other. Consider a classification model where Gmail separates emails into two categories, spam and not spam. If the goal is to catch as many potential spam emails as possible, Gmail might want to prioritize recall. In contrast, if the goal is to only catch the messages that are definitely spam without blocking other emails, Gmail might want to prioritize precision. Vertex AI visualizes the precision recall curve so that it can be adjusted based on the problem that needs to be solved. You'll get the opportunity to practice adjusting precision and recall in the AutoML lab. In addition, to the confusion matrix and the metrics generated to measure recall and precision, the other useful measurement is feature importance. In Vertex AI, feature importance is displayed through a bar chart to illustrate how each feature contributes to a prediction. The longer the bar or the larger the numerical value associated with a feature, the more important it is. This information helps decide which features are included in a machine learning model to predict the goal. You will observe the feature importance chart in the lab as well. Feature importance is just one example of Vertex AI's comprehensive machine learning functionality called explainable. Explainable AI is a set of tools and frameworks to help understand and interpret predictions made by machine learning model. Please check the reading list if you want to learn more about explainable AI on Google Cloud.

#### Model serving

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565182

Let's focus on the third stage of the ML workflow, model serving. The recipes are ready, and now it's time to serve the meal. This represents the final stage of the machine learning workflow, model serving. Model serving consists of two steps. First, model deployment, which you can compare to serving a meal to a hungry customer, and second model monitoring, which is like checking with the wait staff to ensure that the restaurant is operating efficiently. It's important to note that model management exists throughout this whole workflow to manage the underlying machine learning infrastructure. This lets data scientists focus on what to do and not how to do it. Let's start with model deployment, which is the exciting time when the model is implemented and ready to serve. You have two primary options. Deploy the model to an endpoint for real-time predictions or often called online predictions. This option is best when immediate results with low latency are needed, such as making instant recommendations based on a user's browsing habits whenever they're online. A model must be deployed to an endpoint before it can be used to serve real-time predictions. Request the prediction job directly from the model resource for batch prediction. This option is best when no immediate response is required. For example, sending out marketing campaigns every other week, based on the user's recent purchasing behavior and what's currently popular on the market. Batch prediction does not require deploying the model to an endpoint. You can deploy a model either using the UI on Vertex AI or using code by calling APIs. You'll practice building an endpoint later in the lab. Beyond making predictions in the Cloud, deploying the model off Cloud is also possible. This approach is generally adopted when the model needs to be deployed in a specific environment to mitigate latency, ensure privacy, or enable offline functionality. For instance, consider an IOT Internet of Things application like object detection that utilizes a camera feed in a manufacturing plant. In such use case, the added latency of relying on the Cloud can be impractical. Once the model is deployed and begins making predictions or generating contents, it is important to monitor its performance. The backbone of automating ML workflow on Vertex AI is a tool kit called Vertex AI Pipelines. It automates monitors, and governs machine learning systems by orchestrating the workflow in a serverless manner. Imagine you're in a production control room and Vertex AI Pipelines is displaying the production data on screen. If something goes wrong, it automatically triggers and displays a warning based on a pre-defined threshold. With Vertex AI Workbench, which is a notebook tool, you can define your own pipeline using SDKs. You can do this with pre-built pipeline components, which means that you primarily need to specify how the pipeline is put together using components as building blocks. You'll explore more details about Vertex AI Pipelines in the next lesson. It's with these final two steps, model deployment, and model monitoring that you complete the exploration of the machine learning workflow. The restaurant is open and operating smoothly, bon appetit.

#### MLOps and workflow automation

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565183

In this lesson, you explore an advanced topic, MLOps and workflow automation. You learned from the previous lessons to build an ML model through three main stages, data preparation, model development, and model serving. You have two approaches to build an end-to-end workflow, one is codeless through Google Cloud Console like AutoML on Vertex AI. But what if you want to automate this workflow to achieve continuous integration, training, and delivery? Here comes the other option, to code a pipeline that automates the ML workflow. Machine learning operations, or MLOps, play a big role. MLOps combines machine learning development with operations and applies similar principles from DevOps, or development operations, to machine learning models. MLOps aims to solve production challenges related to machine learning. In this case, this refers to building an integrated machine learning system and operating it in production. These are considered to be some of the biggest pain points by the ML practitioners community because both data and code are constantly evolving in machine learning. Practicing MLOps means automating and monitoring each step of the ML system construction and operating it in production to enable continuous integration, training, and delivery. The backbone of MLOps on Vertex AI is a toolkit called Vertex AI Pipelines, which supports both Kubeflow Pipelines, or KFP, and TensorFlow Extended, or TFX. If you already use TensorFlow to build ML models that process terabytes of structured data, it makes sense to use TFX and turn that code into an ML pipeline. Otherwise, KFP can be a good alternative. Learn more about how to choose between Kubeflow Pipelines, SDK, and TFX from the reading list. An ML pipeline contains a series of processes and runs in two different environments. First is the experimentation, development, and test environment, and the second is the staging, pre-production, and production environment. In the development environment, you start from data preparation, which includes data extraction, analysis, and preparation, to model development like training, evaluation, and validation. The result is a trained model that can be entered in model registry. Once the model is trained, the pipeline moves to the staging and production environment where you serve the model, which includes prediction and monitoring. Each of these processes can be a pipeline component, which is a self-contained set of code that performs one task of a workflow. You can think of a component as a function, which is a building block of a pipeline. You can either build a custom component on your own or leverage the pre-built components provided by Google. If you want to accomplish a specific task to tailor your ML workflow, such as determining a special threshold for model deployment, you may need to code a custom component. Before doing so, check the pre-built components offered by Google Cloud, you may find a pipeline component to reuse or customize to suit your needs. Learn more about using Google Cloud pipeline components in your pipeline in the reading list. All these components are like pieces of an ML pipeline. You need to assemble them together to automate the entire ML workflow. Organizations often implement ML automation in three phases. Phase 0 is the starting point, where you have not configured any MLOps. You typically use the GUI, or graphical user interface, workflow such as AutoML for training, deployment, and serving. Phase 0 is critical because it helps you build an end-to-end workflow manually before you automate it. In phase 1, you start automating your ML workflow by building components using Vertex AI Pipelines SDKs. An example of a component would be the training pipeline. It is in this phase that you develop the building blocks for future use. In phase 2, you integrate the separate components to form an entire workflow and to achieve CI, CT, and CD. Let's look at an example, assume you want to build a pipeline to train, evaluate, and deploy an AutoML model that classifies beans into one of seven types based on their characteristics. You have two main steps, build a pipeline and then run it. To build a pipeline, you first plan it as a series of components, which can be a combination of custom and pre-built. To promote reusability, each component should have a single responsibility. Second, you build any custom components that are needed, for example, you create a component called classification model eval metrics. You use this component to compare the evaluation metrics to a threshold after the model is trained and determine whether the model should be deployed. If the model performs well, you deploy it, otherwise, you retrain the model. Third, you assemble the pipeline by adding pre-built components. For example, TabularDatasetCreateOp creates a tabular dataset in Vertex AI given a data source either in cloud storage or BigQuery. AutoMLTabularTrainingJobRunOp kicks off an AutoML training job for a tabular dataset. EndpointCreateOp creates an endpoint in Vertex AI. And ModelDeployOp deploys a given model to an endpoint in Vertex AI. You also include a custom component, classification_model_eval_metrics, from the previous step to compare the performance of the trained model to a threshold. After the pipeline is built, you must compile and run it. First you compile it using the compiler, compiler.compile or compile commands, and then you define and run the pipeline job. The good news is you don't have to create a pipeline from the beginning. Vertex AI provides a few templates, like the one for classification/regression of tabular data with AutoML, to help you start your journey. Now you have an automated pipeline to train, evaluate, and deploy an ML model. This pipeline will check the performance of the model constantly and decide whether it should be deployed or retrained without your intervention. The nice thing is that Google Cloud also visualizes the pipeline based on the coding, with which you can easily check the components and the corresponding artifacts. This example demonstrated the overall process to build a pipeline. To know more about pipeline details, please practice with the coding example in the demo, Introduction to Vertex AI Pipelines, in the reading list for this course.

#### Lab introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565184

Let's put what you've just learned into practice with a hands on lab. In this lab, you'll use AutoML, a no code tool to build a machine learning model to predict loan risk. The data set used in the lab relates to loans from a financial institution and has 2,050 data points. AutoML requires at least 1,000 data points in a dataset. The goal is to practice working through the three phases of the machine learning workflow, data preparation, model development and model serving. Before you start the lab, lets explain the details about model evaluation so that you can interpret the training results. Lets start with a review of the confusion matrix. Youll get a similar result like this in the lab, pause for a second and try to interpret this matrix yourself. What does it tell you? 100% true positive rate, this means the model perfectly identifies everyone who will repay their loan. It never misses a good borrower, that's fantastic. Note that the true positive rate equals true positives divided by the sum of true positives and false negatives. If the terms sound unfamiliar, please refer to the previous example where you learned about the confusion matrix. 87% true negative rate, the model also correctly identifies 87% of people who wont repay defaulters. Note that the true negative rate equals true negatives divided by the sum of false positives and true negatives. 13% false positive rate, this means the model mistakenly identifies 13% of good borrowers, those who actually repay as defaulters. This can lead to the bank rejecting potentially good customers, which is not ideal. Note that the false positive rate equals false positives divided by the sum of false positives and true negatives. Finally 0% false negative rate. This means the model never incorrectly identifies a borrower who will repay their loan as a potential defaulter. Note that the false negative rate equals false negatives divided by the sum of true positives and false negatives. Let's look at the precision recall curve you will encounter in the upcoming AutoML lab. The confidence threshold determines how a machine learning model counts the positive cases. A higher threshold increases the precision but decreases the recall. A lower threshold decreases the precision but increases recall. Moving the confidence threshold to zero produces the highest recall of 100% and the lowest precision of 50%. So what does that mean? That means the model predicts that 100% of loan applicants will be able to repay a loan they take out. However, actually only 50% of people were able to repay the loan. Using this threshold to identify the default cases in this example can be risky, because it means that you only get half of the loan investments back. Now lets view the other extreme by moving the threshold to one. This will produce the highest precision of 100% with the lowest recall of 1%. What does this mean? It means that all the people who were predicted to repay the loan, 100% of them, actually did. However, you rejected 99% of loan applicants by only offering loans to 1% of them, that's a pretty big business loss for your company. These are both extreme examples, but it's important that you always try to set an appropriate threshold for your model. Now that we've reviewed, let's start the lab.

#### How a machine learns

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565185

To understand machine learning, you must first understand how neural networks learn. This includes exploring this learning process and the terms associated with it. How do machines learn and how do they assess their learning? Before you dive into building an ML model, let's take a look at how a neural network learns. You may already know about the various neural networks such as deep neural networks or DNN, convolutional neural networks or CNN, recurrent neural networks or RNN, and more recently, large language models or LLMs. These networks are used to solve different problems. All these models stem from the most basic artificial neural network or ANN. ANNs are also referred to as neural networks or shallow neural networks. Let's focus on ANN to see how a neural network learns. An ANN has three layers, an input layer, a hidden layer, and an output layer. Each node represents a neuron. The lines between neurons stimulate synopsis, which is how information is transmitted in a human brain. For instance, if you input article titles from multiple resources, the neural network can tell you which media outlet or platform the article belongs to, such as GitHub, New York Times, and Tech Crunch. How does an ANN learn from examples and then make predictions? Let's examine how it works in depth. Let's assume you have two input neurons or nodes, one hidden neuron and one output neuron. Above the link between neurons are weights. The weights retain information that a neural network learned through the training process and are the mysteries that a neural network aims to discover. The first step is to calculate the weighted sum. This is done by multiplying each input value by its corresponding weight and then summing the products. It normally includes a bias component, BI. However, to focus on the core idea, ignore it for now. The second step is to apply an activation function to the weighted sum. What is an activated function and why do you need it? Let's pause your curiosity for just a moment and then get back to that soon. In the third step, the weighted sum is calculated for the output layer, assuming multiple neurons in the hidden layers. The fourth step is to apply an activation function to the weighted sum. This activation function can be different from the one applied to the hidden layers. The result is the predicted y, which consists of the output layer. You use y hat to represent the predicted result and y as the actual result. Now, let's get back to the activation functions. What does an activation function do? Well, an activation function is used to prevent linearity or add non-linearity. What does that mean? Think about a neural network. Without activation functions, the predicted result y hat will always be a linear function of the input x, regardless of the number of layers between input and output. Let's walk through this for clarity. Without the activation function, the value of the hidden layer h equals a total of W_1*X_1 and W_2*X_2. Please note that to make this illustration easy, we ignored the bias component B, which you often see in other ML materials. The output y hat, therefore, equals to W_3*h, and eventually equals to a total of constant number A*x_1 and a constant number b*x_2. In other words, the output y is a linear combination of the input x. If y is a linear function of x, you don't need all the hidden layers, but only one input and one output. You might already know that linear models do not perform well when handling comprehensive problems. That's why you must use activation functions to convert a linear network to a non-linear one. What are widely used activation functions? You can use the rectified linear unit or ReLU function, which turns an input value to zero if it's negative or keeps the original value if it's positive. You can use the sigmoid function, which turns the input to a value between zero and one, and hyperbolic tangent Tahn function, which shifts the sigmoid curve and generates a value between -1 and +1. Another interesting and important activation function is called Softmax. Think about Sigmoid. It generates a value 0-1 and is used for binary classification in logistic regression models. An example for this would be deciding whether an email is spam. What if you have multiple categories such as GitHub, New York Times, and Tech Crunch? Here you must use softmax, which is the activation function for multiclass classification. It maps each output to a zero, one range in a way that the total adds up to one. Therefore, the output of softmax is a probability distribution. Skipping the math, you can conclude that softmax is used for multiclass classification, whereas sigmoid is used for binary class classification in logistic regression models. Also, note that you don't need to have the same activation function across different layers. For instance, you can have ReLU for hidden layers and softmax for the output layer. Now that you understand the activation function and get a predicted y, how do you know if the result is correct? You use an assessment called loss function or cost function to measure the difference between the predicted y and the actual y. Loss function is used to calculate errors for a single training instance, whereas cost function is used to calculate errors from the entire training set. Therefore, in step 5, you calculate the cost function to minimize the difference. If the difference is significant, the neural network knows that it did a bad job in predicting and must go back to learn more and adjust parameters. Many different cost functions are used in practice. For regression problems, means squatted error or MSE is a common one used in linear regression models. MSE equals the average of the sum of squared differences between y hat and y. For classification problems, cross-entropy is typically used to measure the difference between the predicted and actual probability distributions in logistic regression models. If the difference between predicted and actual results is significant, you must go back to adjust weights and biases to minimize cost function. The potential sixth step is called back-propagation. The challenge now is how to adjust the weights. The solution is slightly complex, but indeed the most interesting part of a neural network. The idea is to take cost functions and turn them into a search strategy. That's where gradient descent comes in. Gradient descent refers to the process of walking down the surface formed by the cost function and finding the bottom. It turns out that the problem of finding the bottom can be divided into two different and important questions. The first is, which direction should you take? The answer involves the derivative. Let's say you start from the top left. You calculate the derivative of the cost function and find it's negative. This means the angle of the slope is negative and you are at the left side of the curve. To get to the bottom, you must go down and right. Then at one point, you are on the right side of the curve and you calculate the derivative again. This time, the value is positive, and you must slide again to the left. You calculate the derivative of the cost function every time to decide which direction to take. Repeat this process according to gradient descent, and you will eventually reach the regional bottom. The second question in finding the bottom is what size should the steps be? The step size depends on the learning rate, which determines the learning speed of how fast you bounce around to reach the bottom. Step size or learning rate is a hyperparameter that is set before training. If the step size is too small, your training might take too long. If the step size is too large, you might bounce from wall to wall or even bounce out of the curve entirely without converging. When step size is just right, you're set. The seventh and last step is iteration. One complete pass of the training process from step 1 to step 6 is called an epoch. You can set the number of epochs as a hyperparameter in training. Weights or parameters are adjusted until the cost function reaches its optimum. You can tell that the cost function has reached its optimum when the value stops decreasing even after many iterations. This is how a neural network learns. It iterates the learning by continuously adjusting weights to improve behavior until it reaches the best result. This is similar to a human learning lessons from the past. We have illustrated a simple example with two input neurons or nodes, one hidden neuron, and one output neuron. In practice, you might have many neurons in each layer, regardless of the number of neurons in the input, hidden, and output layer, the fundamental process of how a neural network learns remains the same. Learning about neural networks can be exciting, but also overwhelming with a large number of new terms. Let's take a moment to review them. In a neural network, weights and biases are parameters which are learned by the machine during training. You have no control of the parameters except to set the initial values. The number of layers and neurons, activation functions, learning rate, and epochs are hyperparameters, which are decided by a human before training. The hyperparameters determine how a machine learns. For example, the learning rate decides how fast a machine learns and the number of epochs defines how many times the learning iterates. Normally, data scientists choose the hyperparameters and experiment with them to find the optimum combination. However, if you use tools like AutoML, it automatically selects the hyperparameters for you and saves you plenty of experiment time. You also learned about cost or loss functions, which are used to measure the difference between the predicted and actual value. They are used to minimize error and improve performance. You use backpropagation to modify weights and bias if the difference is significant and gradient descent to decide how to tune the weights and bias and when to stop. These terms are your best friend when building an ML model. You'll revisit them in upcoming lessons and labs.

#### Vertex AI: Predicting Loan Risk with AutoML

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/labs/565186

#### Summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565187

This concludes the module on the AI development workflow. Let's do a quick recap. In this module, you learned about the three main stages of the machine learning workflow with the help of the restaurant analogy. In stage 1, data preparation, you uploaded data and applied feature engineering. This translated to gathering our ingredients and then chopping and prepping them in the kitchen. In stage 2, model development development, the model was trained and evaluated. This is where you experimented with the recipes and tasted the meal to ensure that it turned out as expected. And in the final stage model, serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end to end. One is through a user interface like you practiced in the AutoML lab, the other is with code, which you are shown using pre-built SDKs with Vertex AI pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training and delivery. We hope you enjoyed this module. Next, you'll advance to Generative AI, which is the most recent AI development that offers lots of exciting opportunities. See you soon.

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/quizzes/565188

#### Reading

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/documents/565189

### Generative AI

#### Introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565190

In the previous module, you learned how to develop an ML model from beginning to end. More specifically, you walked through the ML workflow and explored how to convert it into an automated pipeline. In this module, you learn about Generative AI, the most recent AI innovation that offers a wide range of exciting opportunities. You explore the generative AI tools that assist AI development and the integration of its capabilities into AI solutions. To begin, it is important to understand what generative AI or GenAI entails, how it functions, and how to create a GenAI project on Google Cloud. You then explore Gemini multimodal, one of the latest foundation models trained by Google, and how to use it with Vertex AI Studio, a primary tool for a developer to access and tune GenAI models. After that, you delve into the art and science of prompt design and a more advanced topic, model tuning. In addition to Vertex AI Studio, you can rely on Model Garden, a model library, to access different GenAI models. Finally, you explore how GenAI is embedded into the AI solutions like CCAI or contact center AI. With the GenAI tools and knowledge in hand, you conclude your learning journey with a hands-on lab using Vertex AI Studio to create prompts and conversations. Let's get started.

#### Generative AI and workflow

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565191

To begin, it is important to understand what generative AI or gen AI entails, how it functions, and how to create a gen AI project on Google Cloud. Imagine you are a marketing manager spending hours creating compelling content and distributing it through multiple channels, both traditional and new media. Or you're a data scientist writing SQL commands to run complex queries on various data sources. Or perhaps you're an app developer creating a chatbot in a specific field like healthcare. Well, generative AI can be your new go to friend for all of these tasks. Generative AI is transforming how we interact with technology. So what is generative AI? It is a type of artificial intelligence that generates content for you. What kind of content? Well, the generated content can be multimodal, including text, code, images, speech, video, and even 3D. When given a prompt or a request, generative AI can help you with a variety of tasks. Such as launching marketing campaigns, generating code, creating chatbots, extracting information, summarizing documents, and providing virtual assistance. And these are just some examples. Then how does AI generate new content? It learns from a massive amount of existing content such as text, image and video. The process of learning from existing content is called training, which results in the creation of a foundation model. A foundation model is usually a large model in the sense of a significant number of parameters, massive size of training data, and high requirements of computational power. Google has a long history innovating in the area of foundation models and gen AI technologies. Starting from Transformer in 2017, which lays the foundation for all modern gen AI applications seen today. To Gemini in 2023, which extends the word general in artificial general intelligence or AGI in the sense of handling multimodal processing. At present, the primary foundation models trained by Google include Gemini for multimodal processing, Gemma, a lightweight open model for language generation, Codey for code generation, and imagen for image processing. Please note that this list is subject to change as foundation models evolve. Additionally, Gemini has the potential to replace some of these models as it can process data across multiple modalities. A foundation model can then be used directly to generate content and solve general problems such as content extraction and document summarization. It can also be trained further with new datasets in your field to solve specific problems such as financial model generation and healthcare consulting. This results in the creation of a new model that is tailored to your specific needs. This leads to the next, pre-trained and fine-tuned. Meaning, to pre-train a foundation model for a general purpose with a large dataset, and then fine-tune it for specific aims with a much smaller dataset. Imagine training a dog. Often you train your dog basic commands such as sit, come, down, and stay. These commands are normally sufficient for general purposes in everyday life and help your dog become a good canine citizen. However, if you need a special service dog, such as a police dog or a guide dog or a hunting dog, you add special trainings. This similar idea applies to pre-trained versus fine-tuned models. For example, a large language model, which is a typical type of generative AI foundation model, is trained for general purposes to solve common language problems. Such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment by using relatively small datasets from these fields. Powered by the foundation models like large language models or LLMs, generative AI is driving new opportunities to enhance productivity, save operational costs, and create new values. You might have seen these opportunities from the use case about coffee on wheels in module one, where you use gen AI capabilities to automate the marketing campaign, generate customer feedback, and optimize truck route. This sounds exciting. How can you access gen AI models and create a gen AI project on Google Cloud? Recall your best friend Vertex AI, which is the end-to-end AI development platform supporting both predictive AI and generative AI. Aside from the capabilities you learned in previous modules for powering the experimentation, training, and implementation of predictive ML models. Vertex AI offers a range of tools for tuning your gen AI models and developing gen AI projects like Vertex AI Studio and Model Garden. Before you dive into these tools, let's walk through the gen AI workflow on Google Cloud. Input prompt, via the vertex AI studio UI input a prompt, a natural language request to gen AI models. Responsible AI and safety measures. The prompt undergoes responsible AI and safety checks configurable through the UI or code. Foundation models, the screened prompt proceeds to foundation models like Gemini, multimodal, or other gen AI models like Imagen and Codey, based on your choices. Model customization, optionally customize gen AI models to fit your data and use cases by further tuning them. Results grounding, gen AI models return results that undergo grounding and citation checks to prevent hallucinations. Final response, the final response appears on the Vertex AI Studio UI after a final check through responsible AI and safety measures. Lets focus on Vertex AI Studio, the interface to assess gen AI models and model garden, the repository of gen AI models in the following lessons.

#### Gemini multimodal

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565192

Let's explore Gemini multimodal, one of the latest foundation models trained by Google. In the previous lesson, you walked through the Gen AI workflow where Vertex AI studio provides an intuitive interface between developers and the foundational models. It enables you to build GenAI applications in a low code or even no-code environment, where you can rapidly test and prototype models, tune and customize models using your own data. Augment them with real world, up to date information deploy models efficiently in production environments with auto generated code. Let's uncover the capabilities of Gemini multimodal and delve into practical ways to access it with vertex AI Studio. So what is a multimodal model? It's a large foundation model that is capable of processing information from multiple modalities, including text, image, and video. The generated content can also be in multiple modalities. For example, you can send the model a photo of a plate of cookies and ask it to give you a recipe for those cookies. Gemini is such a multimodal model trained by Google, you can access or even tune it with vertex AI Studio. How can Gemini help you with your business use cases, Gemini excels at a diverse range of multimodal use cases. Here are some notable description and captioning, Gemini can identify objects in images and videos, providing detailed or concise descriptions as needed. Information extraction, it can read text from images and videos, extracting important information for further processing. Information analysis, it can analyze the information it extracts from images and videos based on specific prompts. For instance, it can classify expenses on a receipt. Information seeking, Gemini can answer questions or generate Q&A based on the information it extracts from text, images, and videos. Content creation, it can create stories or advertisements using images and videos as inspiration. Data conversion, it can convert text responses to various formats such as HTML and JSON. Can you think of a real world use case to apply Gemini multimodal? In light of these exciting advancements, how can developers engage with Gemini and create applications that leverage multimodal capabilities? There are three primary approaches, each essentially achieving the same objective, using a user interface or UI within the Google Cloud console. This no-code solution is ideal for exploring and testing prompts, using predefined SDKs in different languages like Python and Java, with notebooks like Colab and Workbench. Which are seamlessly integrated within the vertex AI platform, utilizing Gemini APIs in conjunction with command line tools like Curl. Regardless which method to access the Gemini multimodal, you start with a prompt. So what is a prompt? In the world of generative AI, a prompt is a natural language request submitted to a model in order to receive a response, you feed the desired input text like questions and instructions to the model. The model will then provide a response based on how you structured your prompt. Therefore, the answers you get depend on the questions you ask. Lets look at the anatomy of a prompt, which includes one or more of the following input, context, and examples. Input (required), an input represents your request for a response from the model. It can take various forms a question that the model can answer, which is a question input a task that the model can perform, which is a task input. An entity that the model can operate on, which is an entity input, partial input that the model can complete or continue, which is a completion input. Context (optional), a context can serve multiple specify instructions to guide the model's behavior. Provide information for the model to use or reference in generating a response. When you need to supply information or limit responses within the scope of your prompt, include contextual information. For instance, you could assume the role of an IT help desk and consistently advise users to restart their computers regardless of the nature of their inquiries. Examples (optional), examples are pairs of inputs and outputs that demonstrate the desired response format to the model. Incorporating examples in the prompt is an effective technique for tailoring the response format. For example, you can establish the context and assume the role of an IT help desk, and then you provide a few examples. This is called few shot prompting. Couldn't log in, suggest a password reset. Lost Internet connection, suggest checking devices like a modem or a router. Screen went black, suggest restarting the computer. Subsequently, when the question becomes what should I do when my computer freezes? The model may suggest restarting the computer. Context and examples are widely used when training or tuning a GenAI model to behave as you desire. The process of figuring out and designing the input text to get the desired response back from the model is called prompt design, which often involves a lot of experimentation. Youll learn more about prompt and prompt design later in this course. Lets explore the Gemini multimodal feature within vertex AI Studio. Navigate to the Vertex AI Studio overview page and click on multimodal powered by Gemini, try it now. You'll see three a prompt field at the top, a response field at the bottom, and a configuration panel on the right. Click insert media and upload an image. For example, let's use a departure board image from an airport. Enter your first prompt, read the text from the image. Before clicking submit let's look at the configuration on the right. You can choose from a list of models. The default model is usually the most recent cutting edge model, which is currently Gemini Pro vision. The temperature setting controls the degree of randomness in the response, with 0 being the most expected answer and 1 being the most creative. The safety settings allow you to adjust the likelihood of receiving a response that could contain harmful content. Content is blocked based on the probability of harmfulness. For example, for hate speech, you can choose from block few, block some, and block most. Adjust these settings based on your use case and click save. In the next lesson, you'll learn more about advanced settings such as Top-k and Top-p. Once you've completed the configuration, it's time to get the response. Click submit and wait a moment. Here's the result. If the result is not easy to read, you can further adjust your prompt to read the text from the image and put it into two columns, time and destination. Does the result look better? You can also be more adventurous and do some analysis. For example, you can change the prompt to calculate the percentage of the flights to different continents and put them in two columns, percentage and continent. Here is the result. If you desire to further develop the application and make the process productionalized, you can click the code located on the top right corner. There you'll find the code describing the prompt and the settings in the user interface. Alternatively, you can retrieve curl, which serves as the API to call in a command line interface. Additionally, you have the option to open a notebook with the SDK's code of your preferred programming languages such as Python. These automated generated coding and the integrated development environment significantly simplify the production process. Hopefully, this provides a straightforward guide on how to utilize Gemini multimodal with vertex AI Studio. At the end of this course, you practice with prompts and settings in a hands on practice. Enjoy learning.

#### Prompt design

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565193

Let's delve into the art and science of prompt design in this lesson, a fundamental skill for utilizing generative AI effectively. To get started experimenting with the GenAI models, click on new prompt. Let's start with a free form prompt. One way to design a prompt is to simply tell the model what you want. In other words, provide an instruction. For example, generate a list of items I need for a camping trip to Joshua Tree National Park. You send this text to the model and you can see that the model outputs a useful list of items you don't want to camp without. This approach of writing a single command so that the model can adopt a certain behavior is called zero-shot prompting. Generally, there are three methods that you can use to shape the model's response in a way that you desire. Zero-shot prompting is a method where the model is given a prompt that describes the task without additional examples. For example, if you want the LLM to answer a question, you just prompt, what is prompt design? One-shot prompting is a method where the LLM is giving a single example of the task that it is being asked to perform. For example, if you want the LLM to write a poem, you might provide a single example poem. Few-shot prompting is a method where the model is given a small number of examples of the task that it is being asked to perform. Recall the examples of an IT help desk in the previous lesson. You can use the structured mode to design the few-shot prompting by providing a context and additional examples for the model to learn from. The structured prompt contains a few more components. First, you have the context, which instructs how the model should respond. The context applies each time you send a request to the model. Let's say you want to use the model to answer questions about the changes in rainforest vegetation in the Amazon. You can paste this background text as the context. Then you add some examples of questions, such as, what does LGM stand for or what did the analysis from the sediment deposits indicate? You also need to include the corresponding answers to demonstrate how you want the model to respond. Then you can test the prompt you've designed by sending a new question as input. There you go. You've prototyped a Q&A system based on few-shot prompting in just a few minutes. Please note a few best practices around prompt design. The prompt needs to be concise, be specific and clearly defined. Ask one task at a time. Additionally, you can improve response quality by including examples. Adding instructions and a few examples tends to yield good results. However, there's currently no best way to write a prompt. You may need to experiment with different structures, formats, and examples to see what works best for your use case. For more information about prompt design, please check prompt design in the reading list. If you designed a prompt that you think is working pretty well, you can save it and return to it later. Your saved prompts will be visible in the prompt gallery, which is a curated collection of sample prompts that show how generative AI models can work for a variety of use cases. Finally, in addition to testing different prompts and prompt structures, there are a few model parameters you can experiment with to try to improve the quality of the responses. First, there are different models you can choose from. Each model is tuned to perform well on specific tasks. You can also specify the temperature, top P, and top K. These parameters all adjust the randomness of responses by controlling how the output tokens are selected. When you send a prompt to a model, it produces an array of probabilities over the words that could come next. From this array, you need some strategy to decide what it should return. A simple strategy might be to select the most likely word at every time step. But this method can result in uninteresting and sometimes repetitive answers. On the contrary, if you randomly sample over the distribution returned by the model, you might get some unlikely responses. By controlling the degree of randomness, you can get more unexpected, and some might say, creative responses. Back to the model parameters, temperature is the number used to tune the degree of randomness. Low temperature means to narrow the range of possible words to those that have high possibilities and are more typical. In this case, those are flowers and the other words that are located at the beginning of the list. This setting is generally better for tasks like questions and answers, and summarization, where you expect a more typical answer with less variability. High temperature. It means to extend the range of possible words to include those that have low possibility and are more unusual. In this case, those are bugs and other words that are located at the end of the list. This setting is good if you want to generate more creative or unexpected content, like an advertisement slogan. In addition to adjusting the temperature, top K lets the model randomly return a word from the top K number of words in terms of possibility. For example, top two means you get a random word from the top two possible words, including flowers and trees. This approach allows the other high scoring word a chance of being selected. However, if the probability distribution of the words is highly skewed and you have one word that is very likely and everything else is very unlikely, this approach can result in some strange responses. The difficulty of selecting the best top K-value leads to another popular approach that dynamically sets the size of the short list of words. Top P allows the model to return a random word from the smallest subset with the sum of the likelihoods that exceeds or equals to P. For instance, P(0.75) means you sample from a set of words that have a cumulative probability greater than 0.75. In this case, it includes three words, flowers, trees, and herbs. This way, the size of the set of words can dynamically increase and decrease according to the probability distribution of the next word on the list. In sum, Vertex AI Studio provides a few model parameters for you to play with, such as the model type, temperature, top K, and top P. Note that you are not required to adjust them constantly, especially top K and top P.

#### Model tuning

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565194

Having explored prompt design, a fundamental aspect of interacting with GenAI models, lets progress to a more advanced topic, model tuning. You explore different types of tuning and walk through a demo tuning a model with Vertex AI Studio. If you've been prototyping with the GenAI models like in LLM, you might wonder if there are ways to enhance the quality of responses and tailor them to specific domains beyond just prompt design. Here comes model tuning. Let's explore the different tuning methods and how to use Vertex AI studio to launch a tuning job. You have different choices to customize and tune a gen AI model, from less technical prompt design to more technical like distilling. Let's explore them one by one. You are already familiar with prompt design, which lets you tune a GenaI model using natural language without any ML background. The prompt is a request to a model for a desired outcome. To enhance the model's performance, you can provide context and examples to guide its responses. Prompt design does not alter the parameters of the pre trained model. Instead, it improves the model's ability to respond appropriately by teaching it how to react. One benefit of prompt design is that it enables rapid experimentation and customization. Another benefit is that it doesn't require specialized machine learning knowledge or coding skills, making it accessible to a wider range of users. However, producing prompts can be tricky. Small changes in wording or word order can affect the model results in ways that aren't totally predictable, and you cant really fit all that many examples into a prompt. Even when you do discover a good prompt for your use case, you might notice the quality of model responses isn't totally consistent. One way to address the issues is to tune the models using your own data. However, fine tuning the entire model can be impractical due to the high computational resources, cost, and time required. As the name large suggests, LLMs have a vast number of parameters, making it computationally demanding to update every weight. Instead, parameter efficient tuning can be employed. This involves making smaller changes to the model, such as training a subset of parameters or adding additional layers and embeddings. This approach has gained significant attention in the research community as it aims to reduce the challenges of fine tuning large language models by only training a subset of parameters. For example, you can have adapter tuning, which is supervised tuning. It lets you use as few as 100 examples to improve model performance. Reinforcement tuning, which is unsupervised reinforcement learning with human feedback. Distillation, a more technical tuning technique, enables training smaller task specific models with less training data and lower serving costs and latency than the original model. This technique is exclusive to Google Cloud. Through Vertex AI, you can access the newest techniques from Google research available with distilling step-by-step. This technology transfers knowledge from a larger model to a smaller model to optimize performance latency and cost. The rationale is to use a larger teacher model that trains smaller student models to perform specific tasks better and with improved reasoning capabilities. The training and distilling process uses labeled examples and rationales generated by the teacher model to fine tune the student model. Rationales are like asking the model to explain why examples are labeled the way they are. Similar to how you learn better by understanding the reason behind an answer, this type of teaching makes the student model more accurate and robust. Now, let's move to Vertex AI studio and see how to start a tuning job. Please note that the UI may change as the product progresses. From the language section of Vertex AI Studio, tune and distil, select Create Tuned Model. For model details, you can choose from either supervised tuning, which is the adapter tuning as mentioned earlier, or unsupervised tuning, which is reinforcement tuning. Give the tuned model a name, choose the base model, the region, and the output directory. Parameter efficient tuning is ideally suited for scenarios where you have modest amounts of training data. The number of examples can be as low as ten. However, it's recommended to have 100 training examples for better performance. Tuning dataset is where you specify the location of your training dataset. Please note, it needs to be on cloud storage. Your training data should be structured as a supervised training dataset in a JSON L file. Each record or row contains a pair of text data, the input text, which is the prompt, and the output text, which is the expected response from the model. This structure allows the model to learn and adapt to your desired behavior. You can then start the tuning job and monitor the status on the Google Cloud console. When the tuning job completes, you see the tuned model in the vertex AI model registry, and you can deploy it to an endpoint for serving or further test it in vertex AI studio.

#### Model Garden

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565195

In addition to Vertex AI studio, Model Garden with Vertex AI offers access to a wide range of gen AI models not limited to those developed by Google. Model Garden is like a model library where you can search, discover and interact with Google's third parties and open source gen AI models. When you want to build, train and fine tune gen AI models in a comprehensive environment supported by both you, UI and coding, Vertex AI studio can be a good start point. However, if you want to quickly find and apply the right model to solve a problem, model garden is a good choice. Model Garden offers a model card for each model, encapsulating critical details such as overview, use cases and relevant documentation. Furthermore, it seamlessly integrates with Vertex AI studio, allowing you to initiate project development through a user friendly interface. Additionally, it provides access to sample code and facilitates code development via notebooks. You can find three major categories of models, foundation models, tasks-specific solutions, and fine-tunable or open source models. Foundation models are pr trained multitask large models that can be tuned or customized for specific tasks by using vertex AI studio, vertex AI APIs and SDKs. Some of these models are Gemini for multimodal processing, embeddings for creating text, and multimodal embeddings imagin for image, chirp for speech, and Codey for cogeneration. Recall that you learned the APIs for these models in the previous lesson when you explored pretrained APIs. Task specific solutions are pretrained models which are optimized to solve a specific problem. These include some tasks you practiced in the previous section by using natural language APIs like entity analysis, sentiment analysis, and syntax analysis. More interesting tasks like object detector and text translation are also task specific solutions. And finally, there are fine-tunable models. These are mostly open source models that you can fine-tune by using a custom notebook or pipeline. To find the model that best suits your requirements you can also use three filters in your search. They are modalities such as language, vision and speech, tasks like generation, classification and detection, and features such as pipeline, notebook, and one-click deployment support. With all these models to choose from, how can you start your workflow? Model garden lets you use Google's foundation models directly through the Google Cloud console with Vertex AI studio or through code with prebuilt APIs. Tune models in generative AI studio, deploy models, customize and use open source models. Let's walk through a use case of using model garden. Assume you are interested in computer vision from the model garden page let's filter for vision related models. Next in the tasks section, select detection. Great, the search results shows there's an owl vision transformer model, an open source model. It says it's a zero-shot, text-condition object detection model that can query an image with one or multiple text queries. Let's dive into this. Here you see the model card, where you can see more details. As a data scientist, you might want to try using this model, so you click on open notebook. This opens up a colab notebook for the Owl Vision transformer model. This notebook in particular shows how you can deploy the model to an endpoint on Vertex AI. Then send an image to the endpoint to get a prediction, which is the text caption describing what's in the image. As this example illustrates, the flow from finding a model to deploying and using it becomes super easy with model garden.

#### AI solutions

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565196

You learned about Vertex AI Studio and Model Garden the tools provided by Vertex AI to access Gen AI models and develop Gen AI projects. How is Gen AI embedded in AI solutions? You'll explore more in this lesson. AI solutions include both vertical and horizontal solutions. Vertical or industry solutions refer to solutions that solve specific problems within certain industries. Examples include Healthcare Data Engine, which generates healthcare insights and provides services across patients, doctors, and hospitals, and Vertex AI Search for Retail, which enables retailers to provide Google quality search and recommendations on their own digital property, which can help increase conversion rate and reduce search abandonment. Horizontal solutions in contrast, usually refer to solutions that solve similar problems across different industries. For instance, contact center AI or CCAI aims to improve customer service in contact centers through the use of artificial intelligence. It can help automate simple interactions, assist human agents, and unlock caller insights. Document AI uses computer vision, optical character recognition, and natural language processing to create pre-trained models that can extract information from documents. This increases the speed and accuracy of document processing, which can help organizations make better decisions faster and reduce costs. Let's dive into CCAI, explore its main features and learn how generative AI supercharges them. Contact Center artificial intelligence is Google's solution to apply AI in contact centers. The goal is to use AI to increase the customer satisfaction and the operational efficiency while requiring minimum AI expertise. At the center of contact center AI is the conversational core, which empowers three major components; virtual agent, agent assist, and insights. Let's briefly look at how contact center AI works. When a customer contacts contact center AI, the system automatically determines whether the request is routine or complex. Routine requests are passed to a virtual agent, a large language model powered bot that can converse naturally with customers. The virtual agent may then process the request and pass it to back end fulfillment. Complex requests are sent to the agent assist, an AI assistant that helps human agents summarize the problem, gather information, provide solutions, and generate insights. Relevant information and insights can also be saved to the knowledge base throughout the process. Now, let's look at how generative AI is used to transform the Contact Center AI Platform. Today, the Contact Center AI Platform is an end to end AI first call center as a service solution that is integrated with a set of pre built tools and features, including virtual agent, agent assist and insights. The platform will soon be upgraded with natural language understanding powered call and chat, as well as ML driven routing. This means that the platform will be able to understand what customers are saying and direct them to the appropriate person or department. It will also be able to learn from the interactions and improve its routing over time. Today, a contact center AI virtual agent provides 24/7 customer self service. It provides a conversational voice and chat bot that a customer can get help from any time using natural language. It also integrates the function to handoff calls to live agents. Through generative AI, the bots will be faster and easier to use. They will also be able to handle a wider range of customer interactions, answer customer questions in a more comprehensive and accurate way and engage in natural conversations with customers. Today, AI agent assist empowers live agents with step by step help during calls and chats. It also provides agents an automated summary of every interaction afterwards. In the near future, the agent assist will be enhanced with a generative AI feature that can coach live agents on demand. Contact Center AI insights currently use natural language processing to analyze conversations such as sentiment analysis, entity identification, and topic detection. These insights can be used to improve the performance of virtual and live agents. They can also be used to create training materials for agents and to identify areas where customer service can be improved. Soon, insights will generate FAQs automatically to improve customer experience with generative AI capabilities. You can learn more about Google Cloud's growing list of AI solutions and their embedded generative AI capabilities at cloud.google.com/solutions/ai. With the rapid development of AI and machine learning, you might hear exciting news every day. What do you think about the future of AI and machine learning? Here are a few of our expectations. The transition from data to AI is inevitable. Data is the fuel that powers AI, and AI extracts insights and possibilities from data. As AI becomes increasingly powerful and capable, it will learn and adapt in ways that data alone cannot. As a result, AI is becoming more and more essential for businesses that want to stay ahead of the competition. Generative AI becomes more important. It will be used to produce content and improve productivity, which will create new opportunities and offer great potential. These opportunities will be more accessible to all as AI and ML development tools become easier to use, which allows people without a technical background to benefit from their capabilities. What do you think, dear learners?

#### Lab introduction

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565197

With a grasp of the gen AI concepts, it's time to play with Vertex AI studio in a hands-on lab where you analyze images with Gemini multimodal. Explore multimodal capabilities. Design prompts with free form and structured mode. Generate conversations. By the end of this lab, you will be able to use the capabilities of Vertex AI studio discussed in this course. Have fun exploring.

#### Get Started with Vertex AI Studio

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/labs/565198

#### Summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565199

This concludes the last module on generative AI. Let's do a quick recap. You began by understanding how Gen AI works. Through extensive training on vast amounts of data, foundation models like large language models or LLMs are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the Gen AI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI; Vertex AI Studio, which is the interface between developers and back end Gen AI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune Gen AI models to solve your business problems, and Model Garden, which is a model library, helps you search, discover, and interact with Google and open source Gen AI models. You then explored vertical and horizontal AI solutions and their embedded generative AI capabilities. You concluded with a hands on lab where you used vertex AI Studio to create prompts and conversations. We hope you enjoyed this module and feel motivated to use generative AI in your projects.

#### Quiz

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/quizzes/565200

#### Reading

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/documents/565201

### Summary

#### Course summary

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/video/565202

Congratulations on completing the course Introduction to AI and Machine Learning on Google Cloud. Regardless of your role, an AI developer, data scientist, ML engineer, or simply a passionate AI and machine learning enthusiast, we trust that this course has provided you with valuable insights to enhance your career. Before the final review, let's take a moment to reflect on what you've learned so far. Can you come up with 3-5 keywords and group them together in a way that makes sense to you? Throughout this course, we aim to help you navigate a comprehensive toolbox provided by Google Cloud so you can effectively utilize these tools and resources to build an AI and ML project. Let's walk through the main concepts. You started from AI Foundations, where you focused on Cloud essentials and data tools. You began with Google Cloud infrastructure. Of the three layers, you explored the middle and top layers. On the middle layer sit compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. On the top layer sit data and AI products which enable you to perform tasks to support the data to AI journey. From data ingestion, storage, and analytics to AI and machine learning. After that, you advance to the fundamental ML concepts, including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. By grasping these concepts, you can make an informed decision when selecting an ML model. You then learned the steps to build an ML model by using BigQuery, Google's widely used data warehouse. Finally, you had a hands-on lab where you applied these steps to build your own ML models with SQL commands. In the second module, you advanced to AI development. Specifically, you learned about the options available to build an ML model. You completed three options from a ready-made approach to low and no-code to do it yourself. You started with pre-trained APIs which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google's unified platform to build a machine learning project from end-to-end. You learned about AutoML, which is a low or no-code tool on Vertex AI and lets you build the ML model with your own data through the Google Cloud Console. You also explored custom training, which is a code-based solution on Vertex AI. It allows you to control the working environment and automate the ML workflow by using programming tools that you're familiar with, such as Python, TensorFlow, and Notebooks. Finally, you practiced with a hands-on lab using the Natural Language API to identify subjects and analyze sentiment in text. With a grasp of the various options for developing an ML project, you are now ready to build your own ML model. In the third module, you walked through the ML workflow and explored how to create an automated pipeline. You began with the three main stages of the machine learning workflow with the help of the restaurant analogy. In Stage 1, data preparation, you uploaded d ta and applied feature engineering. This translates to gathering your ingredients and then prepping them in the kitchen. In Stage 2, model development, the model was trained and evaluated. This is where you experiment with the recipes and taste the meal to ensure that it turned out as expected. In the final stage, model serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end-to-end with Vertex AI. One is through a user interface like you practiced in the AutoML lab. The other is with code, which you were shown using pre-built SDKs with Vertex AI pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training, and delivery. In the end, you completed a lab using AutoML on Vertex AI, where you built an ML model to predict loan risk through the Google Cloud Console. In the final module, you were introduced to the most recent development in AI, generative AI. You explored the GenAI development tools and the integration of the GenAI capabilities into AI solutions. You began by understanding how GenAI works through extensive training on vast amounts of data foundation models like large language models, LLMs, are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the GenAI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI Studio, which is the interface between developers and backend GenAI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune GenAI models to solve your business problems. Model Garden, which is a model library helps you search, discover, and interact with Google and open-source GenAI models. You then explored vertical and horizontal AI solutions and their embedded GenAI capabilities. You concluded with a hands-on lab where you used Vertex AI Studio to create prompts and conversations. We hope that this course is the start of your journey into AI and machine learning. Remember what we suggested at the beginning of the course: apply what you've learned to your own work. This is the best way to develop your skills as an AI practitioner. For more training with machine learning and AI, please explore the options available at cloud.google.com/training/machinelearning-ai. If you are interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you may want to consider pursuing a Google Cloud certification. You can learn more about Google Cloud's certification offerings at cloud.google.com/certifications. Thanks for completing this course. We'll see you next time.

#### Reading

- https://www.cloudskillsboost.google/paths/1336/course_templates/593/documents/565203

### Your Next Steps

## 05: Baseline: Infrastructure

- https://www.cloudskillsboost.google/paths/1336/course_templates/620

### Baseline: Infrastructure

#### Cloud Storage: Qwik Start - Cloud Console

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566694

#### Cloud Storage: Qwik Start - CLI/SDK 

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566695

#### Cloud IAM: Qwik Start

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566696

#### Cloud Monitoring: Qwik Start

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566697

#### Cloud Run Functions: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566698

#### Cloud Run Functions: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566699

#### Pub/Sub: Qwik Start - Console

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566700

#### Pub/Sub: Qwik Start - Command Line

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566701

#### Pub/Sub: Qwik Start - Python

- https://www.cloudskillsboost.google/paths/1336/course_templates/620/labs/566702

### Your Next Steps

